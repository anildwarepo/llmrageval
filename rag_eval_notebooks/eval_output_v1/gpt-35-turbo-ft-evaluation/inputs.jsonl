{"answer":"To initiate a new Azure ML experiment using the Azure CLI, you can use the `az ml experiment submit` command. This command allows you to submit a new experiment run to your Azure Machine Learning workspace.","documents":"[\"The Azure CLI  and the ml extension for Azure Machine Learning .\\nAzure CLI\\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\\nAzure CLI\\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\\nso one cluster can host one or many batch deployments (along with other workloads if\\ndesired).\\nThis article uses a compute created here named batch-cluster. Adjust as needed and\\nreference your compute using azureml:<your-compute-name> or create one as shown.\\nAzure CLIaz extension add -n ml \\nConnect to your workspace\\nAzure CLI\\naz account set --subscription  <subscription>  \\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \\nlocation= <location>  \\nCreate compute\\nAzure CLI\\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \\n0 --max-instances  5 \\n７ Note\", 'Azure CLI\\nIf you\\'re not on the compute instance, omit --identity and follow the prompt to\\nopen a browser window to authenticate.\\n3. Make sure you have the most recent versions of the CLI and the ml extension:\\nAzure CLI\\n4. If you have multiple Azure subscriptions, set the active subscription to the one\\nyou\\'re using for your workspace. (Y ou can skip this step if you only have access to\\na single subscription.) R eplace <SUBSCRIPTION-NAME> with your subscription name.\\nAlso remove the brackets <>.\\nAzure CLI\\n5. Now use CLI to submit the job. If you\\'re doing this on a compute instance in your\\nworkspace, you can use environment variables for the workspace name and\\nresource group as show in the following code. If you aren\\'t on a compute instance,\\nreplace these values with your workspace name and resource group.\\nAzure CLI\\nOnce you\\'ve submitted the job, you can check the status and results in studio:\\n1. Sign in to Azure Machine Learning studio .\\n2. Select your workspace if it isn\\'t already loaded.\\n3. On the left navigation, select Jobs.\\n4. Select the Experiment name  that you used to train your model.\\n5. Select the Display name  of the job to view details and artifacts of the job,\\nincluding metrics, images, child jobs, outputs, logs, and code used in the job.az login --identity\\naz upgrade\\naz account set --subscription  \"<SUBSCRIPTION-NAME>\"\\naz ml job create -f job.yml  --workspace-name  $CI_WORKSPACE  --resource-\\ngroup $CI_RESOURCE_GROUP\\nRegister model', \"Azure CLI\\nIf you don't have it, create a cluster called cpu-cluster by running:\\nAzure CLI\\nNow, create a pipeline job defined in the pipeline.yml file with the following command.\\nThe compute target is referenced in the pipeline.yml file as azureml:cpu-cluster. If your\\ncompute target uses a different name, remember to update it in the pipeline.yml file.\\nAzure CLI\\nYou should receive a JSON dictionary with information about the pipeline job including:\\nKey Descr iption\\nname The GUID-based name of the job.\\nexperiment_name The name under which jobs will be organized in studio.\\nservices.Studio.endpoint A URL for monitoring and reviewing the pipeline job.\\nstatus The status of the job. This will likely be Preparing at this point.\\nOpen the services.Studio.endpoint URL to see a graph visualization of the pipeline.az ml compute list\\n７ Note\\nSkip this step to use serverless comput e.\\naz ml compute create -n cpu-cluster  --type amlcompute --min-instances  0 --\\nmax-instances  10\\naz ml job create --file pipeline.yml\\nﾉExpand table\", \"The Azure CLI  and the ml extension for Azure Machine Learning .\\nAzure CLI\\nThe workspace is the top-level resource for Azure Machine Learning, providing a\\ncentralized place to work with all the artifacts you create when you use Azure Machine\\nLearning. In this section, we'll connect to the workspace in which you'll perform\\ndeployment tasks.\\nPass in the values for your subscription ID, workspace, location, and resource group\\nin the following code:\\nAzure CLI\\nIn this example, we are going to create a deployment that can write directly to the\\noutput folder of the batch deployment job. The deployment will use this feature to write\\ncustom parquet files.Azure CLI\\naz extension add -n ml\\n７ Note\\nPipeline component deployments for Batch Endpoints were introduced in\\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\\nname ml to get the last version of it.\\nConnect to your workspace\\nAzure CLI\\naz account set --subscription  <subscription>\\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \\nlocation= <location>\\nCreating a batch deployment with a custom\\noutput\", \"The Azure CLI  and the ml extension for Azure Machine Learning .\\nAzure CLI\\nThe workspace is the top-level resource for Azure Machine Learning, providing a\\ncentralized place to work with all the artifacts you create when you use Azure Machine\\nLearning. In this section, we'll connect to the workspace in which you'll perform\\ndeployment tasks.\\nPass in the values for your subscription ID, workspace, location, and resource group\\nin the following code:\\nAzure CLI\\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\\ninference over new data:\\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\\n７ Note\\nPipeline component deployments for Batch Endpoints were introduced in\\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\\nname ml to get the last version of it.\\nConnect to your workspace\\nAzure CLI\\naz account set --subscription  <subscription>\\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \\nlocation= <location>\\nSteps\"]","question":"How do you initiate a new Azure ML experiment using the Azure CLI?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":1}
{"answer":"To create a new compute instance via the Azure ML CLI, you need to specify parameters such as the instance name, the virtual network and subnet, and the size of the instance. Additionally, you may need to provide authentication details and specify the Docker image to use.","documents":"['Create an Azure Machine Learning\\ncompute instance\\nArticle •12\/08\/2023\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (current)  \\n  Python SDK azure-ai-ml v2\\n(current)\\nLearn how to create a compute instance  in your Azure Machine Learning workspace.\\nUse a compute instance as your fully configured and managed development\\nenvironment in the cloud. For development and testing, you can also use the instance as\\na training compute target . A compute instance can run multiple jobs in parallel and has\\na job queue. As a development environment, a compute instance can\\'t be shared with\\nother users in your workspace.\\nIn this article, you learn how to create a compute instance. See Manage an Azure\\nMachine Learning compute instance  for steps to manage start, stop, restart, delete a\\ncompute instance.\\nYou can also use a setup script  to create the compute instance with your own custom\\nenvironment.\\nCompute instances can run jobs securely in a virtual network environment , without\\nrequiring enterprises to open up SSH ports. The job executes in a containerized\\nenvironment and packages your model dependencies in a Docker container.\\nAn Azure Machine Learning workspace. For more information, see Create an Azure\\nMachine Learning workspace . In the storage account, the \"Allow storage account\\nkey access\" option must be enabled for compute instance creation to be\\nsuccessful.\\nChoose the tab for the environment you\\'re using for other prerequisites.\\n７ Note\\nThis article uses CLI v2 in some examples. If you are still using CLI v1, see Create an\\nAzur e Machine Learning comput e clust er CLI v1) .\\nPrerequisites', \"For more information on creating a compute cluster and compute cluster, including how\\nto do so with Python and the CLI, see the following articles:\\nCreate a compute cluster\\nCreate a compute instance\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (curr ent)\\nWhen Azure Container R egistry is behind the virtual network, Azure Machine Learning\\ncan't use it to directly build Docker images (used for training and deployment). Instead,\\nconfigure the workspace to use the compute cluster you created earlier. Use the\\nfollowing steps to create a compute cluster and configure the workspace to use it to\\nbuild images:\\n1. Navigate to https:\/\/shell.azure.com\/  to open the Azure Cloud Shell.\\n2. From the Cloud Shell, use the following command to install the 2.0 CLI for Azure\\nMachine Learning:\\nAzure CLIAllow inbound T CP traffic on ports 29876-29877 from the\\nBatchNodeManagement service tag.\\nAllow inbound T CP traffic on port 44224 from the AzureMachineLearning\\nservice tag.\\nThe following screenshot shows an example of these rules:\\nConfigure image builds\\naz extension add -n ml\", \"The Azure CLI  and the ml extension for Azure Machine Learning .\\nAzure CLI\\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\\nAzure CLI\\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\\nso one cluster can host one or many batch deployments (along with other workloads if\\ndesired).\\nThis article uses a compute created here named batch-cluster. Adjust as needed and\\nreference your compute using azureml:<your-compute-name> or create one as shown.\\nAzure CLIaz extension add -n ml \\nConnect to your workspace\\nAzure CLI\\naz account set --subscription  <subscription>  \\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \\nlocation= <location>  \\nCreate compute\\nAzure CLI\\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \\n0 --max-instances  5 \\n７ Note\", \"To create a workspace using CLI v2, use the following command:\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (curr ent)\\nBash\\nFor more information, see workspace Y AML schema .\\nA compute is a designated compute resource where you run your job or host your\\nendpoint. Azure Machine Learning supports the following types of compute:\\nComput e clust er - a managed-compute infrastructure that allows you to easily\\ncreate a cluster of CPU or GPU compute nodes in the cloud.\\nComput e instance  - a fully configured and managed development environment in\\nthe cloud. Y ou can use the instance as a training or inference compute for\\ndevelopment and testing. It's similar to a virtual machine on the cloud.\\nInfer ence clust er - used to deploy trained machine learning models to Azure\\nKubernetes Service. Y ou can create an Azure Kubernetes Service (AKS) cluster from\\nyour Azure Machine Learning workspace, or attach an existing AKS cluster.\\nAttached comput e - You can attach your own compute resources to your\\nworkspace and use them for training and inference.\\nTo create a compute using CLI v2, use the following command:\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (curr ent)Azure CLI\\naz ml workspace create --file my_workspace.yml  \\nCompute\\n７ Note\\nInstead of creating a compute cluster, use serverless comput e (preview) to\\noffload compute lifecycle management to Azure Machine Learning.\\nAzure CLI\", 'Manage an Azure Machine Learning\\ncompute instance\\nArticle •07\/06\/2023\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (current)  \\n  Python SDK azure-ai-ml v2\\n(current)\\nLearn how to manage a compute instance  in your Azure Machine Learning workspace.\\nUse a compute instance as your fully configured and managed development\\nenvironment in the cloud. For development and testing, you can also use the instance as\\na training compute target . A compute instance can run multiple jobs in parallel and has\\na job queue. As a development environment, a compute instance can\\'t be shared with\\nother users in your workspace.\\nIn this article, you learn how to start, stop, restart, delete) a compute instance. See\\nCreate an Azure Machine Learning compute instance  to learn how to create a compute\\ninstance.\\nAn Azure Machine Learning workspace. For more information, see Create an Azure\\nMachine Learning workspace . In the storage account, the \"Allow storage account\\nkey access\" option must be enabled for compute instance creation to be\\nsuccessful.\\nThe Azure CLI extension for Machine Learning service (v2) , Azure Machine\\nLearning Python SDK (v2) , or the Azure Machine Learning Visual S tudio Code\\nextension .\\nIf using the Python SDK, set up your development environment with a workspace .\\nOnce your environment is set up, attach to the workspace in your Python script:\\nAPPLIES T O: \\n  Python SDK azure-ai-ml v2 (curr ent)\\n７ Note\\nThis article shows CLI v2 in the sections below. If you are still using CLI v1, see\\nCreate an Azur e Machine Learning comput e clust er CLI v1) .\\nPrerequisites']","question":"Describe the process of creating a new compute instance via the Azure ML CLI. What parameters are necessary?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":2}
{"answer":"To submit a training job using Python and the Azure Machine Learning SDK v2, you need to define a script run configuration, which includes the training script, environment, and compute target. Here's an example of how to do this:\n\n```python\nfrom azureml.core import Workspace, Experiment, ScriptRunConfig\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.environment import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\n# Load workspace\nws = Workspace.from_config()\n\n# Define the experiment\nexperiment_name = 'training-job-example'\nexperiment = Experiment(workspace=ws, name=experiment_name)\n\n# Define the compute target\ncompute_target = ComputeTarget(workspace=ws, name='my-compute')\n\n# Create a Python environment\nenv = Environment('training-env')\nenv.python.user_managed_dependencies = True\nenv.docker.enabled = False\ndeps = CondaDependencies.create(pip_packages=['azureml-defaults', 'scikit-learn'])\nenv.python.conda_dependencies = deps\n\n# Define the script run configuration\nscript_config = ScriptRunConfig(source_directory='.',\n                                script='train.py',\n                                compute_target=compute_target,\n                                environment=env)\n\n# Submit the job\nrun = experiment.submit(script_config)\n```\n\nIn this example, `train.py` is the training script, `my-compute` is the name of the compute target, and `training-env` is the name of the environment.","documents":"[\"1. On the top bar above your opened notebook, create a compute instance if you\\ndon't already have one.\\n2. If the compute instance is stopped, select Start comput e and wait until it is\\nrunning.\\n3. Make sure that the kernel, found on the top right, is Python 3.10 - SDK v2. If not,\\nuse the dropdown to select this kernel.\\n4. If you see a banner that says you need to be authenticated, select Authenticat e.\\nTo train a model, you need to submit a job. The type of job you'll submit in this tutorial\\nis a command job . Azure Machine Learning offers several different types of jobs to train\\nmodels. Users can select their method of training based on complexity of the model,\\ndata size, and training speed requirements. In this tutorial, you'll learn how to submit a\\ncommand job  to run a training s cript.\\nA command job is a function that allows you to submit a custom training script to train\\nyour model. This can also be defined as a custom training job. A command job in Azure\\nMachine Learning is a type of job that runs a script or command in a specified\\nenvironment. Y ou can use command jobs to train models, process data, or any other\\ncustom code you want to execute in the cloud.Set your kernel\\n\\uf80a\\n\\uf80a\\n\\uf80a\\n） Impor tant\\nThe rest of this tutorial contains cells of the tutorial notebook. Copy\/paste them\\ninto your new notebook, or switch to the notebook now if you cloned it.\\nUse a command job to train a model in Azure\\nMachine Learning\", 'For detailed instructions on how to complete these steps, see Secure a training\\nenvironment .\\nIn this section, you learn how Azure Machine Learning securely communicates between\\nservices to submit a training job. This example shows you how all your configurations\\nwork together to secure communication.\\n1. The client uploads training scripts and training data to storage accounts that are\\nsecured with a service or private endpoint.\\n2. The client submits a training job to the Azure Machine Learning workspace\\nthrough the private endpoint.\\n3. Azure Batch service receives the job from the workspace. It then submits the\\ntraining job to the compute environment through the public load balancer for the\\ncompute resource.\\n4. The compute resource receives the job and begins training. The compute resource\\nuses information stored in key vault to access storage accounts to download\\ntraining files and upload output.Example training job submission', 'Python\\nView more examples of training with serverless compute at:-\\nQuick S tart\\nTrain Model\\nThere\\'s no need to specify compute for AutoML jobs. R esources can be optionally\\nspecified. If instance count isn\\'t specified, then it\\'s defaulted based on\\nmax_concurrent_trials and max_nodes parameters. If you submit an AutoML image\\nclassification or NLP task with no instance type, the GPU VM size is automatically\\nselected. It\\'s possible to submit AutoML job through CLIs, SDK, or S tudio. T o submit\\nAutoML jobs with serverless compute in studio first enable the submit a training job in\\nstudio (preview)  feature in the preview panel.Python SDK\\nfrom azure.ai.ml import command\\nfrom azure.ai.ml import MLClient      # Handle to the workspace\\nfrom azure.identity import DefaultAzureCredential     # Authentication  \\npackage\\nfrom azure.ai.ml.entities import ResourceConfiguration\\nfrom azure.ai.ml.entities import UserIdentityConfiguration \\ncredential = DefaultAzureCredential()\\n# Get a handle to the workspace. You can find the info on the workspace  \\ntab on ml.azure.com\\nml_client = MLClient(\\n    credential=credential,\\n    subscription_id= \"<Azure subscription id>\" , \\n    resource_group_name= \"<Azure resource group>\" ,\\n    workspace_name= \"<Azure Machine Learning Workspace>\" ,\\n)\\njob = command(\\n    command= \"echo \\'hello world\\'\" ,\\n    environment= \"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\" ,\\n         identity=UserIdentityConfiguration(),\\n    queue_settings={\\n      \"job_tier\" : \"Standard\"   \\n    }\\n)\\njob.resources = ResourceConfiguration(instance_type= \"Standard_E4s_v3\" , \\ninstance_count= 1)\\n# submit the command job\\nml_client.create_or_update(job)\\nAutoML job', \"Designer : Azure Machine Learning designer provides an easy entry-point into\\nmachine learning for building proof of concepts, or for users with little coding\\nexperience. It allows you to train models using a drag and drop web-based UI. Y ou\\ncan use Python code as part of the design, or train models without writing any\\ncode.\\nAzur e CLI : The machine learning CLI provides commands for common tasks with\\nAzure Machine Learning, and is often used for scripting and aut omating tasks . For\\nexample, once you've created a training script or pipeline, you might use the Azure\\nCLI to start a training job on a schedule or when the data files used for training are\\nupdated. For training models, it provides commands that submit training jobs. It\\ncan submit jobs using run configurations or pipelines.\\nEach of these training methods can use different types of compute resources for\\ntraining. Collectively, these resources are referred to as comput e targets. A compute\\ntarget can be a local machine or a cloud resource, such as an Azure Machine Learning\\nCompute, Azure HDInsight, or a remote virtual machine.\\nThe Azure Machine Learning SDK for Python allows you to build and run machine\\nlearning workflows with Azure Machine Learning. Y ou can interact with the service from\\nan interactive Python session, Jupyter Notebooks, Visual S tudio Code, or other IDE.\\nInstall\/update the SDK\\nConfigure a development environment for Azure Machine Learning\\nA generic training job with Azure Machine Learning can be defined using the\\ncommand() . The command is then used, along with your training script(s) to train a\\nmodel on the specified compute target.\\nYou may start with a command for your local computer, and then switch to one for a\\ncloud-based compute target as needed. When changing the compute target, you only\\nchange the compute parameter in the command that you use. A run also logs\\ninformation about the training job, such as the inputs, outputs, and logs.\\nTutorial: T rain your first ML model\\nExamples: Jupyter Notebook and Python examples of training modelsPython SDK\\nSubmit a command\\nAutomated Machine Learning\", 'An Azure Machine Learning compute cluster is a fully managed compute resource that\\ncan be used to run the training job. In the following examples, a compute cluster named\\ncpu-compute is created.\\nPython\\nTo run this script, you\\'ll use a command that executes main.py Python script located\\nunder .\/sdk\/python\/jobs\/single-step\/lightgbm\/iris\/src\/. The command will be runworkspace = \\'<AZUREML_WORKSPACE_NAME>\\'\\n#connect to the workspace\\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \\nresource_group, workspace)\\n2. Create a compute resource for training\\n７ Note\\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\\ntraining job .\\nPython SDK\\nfrom azure.ai.ml.entities import AmlCompute\\n# specify aml compute name.\\ncpu_compute_target = \"cpu-cluster\"\\ntry:\\n    ml_client.compute.get(cpu_compute_target)\\nexcept Exception:\\n    print(\"Creating a new cpu compute target...\" )\\n    compute = AmlCompute(\\n        name=cpu_compute_target, size= \"STANDARD_D2_V2\" , min_instances= 0, \\nmax_instances= 4\\n    )\\n    ml_client.compute.begin_create_or_update(compute).result()\\n4. Submit the training job\\nPython SDK']","question":"Explain how to submit a training job using Python and the AML SDK v2. Include an example of specifying a compute target.","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":3}
{"answer":"The command to list all the models registered in an Azure ML Workspace using the Azure CLI is `az ml model list`.","documents":"['then select Regist er.\\nThe SDK and CLI (v2) also allow you to manage the lifecycle of your Azure Machine\\nLearning model assets.\\nList all the models in your workspace:\\ncli\\nList all the model versions under a given name:\\ncli\\n\\uf80a\\nManage models\\nList\\nAzure CLI\\naz ml model list  \\nAzure CLI\\naz ml model list --name run-model-example', 'Install the Azure CLI and the ml extension to the Azure CLI. For more\\ninformation on installing the CLI, see Install and set up the CLI (v2) .\\nFirst, connect to the Azure Machine Learning workspace where you\\'ll work.\\nAzure CLI\\nYou can deploy only registered models to online endpoints. In this case, you already\\nhave a local copy of the model in the repository, so you only need to publish the model\\nto the registry in the workspace. Y ou can skip this step if the model you\\'re trying to\\ndeploy is already registered.\\nAzure CLI\\nIf your model was logged inside of a run, you can register it directly.\\nTo register the model, you need to know the location where it is stored. If you\\'re using\\nMLflow\\'s autolog feature, the path to the model depends on the model type and\\nframework. Y ou should check the jobs output to identify the name of the model\\'s folder.\\nThis folder contains a file named MLModel.Connect to your workspace\\nAzure CLI\\naz account set --subscription  <subscription>\\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \\nlocation= <location>\\nRegister the model\\nAzure CLI\\nMODEL_NAME= \\'sklearn-diabetes\\'\\naz ml model create --name $MODEL_NAME  --type \"mlflow_model\"  --path \\n\"endpoints\/online\/ncd\/sklearn-diabetes\/model\"\\nWhat if your model was logged inside of a run?', 'Install the Azure CLI and the ml extension to the Azure CLI. For more\\ninformation on installing the CLI, see Install and set up the CLI (v2) .\\nFirst, connect to the Azure Machine Learning workspace where you\\'ll work.\\nAzure CLI\\nYou can deploy only registered models to online endpoints. In this case, you already\\nhave a local copy of the model in the repository, so you only need to publish the model\\nto the registry in the workspace. Y ou can skip this step if the model you\\'re trying to\\ndeploy is already registered.\\nAzure CLI\\nIf your model was logged inside of a run, you can register it directly.\\nTo register the model, you need to know the location where it is stored. If you\\'re using\\nMLflow\\'s autolog feature, the path to the model depends on the model type and\\nframework. Y ou should check the jobs output to identify the name of the model\\'s folder.\\nThis folder contains a file named MLModel.Connect to your workspace\\nAzure CLI\\naz account set --subscription  <subscription>\\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \\nlocation= <location>\\nRegister the model\\nAzure CLI\\nMODEL_NAME= \\'sklearn-diabetes\\'\\naz ml model create --name $MODEL_NAME  --type \"mlflow_model\"  --path \\n\"endpoints\/online\/ncd\/sklearn-diabetes\/model\"\\nWhat if your model was logged inside of a run?', 'The az ml model command can be used for managing Azure Machine Learning models.\\nExamples are available in the examples GitHub repository . Several are shown below.\\nYAML\\nYAML\\nInstall and use the CLI (v2)Remarks\\nExamples\\nYAML: local file\\n$schema:  https:\/\/azuremlschemas.azureedge.net\/latest\/model.schema.json  \\nname: local-file-example  \\npath: mlflow-model\/model.pkl  \\ndescription:  Model created from local file. \\nYAML: local folder in MLflow format\\n$schema:  https:\/\/azuremlschemas.azureedge.net\/latest\/model.schema.json  \\nname: local-mlflow-example  \\npath: mlflow-model  \\ntype: mlflow_model  \\ndescription:  Model created from local MLflow model directory.', \"The Azure CLI  and the ml extension for Azure Machine Learning .\\nAzure CLI\\nThe workspace is the top-level resource for Azure Machine Learning, providing a\\ncentralized place to work with all the artifacts you create when you use Azure Machine\\nLearning. In this section, we'll connect to the workspace in which you'll perform\\ndeployment tasks.\\nPass in the values for your subscription ID, workspace, location, and resource group\\nin the following code:\\nAzure CLI\\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\\ninference over new data:\\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\\n７ Note\\nPipeline component deployments for Batch Endpoints were introduced in\\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\\nname ml to get the last version of it.\\nConnect to your workspace\\nAzure CLI\\naz account set --subscription  <subscription>\\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \\nlocation= <location>\\nSteps\"]","question":"What is the command to list all the models registered in an Azure ML Workspace using the Azure CLI?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":4}
{"answer":"To stream logs from a running experiment using Azure Machine Learning SDK v2 in Python, you can use the `Stream` class from the `azure.ai.ml` package. Here's an example of how to achieve this:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\n\n# Connect to the workspace\nml_client = MLClient.from_config(DefaultAzureCredential())\n\n# Get the run ID of the experiment\nrun_id = \"<your_run_id>\"\n\n# Stream logs from the running experiment\nstream = ml_client.streams.get(run_id)\nfor line in stream:\n    print(line)\n```\n\nReplace `<your_run_id>` with the actual run ID of the experiment you want to stream logs from.","documents":"['SDK v2\\nPython\\nFunctionality in SDK v1 Rough mapping in SDK v2\\nexperiment.submit MLCLient.jobs.create_or_update\\nScriptRunConfig() command()\\nFor more information, see:\\nV1 - Experiment\\nV2 - Command Jobaml_url = run.get_portal_url()  \\nprint(aml_url)  \\n#import required libraries  \\nfrom azure.ai.ml import MLClient, command  \\nfrom azure.ai.ml.entities import Environment  \\nfrom azure.identity import DefaultAzureCredential  \\n#connect to the workspace  \\nml_client = MLClient.from_config(DefaultAzureCredential())  \\n# set up pytorch environment  \\nenv = Environment(  \\n    image= \"mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04\" , \\n    conda_file= \"pytorch-env.yml\" , \\n    name= \"pytorch-env\"  \\n) \\n# define the command  \\ncommand_job = command(  \\n    code= \".\/src\", \\n    command= \"train.py\" , \\n    environment=env,  \\n    compute= \"cpu-cluster\" , \\n) \\nreturned_job = ml_client.jobs.create_or_update(command_job)  \\nreturned_job  \\nMapping of key functionality in v1 and v2\\nNext steps', \"Explore Azu re Machine Learning with\\nJupyter Notebooks\\nArticle •02\/06\/2024\\nAPPLIES T O: \\n  Python SDK azure-ai-ml v2 (curr ent)\\nThe AzureML-Examples  repository includes the latest (v2) Azure Machine Learning\\nPython CLI and SDK samples. For information on the various example types, see the\\nreadme .\\nThis article shows you how to access the repository from the following environments:\\nAzure Machine Learning compute instance\\nYour own compute resource\\nData Science Virtual Machine\\nThe easiest way to get started with the samples is to complete the Create resources to\\nget started . Once completed, you'll have a dedicated notebook server preloaded with\\nthe SDK and the Azure Machine Learning Notebooks repository. No downloads or\\ninstallation necessary.\\nTo view example notebooks:\\n1. Sign in to studio  and select your workspace if necessary.\\n2. Select Notebooks .\\n3. Select the Samples  tab. Use the SDK v2  folder for examples using Python SDK v2.\\n4. Open the notebook you want to run. Select Clone this not ebook  to create a copy\\nin your workspace file share. This action will copy the notebook along with any\\ndependent resources.\\nIf you'd like to bring your own notebook server for local development, follow these\\nsteps on your computer.\\n1. Use the instructions at Azure Machine Learning SDK  to install the Azure Machine\\nLearning SDK (v2) for Python\\nOption 1: Access on Azure Machine Learning\\ncompute instance (recommended)\\nOption 2: Access on your own notebook server\", \"Query & co mpare experiments and runs\\nwith MLflow\\nArticle •06\/26\/2023\\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\\nYou don't need to install any specific SDK to manage what happens inside of a training\\njob, creating a more seamless transition between local runs and the cloud by removing\\ncloud-specific dependencies. In this article, you'll learn how to query and compare\\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\\nin Python.\\nMLflow allows you to:\\nCreate, query, delete and search for experiments in a workspace.\\nQuery, delete, and search for runs in a workspace.\\nTrack and retrieve metrics, parameters, artifacts and models from runs.\\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\\ndetailed comparison between MLflow Open-Source and MLflow when connected to\\nAzure Machine Learning.\\nQuery and searching experiments and runs is also available using the MLflow REST API.\\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\\nconsume it.\\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\\nMLflow azureml-mlflow.７ Note\\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\\ntracking capabilities. This applies not just for logging but also for querying the\\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\\nexplains how to use MLflow to manage experiments and runs in Azure Machine\\nLearning.\\nREST API\\nPrerequisites\", \"Query & co mpare experiments and runs\\nwith MLflow\\nArticle •06\/26\/2023\\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\\nYou don't need to install any specific SDK to manage what happens inside of a training\\njob, creating a more seamless transition between local runs and the cloud by removing\\ncloud-specific dependencies. In this article, you'll learn how to query and compare\\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\\nin Python.\\nMLflow allows you to:\\nCreate, query, delete and search for experiments in a workspace.\\nQuery, delete, and search for runs in a workspace.\\nTrack and retrieve metrics, parameters, artifacts and models from runs.\\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\\ndetailed comparison between MLflow Open-Source and MLflow when connected to\\nAzure Machine Learning.\\nQuery and searching experiments and runs is also available using the MLflow REST API.\\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\\nconsume it.\\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\\nMLflow azureml-mlflow.７ Note\\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\\ntracking capabilities. This applies not just for logging but also for querying the\\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\\nexplains how to use MLflow to manage experiments and runs in Azure Machine\\nLearning.\\nREST API\\nPrerequisites\", 'SDK v2. Full sample link\\nPython\\'--learning_rate\\' , 0.1,\\'--model_output\\' , model_output],\\n    source_directory=script_folder,\\n    compute_target=aml_compute,\\n    allow_reuse= True,\\n)\\nscore_output = OutputFileDatasetConfig( \\'score_output\\' )\\nscore_step = PythonScriptStep(\\n    name=\"score step\" ,\\n    script_name= \"score.py\" ,\\n    arguments=[ \\'--model_input\\' ,model_output.as_input( \\'model_input\\' ), \\'-\\n-test_data\\' , input_ds.as_named_input( \\'test_data\\' ).as_mount(), \\'--\\nscore_output\\' , score_output],\\n    source_directory=script_folder,\\n    compute_target=aml_compute,\\n    allow_reuse= True,\\n)\\neval_output = OutputFileDatasetConfig( \\'eval_output\\' )\\neval_step = PythonScriptStep(\\n    name=\"eval step\" ,\\n    script_name= \"eval.py\" ,\\n    arguments=[ \\'--\\nscoring_result\\' ,score_output.as_input( \\'scoring_result\\' ), \\'--\\neval_output\\' , eval_output],\\n    source_directory=script_folder,\\n    compute_target=aml_compute,\\n    allow_reuse= True,\\n)\\n# built pipeline\\nfrom azureml.pipeline.core import Pipeline\\npipeline_steps = [train_step, score_step, eval_step]\\npipeline = Pipeline(workspace = workspace, steps=pipeline_steps)\\nprint(\"Pipeline is built.\" )\\npipeline_run = experiment.submit(pipeline, regenerate_outputs= False)\\nprint(\"Pipeline submitted for execution.\" )\\n# import required libraries\\nfrom azure.identity import DefaultAzureCredential,  \\nInteractiveBrowserCredential\\nfrom azure.ai.ml import MLClient, Input\\nfrom azure.ai.ml.dsl import pipeline']","question":"Provide an example of how to use the AML SDK v2 to stream logs from a running experiment in Python.","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":5}
{"answer":"To configure an AutoML run for a classification task using the AML SDK v2, you can use the provided sample code as a reference. Here's a breakdown of the key steps:\n\n1. Define the AutoML settings, including the primary metric, training data, and other configurations.\n2. Submit the AutoML run to the experiment.\n3. Retrieve the URL for monitoring the run's progress and results.\n\nThe provided code snippet demonstrates these steps, including the use of the `classification` factory function to create the AutoML classification job.","documents":"['SDK v2: Below is a sample AutoML classification task. For the entire code, check\\nout our examples repo .\\nPython    debug_log= \"automl_errors.log\" , \\n    compute_target=compute_target,  \\n    training_data=training_data,  \\n    label_column_name=label_column_name,  \\n    **automl_settings,  \\n) \\n# Submit run  \\nremote_run = experiment.submit(automl_config, show_output= False) \\nazureml_url = remote_run.get_portal_url()  \\nprint(azureml_url)  \\n# Imports  \\nfrom azure.ai.ml import automl, Input, MLClient  \\nfrom azure.ai.ml.constants import AssetTypes  \\nfrom azure.ai.ml.automl import ( \\n    classification,  \\n    ClassificationPrimaryMetrics,\\n    ClassificationModels,  \\n) \\n# Create MLTables for training dataset  \\n# Note that AutoML Job can also take in tabular data  \\nmy_training_data_input = Input(  \\n    type=AssetTypes.MLTABLE, path= \".\/data\/training-mltable-folder\"  \\n) \\n# Create the AutoML classification job with the related factory-\\nfunction.  \\nclassification_job = automl.classification(  \\n    compute= \"<compute_name>\" , \\n    experiment_name= \"<exp_name?\" , \\n    training_data=my_training_data_input,  \\n    target_column_name= \"<name_of_target_column>\" , \\n    primary_metric= \"accuracy\" , \\n    n_cross_validations= 5, \\n    enable_model_explainability= True, \\n    tags={ \"my_custom_tag\" : \"My custom value\" }, \\n) \\n# Limits are all optional  \\nclassification_job.set_limits(  \\n    timeout_minutes= 600, \\n    trial_timeout_minutes= 20, \\n    max_trials= 5, \\n    max_concurrent_trials = 4,', 'Upgrade AutoML to SDK v2\\nArticle •04\/04\/2023\\nIn SDK v2, \"experiments\" and \"runs\" are consolidated into jobs.\\nIn SDK v1, AutoML was primarily configured and run using the AutoMLConfig class. In\\nSDK v2, this class has been converted to an AutoML job. Although there are some\\ndifferences in the configuration options, by and large, naming & functionality has been\\npreserved in V2.\\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\\nSDK v1: Below is a sample AutoML classification task. For the entire code, check\\nout our examples repo .\\nPythonSubmit AutoML run\\n# Imports  \\nimport azureml.core  \\nfrom azureml.core.experiment import Experiment  \\nfrom azureml.core.workspace import Workspace  \\nfrom azureml.core.dataset import Dataset  \\nfrom azureml.train.automl import AutoMLConfig  \\nfrom azureml.train.automl.run import AutoMLRun    \\n# Load tabular dataset  \\ndata = \"<url_to_data>\"  \\ndataset = Dataset.Tabular.from_delimited_files(data)  \\ntraining_data, validation_data = dataset.random_split(percentage= 0.8, \\nseed=223) \\nlabel_column_name = \"Class\" \\n# Configure Auto ML settings  \\nautoml_settings = {  \\n    \"n_cross_validations\" : 3, \\n    \"primary_metric\" : \"average_precision_score_weighted\" , \\n    \"enable_early_stopping\" : True, \\n    \"max_concurrent_iterations\" : 2,   \\n    \"experiment_timeout_hours\" : 0.25,   \\n    \"verbosity\" : logging.INFO,  \\n} \\n# Put together an AutoML job constructor  \\nautoml_config = AutoMLConfig(  \\n    task= \"classification\" ,', '3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\\nMachine Learning in many different ways .\\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\\niterations over different models, hyperparameter settings, advanced\\npreprocessing\/featurization, and what metrics to look at when determining the\\nbest model.\\n5. Submit the training job.\\n6. Review the r esults\\nThe following diagram illustrates this process. \\nYou can also inspect the logged job information, which contains metrics  gathered\\nduring the job. The training job produces a Python serialized object ( .pkl file) that\\ncontains the model and data preprocessing.\\nWhile model building is automated, you can also learn how important or relevant\\nfeatures are  to the generated models.\\nApply automated ML when you want Azure Machine Learning to train and tune a model\\nfor you using the target metric you specify. Automated ML democratizes the machine\\nlearning model development process, and empowers its users, no matter their data\\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\\nML professionals and developers across industries can use automated ML to:\\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\\nforecasting, computer vision & NLP', 'iii. Select Create to create your compute target.\\nThis tak es a couple minut es to complet e.\\niv. After creation, select your new compute target from the drop-down list.\\ne. Select Next .\\n3. On the Select task and settings  form, complete the setup for your automated ML\\nexperiment by specifying the machine learning task type and configuration\\nsettings.\\na. Select Classification  as the machine learning task type.\\nb. Select View additional configuration settings  and populate the fields as\\nfollows. These settings are to better control the training job. Otherwise, defaults\\nare applied based on experiment selection and data.\\nAdditional\\xa0configurations Descr iption Value\\xa0for\\xa0tut orial\\nPrimary metric Evaluation metric that the\\nmachine learning algorithm\\nwill be measured by.AUC_weighted\\nExplain best model Automatically shows\\nexplainability on the bestEnable', \"An Azure subscription. (If you don't have an Azure subscription, create a free\\naccount  before you begin)\\nAn Azure Machine Learning experiment created with either:\\nThe Azure Machine Learning studio  (no code required)\\nThe Azure Machine Learning Python SDK\\nAfter your automated ML experiment completes, a history of the jobs can be found via:\\nA browser with Azure Machine Learning studio\\nA Jupyter notebook using the JobDetails Jupyter widget\\nThe following steps and video, show you how to view the run history and model\\nevaluation metrics and charts in the studio:\\n1. Sign into the studio  and navigate to your workspace.\\n2. In the left menu, select Jobs.\\n3. Select your experiment from the list of experiments.\\n4. In the table at the bottom of the page, select an automated ML job.\\n5. In the Models  tab, select the Algorithm name  for the model you want to evaluate.\\n6. In the Metrics  tab, use the checkboxes on the left to view metrics and charts.\\nAutomated ML calculates performance metrics for each classification model generated\\nfor your experiment. These metrics are based on the scikit learn implementation.\\nMany classification metrics are defined for binary classification on two classes, and\\nrequire averaging over classes to produce one score for multi-class classification. Scikit-\\nlearn provides several averaging methods, three of which automated ML exposes:\\nmacr o, micro, and weight ed.\\nMacr o - Calculate the metric for each class and take the unweighted average\\nMicro - Calculate the metric globally by counting the total true positives, false\\nnegatives, and false positives (independent of classes).\\nWeight ed - Calculate the metric for each class and take the weighted average\\nbased on the number of samples per class.\\nWhile each averaging method has its benefits, one common consideration when\\nselecting the appropriate method is class imbalance. If classes have different numbers of\\nsamples, it might be more informative to use a macro average where minority classes\\nView job results\\nClassification metrics\"]","question":"How can you configure an AutoML run for a classification task using the AML SDK v2, including specifying primary metric and training data?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":6}
{"answer":"To retrieve the best model from an AutoML run using the Azure Machine Learning SDK v2, you can follow these steps:\n\n1. **Get the Best Model**: Use the `get_output` method on the AutoML run to retrieve the best model and its associated metrics.\n\n2. **Register the Model**: Optionally, you can register the best model with the Azure Machine Learning workspace using the `register_model` method.\n\n3. **Download the Model**: If needed, you can download the best model to your local environment using the `download` method.\n\nWould you like more detailed information on any of these steps?","documents":"['Learn more about how and where to deploy a model .\\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps', \"Navigate to the Models  tab to see the algorithms (models) tested. By default, the\\nmodels are ordered by metric score as they complete. For this tutorial, the model that\\nscores the highest based on the chosen Normalized r oot mean squar ed err or metric is\\nat the top of the list.\\nWhile you wait for all of the experiment models to finish, select the Algorithm name  of\\na completed model to explore its performance details.\\nThe following example navigates to select a model from the list of models that the job\\ncreated. Then, you select the Overview  and the Metrics  tabs to view the selected\\nmodel's properties, metrics and performance charts.\\nAutomated machine learning in Azure Machine Learning studio allows you to deploy the\\nbest model as a web service in a few steps. Deployment is the integration of the model\\nso it can predict on new data and identify potential areas of opportunity.\\nFor this experiment, deployment to a web service means that the bike share company\\nnow has an iterative and scalable web solution for forecasting bike share rental demand.\\nOnce the job is complete, navigate back to parent job page by selecting Job 1  at the top\\nof your screen.\\nIn the Best model summar y section, the best model in the context of this experiment, is\\nselected based on the Normalized r oot mean squar ed err or metric.\\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\\nThe deployment process entails several steps including registering the model,Deploy the model\", \"To run a fine-tuned model job from Machine Learning, in order to deploy an Azure\\nOpenAI model:\\n1. After you've finished fine-tuning an Azure OpenAI model, find the registered\\nmodel in the Models  list with the name provided during fine-tuning and select the\\nmodel you want to deploy.\\n2. Select Deploy  and name the deployment. The model is deployed to the default\\nAzure OpenAI resource linked to your workspace.\\nTo enable users to quickly get started with code-based fine-tuning, we've published\\nsamples (both Python notebooks and Azure CLI examples) to the azureml-ex amples\\nGitHub repo:\\nSDK example\\nCLI exampleDeploy fine-tuned models\\nFine-tuning by using code-based samples\", \"Option 2: T o deploy a specific model iteration from this experiment.\\na. Select the desired model from the Models  tab\\nb. Select Deploy  on the top left of the window.\\n2. Populate the Deploy model  pane.\\nField Value\\nName Enter a unique name for your deployment.\\nDescription Enter a description to better identify what this deployment is for.\\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\\nService (AKS)  or Azure Container Inst ance (ACI).\\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\\ndeploy to.\\nEnable\\nauthenticationSelect to allow for token-based or key-based authentication.\\nUse custom\\ndeployment assetsEnable this feature if you want to upload your own scoring script and\\nenvironment file. Otherwise, automated ML provides these assets for\\nyou by default. Learn more about scoring scripts .\\nThe Advanced menu offers default deployment features such as data collection  and\\nresource utilization settings. If you wish to override these defaults do so in this\\nmenu.\\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\\ndeployment begins, the Model summar y tab appears. See the deployment\\nprogress under the Deploy status  section.\\nNow you have an operational web service to generate predictions! Y ou can test the\\npredictions by querying the service from Power BI's built in Azure Machine Learning\\nsupport .ﾉExpand table\\n） Impor tant\\nFile names must be under 32 characters and must begin and end with\\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\\nbetween. Spaces are not allowed.\", 'The automated machine learning interface allows you to deploy the best model as a\\nweb service in a few steps. Deployment is the integration of the model so it can predict\\non new data and identify potential areas of opportunity.\\nFor this experiment, deployment to a web service means that the financial institution\\nnow has an iterative and scalable web solution for identifying potential fixed term\\ndeposit customers.\\nCheck to see if your experiment run is complete. T o do so, navigate back to the parent\\njob page by selecting Job 1  at the top of your screen. A Complet ed status is shown on\\nthe top left of the screen.\\nOnce the experiment run is complete, the Details  page is populated with a Best model\\nsummar y section. In this experiment context, VotingEnsemble  is considered the best\\nmodel, based on the AUC_w eight ed metric.\\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\\nThe deployment process entails several steps including registering the model,\\ngenerating resources, and configuring them for the web service.\\n1. Select VotingEnsemble  to open the model-specific page.\\n2. Select the Deploy  menu in the top-left and select Deploy t o web ser vice.\\n3. Populate the Deploy a model  pane as follows:\\nField Value\\nDeployment name my-automl-deploy\\nDeployment\\ndescriptionMy first automated machine learning experiment deployment\\nCompute type Select Azure Container Instance (A CI)\\nEnable\\nauthenticationDisable.\\nUse custom\\ndeploymentsDisable. Allows for the default driver file (scoring script) and\\nenvironment file to be auto-generated.\\nFor this example, we use the defaults provided in the Advanced menu.\\n4. Select Deploy .Deploy the best model']","question":"Describe the steps to retrieve the best model from an AutoML run using the AML SDK v2.","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":7}
{"answer":"Azure Machine Learning's Automated ML provides capabilities to handle imbalanced data by default. It offers charts and metrics to help identify imbalanced data risks and implements best practices to mitigate them. Additionally, it can automatically apply techniques such as class balancing, resampling, and weighted loss functions to address imbalanced data during model training.","documents":"[\"Prevent overfitting and imbalanced data\\nwith Automated ML\\nArticle •06\/16\/2023\\nOverfitting and imbalanced data are common pitfalls when you build machine learning\\nmodels. By default, Azure Machine Learning's Automated ML provides charts and\\nmetrics to help you identify these risks, and implements best practices to help mitigate\\nthem.\\nOverfitting in machine learning occurs when a model fits the training data too well, and\\nas a result can't accurately predict on unseen test data. In other words, the model has\\nmemorized specific patterns and noise in the training data, but is not flexible enough to\\nmake predictions on real data.\\nConsider the following trained models and their corresponding train and test accuracies.\\nModel Train accuracy Test accuracy\\nA 99.9% 95%\\nB 87% 87%\\nC 99.9% 45%\\nConsider model A, there is a common misconception that if test accuracy on unseen\\ndata is lower than training accuracy, the model is overfitted. However, test accuracy\\nshould always be less than training accuracy, and the distinction for overfit vs.\\nappropriately fit comes down to how much  less accurate.\\nCompare models A and B, model A is a better model because it has higher test\\naccuracy, and although the test accuracy is slightly lower at 95%, it is not a significant\\ndifference that suggests overfitting is present. Y ou wouldn't choose model B because\\nthe train and test accuracies are closer together.\\nModel C represents a clear case of overfitting; the training accuracy is high but the test\\naccuracy isn't anywhere near as high. This distinction is subjective, but comes from\\nknowledge of your problem and data, and what magnitudes of error are acceptable.Identify overfitting\\nPrevent overfitting\", \"If it ascertains a better performance through this experiment, then this remedy is\\napplied.\\nUse a performance metric that deals better with imbalanced data. For example, the\\nAUC_weighted is a primary metric that calculates the contribution of every class\\nbased on the relative number of samples representing that class, hence is more\\nrobust against imbalance.\\nThe following techniques are additional options to handle imbalanced data outside of\\nAutomated ML.\\nResampling to even the class imbalance, either by up-sampling the smaller classes\\nor down-sampling the larger classes. These methods require expertise to process\\nand analyze.\\nReview performance metrics for imbalanced data. For example, the F1 score is the\\nharmonic mean of precision and recall. Precision measures a classifier's exactness,\\nwhere higher precision indicates fewer false positives, while recall measures a\\nclassifier's completeness, where higher recall indicates fewer false negatives.\\nSee examples and learn how to build models using Automated ML:\\nFollow the Tutorial: T rain an object detection model with automated machine\\nlearning and Python .\\nConfigure the settings for automatic training experiment:\\nIn Azure Machine Learning studio, use these steps .\\nWith the Python SDK, use these steps .Next steps\", 'Imbalanced data is commonly found in data for machine learning classification\\nscenarios, and refers to data that contains a disproportionate ratio of observations in\\neach class. This imbalance can lead to a falsely perceived positive effect of a model\\'s\\naccuracy, because the input data has bias towards one class, which results in the trained\\nmodel to mimic that bias.\\nIn addition, Automated ML jobs generate the following charts automatically. These\\ncharts help you understand the correctness of the classifications of your model, and\\nidentify models potentially impacted by imbalanced data.\\nChar t Descr iption\\nConfusion\\nMatrixEvaluates the correctly classified labels against the actual labels of the data.\\nPrecision-recall Evaluates the ratio of correct labels against the ratio of found label instances of\\nthe data\\nROC Curves Evaluates the ratio of correct labels against the ratio of false-positive labels.\\nAs part of its goal of simplifying the machine learning workflow, Automated ML has built\\nin capabilities to help deal with imbalanced data such as,\\nA weight column: Automated ML creates a column of weights as input to cause\\nrows in the data to be weighted up or down, which can be used to make a class\\nmore or less \"important.\"\\nThe algorithms used by Automated ML detect imbalance when the number of\\nsamples in the minority class is equal to or fewer than 20% of the number of\\nsamples in the majority class, where minority class refers to the one with fewest\\nsamples and majority class refers to the one with most samples. Subsequently,\\nautomated machine learning will run an experiment with subsampled data to\\ncheck if using class weights would remedy this problem and improve performance.Cross-validation isn\\'t enabled by default; it must be configured in Automated\\nmachine learning settings. However, after cross-validation is configured and a\\nvalidation data set has been provided, the process is automated for you.\\nIdentify models with imbalanced data\\nHandle imbalanced data', 'What is automated machine learning\\n(AutoML)?\\nArticle •04\/13\/2023\\nAPPLIES T O: \\n  Python SDK azure-ai-ml v2 (curr ent)\\nAutomated machine learning, also referred to as automated ML or AutoML, is the\\nprocess of automating the time-consuming, iterative tasks of machine learning model\\ndevelopment. It allows data scientists, analysts, and developers to build ML models with\\nhigh scale, efficiency, and productivity all while sustaining model quality. Automated ML\\nin Azure Machine Learning is based on a breakthrough from our Microsoft R esearch\\ndivision .\\nFor code-experienced customers, Azure Machine Learning Python SDK . Get\\nstarted with Tutorial: T rain an object detection model (preview) with AutoML and\\nPython .\\nDuring training, Azure Machine Learning creates a number of pipelines in parallel that\\ntry different algorithms and parameters for you. The service iterates through ML\\nalgorithms paired with feature selections, where each iteration produces a model with a\\ntraining score. The better the score for the metric you want to optimize for, the better\\nthe model is considered to \"fit\" your data. It will stop once it hits the exit criteria defined\\nin the experiment.\\nUsing Azur e Machine Learning , you can design and run your automated ML training\\nexperiments with these steps:\\n1. Identif y the ML pr oblem  to be solved: classification, forecasting, regression,\\ncomputer vision or NLP.\\n2. Choose whether y ou want a code-fir st experience or a no-code studio w eb\\nexperience : Users who prefer a code-first experience can use the Azure Machine\\nLearning SDK v2 or the Azure Machine Learning CLIv2 . Get started with Tutorial:\\nTrain an object detection model with AutoML and Python . Users who prefer a\\nlimited\/no-code experience can use the web interface  in Azure Machine Learning\\nstudio at https:\/\/ml.azure.com . Get started with Tutorial: Create a classification\\nmodel with automated ML in Azure Machine Learning .\\nHow does AutoML work?', 'Automated ML in Azure Machine Learning supports model training for computer vision\\ntasks like image classification and object detection. T o debug AutoML vision models and\\nexplain model predictions, AutoML models for computer vision are integrated with\\nResponsible AI dashboard. T o generate R esponsible AI insights for AutoML computer\\nvision models, register your best AutoML model in the Azure Machine Learning\\nworkspace and run it through the R esponsible AI vision insights pipeline. T o learn, see\\nhow to set up AutoML to train computer vision models .\\nNotebooks related to the AutoML supported computer vision tasks can be found in\\nazureml-examples  repository.\\nThe R esponsible AI vision Insights pipeline could be submitted through one of the\\nfollowing methods\\nPython SDK: T o learn how to submit the pipeline through Python, see the AutoML\\nImage Classification scenario with RAI Dashboard sample notebook . For\\nconstructing the pipeline, refer to section 5.1 in the notebook.\\nAzure CLI: T o submit the pipeline via Azure-CLI, see the component Y AML in\\nsection 5.2 of the example notebook linked above.\\nUI (via Azure Machine Learning studio): From the Designer in Azure Machine\\nLearning studio, the RAI-vision insights component can be used to create and\\nsubmit a pipeline.\\nIn addition to the list of R esponsible AI vision insights parameters provided in the\\nprevious section, the following are parameters to set specifically for AutoML models.Integration with AutoML Image\\nMode of submitting the Responsible AI vision insights\\npipeline\\nResponsible AI vision insights component parameter\\n(AutoML specific)\\n７ Note\\nA few parameters are specific to the XAI algorithm chosen and are optional for\\nother algorithms.']","question":"What are the AutoML capabilities for handling imbalanced data in Azure ML?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":8}
{"answer":"To use AutoML for forecasting time series data, you should specify the time column and the target column in your dataset. Additionally, you can adjust the forecast horizon, the maximum forecast horizon, and the time series identifier columns. These settings can be adjusted in the AutoMLConfig object when creating an experiment.","documents":"[\"Frequently asked questions about\\nforecasting in AutoML\\nArticle •08\/01\/2023\\nAPPLIES T O: \\n  Python SDK azure-ai-ml v2 (curr ent)\\nThis article answers common questions about forecasting in automatic machine learning\\n(AutoML). For general information about forecasting methodology in AutoML, see the\\nOverview of forecasting methods in AutoML  article.\\nYou can start by reading the Set up AutoML to train a time-series forecasting model\\narticle. Y ou can also find hands-on examples in several Jupyter notebooks:\\nBike share example\\nForecasting using deep learning\\nMany Models solution\\nForecasting recipes\\nAdvanced forecasting scenarios\\nWe're always working to make AutoML faster and more scalable. T o work as a general\\nforecasting platform, AutoML does extensive data validations and complex feature\\nengineering, and it searches over a large model space. This complexity can require a lot\\nof time, depending on the data and the configuration.\\nOne common source of slow runtime is training AutoML with default settings on data\\nthat contains numerous time series. The cost of many forecasting methods scales with\\nthe number of series. For example, methods like Exponential Smoothing and Prophet\\ntrain a model for each time series  in the training data.\\nThe Many Models feature of AutoML scales to these scenarios by distributing training\\njobs across a compute cluster. It has been successfully applied to data with millions of\\ntime series. For more information, see the many models  article section. Y ou can also\\nread about the success of Many Models  on a high-profile competition dataset.\\nHow do I start building forecasting models in\\nAutoML?\\nWhy is AutoML slow on my data?\", 'Learn more about how and where to deploy a model .\\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps', \"If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\\nLearn more about how to set up AutoML to train a time-series forecasting model .\\nLearn about calendar features for time series forecasting in AutoML .\\nLearn about how AutoML uses machine learning to build forecasting models .\\nLearn about AutoML forecasting for lagged features .Next steps\", \"Set up AutoML to train a time-series\\nforecasting model with SDK and CLI\\nArticle •08\/02\/2023\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (current)  \\n  Python SDK azure-ai-ml v2\\n(current)\\nIn this article, you'll learn how to set up AutoML for time-series forecasting with Azure\\nMachine Learning automated ML in the Azure Machine Learning Python SDK .\\nTo do so, you:\\nFor a low code experience, see the Tutorial: Forecast demand with automated machine\\nlearning  for a time-series forecasting example using automated ML in the Azure\\nMachine Learning studio .\\nAutoML uses standard machine learning models along with well-known time series\\nmodels to create forecasts. Our approach incorporates historical information about the\\ntarget variable, user-provided features in the input data, and automatically engineered\\nfeatures. Model search algorithms then work to find a model with the best predictive\\naccuracy. For more details, see our articles on forecasting methodology  and model\\nsearch .\\nFor this article you need,\\nAn Azure Machine Learning workspace. T o create the workspace, see Create\\nworkspace resources .\\nThe ability to launch AutoML training jobs. Follow the how-to guide for setting up\\nAutoML  for details.\\nPrepare data for training.＂\\nConfigure specific time-series parameters in a Forecasting Job . ＂\\nOrchestrate training, inference, and model evaluation using components and\\npipelines.＂\\nPrerequisites\\nTraining  and validation data\", \"See the Why is AutoML slow on my data?  answer to understand why AutoML might be\\nslow in your case.\\nConsider the following configuration changes that might speed up your job:\\nBlock time series models  like ARIMA and Prophet.\\nTurn off look-back features like lags and rolling windows.\\nReduce:\\nThe number of trials\/iterations.\\nTrial\/iteration timeout.\\nExperiment timeout.\\nThe number of cross-validation folds.\\nEnsure that early termination is enabled.\\nAutoML forecasting supports four basic configurations:\\nConfiguration Scenar io Pros Cons\\nDefault\\nAutoMLRecommended if the dataset\\nhas a small number of time\\nseries that have roughly\\nsimilar historical behavior.- Simple to configure\\nfrom code\/SDK or\\nAzure Machine\\nLearning studio.\\n- AutoML can learn\\nacross different time\\nseries because the\\nregression models\\npool all series together\\nin training. For more\\ninformation, see\\nModel grouping .- Regression models\\nmight be less\\naccurate if the time\\nseries in the training\\ndata have divergent\\nbehavior.\\n- Time series models\\nmight take a long\\ntime to train if the\\ntraining data has a\\nlarge number of\\nseries. For more\\ninformation, see the\\nWhy is AutoML slow\\non my data?  answer.\\nAutoML with\\ndeep learningRecommended for datasets\\nwith more than 1,000\\nobservations and, potentially,\\nnumerous time series that\\nexhibit complex patterns.\\nWhen it's enabled, AutoML\\nwill sweep over temporal- Simple to configure\\nfrom code\/SDK or\\nAzure Machine\\nLearning studio.\\n- Cross-learning\\nopportunities, because- Training can take\\nmuch longer\\nbecause of the\\ncomplexity of DNN\\nmodels.\\n- Series with smallHow can I make AutoML faster?\\nWhat modeling configuration should I use?\"]","question":"Explain how to use AutoML for forecasting time series data. What specific settings should be adjusted for time series problems?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":9}
{"answer":"To create a custom deep learning environment with specific Python packages using the AML SDK v2, you can define a custom environment configuration and register it with your workspace. Here's an example of how to do this:\n\n```python\nfrom azure.ai.ml.entities import Environment\n\n# Define the custom environment\ncustom_env_name = \"keras-env\"\njob_env = Environment(\n    name=custom_env_name,\n    description=\"Custom environment for keras image classification\",\n    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n    image=\"mcr.microsoft.com\/azureml\/openmpi4.1.0-ubuntu20.04:latest\",\n)\n\n# Register the custom environment\njob_env = ml_client.environments.create_or_update(job_env)\nprint(\n    f\"Environment with name {job_env.name} is registered to workspace, the environment version is {job_env.version}\"\n)\n```\n\nIn this example, you define a custom environment named \"keras-env\" with a description and specify the required Python packages in a Conda environment file. Then, you register the environment with your workspace.","documents":"['The specification contains some usual packages (such as numpy and pip) that you\\'ll use\\nin your job.\\nNext, use the Y AML file to create and register this custom environment in your\\nworkspace. The environment will be packaged into a Docker container at runtime.\\nPython\\nFor more information on creating and using environments, see Create and use software\\nenvironments in Azure Machine Learning .\\nIn this section, we\\'ll begin by introducing the data for training. W e\\'ll then cover how to\\nrun a training job, using a training script that we\\'ve provided. Y ou\\'ll learn to build the\\ntraining job by configuring the command for running the training script. Then, you\\'ll\\nsubmit the training job to run in Azure Machine Learning.\\nYou\\'ll use data from the Modified National Institute of S tandards and T echnology\\n(MNIST) database of handwritten digits. This data is sourced from Y an LeCun\\'s website\\nand stored in an Azure storage account.\\nPythonfrom azure.ai.ml.entities import Environment  \\ncustom_env_name = \"keras-env\"  \\njob_env = Environment(  \\n    name=custom_env_name,  \\n    description= \"Custom environment for keras image classification\" , \\n    conda_file=os.path.join(dependencies_dir, \"conda.yaml\" ), \\n    image= \"mcr.microsoft.com\/azureml\/openmpi4.1.0-ubuntu20.04:latest\" , \\n) \\njob_env = ml_client.environments.create_or_update(job_env)  \\nprint( \\n    f\"Environment with name {job_env.name}  is registered to workspace, the  \\nenvironment version is {job_env.version} \" \\n)\\nConfigure and submit your training  job\\nObtain the training data\\nweb_path = \"wasbs:\/\/datasets@azuremlexamples.blob.core.windows.net\/mnist\/\"', \"In the samples deep learning folder on the notebook server, find a\\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\\ndeploy-with-k eras.\\nYour Jupyter notebook server\\nInstall the Azure Machine Learning SDK (v2) .\\nDownload the training scripts keras_mnist.py  and utils.py .\\nYou can also find a completed Jupyter Notebook version  of this guide on the GitHub\\nsamples page.\\nBefore you can run the code in this article to create a GPU cluster, you'll need to request\\na quota increase  for your workspace.\\nThis section sets up the job for training by loading the required Python packages,\\nconnecting to a workspace, creating a compute resource to run a command job, and\\ncreating an environment to run the job.\\nFirst, you'll need to connect to your Azure Machine Learning workspace. The Azure\\nMachine Learning workspace  is the top-level resource for the service. It provides you\\nwith a centralized place to work with all the artifacts you create when you use Azure\\nMachine Learning.\\nWe're using DefaultAzureCredential to get access to the workspace. This credential\\nshould be capable of handling most Azure SDK authentication scenarios.\\nIf DefaultAzureCredential doesn't work for you, see azure-identity reference\\ndocumentation  or Set up authentication  for more available credentials.\\nPython\\nSet up the job\\nConnect to the workspace\\n# Handle to the workspace  \\nfrom azure.ai.ml import MLClient  \\n# Authentication package  \\nfrom azure.identity import DefaultAzureCredential  \\ncredential = DefaultAzureCredential()\", 'Azure Machine Learning allows you to either use a curated (or ready-made)\\nenvironment or create a custom environment using a Docker image or a Conda\\nconfiguration. In this article, you reuse the curated Azure Machine Learning environment\\nAzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu. Use the latest version of this\\nenvironment using the @latest directive.\\nPython\\nIn this section, we begin by introducing the data for training. W e then cover how to run\\na training job, using a training script that we\\'ve provided. Y ou\\'ll learn to build the\\ntraining job by configuring the command for running the training script. Then, you\\'ll\\nsubmit the training job to run in Azure Machine Learning.\\nYou can use the dataset in this zipped file . This dataset consists of about 120 training\\nimages each for two classes (turkeys and chickens), with 100 validation images for each\\nclass. The images are a subset of the Open Images v5 Dataset . The training script\\npytorch_tr ain.p y downloads and extracts the dataset.\\nIn the prerequisites section, we provided the training script pytorch_tr ain.p y. In practice,\\nyou should be able to take any custom training script as is and run it with Azure\\nMachine Learning without having to modify your code.\\nThe provided training script downloads the data, trains a model, and registers the\\nmodel.\\nNow that you have all the assets required to run your job, it\\'s time to build it using the\\nAzure Machine Learning Python SDK v2. For this example, we create a command.\\nAn Azure Machine Learning command is a resource that specifies all the details needed to\\nexecute your training code in the cloud. These details include the inputs and outputs,curated_env_name = \"AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu@latest\"\\nConfigure and submit your training  job\\nObtain the training data\\nPrepare the training script\\nBuild the training job', \"2. Create an Azure Machine Learning workspace .\\n3. Clone the AzureML-Examples repository .\\nBash\\n4. Start the notebook server from the directory containing your clone.\\nBash\\nThese instructions install the base SDK packages necessary for the quickstart and tutorial\\nnotebooks. Other sample notebooks might require you to install extra components. For\\nmore information, see Install the Azure Machine Learning SDK for Python .\\nThe Data Science Virtual Machine (DSVM) is a customized VM image built specifically for\\ndoing data science. If you create a DSVM , the SDK and notebook server are installed and\\nconfigured for you. However, you'll still need to create a workspace and clone the\\nsample repository.\\n1. Create an Azure Machine Learning workspace .\\n2. Clone the the AzureML-Examples repository .\\nBash\\n3. Start the notebook server from the directory that contains the clone.\\nBash\\ngit clone https:\/\/github.com\/Azure\/azureml-examples.git --depth 1\\njupyter notebook\\nOption 3: Access on a DSVM\\ngit clone https:\/\/github.com\/Azure\/azureml-examples.git --depth 1\\njupyter notebook\\nConnect to a workspace\", \"Manage Azu re Machine Learning\\nenvironments with the CLI & SDK (v2)\\nArticle •01\/03\/2024\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (current)  \\n  Python SDK azure-ai-ml v2\\n(current)\\nAzure Machine Learning environments define the execution environments for your jobs\\nor deployments and encapsulate the dependencies for your code. Azure Machine\\nLearning uses the environment specification to create the Docker container that your\\ntraining or scoring code runs in on the specified compute target. Y ou can define an\\nenvironment from a conda specification, Docker image, or Docker build context.\\nIn this article, learn how to create and manage Azure Machine Learning environments\\nusing the SDK & CLI (v2).\\nBefore following the steps in this article, make sure you have the following prerequisites:\\nAn Azure Machine Learning workspace. If you don't have one, use the steps in the\\nQuickstart: Create workspace resources  article to create one.\\nThe Azure CLI and the ml extension or the Azure Machine Learning Python SDK v2:\\nTo install the Azure CLI and extension, see Install, set up, and use the CLI (v2) .\\nTo install the Python SDK v2, use the following command:\\nBash\\nPrerequisites\\n） Impor tant\\nThe CLI examples in this article assume that you are using the Bash (or\\ncompatible) shell. For example, from a Linux system or Windows\\nSubsyst em for Linux .\\npip install azure-ai-ml azure-identity\"]","question":"How do you create a custom deep learning environment with specific Python packages using the AML SDK v2?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":10}
{"answer":"To train a deep learning model on a GPU compute cluster using Azure ML, you would follow these general steps:\n\n1. **Create or Connect to an Azure ML Workspace**: This is the central place where you manage your Azure ML resources.\n\n2. **Define a GPU Compute Target**: You would define a compute target that specifies the GPU resources required for training.\n\n3. **Prepare Training Script**: Create a training script that specifies the deep learning model, data, and any dependencies.\n\n4. **Submit the Training Job**: Submit the training job to the Azure ML service, specifying the compute target and other job settings.\n\n5. **Monitor and Manage the Job**: Monitor the job's progress and manage the resources used during training.\n\n6. **Retrieve and Register Model**: Once training is complete, retrieve the trained model and register it in the Azure ML workspace for later use.\n\nLet me know if you need more detailed information on any of these steps.","documents":"['An Azure Machine Learning compute cluster is a fully managed compute resource that\\ncan be used to run the training job. In the following examples, a compute cluster named\\ncpu-compute is created.\\nPython\\nTo run this script, you\\'ll use a command that executes main.py Python script located\\nunder .\/sdk\/python\/jobs\/single-step\/lightgbm\/iris\/src\/. The command will be runworkspace = \\'<AZUREML_WORKSPACE_NAME>\\'\\n#connect to the workspace\\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \\nresource_group, workspace)\\n2. Create a compute resource for training\\n７ Note\\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\\ntraining job .\\nPython SDK\\nfrom azure.ai.ml.entities import AmlCompute\\n# specify aml compute name.\\ncpu_compute_target = \"cpu-cluster\"\\ntry:\\n    ml_client.compute.get(cpu_compute_target)\\nexcept Exception:\\n    print(\"Creating a new cpu compute target...\" )\\n    compute = AmlCompute(\\n        name=cpu_compute_target, size= \"STANDARD_D2_V2\" , min_instances= 0, \\nmax_instances= 4\\n    )\\n    ml_client.compute.begin_create_or_update(compute).result()\\n4. Submit the training job\\nPython SDK', 'Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', \"Manage and optimize Azure Machine\\nLearning costs\\nArticle •08\/01\/2023\\nLearn how to manage and optimize costs when training and deploying machine learning\\nmodels to Azure Machine Learning.\\nUse the following tips to help you manage and optimize your compute resource costs.\\nConfigure your training clusters for autoscaling\\nSet quotas on your subscription and workspaces\\nSet termination policies on your training job\\nUse low-priority virtual machines (VM)\\nSchedule compute instances to shut down and start up automatically\\nUse an Azure R eserved VM Instance\\nTrain locally\\nParallelize training\\nSet data retention and deletion policies\\nDeploy resources to the same region\\nFor information on planning and monitoring costs, see the plan to manage costs for\\nAzure Machine Learning  guide.\\nWith constantly changing data, you need fast and streamlined model training and\\nretraining to maintain accurate models. However, continuous training comes at a cost,\\nespecially for deep learning models on GPUs.） Impor tant\\nItems marked (preview) in this article are currently in public preview. The preview\\nversion is provided without a service level agreement, and it's not recommended\\nfor production workloads. Certain features might not be supported or might have\\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\\nMicrosoft Azur e Previews .\\nUse Azure Machine Learning compute cluster\\n(AmlCompute)\", 'For detailed instructions on how to complete these steps, see Secure a training\\nenvironment .\\nIn this section, you learn how Azure Machine Learning securely communicates between\\nservices to submit a training job. This example shows you how all your configurations\\nwork together to secure communication.\\n1. The client uploads training scripts and training data to storage accounts that are\\nsecured with a service or private endpoint.\\n2. The client submits a training job to the Azure Machine Learning workspace\\nthrough the private endpoint.\\n3. Azure Batch service receives the job from the workspace. It then submits the\\ntraining job to the compute environment through the public load balancer for the\\ncompute resource.\\n4. The compute resource receives the job and begins training. The compute resource\\nuses information stored in key vault to access storage accounts to download\\ntraining files and upload output.Example training job submission', 'like Spark.\\nIn the following example script, we provision a Linux compute cluster . You can see the\\nAzure Machine Learning pricing  page for the full list of VM sizes and prices. Since we\\nneed a GPU cluster for this example, let\\'s pick a STAND ARD_NC6  model and create an\\nAzure Machine Learning compute.\\nPython\\nfrom azure.ai.ml.entities import AmlCompute  \\ngpu_compute_target = \"gpu-cluster\"  \\ntry: \\n    # let\\'s see if the compute target already exists  \\n    gpu_cluster = ml_client.compute.get(gpu_compute_target)  \\n    print(  \\n        f\"You already have a cluster named {gpu_compute_target} , we\\'ll reuse  \\nit as is.\"  \\n    ) \\nexcept Exception:  \\n    print( \"Creating a new gpu compute target...\" ) \\n    # Let\\'s create the Azure ML compute object with the intended parameters  \\n    gpu_cluster = AmlCompute(  \\n        # Name assigned to the compute cluster  \\n        name= \"gpu-cluster\" , \\n        # Azure ML Compute is the on-demand VM service  \\n        type= \"amlcompute\" , \\n        # VM Family  \\n        size= \"STANDARD_NC6\" , \\n        # Minimum running nodes when there is no job running  \\n        min_instances= 0, \\n        # Nodes in cluster  \\n        max_instances= 4, \\n        # How many seconds will the node running after the job termination  \\n        idle_time_before_scale_down= 180, \\n        # Dedicated or LowPriority. The latter is cheaper but there is a  \\nchance of job termination  \\n        tier= \"Dedicated\" , \\n    ) \\n    # Now, we pass the object to MLClient\\'s create_or_update method  \\n    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()  \\nprint( \\n    f\"AMLCompute with name {gpu_cluster.name}  is created, the compute size  \\nis {gpu_cluster.size} \" \\n)']","question":"Describe the process of using Azure ML to train a deep learning model on a GPU compute cluster.","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":11}
{"answer":"To deploy a TensorFlow model as a web service in Azure ML, you would typically follow these steps:\n\n1. **Prepare the Model**: Ensure that your TensorFlow model is trained and saved in a format that Azure ML can use for deployment, such as a SavedModel or a TensorFlow Lite model.\n\n2. **Create an Inference Configuration**: Define the environment and entry script for your model. This includes specifying the dependencies required to run your model, such as the TensorFlow runtime.\n\n3. **Create a Deployment Configuration**: Specify the compute target and other settings for the deployment, such as the number of instances and the type of instance to use.\n\n4. **Deploy the Model**: Use Azure ML to deploy the model as a web service, which will create an endpoint that can be used to make predictions.\n\nKey considerations for deployment include:\n- **Scalability**: Consider the expected load on your service and choose an appropriate compute target and instance type.\n- **Security**: Ensure that your endpoint is secured and that any sensitive data is handled appropriately.\n- **Monitoring**: Set up monitoring to track the performance and usage of your endpoint.\n- **Versioning**: Consider how you will manage and update different versions of your model.\n\nWould you like more detailed information on any of these steps or considerations?","documents":"['What are Azure Machine Learning endpoints?\\nDeploy and score a model with an online endpointNext steps', 'Tutorial Descr iption\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', 'To learn more, review these articles:\\nDeploy models with REST\\nCreate and use managed online endpoints in the studio\\nSafe rollout for online endpoints\\nHow to autoscale managed online endpoints\\nView costs for an Azure Machine Learning managed online endpoint\\nAccess Azure resources with a managed online endpoint and managed identity\\nTroubleshoot managed online endpoints deployment', \"Option 2: T o deploy a specific model iteration from this experiment.\\na. Select the desired model from the Models  tab\\nb. Select Deploy  on the top left of the window.\\n2. Populate the Deploy model  pane.\\nField Value\\nName Enter a unique name for your deployment.\\nDescription Enter a description to better identify what this deployment is for.\\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\\nService (AKS)  or Azure Container Inst ance (ACI).\\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\\ndeploy to.\\nEnable\\nauthenticationSelect to allow for token-based or key-based authentication.\\nUse custom\\ndeployment assetsEnable this feature if you want to upload your own scoring script and\\nenvironment file. Otherwise, automated ML provides these assets for\\nyou by default. Learn more about scoring scripts .\\nThe Advanced menu offers default deployment features such as data collection  and\\nresource utilization settings. If you wish to override these defaults do so in this\\nmenu.\\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\\ndeployment begins, the Model summar y tab appears. See the deployment\\nprogress under the Deploy status  section.\\nNow you have an operational web service to generate predictions! Y ou can test the\\npredictions by querying the service from Power BI's built in Azure Machine Learning\\nsupport .ﾉExpand table\\n） Impor tant\\nFile names must be under 32 characters and must begin and end with\\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\\nbetween. Spaces are not allowed.\", 'In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\\nmodel to an online endpoint. See these other articles to learn more about Azure\\nMachine Learning.\\nTrack run metrics during training\\nTune hyperparameters\\nReference architecture for distributed deep learning training in AzureNext steps']","question":"Explain how to deploy a TensorFlow model as a web service in Azure ML. What are the key considerations for deployment?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":12}
{"answer":"To perform distributed deep learning training using the Azure Machine Learning SDK v2, you would typically need to define a multi-node configuration, specify the required resources, and set up the necessary environment. Here's a high-level example of how you might achieve this:\n\n1. Define a multi-node configuration:\n   - Specify the number of worker nodes and their configurations.\n   - Define the distribution strategy (e.g., data parallelism) and any additional settings for the distributed training job.\n\n2. Specify the required resources:\n   - Define the compute target for the distributed training job, such as an Azure Machine Learning compute cluster.\n\n3. Set up the environment:\n   - Define the training script and any dependencies required for the distributed training job, such as the deep learning framework (e.g., PyTorch or TensorFlow) and any additional libraries.\n\n4. Submit the job:\n   - Submit the distributed training job to the Azure Machine Learning service, specifying the defined configuration, resources, and environment settings.\n\nThis is a simplified overview, and the specific implementation details would depend on the deep learning framework, the specific requirements of the model, and the Azure Machine Learning SDK v2 features.","documents":"[\"Distributed training with Azure Machine\\nLearning\\nArticle •03\/27\/2023\\nIn this article, you learn about distributed training and how Azure Machine Learning\\nsupports it for deep learning models.\\nIn distributed training the workload to train a model is split up and shared among\\nmultiple mini processors, called worker nodes. These worker nodes work in parallel to\\nspeed up model training. Distributed training can be used for traditional ML models, but\\nis better suited for compute and time intensive tasks, like deep learning  for training\\ndeep neural networks.\\nThere are two main types of distributed training: data parallelism  and model parallelism .\\nFor distributed training on deep learning models, the Azure Machine Learning SDK in\\nPython  supports integrations with popular frameworks, PyT orch and T ensorFlow. Both\\nframeworks employ data parallelism for distributed training, and can leverage\\nhorovod  for optimizing compute speeds.\\nDistributed training with PyT orch\\nDistributed training with T ensorFlow\\nFor ML models that don't require distributed training, see train models with Azure\\nMachine Learning  for the different ways to train models using the Python SDK.\\nData parallelism is the easiest to implement of the two distributed training approaches,\\nand is sufficient for most use cases.\\nIn this approach, the data is divided into partitions, where the number of partitions is\\nequal to the total number of available nodes, in the compute cluster or serverless\\ncompute . The model is copied in each of these worker nodes, and each worker operates\\non its own subset of the data. K eep in mind that each node has to have the capacity to\\nsupport the model that's being trained, that is the model has to entirely fit on each\\nnode. The following diagram provides a visual demonstration of this approach.Deep learning and distributed training\\nData parallelism\", \"Distributed GPU training guide (SDK v2)\\nArticle •02\/02\/2024\\nAPPLIES T O: \\n  Python SDK azure-ai-ml v2 (curr ent)\\nLearn more about using distributed GPU training code in Azure Machine Learning. This\\narticle helps you run your existing distributed training code, and offers tips and\\nexamples for you to follow for each framework:\\nMessage P assing Interface (MPI)\\nHorovod\\nEnvironment variables from Open MPI\\nPyTorch\\nTensorFlow\\nAccelerate GPU training with InfiniBand\\nReview the basic concepts of distributed GPU training , such as data parallelism ,\\ndistributed dat a parallelism , and model p arallelism .\\nAzure Machine Learning offers an MPI job  to launch a given number of processes in\\neach node. Azure Machine Learning constructs the full MPI launch command ( mpirun)\\nbehind the scenes. Y ou can't provide your own full head-node-launcher commands like\\nmpirun or DeepSpeed launcher.\\nPrerequisites\\n\\uea80 Tip\\nIf you don't know which type of parallelism to use, more than 90% of the time you\\nshould use distribut ed data p arallelism .\\nMPI\\n\\uea80 Tip\\nThe base Docker image used by an Azure Machine Learning MPI job needs to have\\nan MPI library installed. Open MPI  is included in all the Azur e Machine Learning\\nGPU b ase images . When you use a custom Docker image, you are responsible\\nfor making sure the image includes an MPI library. Open MPI is recommended, but\", 'Distributed DNN training achieves scalability using a data partitioning algorithm that\\nrespects time series boundaries. The following diagram illustrates a simple example with\\ntwo partitions:\\nDuring training, the DNN data loaders on each compute load just what they need to\\ncomplete an iteration of back-propagation; the whole dataset is nev er read int o\\nmemor y. The partitions are further distributed across multiple compute cores (usually\\nGPUs) on possibly multiple nodes to accelerate training. Coordination across computes\\nis provided by the Horovod  framework.\\nLearn more about how to set up AutoML to train a time-series forecasting model .\\nLearn about how AutoML uses machine learning to build forecasting models .\\nLearn about deep learning models  for forecasting in AutoML\\nNext steps', 'For more information, see What is automated machine learning? .\\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\\nLearning can automate this task for arbitrary parameterized commands with little\\nmodification to your job definition. R esults are visualized in the studio.\\nFor more information, see Tune hyperparameters .\\nEfficiency of training for deep learning and sometimes classical machine learning\\ntraining jobs can be drastically improved via multinode distributed training. Azure\\nMachine Learning compute clusters and serverless compute  offer the latest GPU\\noptions.\\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\\nclusters, and serverless compute :\\nPyTorch\\nTensorFlow\\nMPI\\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\\nAzure S ynapse Analytics Spark clusters.\\nFor more information, see Distributed training with Azure Machine Learning .\\nScaling an ML project might require scaling embarrassingly parallel model training. This\\npattern is common for scenarios like forecasting demand, where a model might be\\ntrained for many stores.\\nTo bring a model into production, you deploy the model. The Azure Machine Learning\\nmanaged endpoints abstract the required infrastructure for both batch or real-time\\n(online) model scoring (inferencing).Hyperparameter optimization\\nMultinode distributed training\\nEmbarrassingly parallel training\\nDeploy models', \"Train TensorFlow models at scale with\\nAzure Machine Lea rning\\nArticle •04\/04\/2023\\nAPPLIES T O: \\n  Python SDK azure-ai-ml v2 (curr ent)\\nIn this article, learn how to run your TensorFlow  training scripts at scale using Azure\\nMachine Learning Python SDK v2.\\nThe example code in this article train a T ensorFlow model to classify handwritten digits,\\nusing a deep neural network (DNN); register the model; and deploy it to an online\\nendpoint.\\nWhether you're developing a T ensorFlow model from the ground-up or you're bringing\\nan existing model into the cloud, you can use Azure Machine Learning to scale out\\nopen-source training jobs using elastic cloud compute resources. Y ou can build, deploy,\\nversion, and monitor production-grade models with Azure Machine Learning.\\nTo benefit from this article, you'll need to:\\nAccess an Azure subscription. If you don't have one already, create a free\\naccount .\\nRun the code in this article using either an Azure Machine Learning compute\\ninstance or your own Jupyter notebook.\\nAzure Machine Learning compute instance—no downloads or installation\\nnecessary\\nComplete the Create resources to get started  to create a dedicated notebook\\nserver pre-loaded with the SDK and the sample repository.\\nIn the samples deep learning folder on the notebook server, find a\\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\\ndeploy-with-t ensor flow.\\nYour Jupyter notebook server\\nInstall the Azure Machine Learning SDK (v2) .\\nDownload the following files:\\ntraining script tf_mnist.py\\nscoring script score.py\\nsample request file sample-request.json\\nPrerequisites\"]","question":"Provide an example of how to use the AML SDK v2 for distributed deep learning training. What configurations are necessary for multi-node training?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":13}
{"answer":"Azure ML's HyperDrive can be used to tune hyperparameters of a deep learning model by defining a hyperparameter search space and specifying the primary metric to optimize. Here's an example of specifying the search space for HyperDrive:\n\n```python\nfrom azureml.train.hyperdrive import RandomParameterSampling, choice, uniform\n\nparam_sampling = RandomParameterSampling( {\n        '--batch-size': choice(16, 32, 64),\n        '--learning-rate': uniform(0.01, 0.1)\n    }\n)\n```\n\nIn this example, we define a search space for the hyperparameters `batch-size` and `learning-rate` with specific choices and ranges.","documents":"['For more information, see What is automated machine learning? .\\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\\nLearning can automate this task for arbitrary parameterized commands with little\\nmodification to your job definition. R esults are visualized in the studio.\\nFor more information, see Tune hyperparameters .\\nEfficiency of training for deep learning and sometimes classical machine learning\\ntraining jobs can be drastically improved via multinode distributed training. Azure\\nMachine Learning compute clusters and serverless compute  offer the latest GPU\\noptions.\\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\\nclusters, and serverless compute :\\nPyTorch\\nTensorFlow\\nMPI\\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\\nAzure S ynapse Analytics Spark clusters.\\nFor more information, see Distributed training with Azure Machine Learning .\\nScaling an ML project might require scaling embarrassingly parallel model training. This\\npattern is common for scenarios like forecasting demand, where a model might be\\ntrained for many stores.\\nTo bring a model into production, you deploy the model. The Azure Machine Learning\\nmanaged endpoints abstract the required infrastructure for both batch or real-time\\n(online) model scoring (inferencing).Hyperparameter optimization\\nMultinode distributed training\\nEmbarrassingly parallel training\\nDeploy models', 'Hyperparameter tuning a model (v2)\\nArticle •04\/04\/2023\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (curr ent)\\nAPPLIES T O: \\n  Python SDK azure-ai-ml v2 (curr ent)\\nAutomate efficient hyperparameter tuning using Azure Machine Learning SDK v2 and\\nCLI v2 by way of the S weepJob type.\\n1. Define the parameter search space for your trial\\n2. Specify the sampling algorithm for your sweep job\\n3. Specify the objective to optimize\\n4. Specify early termination policy for low-performing jobs\\n5. Define limits for the sweep job\\n6. Launch an experiment with the defined configuration\\n7. Visualize the training jobs\\n8. Select the best configuration for your model\\nHyperp aramet ers are adjustable parameters that let you control the model training\\nprocess. For example, with neural networks, you decide the number of hidden layers and\\nthe number of nodes in each layer. Model performance depends heavily on\\nhyperparameters.\\nHyperp aramet er tuning , also called hyperp aramet er optimization , is the process of\\nfinding the configuration of hyperparameters that results in the best performance. The\\nprocess is typically computationally expensive and manual.\\nAzure Machine Learning lets you automate hyperparameter tuning and run experiments\\nin parallel to efficiently optimize hyperparameters.\\nTune hyperparameters by exploring the range of values defined for each\\nhyperparameter.\\nHyperparameters can be discrete or continuous, and has a distribution of values\\ndescribed by a parameter expression .\\nWhat is hyperparameter tuning ?\\nDefine the search space', 'When training computer vision models, model performance depends heavily on the\\nhyperparameter values selected. Often, you might want to tune the hyperparameters to\\nget optimal performance. For computer vision tasks, you can sweep hyperparameters to\\nfind the optimal settings for your model. This feature applies the hyperparameter tuning\\ncapabilities in Azure Machine Learning. Learn how to tune hyperparameters .\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (curr ent)\\nYAML\\nYou can define the model architectures and hyperparameters to sweep in the parameter\\nspace. Y ou can either specify a single model architecture or multiple ones.\\nSee Individual trials  for the list of supported model architectures for each task type.\\nSee Hyperparameters for computer vision tasks  hyperparameters for each\\ncomputer vision task type.Azure CLI\\nsearch_space:\\n  - model_name:\\n      type: choice\\n      values: [yolov5]\\n    learning_rate:\\n      type: uniform\\n      min_value:  0.0001\\n      max_value:  0.01\\n    model_size:\\n      type: choice\\n      values: [small, medium]\\n  - model_name:\\n      type: choice\\n      values: [fasterrcnn_resnet50_fpn]\\n    learning_rate:\\n      type: uniform\\n      min_value:  0.0001\\n      max_value:  0.001\\n    optimizer:\\n      type: choice\\n      values: [sgd, adam, adamw]\\n    min_size:\\n      type: choice\\n      values: [600, 800]\\nDefine the parameter search space', 'To tune the model\\'s hyperparameters, define the parameter space in which to search\\nduring training. Y ou\\'ll do this by replacing some of the parameters ( batch_size,\\nfirst_layer_neurons, second_layer_neurons, and learning_rate) passed to the training\\njob with special inputs from the azure.ml.sweep package.\\nPython\\nThen, you\\'ll configure sweep on the command job, using some sweep-specific\\nparameters, such as the primary metric to watch and the sampling algorithm to use.\\nIn the following code, we use random sampling to try different configuration sets of\\nhyperparameters in an attempt to maximize our primary metric, validation_acc.\\nWe also define an early termination policy—the BanditPolicy. This policy operates by\\nchecking the job every two iterations. If the primary metric, validation_acc, falls outside\\nthe top ten percent range, Azure Machine Learning will terminate the job. This saves the\\nmodel from continuing to explore hyperparameters that show no promise of helping to\\nreach the target metric.\\nPythonfrom azure.ai.ml.sweep import Choice, LogUniform  \\n# we will reuse the command_job created before. we call it as a function so  \\nthat we can apply inputs  \\n# we do not apply the \\'iris_csv\\' input again -- we will just use what was  \\nalready defined earlier  \\njob_for_sweep = job(  \\n    batch_size=Choice(values=[ 25, 50, 100]), \\n    first_layer_neurons=Choice(values=[ 10, 50, 200, 300, 500]), \\n    second_layer_neurons=Choice(values=[ 10, 50, 200, 500]), \\n    learning_rate=LogUniform(min_value= -6, max_value= -1), \\n)\\nfrom azure.ai.ml.sweep import BanditPolicy  \\nsweep_job = job_for_sweep.sweep(  \\n    compute=gpu_compute_target,  \\n    sampling_algorithm= \"random\" , \\n    primary_metric= \"Accuracy\" , \\n    goal= \"Maximize\" , \\n    max_total_trials= 20, \\n    max_concurrent_trials= 4, \\n    early_termination_policy=BanditPolicy(slack_factor= 0.1, \\nevaluation_interval= 2), \\n)', 'You trained the model with one set of parameters, let\\'s now see if you can further\\nimprove the accuracy of your model. Y ou can tune and optimize your model\\'s\\nhyperparameters using Azure Machine Learning\\'s sweep  capabilities.\\nTo tune the model\\'s hyperparameters, define the parameter space in which to search\\nduring training. Y ou do this by replacing some of the parameters passed to the training\\njob with special inputs from the azure.ml.sweep package.\\nSince the training script uses a learning rate schedule to decay the learning rate every\\nseveral epochs, you can tune the initial learning rate and the momentum parameters.\\nPython\\nThen, you can configure sweep on the command job, using some sweep-specific\\nparameters, such as the primary metric to watch and the sampling algorithm to use.\\nIn the following code, we use random sampling to try different configuration sets of\\nhyperparameters in an attempt to maximize our primary metric, best_val_acc.\\nWe also define an early termination policy, the BanditPolicy, to terminate poorly\\nperforming runs early. The BanditPolicy terminates any run that doesn\\'t fall within the\\nslack factor of our primary evaluation metric. Y ou apply this policy every epoch (since we\\nreport our best_val_acc metric every epoch and evaluation_interval=1). Notice we\\ndelay the first policy evaluation until after the first 10 epochs ( delay_evaluation=10).\\nPythonfrom azure.ai.ml.sweep import Uniform\\n# we will reuse the command_job created before. we call it as a function so  \\nthat we can apply inputs\\njob_for_sweep = job(\\n    learning_rate=Uniform(min_value= 0.0005, max_value= 0.005),\\n    momentum=Uniform(min_value= 0.9, max_value= 0.99),\\n)\\nfrom azure.ai.ml.sweep import BanditPolicy\\nsweep_job = job_for_sweep.sweep(\\n    compute= \"gpu-cluster\" ,\\n    sampling_algorithm= \"random\" ,\\n    primary_metric= \"best_val_acc\" ,\\n    goal=\"Maximize\" ,\\n    max_total_trials= 8,\\n    max_concurrent_trials= 4,\\n    early_termination_policy=BanditPolicy(\\n        slack_factor= 0.15, evaluation_interval= 1, delay_evaluation= 10']","question":"How can Azure ML's HyperDrive be used to tune hyperparameters of a deep learning model? Give an example of specifying the search space.","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":14}
{"answer":"To update an existing model's properties, such as description or tags, using the Azure CLI, you can use the following command:\n\n```bash\naz ml model update --name run-model-example --version 1 --set description=\"This is an updated description.\" --set tags.stage=\"Prod\"\n```","documents":"['Get the details of a specific model:\\ncli\\nUpdate mutable properties of a specific model:\\ncli\\nArchiving a model will hide it by default from list queries ( az ml model list). You can\\nstill continue to reference and use an archived model in your workflows. Y ou can archive\\neither all versions of a model or only a specific version.\\nIf you don\\'t specify a version, all versions of the model under that given name will be\\narchived. If you create a new model version under an archived model container, that\\nnew version will automatically be set as archived as well.\\nArchive all versions of a model:Show\\nAzure CLI\\naz ml model show --name run-model-example --version 1  \\nUpdate\\nAzure CLI\\naz ml model update --name  run-model-example --version 1 --set  \\ndescription=\"This is an updated description.\" --set tags.stage=\"Prod\"  \\n） Impor tant\\nFor model, only description and tags can be updated. All other properties are\\nimmutable; if you need to change any of those properties you should create a new\\nversion of the model.\\nArchive', 'Execute the following command in the Azure CLI, and update the <> placeholders\\nwith your data asset name, version and key-value pair for the tag.\\nAzure CLI\\nTypically, your ETL processes organize your folder structure on Azure storage by time,\\nfor example:\\ntext\\nThe combination of time\/version structured folders and Azure Machine Learning T ables\\n(MLTable) allow you to construct versioned datasets. T o show how to achieve versioned\\ndata with Azure Machine Learning T ables, we use a hypothetical ex ample . Suppose you\\nhave a process that uploads camera images to Azure Blob storage every week, in the\\nfollowing structure:\\ntextaz ml data update --name <DATA ASSET NAME>  --version  <VERSION>  --set \\ntags.<KEY>=<VALUE> \\nVersioning best practices\\n\/ \\n└── 📁 mydata \\n    ├── 📁  year=2022  \\n    │   ├── 📁  month=11  \\n    │   │   └── 📄  file1 \\n    │   │   └── 📄  file2 \\n    │   └── 📁  month=12  \\n    │       └── 📄  file1 \\n    │   │   └── 📄  file2 \\n    └── 📁  year=2023  \\n        └── 📁  month=1  \\n            └── 📄  file1 \\n    │   │   └── 📄  file2 \\n\/myimages  \\n└── 📁 year=2022  \\n    ├── 📁  week52 \\n    │   ├── 📁  camera1  \\n    │   │   └── 🖼  file1.jpeg  \\n    │   │   └── 🖼  file2.jpeg  \\n    │   └── 📁  camera2  \\n    │       └── 🖼  file1.jpeg  \\n    │   │   └── 🖼  file2.jpeg', 'The az ml model command can be used for managing Azure Machine Learning models.\\nExamples are available in the examples GitHub repository . Several are shown below.\\nYAML\\nYAML\\nInstall and use the CLI (v2)Remarks\\nExamples\\nYAML: local file\\n$schema:  https:\/\/azuremlschemas.azureedge.net\/latest\/model.schema.json  \\nname: local-file-example  \\npath: mlflow-model\/model.pkl  \\ndescription:  Model created from local file. \\nYAML: local folder in MLflow format\\n$schema:  https:\/\/azuremlschemas.azureedge.net\/latest\/model.schema.json  \\nname: local-mlflow-example  \\npath: mlflow-model  \\ntype: mlflow_model  \\ndescription:  Model created from local MLflow model directory.', 'If you want to update the code, model, or environment, update the Y AML file, and\\nthen run the az ml online-endpoint update command.\\nTo understand how update works:\\n1. Open the file online\/model-1\/onlines coring\/s core.py.\\n2. Change the last line of the init() function: After logging.info(\"Init\\ncomplete\"), add logging.info(\"Updated successfully\").\\n3. Save the file.\\n4. Run this command:\\nAzure CLI(Optional) Update the deployment\\nAzure CLI\\n７ Note\\nIf you update instance count (to scale your deployment) along with other\\nmodel settings (such as code, model, or environment) in a single update\\ncommand, the scaling operation will be performed first, then the other updates\\nwill be applied. It\\'s a good practice to perform these operations separately in a\\nproduction environment.\\naz ml online-deployment update -n blue --endpoint  $ENDPOINT_NAME  -f \\nendpoints\/online\/managed\/sample\/blue -deployment .yml\\n７ Note\\nUpdating by using Y AML is declarative. That is, changes in the Y AML are\\nreflected in the underlying Azure R esource Manager resources (endpoints\\nand deployments). A declarative approach facilitates GitOps : All\\nchanges to endpoints and deployments (even instance_count) go\\nthrough the Y AML.\\n\\uea80 Tip', 'then select Regist er.\\nThe SDK and CLI (v2) also allow you to manage the lifecycle of your Azure Machine\\nLearning model assets.\\nList all the models in your workspace:\\ncli\\nList all the model versions under a given name:\\ncli\\n\\uf80a\\nManage models\\nList\\nAzure CLI\\naz ml model list  \\nAzure CLI\\naz ml model list --name run-model-example']","question":"What is the command to update an existing model's properties, such as description or tags, using the Azure CLI?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":15}
{"answer":"Azure ML integrates with Azure DevOps for CI\/CD by leveraging Azure Pipelines. The steps involved include defining build and release pipelines, setting up triggers, and configuring environment variables for model deployment. Would you like a detailed walkthrough of these steps?","documents":"['Continuous integration and deployment of machine learning models with Azure\\nPipelines\\nMachine Learning ML Ops  repository\\nLearn more by reading and exploring the following resources:\\nSet up ML Ops with Azure DevOps\\nLearning path: End-to-end ML Ops with Azure Machine Learning\\nHow to deploy a model to an online endpoint  with Machine Learning\\nTutorial: T rain and deploy a model\\nCI\/CD of machine learning models with Azure Pipelines\\nMachine learning at scale\\nAzure AI reference architectures and best practices repo\\nNext steps', 'Some key features enabling ML Ops include:\\ngit integration.\\nMLflow integration.\\nMachine learning pipeline scheduling.\\nAzure Event Grid integration for custom triggers.\\nEase of use with CI\/CD tools like GitHub Actions or Azure DevOps.\\nMachine Learning also includes features for monitoring and auditing:\\nJob artifacts, such as code snapshots, logs, and other outputs.\\nLineage between jobs and assets, such as containers, data, and compute resources.\\nIf you use Apache Airflow, the airflow-provider-azure-machinelearning  package is a\\nprovider that enables you to submit workflows to Azure Machine Learning from Apache\\nAirFlow.\\nStart using Azure Machine Learning:\\nSet up an Azure Machine Learning workspace\\nTutorial: Build a first machine learning project\\nRun training jobs\\nNext steps', 'What are Azu re Machine Learning\\npipelines?\\nArticle •01\/31\/2024\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (current)  \\n  Python SDK azure-ai-ml v2\\n(current)\\nAn Azure Machine Learning pipeline is an independently executable workflow of a\\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\\nstandardize the best practices of producing a machine learning model, enables the team\\nto execute at scale, and improves the model building efficiency.\\nThe core of a machine learning pipeline is to split a complete machine learning task into\\na multistep workflow. Each step is a manageable component that can be developed,\\noptimized, configured, and automated individually. S teps are connected through well-\\ndefined interfaces. The Azure Machine Learning pipeline service automatically\\norchestrates all the dependencies between pipeline steps. This modular approach brings\\ntwo key benefits:\\nStandardize the Machine learning operation (ML Ops) practice and support scalable\\nteam collaboration\\nTraining efficiency and cost reduction\\nMachine learning operation (ML Ops) automates the process of building machine\\nlearning models and taking the model to production. This is a complex process. It\\nusually requires collaboration from different teams with different skills. A well-defined\\nmachine learning pipeline can abstract this complex process into a multiple steps\\nworkflow, mapping each step to a specific task such that each team can work\\nindependently.\\nFor example, a typical machine learning project includes the steps of data collection,\\ndata preparation, model training, model evaluation, and model deployment. Usually, the\\ndata engineers concentrate on data steps, data scientists spend most time on model\\nWhy are Azure Machine Learning pipelines\\nneeded?\\nStandardize the MLOps practice and support scalable\\nteam collaboration', 'What are Azu re Machine Learning\\npipelines?\\nArticle •01\/31\/2024\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (current)  \\n  Python SDK azure-ai-ml v2\\n(current)\\nAn Azure Machine Learning pipeline is an independently executable workflow of a\\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\\nstandardize the best practices of producing a machine learning model, enables the team\\nto execute at scale, and improves the model building efficiency.\\nThe core of a machine learning pipeline is to split a complete machine learning task into\\na multistep workflow. Each step is a manageable component that can be developed,\\noptimized, configured, and automated individually. S teps are connected through well-\\ndefined interfaces. The Azure Machine Learning pipeline service automatically\\norchestrates all the dependencies between pipeline steps. This modular approach brings\\ntwo key benefits:\\nStandardize the Machine learning operation (ML Ops) practice and support scalable\\nteam collaboration\\nTraining efficiency and cost reduction\\nMachine learning operation (ML Ops) automates the process of building machine\\nlearning models and taking the model to production. This is a complex process. It\\nusually requires collaboration from different teams with different skills. A well-defined\\nmachine learning pipeline can abstract this complex process into a multiple steps\\nworkflow, mapping each step to a specific task such that each team can work\\nindependently.\\nFor example, a typical machine learning project includes the steps of data collection,\\ndata preparation, model training, model evaluation, and model deployment. Usually, the\\ndata engineers concentrate on data steps, data scientists spend most time on model\\nWhy are Azure Machine Learning pipelines\\nneeded?\\nStandardize the MLOps practice and support scalable\\nteam collaboration', 'Use Azu re Pipelines with Azure Machine\\nLearning\\nArticle •09\/29\/2023\\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\\nof the operations you can automate are:\\nData preparation (extract, transform, load operations)\\nTraining machine learning models with on-demand scale-out and scale-up\\nDeployment of machine learning models as public or private web services\\nMonitoring deployed machine learning models (such as for performance or data-\\ndrift analysis)\\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\\nmachine learning model to Azure Machine Learning .\\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\\nv2.\\nComplete the Create resources to get started  to:\\nCreate a workspace\\nCreate a cloud-based compute cluster  to use for training your model\\nAzure Machine Learning extension for Azure Pipelines. This extension can be\\ninstalled from the Visual S tudio marketplace at\\nhttps:\/\/marketplace.visualstudio.com\/items?itemName=ms-air-aiagility.azureml-\\nv2.\\nFork the following repo at GitHub:Prerequisites\\nStep 1: Get the code\\nhttps:\/\/github.com\/azure\/azureml-examples']","question":"How does Azure ML integrate with Azure DevOps for implementing CI\/CD pipelines for ML models? Describe the steps involved.","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":16}
{"answer":"To set up a real-time inferencing endpoint using a deep learning model in Azure ML, you would follow these steps:\n\n1. **Model Training**: Train your deep learning model using Azure ML's training capabilities, ensuring it's compatible with real-time inferencing requirements.\n\n2. **Model Registration**: Register the trained model in your Azure ML workspace, which makes it available for deployment.\n\n3. **Deployment Configuration**: Define the deployment configuration, specifying the compute target, environment, and other settings for the real-time endpoint.\n\n4. **Endpoint Deployment**: Deploy the model as a real-time endpoint, which will make it accessible for inferencing via HTTP requests.\n\nRegarding performance tuning options, Azure ML provides several options to optimize real-time inferencing performance:\n\n- **Scaling**: Azure ML allows you to scale the underlying infrastructure to handle increased inferencing loads.\n\n- **Hardware Selection**: You can choose the appropriate hardware for your inferencing needs, including CPU and GPU options.\n\n- **Monitoring and Logging**: Azure ML provides monitoring and logging capabilities to track and optimize endpoint performance over time.\n\n- **Auto-scaling**: Azure ML can automatically scale the endpoint based on usage patterns, ensuring optimal performance and cost efficiency.\n\nThese options enable you to fine-tune the performance of your real-time inferencing endpoint to meet your specific requirements.","documents":"['Online endpoints and deployments for\\nreal-time inference\\nArticle •10\/24\/2023\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (current)  \\n  Python SDK azure-ai-ml v2\\n(current)\\nAzure Machine Learning allows you to perform real-time inferencing on data by using\\nmodels that are deployed to online endpoints . Inferencing is the process of applying new\\ninput data to a machine learning model to generate outputs. While these outputs are\\ntypically referred to as \"predictions,\" inferencing can be used to generate outputs for\\nother machine learning tasks, such as classification and clustering.\\nOnline endpoints  deploy models to a web server that can return predictions under the\\nHTTP protocol. Use online endpoints to operationalize models for real-time inference in\\nsynchronous low-latency requests. W e recommend using them when:\\nTo define an endpoint, you need to specify:\\nEndpoint name : This name must be unique in the Azure region. For more\\ninformation on the naming rules, see endpoint limits .\\nAuthentication mode : You can choose between key-based authentication mode\\nand Azure Machine Learning token-based authentication mode for the endpoint. A\\nkey doesn\\'t expire, but a token does expire. For more information on\\nauthenticating, see Authenticate to an online endpoint .\\nAzure Machine Learning provides the convenience of using managed online endpoints\\nfor deploying your ML models in a turnkey manner. This is the recommended  way to use\\nonline endpoints in Azure Machine Learning. Managed online endpoints work with\\npowerful CPU and GPU machines in Azure in a scalable, fully managed way. These\\nendpoints also take care of serving, scaling, securing, and monitoring your models, to\\nfree you from the overhead of setting up and managing the underlying infrastructure. T o\\nOnline  endpoints\\nYou have low-latency requirements＂\\nYour model can answer the request in a relatively short amount of time＂\\nYour model\\'s inputs fit on the HT TP payload of the request ＂\\nYou need to scale up in terms of number of requests＂', 'In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\\nmodel to an online endpoint. See these other articles to learn more about Azure\\nMachine Learning.\\nTrack run metrics during training\\nTune hyperparameters\\nReference architecture for distributed deep learning training in AzureNext steps', \"Batch scoring, or batch infer encing , involves invoking an endpoint with a reference to\\ndata. The batch endpoint runs jobs asynchronously to process data in parallel on\\ncompute clusters and store the data for further analysis.\\nReal-time s coring, or online infer encing , involves invoking an endpoint with one or more\\nmodel deployments and receiving a response in near real time via HT TPS. T raffic can be\\nsplit across multiple deployments, allowing for testing new model versions by diverting\\nsome amount of traffic initially and increasing after confidence in the new model is\\nestablished.\\nFor more information, see:\\nDeploy a model with a real-time managed endpoint\\nUse batch endpoints for scoring\\nDevOps for ML models, often called ML Ops, is a process for developing models for\\nproduction. A model's lifecycle from training to deployment must be auditable if not\\nreproducible.\\nLearn more about MLOps in Azure Machine Learning .\\nMachine Learning is built with the model lifecycle in mind. Y ou can audit the model\\nlifecycle down to a specific commit and environment.Real-time and batch scoring (inferencing)\\nMLOps: DevOps for machine learning\\nML model lifecycle\\nIntegrations enabling MLOPs\", 'What are Azure Machine Learning endpoints?\\nDeploy and score a model with an online endpointNext steps', \"Currently, Azure Machine Learning supports fine-tuning models for the following\\nlanguage tasks:\\nText classification\\nToken classification\\nQuestion answering\\nSummarization\\nTranslation\\nTo enable users to quickly get started with fine-tuning, we have published samples (both\\nPython notebooks and CLI examples) for each task in the azureml-examples git repo\\nFinetune samples . Each model card also links to fine-tuning samples for supported\\nfine-tuning tasks.\\nYou can deploy foundation models (both pre-trained models from the model catalog,\\nand fine-tuned models, once they're registered to your workspace) to an endpoint that\\ncan then be used for inferencing. Deployment to both real time endpoints and batch\\nendpoints is supported. Y ou can deploy these models by using either the Deploy UI\\nwizard or by using the code based samples linked from the model card.\\nYou can invoke the Deploy UI form by selecting the Deploy  button on the model card\\nfor any foundation model, and selecting either R eal-time endpoint or Batch endpoint\\nDeploying foundation models to endpoints for\\ninferencing\\nDeploying using the studio\\n\\uf80a\\nDeployment settings\"]","question":"Explain the process of setting up a real-time inferencing endpoint using a deep learning model in Azure ML. What are the performance tuning options available?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":17}
{"answer":"To automate the process of model retraining and deployment using Azure ML Pipelines, you can define a pipeline that includes the necessary steps for data preparation, model training, and model deployment. This pipeline can be executed on a schedule or triggered by specific events. The Azure ML Pipelines provide a way to orchestrate and automate the end-to-end machine learning lifecycle, including data preparation, model training, and deployment.","documents":"['Use Azu re Pipelines with Azure Machine\\nLearning\\nArticle •09\/29\/2023\\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\\nof the operations you can automate are:\\nData preparation (extract, transform, load operations)\\nTraining machine learning models with on-demand scale-out and scale-up\\nDeployment of machine learning models as public or private web services\\nMonitoring deployed machine learning models (such as for performance or data-\\ndrift analysis)\\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\\nmachine learning model to Azure Machine Learning .\\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\\nv2.\\nComplete the Create resources to get started  to:\\nCreate a workspace\\nCreate a cloud-based compute cluster  to use for training your model\\nAzure Machine Learning extension for Azure Pipelines. This extension can be\\ninstalled from the Visual S tudio marketplace at\\nhttps:\/\/marketplace.visualstudio.com\/items?itemName=ms-air-aiagility.azureml-\\nv2.\\nFork the following repo at GitHub:Prerequisites\\nStep 1: Get the code\\nhttps:\/\/github.com\/azure\/azureml-examples', 'Continuous integration and deployment of machine learning models with Azure\\nPipelines\\nMachine Learning ML Ops  repository\\nLearn more by reading and exploring the following resources:\\nSet up ML Ops with Azure DevOps\\nLearning path: End-to-end ML Ops with Azure Machine Learning\\nHow to deploy a model to an online endpoint  with Machine Learning\\nTutorial: T rain and deploy a model\\nCI\/CD of machine learning models with Azure Pipelines\\nMachine learning at scale\\nAzure AI reference architectures and best practices repo\\nNext steps', \"6. Select main as a branch and choose \/mlops\/devops-pipelines\/deploy-model-\\ntraining-pipeline.yml, then select Continue .\\n7. Save and Run  the pipeline\\nThis scenario includes prebuilt workflows for two approaches to deploying a trained\\nmodel, batch scoring or a deploying a model to an endpoint for real-time scoring. Y ou\\nmay run either or both of these workflows to test the performance of the model in your\\nAzure ML workspace. IN this example we will be using real-time scoring.\\n1. Go to ADO pipelines\\n2. Select New Pipeline .\\n3. Select Azur e Repos Git .７ Note\\nAt this point, the infrastructure is configured and the Prototyping Loop of the\\nMLOps Architecture is deployed. you're ready to move to our trained model to\\nproduction.\\nDeploying the Trained model\\nDeploy ML model endpoint\", '3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\\nMachine Learning in many different ways .\\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\\niterations over different models, hyperparameter settings, advanced\\npreprocessing\/featurization, and what metrics to look at when determining the\\nbest model.\\n5. Submit the training job.\\n6. Review the r esults\\nThe following diagram illustrates this process. \\nYou can also inspect the logged job information, which contains metrics  gathered\\nduring the job. The training job produces a Python serialized object ( .pkl file) that\\ncontains the model and data preprocessing.\\nWhile model building is automated, you can also learn how important or relevant\\nfeatures are  to the generated models.\\nApply automated ML when you want Azure Machine Learning to train and tune a model\\nfor you using the target metric you specify. Automated ML democratizes the machine\\nlearning model development process, and empowers its users, no matter their data\\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\\nML professionals and developers across industries can use automated ML to:\\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\\nforecasting, computer vision & NLP', '4. When the pipeline has complete successfully, you can find your Azure Machine\\nLearning W orkspace and associated resources by logging in to the Azure P ortal.\\nNext, a model training and scoring pipelines will be deployed into the new\\nMachine Learning environment.\\nThe solution accelerator includes code and data for a sample end-to-end machine\\nlearning pipeline which runs a linear regression to predict taxi fares in NY C. The pipeline\\nis made up of components, each serving different functions, which can be registered\\nwith the workspace, versioned, and reused with various inputs and outputs. Sample\\npipelines and workflows for the Computer Vision and NLP scenarios will have different\\nsteps and deployment steps.\\nThis training pipeline contains the following steps:\\nPrepare Data\\nThis component takes multiple taxi datasets (yellow and green) and merges\/filters\\nthe data, and prepare the train\/val and evaluation datasets.\\nInput: Local data under .\/data\/ (multiple .csv files)\\nOutput: Single prepared dataset (.csv) and train\/val\/test datasets.\\nTrain Model\\nThis component trains a Linear R egressor with the training set.\\nInput: T raining dataset\\nOutput: T rained model (pickle format)\\nEvaluat e Model\\nThis component uses the trained model to predict taxi fares on the test set.\\nInput: ML model and T est dataset\\nOutput: P erformance of model and a deploy flag whether to deploy or not.Sample Training  and Deployment Scenario']","question":"How can you automate the process of model retraining and deployment using Azure ML Pipelines?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":18}
{"answer":"To implement a recommendation system with Azure ML, you would typically use collaborative filtering algorithms and work with user-item interaction data. Azure ML provides the infrastructure to train, deploy, and manage recommendation models at scale. You can use Azure ML to preprocess and feature engineer your data, train recommendation models using collaborative filtering or deep learning algorithms, and deploy these models as endpoints for real-time or batch inference.","documents":"['Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', 'Tell us about y our PDF experience.\\nAzur e Machine Learning documentation\\nLearn how to train and deploy models and manage the ML lifecycle (ML Ops) with Azure\\nMachine Learning. Tutorials, code examples, API references, and more.\\nOverview\\nｅOVERVIE W\\nWhat is Azure Machine Learning?\\nSetup & quickstar t\\nｆQUICKST ART\\nCreate resources\\nGet started with Azure Machine Learning\\nStart with the b asics\\nｇTUTORIAL\\nPrepare and explore data\\nDevelop on a cloud workstation\\nTrain a model\\nDeploy a model\\nSet up a reusable pipeline\\nWork with data\\nｃHOW-T O GUIDE\\nUse Apache Spark in Azure Machine Learning\\nCreate data assets', \"What is Azure Machine Learning?\\nArticle •01\/29\/2024\\nAzure Machine Learning is a cloud service for accelerating and managing the machine\\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\\nin their day-to-day workflows to train and deploy models and manage machine learning\\noperations (ML Ops).\\nYou can create a model in Machine Learning or use a model built from an open-source\\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\\nretrain, and redeploy models.\\nMachine Learning is for individuals and teams implementing ML Ops within their\\norganization to bring ML models into production in a secure and auditable production\\nenvironment.\\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\\nday workflows. Application developers can use tools for integrating models into\\napplications or services. Platform developers can use a robust set of tools, backed by\\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\\nbased access control for infrastructure. Y ou can set up a project to deny access to\\nprotected data and select operations.\\nML projects often require a team with a varied skill set to build and maintain. Machine\\nLearning has tools that help enable you to:\\uea80 Tip\\nFree trial!  If you don't have an Azure subscription, create a free account before you\\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\\nto spend on Azure services. After they're used up, you can keep the account and\\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\\nchange your settings and ask to be charged.\\nWho is Azure Machine Learning for?\\nProductivity for everyone on the team\", \"If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\\nLearn more about how to set up AutoML to train a time-series forecasting model .\\nLearn about calendar features for time series forecasting in AutoML .\\nLearn about how AutoML uses machine learning to build forecasting models .\\nLearn about AutoML forecasting for lagged features .Next steps\", 'For training in developing AI and Machine Learning solutions with Microsoft, see\\nMicrosoft Learn training .\\nChoose a Microsoft cognitive services technology\\nArtificial intelligence (AI) architecture design\\nHow Azure Machine Learning works: resources and assetsRelated resources']","question":"Describe the approach to use Azure ML for implementing a recommendation system. What kind of data and algorithms are typically involved?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":19}
{"answer":"Azure ML's built-in support for ONNX models allows you to leverage interoperability across different frameworks by enabling you to train your preferred model in any framework and then convert it to ONNX format. Once in ONNX format, the model can be deployed and run on a variety of platforms and devices, including Azure ML, without the need for additional conversion or optimization steps.","documents":"[\"ONNX and Azure Machine Learning:\\nCreate and accelerate ML models\\nArticle •06\/13\/2023\\nLearn how using the Open Neural Network Exchange  (ONNX) can help optimize the\\ninference of your machine learning model. Inference, or model scoring, is the phase\\nwhere the deployed model is used for prediction, most commonly on production data.\\nOptimizing machine learning models for inference (or model scoring) is difficult since\\nyou need to tune the model and the inference library to make the most of the hardware\\ncapabilities. The problem becomes extremely hard if you want to get optimal\\nperformance on different kinds of platforms (cloud\/edge, CPU\/GPU, etc.), since each one\\nhas different capabilities and characteristics. The complexity increases if you have\\nmodels from a variety of frameworks that need to run on a variety of platforms. It's very\\ntime consuming to optimize all the different combinations of frameworks and hardware.\\nA solution to train once in your preferred framework and run anywhere on the cloud or\\nedge is needed. This is where ONNX comes in.\\nMicrosoft and a community of partners created ONNX as an open standard for\\nrepresenting machine learning models. Models from many frameworks  including\\nTensorFlow, PyT orch, SciKit-Learn, K eras, Chainer, MXNet, MA TLAB, and SparkML can be\\nexported or converted to the standard ONNX format. Once the models are in the ONNX\\nformat, they can be run on a variety of platforms and devices.\\nONNX Runtime  is a high-performance inference engine for deploying ONNX models\\nto production. It's optimized for both cloud and edge and works on Linux, Windows,\\nand Mac. Written in C++, it also has C, Python, C#, Java, and JavaScript (Node.js) APIs\\nfor usage in a variety of environments. ONNX Runtime supports both DNN and\\ntraditional ML models and integrates with accelerators on different hardware such as\\nTensorR T on NVidia GPUs, OpenVINO on Intel processors, DirectML on Windows, and\\nmore. By using ONNX Runtime, you can benefit from the extensive production-grade\\noptimizations, testing, and ongoing improvements.\\nONNX Runtime is used in high-scale Microsoft services such as Bing, Office, and Azure\\nAI. Performance gains are dependent on a number of factors, but these Microsoft\\nservices have seen an average 2x per formance gain on CPU . In addition to Azure\\nMachine Learning services, ONNX Runtime also runs in other products that support\\nMachine Learning workloads, including:\\nWindows: The runtime is built into Windows as part of Windows Machine Learning\\nand runs on hundreds of millions of devices.\", 'Azure SQL product family: Run native scoring on data in Azure SQL Edge  and\\nAzure SQL Managed Instance .\\nML.NET: Run ONNX models in ML.NET .\\nYou can obtain ONNX models in several ways:\\nTrain a new ONNX model in Azure Machine Learning (see examples at the bottom\\nof this article) or by using automated Machine Learning capabilities\\nConvert existing model from another format to ONNX (see the tutorials )\\nGet a pre-trained ONNX model from the ONNX Model Zoo\\nGenerate a customized ONNX model from Azure Custom Vision service\\nMany models including image classification, object detection, and text processing can\\nbe represented as ONNX models. If you run into an issue with a model that cannot be\\nconverted successfully, please file an issue in the GitHub of the respective converter that\\nyou used. Y ou can continue using your existing format model until the issue is\\naddressed.\\nWith Azure Machine Learning, you can deploy, manage, and monitor your ONNX\\nmodels. Using the standard deployment workflow  and ONNX Runtime, you can create a\\nREST endpoint hosted in the cloud. See example Jupyter notebooks at the end of this\\narticle to try it out for yourself.\\n\\uf80a\\nGet ONNX models\\nDeploy ONNX models in Azure\\nInstall and use ONNX Runtime with Python', 'See the AutoML package  for changing default ensemble settings in automated machine\\nlearning.\\nWith Azure Machine Learning, you can use automated ML to build a Python model and\\nhave it converted to the ONNX format. Once the models are in the ONNX format, they\\ncan be run on a variety of platforms and devices. Learn more about accelerating ML\\nmodels with ONNX .\\nSee how to convert to ONNX format in this Jupyter notebook example . Learn which\\nalgorithms are supported in ONNX .\\nThe ONNX runtime also supports C#, so you can use the model built automatically in\\nyour C# apps without any need for recoding or any of the network latencies that REST\\nendpoints introduce. Learn more about using an AutoML ONNX model in a .NET\\napplication with ML.NET  and inferencing ONNX models with the ONNX runtime C#\\nAPI .\\nThere are multiple resources to get you up and running with AutoML.\\nTutorials are end-to-end introductory examples of AutoML scenarios.\\nFor a code fir st experience , follow the Tutorial: T rain an object detection model\\nwith AutoML and Python\\nFor a low or no-code experience , see the Tutorial: T rain a classification model with\\nno-code AutoML in Azure Machine Learning studio .\\nHow-to articles provide additional detail into what functionality automated ML offers.\\nFor example,\\nConfigure the settings for automatic training experiments\\nWithout code in the Azure Machine Learning studio .\\nWith the Python SDK .\\nLearn how to train computer vision models with Python .\\nLearn how to view the generated code from your automated ML models (SDK v1) .AutoML & ONNX\\nNext steps\\nTutorials\/ how-tos', \"Make predictions with ONNX on\\ncomputer vision models from AutoML\\nArticle •04\/04\/2023\\nAPPLIES T O: \\n  Python SDK azure-ai-ml v2 (curr ent)\\nIn this article, you will learn how to use Open Neural Network Exchange (ONNX) to\\nmake predictions on computer vision models generated from automated machine\\nlearning (AutoML) in Azure Machine Learning.\\nTo use ONNX for predictions, you need to:\\n1. Download ONNX model files from an AutoML training run.\\n2. Understand the inputs and outputs of an ONNX model.\\n3. Preprocess your data so that it's in the required format for input images.\\n4. Perform inference with ONNX Runtime for Python.\\n5. Visualize predictions for object detection and instance segmentation tasks.\\nONNX  is an open standard for machine learning and deep learning models. It enables\\nmodel import and export (interoperability) across the popular AI frameworks. For more\\ndetails, explore the ONNX GitHub project .\\nONNX Runtime  is an open-source project that supports cross-platform inference.\\nONNX Runtime provides APIs across programming languages (including Python, C++,\\nC#, C, Java, and JavaScript). Y ou can use these APIs to perform inference on input\\nimages. After you have the model that has been exported to ONNX format, you can use\\nthese APIs on any programming language that your project needs.\\nIn this guide, you'll learn how to use Python APIs for ONNX Runtime  to make\\npredictions on images for popular vision tasks. Y ou can use these ONNX exported\\nmodels across languages.\\nGet an AutoML-trained computer vision model for any of the supported image\\ntasks: classification, object detection, or instance segmentation. Learn more about\\nAutoML support for computer vision tasks .\\nInstall the onnxruntime  package. The methods in this article have been tested\\nwith versions 1.3.0 to 1.8.0.\\nPrerequisites\", \"runs from experimentation in the studio user interface. Or they can use versioned assets\\nfor jobs like environments and storage references.\\nFor more information, see Manage Azure Machine Learning workspaces .\\nWhen a project is ready for operationalization, users' work can be automated in an ML\\npipeline and triggered on a schedule or HT TPS request.\\nYou can deploy models to the managed inferencing solution, for both real-time and\\nbatch deployments, abstracting away the infrastructure management typically required\\nfor deploying models.\\nIn Azure Machine Learning, you can run your training script in the cloud or build a\\nmodel from scratch. Customers often bring models they've built and trained in open-\\nsource frameworks so that they can operationalize them in the cloud.\\nData scientists can use models in Azure Machine Learning that they've created in\\ncommon Python frameworks, such as:\\nPyTorch\\nTensorFlow\\nscikit-learn\\nXGBoost\\nLightGBM\\nOther languages and frameworks are also supported:\\nR\\n.NET\\nFor more information, see Open-source integration with Azure Machine Learning .\\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\\nexperience and intuition to select the right data featurization and algorithm for training.\\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\\nLearning studio UI or the Python SDK.Train models\\nOpen and interoperable\\nAutomated featurization and algorithm selection\"]","question":"How can you leverage Azure ML's built-in support for ONNX models for interoperability across different frameworks?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":20}
{"answer":"Azure Machine Learning can integrate with various Azure storage services such as Azure Blob Storage, Azure File Storage, and Azure Data Lake Storage for storing datasets.","documents":"['Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', 'You can connect to storage services via identity-based data access with Azure Machine\\nLearning datastores .\\nWhen you use identity-based data access, Azure Machine Learning prompts you for\\nyour Microsoft Entra token for data access authentication instead of keeping your\\ncredentials in the datastore. That approach allows for data access management at the\\nstorage level and keeps credentials confidential.\\nThe same behavior applies when you work with data interactively via a Jupyter\\nNotebook on your local computer or compute instance .\\nTo help ensure that you securely connect to your storage service on Azure, Azure\\nMachine Learning requires that you have permission to access the corresponding data\\nstorage.\\nIdentity-based data access supports connections to only the following storage services.\\nAzure Blob S torage\\nAzure Data Lake S torage Gen1\\nAzure Data Lake S torage Gen2\\nTo access these storage services, you must have at least Storage Blob Data R eader\\naccess to the storage account. Only storage account owners can change your access\\nlevel via the Azure portal .７ Note\\nCredentials stored via credential-based authentication include subscription IDs,\\nshared access signature (SAS) tokens, and storage access key and service principal\\ninformation, like client IDs and tenant IDs.\\n２ Warning\\nCross tenant access to storage accounts is not supported. If cross tenant access is\\nneeded for your scenario, please reach out to the Azure Machine Learning Data\\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\\ncode solution.\\nAccess data for training jobs on compute using managed\\nidentity', \"Data concepts in Azure Machine\\nLearning\\nArticle •07\/13\/2023\\nWith Azure Machine Learning, you can import data from a local machine or an existing\\ncloud-based storage resource. This article describes key Azure Machine Learning data\\nconcepts.\\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\\naccount. An Azure Machine Learning datastore offers these benefits:\\nA common, easy-to-use API that interacts with different storage types\\n(Blob\/Files\/ADLS).\\nEasier discovery of useful datastores in team operations.\\nFor credential-based access (service principal\/SAS\/key), Azure Machine Learning\\ndatastore secures connection information. This way, you won't need to place that\\ninformation in your scripts.\\nWhen you create a datastore with an existing Azure storage account, you can choose\\nbetween two different authentication methods:\\nCredential-b ased  - authenticate data access with a service principal, shared access\\nsignature (SAS) token, or account key. Users with Reader  workspace access can\\naccess the credentials.\\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\\nauthenticate data access.\\nThe following table summarizes the Azure cloud-based storage services that an Azure\\nMachine Learning datastore can create. Additionally, the table summarizes the\\nauthentication types that can access those services:\\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\\nAzure Blob Container ✓ ✓\\nAzure File Share ✓\\nAzure Data Lake Gen1 ✓ ✓\\nAzure Data Lake Gen2 ✓ ✓Datastore\", \"Data encryption with Azure Machine\\nLearning\\nArticle •01\/31\/2024\\nAzure Machine Learning relies on various Azure data storage services and compute\\nresources when you're training models and performing inferences. In this article, learn\\nabout the data encryption for each service both at rest and in transit.\\nFor production-grade encryption during training, we recommend that you use an Azure\\nMachine Learning compute cluster. For production-grade encryption during inference,\\nwe recommend that you use Azure Kubernetes Service (AKS).\\nAn Azure Machine Learning compute instance is a dev\/test environment. When you use\\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\\nStore your data in a datastore.\\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\\nmethods for such services.\\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\\naccount (default storage account) that's tied to the Azure Machine Learning workspace\\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\\nMicrosoft-managed keys.\\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\\nTraining data is typically also stored in Azure Blob S torage so that training compute\\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\\nis mounted to compute targets as a remote file system.\\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\\nAzure Blob Storage\", \"Service Usage Example datanumber of nodes; datastore names and tags, and\\ndescriptions on assets like models; and data label names.\\nAzure AI\\nSearchStores indexes that\\nhelp with querying\\nyour machine learning\\ncontent.These indexes are built on top of the data stored in Azure\\nCosmos DB.\\nAzure\\nStorageStores metadata\\nrelated to Azure\\nMachine Learning\\npipeline data.Data can include designer pipeline names, pipeline layout,\\nand execution properties.\\nFrom the perspective of data lifecycle management, data in the preceding resources is\\ncreated and deleted as you create and delete corresponding objects in Azure Machine\\nLearning.\\nYour Azure Machine Learning workspace reads and writes data by using its managed\\nidentity. This identity is granted access to the resources through a role assignment\\n(Azure role-based access control) on the data resources. The encryption key that you\\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\\nused to create indexes for Azure AI Search at runtime.\\nExtra networking controls are configured when you create a private link endpoint on\\nyour workspace to allow for inbound connectivity. This configuration includes the\\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\\nNetwork access is restricted to only trusted Microsoft services.\\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\\nthe data.\\nWhen you use a customer-managed key, the resources are in your Azure subscription\\nand encrypted with your key. While these resources exist in your subscription, Microsoft\\nmanages them. They're automatically created and configured when you create your\\nAzure Machine Learning workspace.\\nThese Microsoft-managed resources are located in a new Azure resource group that's\\ncreated in your subscription. This resource group is separate from the resource group\\nfor your workspace. It contains the Microsoft-managed resources that your key is used\\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\\nresource group name><GUID>.Customer-managed keys\"]","question":"What Azure storage services can be integrated with Azure Machine Learning for storing datasets?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":21}
{"answer":"To authenticate Azure Machine Learning with Azure Blob Storage for dataset access, you can use identity-based data access. This approach prompts you for your Microsoft Entra token for data access authentication, ensuring that your credentials are kept confidential. Additionally, you must have at least Storage Blob Data Reader access to the storage account.","documents":"['You can connect to storage services via identity-based data access with Azure Machine\\nLearning datastores .\\nWhen you use identity-based data access, Azure Machine Learning prompts you for\\nyour Microsoft Entra token for data access authentication instead of keeping your\\ncredentials in the datastore. That approach allows for data access management at the\\nstorage level and keeps credentials confidential.\\nThe same behavior applies when you work with data interactively via a Jupyter\\nNotebook on your local computer or compute instance .\\nTo help ensure that you securely connect to your storage service on Azure, Azure\\nMachine Learning requires that you have permission to access the corresponding data\\nstorage.\\nIdentity-based data access supports connections to only the following storage services.\\nAzure Blob S torage\\nAzure Data Lake S torage Gen1\\nAzure Data Lake S torage Gen2\\nTo access these storage services, you must have at least Storage Blob Data R eader\\naccess to the storage account. Only storage account owners can change your access\\nlevel via the Azure portal .７ Note\\nCredentials stored via credential-based authentication include subscription IDs,\\nshared access signature (SAS) tokens, and storage access key and service principal\\ninformation, like client IDs and tenant IDs.\\n２ Warning\\nCross tenant access to storage accounts is not supported. If cross tenant access is\\nneeded for your scenario, please reach out to the Azure Machine Learning Data\\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\\ncode solution.\\nAccess data for training jobs on compute using managed\\nidentity', 'Data administration\\nArticle •09\/26\/2023\\nLearn how to manage data access and how to authenticate in Azure Machine Learning\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (current)  \\n  Python SDK azure-ai-ml v2\\n(current)\\nIn general, data access from studio involves these checks:\\nWhich user wants to access the resources?\\nDepending on the storage type, different types of authentication are available,\\nfor example\\naccount key\\ntoken\\nservice principal\\nmanaged identity\\nuser identity\\nFor authentication based on a user identity, you must know which  specific user\\ntried to access the storage resource. For more information about user\\nauthentication, see authentication for Azure Machine Learning . For more\\ninformation about service-level authentication, see authentication between\\nAzure Machine Learning and other services .\\nDoes this user have permission?\\nDoes the user have the correct credentials? If yes, does the service principal,\\nmanaged identity, etc., have the necessary permissions for that storage\\nresource? P ermissions are granted using Azure role-based access controls\\n(Azure RBA C).\\nThe storage account Reader  reads the storage metadata.\\nThe Storage Blob Data R eader  reads data within a blob container.\\nThe Contributor  allows write access to a storage account.\\nMore roles may be required, depending on the type of storage.\\nWhere does the access come from?\\nUser: Is the client IP address in the VNet\/subnet range?\\n） Impor tant\\nThis article is intended for Azure administrators who want to create the required\\ninfrastructure for an Azure Machine Learning solution.', 'means that in order to successfully read data from storage, the managed identity of the\\ncompute cluster where the deployment is running must have at least Storage Blob Data\\nReader  access to the storage account.\\nTo configure the compute cluster for data access, follow these steps:\\n1. Go to Azure Machine Learning studio .\\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\\ndeployment is using.\\n3. Assign a managed identity to the compute cluster:\\na. In the Managed identity  section, verify if the compute has a managed identity\\nassigned. If not, select the option Edit.\\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\\nusing a S ystem-Assigned Managed Identity, it is named as \"[workspace\\nname]\/computes\/[compute cluster name]\".\\nc. Save the changes.\\n4. Go to the Azure portal  and navigate to the associated storage account where the\\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\\nstorage account where those assets are placed.', \"When you use an Azure S torage Account from Azure Machine Learning studio, you must\\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\\naccount:\\nBlob Data R eader\\nIf the storage account uses a private endpoint to connect to the VNet, you must\\ngrant the Reader  role for the storage account private endpoint to the managed\\nidentity.\\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\\nNetwork .\\nThe following sections explain the limitations of using an Azure S torage Account, with\\nyour workspace, in a VNet.\\nTo secure communication between Azure Machine Learning and Azure S torage\\nAccounts, configure the storage to Grant access to trusted Azure services .\\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\\ncan normally be used to allow your client to directly connect over the internet. However,\\nwhen using studio, your client doesn't connect to the storage account. The Azure\\nMachine Learning service that makes the request connects to the storage account. The\\nIP address of the service isn't documented, and it changes frequently. Enabling the\\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\\nconfiguration .\\uea80 Tip\\nTo access data from outside Azure Machine Learning, for example with Azure\\nStorage Explorer, that access probably relies on the user identity. For specific\\ninformation, review the documentation for the tool or service you're using. For\\nmore information about how Azure Machine Learning works with data, see Setup\\nauthentication betw een Azur e Machine Learning and other ser vices .\\nAzure Storage Account\\nSecure communication with Azure Storage Account\\nAzure Storage firewall\", \"Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\\ninformation, see Data encryption with Azure Machine Learning .\\nWhen you access your workspace from the studio, the network traffic flows are as\\nfollows:\\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\\nFor management and deployment operations, the configuration uses Azure\\nResource Manager.\\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\\nAzure Machine Learning service.\\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\\nDoor.\\nFor most storage operations, traffic flows through the private endpoint of the\\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\\nand the datastore from the studio  section of this article discusses exceptions.\\nYou also need to configure a DNS solution that allows you to resolve the names of\\nthe resources within the virtual network. For more information, see Use your\\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\\n７ Note\\nThe information in this section is specific to using the workspace from Azure\\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\\nto you.\"]","question":"How do you authenticate Azure Machine Learning with Azure Blob Storage to access datasets?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":22}
{"answer":"To upload a dataset to Azure Blob Storage and access it from an Azure Machine Learning experiment, you would typically follow these steps:\n\n1. **Upload Data to Azure Blob Storage**: Use Azure Portal, Azure CLI, or Azure SDK to upload your data to Azure Blob Storage.\n\n2. **Register Datastore**: Register the Azure Blob Storage as a Datastore in your Azure Machine Learning workspace.\n\n3. **Create Dataset**: Create a dataset in your Azure Machine Learning workspace, referencing the registered Datastore and the path to your data.\n\n4. **Access Dataset in Experiment**: In your experiment script, reference the dataset by its name to access the data during the experiment run.\n\nWould you like more detailed information on any of these steps?","documents":"['Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', \"In many cases, you can upload local files. However, Azure S torage Explorer  provides a\\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\\nExplorer as the default way to move files.\\nTo create a dataset from data that's already stored in Blob S torage:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Choose the Dataset type :\\nIf you're using a .csv or .tsv file and each row contains a response, select\\nTabular .\\nIf you're using separate .txt files for each response, select File.\\n4. Select Next .\\n5. Select From Azur e storage , and then select Next .\\n6. Select the datastore, and then select Next .\\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\\nTo include all the files in the subfolders of the selected path, append \/** to\\nthe path.\\nTo include all the data in the current container and its subfolders, append\\n**\/*.* to the path.\\n8. Select Create.\\n9. Select the data asset you created.\\nTo directly upload your data:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Choose the Dataset type :\\nIf you're using a .csv or .tsv file and each row contains a response, select\\nTabular .\\nIf you're using separate .txt files for each response, select File.\\n4. Select Next .\\n5. Select From local files , and then select Next .\\n6. (Optional) Select a datastore. The default uploads to the default blob store\\n(worksp aceblobst ore) for your Machine Learning workspace.\\n7. Select Next .\\nCreate a dataset from uploaded data\", \"Tutorial: Upload, access and explore\\nyour data in Azure Machine Learning\\nArticle •12\/27\/2023\\nAPPLIES T O: \\n  Python SDK azure-ai-ml v2 (curr ent)\\nIn this tutorial you learn how to:\\nThe start of a machine learning project typically involves exploratory data analysis (ED A),\\ndata-preprocessing (cleaning, feature engineering), and the building of Machine\\nLearning model prototypes to validate hypotheses. This prototyping  project phase is\\nhighly interactive. It lends itself to development in an IDE or a Jupyter notebook, with a\\nPython int eractive console. This tutorial describes these ideas.\\nThis video shows how to get started in Azure Machine Learning studio so that you can\\nfollow the steps in the tutorial. The video shows how to create a notebook, clone the\\nnotebook, create a compute instance, and download the data needed for the tutorial.\\nThe steps are also described in the following sections.\\n1. To use Azure Machine Learning, you'll first need a workspace. If you don't have\\none, complete Create resources you need to get started  to create a workspace and\\nlearn more about using it.\\n2. Sign in to studio  and select your workspace if it's not already open.\\n3. Open or create a notebook in your workspace:\\nCreate a new notebook , if you want to copy\/paste code into cells.\\nOr, open tutorials\/get -star ted-not ebooks\/explor e-data.ipynb  from the\\nSamples  section of studio. Then select Clone  to add the notebook to your\\nFiles. (See where to find Samples .)\\nUpload your data to cloud storage＂\\nCreate an Azure Machine Learning data asset＂\\nAccess your data in a notebook for interactive development＂\\nCreate new versions of data assets＂\\nhttps:\/\/learn-video.azurefd.net\/vod\/player?id=514a29e2-0ae7-4a5d-a537-\\n8f10681f5545&locale=en-us&embedUrl=%2F azure%2Fmachine-learning%2Ftutorial-\\nexplore-data\\nPrerequisites\", '2. Select your subscription and the workspace you created.\\n3. Select Get star ted.\\n4. In the left pane, select Automat ed ML  under the Author  section.\\n5. Select +New aut omat ed ML job .\\nBefore you configure your experiment, upload your data file to your workspace in the\\nform of an Azure Machine Learning dataset. Doing so, allows you to ensure that your\\ndata is formatted appropriately for your experiment.\\n1. On the Select dataset  form, select From local files  from the +Create dataset  drop-\\ndown.\\na. On the Basic info  form, give your dataset a name and provide an optional\\ndescription. The dataset type should default to Tabular , since automated ML in\\nAzure Machine Learning studio currently only supports tabular datasets.\\nb. Select Next  on the bottom left\\nc. On the Datast ore and file selection  form, select the default datastore that was\\nautomatically set up during your workspace creation, worksp aceblobst ore\\n(Azur e Blob S torage) . This is the storage location where you upload your data\\nfile.\\nd. Select Uplo ad files  from the Uplo ad drop-down.\\ne. Choose the bike-no.csv  file on your local computer. This is the file you\\ndownloaded as a prerequisite .\\nf. Select Next\\nWhen the upload is complete, the Settings and preview form is pre-populated\\nbased on the file type.\\ng. Verify that the Settings and pr eview  form is populated as follows and select\\nNext .Create and load dataset\\nﾉExpand table', \"In many cases, you can upload local files. However, Azure S torage Explorer  provides a\\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\\nExplorer as the default way to move files.\\nTo create a dataset from data that's already stored in Blob S torage:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\\nimages.\\n4. Select Next .\\n5. Select From Azur e storage , and then select Next .\\n6. Select the datastore, and then select Next .\\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\\nTo include all the files in the subfolders of the selected path, append \/** to\\nthe path.\\nTo include all the data in the current container and its subfolders, append\\n**\/*.* to the path.\\n8. Select Create.\\n9. Select the data asset you created.\\nTo directly upload your data:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\\nimages.\\n4. Select Next .\\n5. Select From local files , and then select Next .\\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\\n7. Select Next .\\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\\nfolders to upload.\\n9. In the browser window, find your files or folders, and then select Open .\\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\\nCreate a dataset from uploaded data\"]","question":"What steps are involved in uploading a dataset to Azure Blob Storage and then accessing it from an Azure Machine Learning experiment?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":23}
{"answer":"Yes, Azure Machine Learning can directly access data stored in Azure Data Lake Storage Gen2. To do so, you would typically define a Datastore in your Azure Machine Learning workspace that points to your Azure Data Lake Storage Gen2 account. This Datastore can then be used to reference data in your training scripts, pipelines, and other AML artifacts.","documents":"[\"For information on regenerating the access keys, see Regenerate storage account access\\nkeys.\\nAzure Data Lake S torage Gen2 is built on top of Azure Blob S torage and is designed for\\nbig data analytics in enterprises. Data Lake S torage Gen2 is used as a datastore for\\nAzure Machine Learning. Like Azure Blob S torage, the data at rest is encrypted with\\nMicrosoft-managed keys.\\nFor information on how to use your own keys for data stored in Azure Data Lake\\nStorage, see Azure S torage encryption with customer-managed keys in Azure K ey Vault.\\nThe Azure Machine Learning service supports data from the following data sources.\\nTransparent data encryption helps protect Azure SQL Database against the threat of\\nmalicious offline activity by encrypting data at rest. By default, transparent data\\nencryption is enabled for all newly deployed SQL databases that use Microsoft-\\nmanaged keys.\\nFor information on how to use customer-managed keys for transparent data encryption,\\nsee Azure SQL Database transparent data encryption .Azure Data Lake Storage\\n７ Note\\nOn Feb 29, 2024 , Azure Data Lake S torage Gen1 will be retired. For more\\ninformation, see the official announcement . If you use Azure Data Lake S torage\\nGen1, make sure to migrate to Azure Data Lake S torage Gen2 before that date. T o\\nlearn how, see Migrat e Azur e Data Lak e Storage fr om Gen1 t o Gen2 by using the\\nAzur e por tal.\\nUnless you already have an Azure Data Lake S torage Gen1 account, you can't create\\nnew ones.\\nAzure relational databases\\nAzure SQL Database\\nAzure Database for PostgreSQL\", 'You can connect to storage services via identity-based data access with Azure Machine\\nLearning datastores .\\nWhen you use identity-based data access, Azure Machine Learning prompts you for\\nyour Microsoft Entra token for data access authentication instead of keeping your\\ncredentials in the datastore. That approach allows for data access management at the\\nstorage level and keeps credentials confidential.\\nThe same behavior applies when you work with data interactively via a Jupyter\\nNotebook on your local computer or compute instance .\\nTo help ensure that you securely connect to your storage service on Azure, Azure\\nMachine Learning requires that you have permission to access the corresponding data\\nstorage.\\nIdentity-based data access supports connections to only the following storage services.\\nAzure Blob S torage\\nAzure Data Lake S torage Gen1\\nAzure Data Lake S torage Gen2\\nTo access these storage services, you must have at least Storage Blob Data R eader\\naccess to the storage account. Only storage account owners can change your access\\nlevel via the Azure portal .７ Note\\nCredentials stored via credential-based authentication include subscription IDs,\\nshared access signature (SAS) tokens, and storage access key and service principal\\ninformation, like client IDs and tenant IDs.\\n２ Warning\\nCross tenant access to storage accounts is not supported. If cross tenant access is\\nneeded for your scenario, please reach out to the Azure Machine Learning Data\\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\\ncode solution.\\nAccess data for training jobs on compute using managed\\nidentity', 'Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', 'Data administration\\nArticle •09\/26\/2023\\nLearn how to manage data access and how to authenticate in Azure Machine Learning\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (current)  \\n  Python SDK azure-ai-ml v2\\n(current)\\nIn general, data access from studio involves these checks:\\nWhich user wants to access the resources?\\nDepending on the storage type, different types of authentication are available,\\nfor example\\naccount key\\ntoken\\nservice principal\\nmanaged identity\\nuser identity\\nFor authentication based on a user identity, you must know which  specific user\\ntried to access the storage resource. For more information about user\\nauthentication, see authentication for Azure Machine Learning . For more\\ninformation about service-level authentication, see authentication between\\nAzure Machine Learning and other services .\\nDoes this user have permission?\\nDoes the user have the correct credentials? If yes, does the service principal,\\nmanaged identity, etc., have the necessary permissions for that storage\\nresource? P ermissions are granted using Azure role-based access controls\\n(Azure RBA C).\\nThe storage account Reader  reads the storage metadata.\\nThe Storage Blob Data R eader  reads data within a blob container.\\nThe Contributor  allows write access to a storage account.\\nMore roles may be required, depending on the type of storage.\\nWhere does the access come from?\\nUser: Is the client IP address in the VNet\/subnet range?\\n） Impor tant\\nThis article is intended for Azure administrators who want to create the required\\ninfrastructure for an Azure Machine Learning solution.', \"Data concepts in Azure Machine\\nLearning\\nArticle •07\/13\/2023\\nWith Azure Machine Learning, you can import data from a local machine or an existing\\ncloud-based storage resource. This article describes key Azure Machine Learning data\\nconcepts.\\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\\naccount. An Azure Machine Learning datastore offers these benefits:\\nA common, easy-to-use API that interacts with different storage types\\n(Blob\/Files\/ADLS).\\nEasier discovery of useful datastores in team operations.\\nFor credential-based access (service principal\/SAS\/key), Azure Machine Learning\\ndatastore secures connection information. This way, you won't need to place that\\ninformation in your scripts.\\nWhen you create a datastore with an existing Azure storage account, you can choose\\nbetween two different authentication methods:\\nCredential-b ased  - authenticate data access with a service principal, shared access\\nsignature (SAS) token, or account key. Users with Reader  workspace access can\\naccess the credentials.\\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\\nauthenticate data access.\\nThe following table summarizes the Azure cloud-based storage services that an Azure\\nMachine Learning datastore can create. Additionally, the table summarizes the\\nauthentication types that can access those services:\\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\\nAzure Blob Container ✓ ✓\\nAzure File Share ✓\\nAzure Data Lake Gen1 ✓ ✓\\nAzure Data Lake Gen2 ✓ ✓Datastore\"]","question":"Can Azure Machine Learning directly access data stored in Azure Data Lake Storage Gen2? If so, how?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":24}
{"answer":"Azure Machine Learning supports versioning of datasets stored in Azure Blob Storage by leveraging the inherent versioning capabilities of Azure Blob Storage. When you upload a new version of a dataset to Azure Blob Storage, it automatically creates a new version of the blob, allowing you to maintain a history of changes. Additionally, Azure Machine Learning provides tools to manage and track dataset versions within your machine learning workflows.","documents":"[\"Data encryption with Azure Machine\\nLearning\\nArticle •01\/31\/2024\\nAzure Machine Learning relies on various Azure data storage services and compute\\nresources when you're training models and performing inferences. In this article, learn\\nabout the data encryption for each service both at rest and in transit.\\nFor production-grade encryption during training, we recommend that you use an Azure\\nMachine Learning compute cluster. For production-grade encryption during inference,\\nwe recommend that you use Azure Kubernetes Service (AKS).\\nAn Azure Machine Learning compute instance is a dev\/test environment. When you use\\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\\nStore your data in a datastore.\\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\\nmethods for such services.\\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\\naccount (default storage account) that's tied to the Azure Machine Learning workspace\\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\\nMicrosoft-managed keys.\\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\\nTraining data is typically also stored in Azure Blob S torage so that training compute\\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\\nis mounted to compute targets as a remote file system.\\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\\nAzure Blob Storage\", 'Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', \"Data concepts in Azure Machine\\nLearning\\nArticle •07\/13\/2023\\nWith Azure Machine Learning, you can import data from a local machine or an existing\\ncloud-based storage resource. This article describes key Azure Machine Learning data\\nconcepts.\\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\\naccount. An Azure Machine Learning datastore offers these benefits:\\nA common, easy-to-use API that interacts with different storage types\\n(Blob\/Files\/ADLS).\\nEasier discovery of useful datastores in team operations.\\nFor credential-based access (service principal\/SAS\/key), Azure Machine Learning\\ndatastore secures connection information. This way, you won't need to place that\\ninformation in your scripts.\\nWhen you create a datastore with an existing Azure storage account, you can choose\\nbetween two different authentication methods:\\nCredential-b ased  - authenticate data access with a service principal, shared access\\nsignature (SAS) token, or account key. Users with Reader  workspace access can\\naccess the credentials.\\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\\nauthenticate data access.\\nThe following table summarizes the Azure cloud-based storage services that an Azure\\nMachine Learning datastore can create. Additionally, the table summarizes the\\nauthentication types that can access those services:\\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\\nAzure Blob Container ✓ ✓\\nAzure File Share ✓\\nAzure Data Lake Gen1 ✓ ✓\\nAzure Data Lake Gen2 ✓ ✓Datastore\", \"Only Azure Blob S torage Accounts are supported for outputs. For instance, Azure Data Lake\\nStorage Gen2 isn't supported as output in batch deployment jobs. If you need to output the\\ndata to a different location\/sink, use the Azure Data F actory Copy activity after the execution of\\nthe batch job.\\nUse low priority VMs in batch deployments\\nAuthorization on batch endpoints\\nNetwork isolation in batch endpointsNext steps\", 'You can connect to storage services via identity-based data access with Azure Machine\\nLearning datastores .\\nWhen you use identity-based data access, Azure Machine Learning prompts you for\\nyour Microsoft Entra token for data access authentication instead of keeping your\\ncredentials in the datastore. That approach allows for data access management at the\\nstorage level and keeps credentials confidential.\\nThe same behavior applies when you work with data interactively via a Jupyter\\nNotebook on your local computer or compute instance .\\nTo help ensure that you securely connect to your storage service on Azure, Azure\\nMachine Learning requires that you have permission to access the corresponding data\\nstorage.\\nIdentity-based data access supports connections to only the following storage services.\\nAzure Blob S torage\\nAzure Data Lake S torage Gen1\\nAzure Data Lake S torage Gen2\\nTo access these storage services, you must have at least Storage Blob Data R eader\\naccess to the storage account. Only storage account owners can change your access\\nlevel via the Azure portal .７ Note\\nCredentials stored via credential-based authentication include subscription IDs,\\nshared access signature (SAS) tokens, and storage access key and service principal\\ninformation, like client IDs and tenant IDs.\\n２ Warning\\nCross tenant access to storage accounts is not supported. If cross tenant access is\\nneeded for your scenario, please reach out to the Azure Machine Learning Data\\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\\ncode solution.\\nAccess data for training jobs on compute using managed\\nidentity']","question":"How does Azure Machine Learning support versioning of datasets stored in Azure Blob Storage?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":25}
{"answer":"The recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage is to use Azure Storage Explorer for faster and more robust data transfer. This is the default way to move files and create datasets from data stored in Blob Storage.","documents":"[\"In many cases, you can upload local files. However, Azure S torage Explorer  provides a\\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\\nExplorer as the default way to move files.\\nTo create a dataset from data that's already stored in Blob S torage:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Choose the Dataset type :\\nIf you're using a .csv or .tsv file and each row contains a response, select\\nTabular .\\nIf you're using separate .txt files for each response, select File.\\n4. Select Next .\\n5. Select From Azur e storage , and then select Next .\\n6. Select the datastore, and then select Next .\\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\\nTo include all the files in the subfolders of the selected path, append \/** to\\nthe path.\\nTo include all the data in the current container and its subfolders, append\\n**\/*.* to the path.\\n8. Select Create.\\n9. Select the data asset you created.\\nTo directly upload your data:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Choose the Dataset type :\\nIf you're using a .csv or .tsv file and each row contains a response, select\\nTabular .\\nIf you're using separate .txt files for each response, select File.\\n4. Select Next .\\n5. Select From local files , and then select Next .\\n6. (Optional) Select a datastore. The default uploads to the default blob store\\n(worksp aceblobst ore) for your Machine Learning workspace.\\n7. Select Next .\\nCreate a dataset from uploaded data\", 'Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', \"Data encryption with Azure Machine\\nLearning\\nArticle •01\/31\/2024\\nAzure Machine Learning relies on various Azure data storage services and compute\\nresources when you're training models and performing inferences. In this article, learn\\nabout the data encryption for each service both at rest and in transit.\\nFor production-grade encryption during training, we recommend that you use an Azure\\nMachine Learning compute cluster. For production-grade encryption during inference,\\nwe recommend that you use Azure Kubernetes Service (AKS).\\nAn Azure Machine Learning compute instance is a dev\/test environment. When you use\\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\\nStore your data in a datastore.\\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\\nmethods for such services.\\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\\naccount (default storage account) that's tied to the Azure Machine Learning workspace\\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\\nMicrosoft-managed keys.\\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\\nTraining data is typically also stored in Azure Blob S torage so that training compute\\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\\nis mounted to compute targets as a remote file system.\\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\\nAzure Blob Storage\", \"In many cases, you can upload local files. However, Azure S torage Explorer  provides a\\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\\nExplorer as the default way to move files.\\nTo create a dataset from data that's already stored in Blob S torage:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\\nimages.\\n4. Select Next .\\n5. Select From Azur e storage , and then select Next .\\n6. Select the datastore, and then select Next .\\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\\nTo include all the files in the subfolders of the selected path, append \/** to\\nthe path.\\nTo include all the data in the current container and its subfolders, append\\n**\/*.* to the path.\\n8. Select Create.\\n9. Select the data asset you created.\\nTo directly upload your data:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\\nimages.\\n4. Select Next .\\n5. Select From local files , and then select Next .\\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\\n7. Select Next .\\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\\nfolders to upload.\\n9. In the browser window, find your files or folders, and then select Open .\\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\\nCreate a dataset from uploaded data\", 'You can connect to storage services via identity-based data access with Azure Machine\\nLearning datastores .\\nWhen you use identity-based data access, Azure Machine Learning prompts you for\\nyour Microsoft Entra token for data access authentication instead of keeping your\\ncredentials in the datastore. That approach allows for data access management at the\\nstorage level and keeps credentials confidential.\\nThe same behavior applies when you work with data interactively via a Jupyter\\nNotebook on your local computer or compute instance .\\nTo help ensure that you securely connect to your storage service on Azure, Azure\\nMachine Learning requires that you have permission to access the corresponding data\\nstorage.\\nIdentity-based data access supports connections to only the following storage services.\\nAzure Blob S torage\\nAzure Data Lake S torage Gen1\\nAzure Data Lake S torage Gen2\\nTo access these storage services, you must have at least Storage Blob Data R eader\\naccess to the storage account. Only storage account owners can change your access\\nlevel via the Azure portal .７ Note\\nCredentials stored via credential-based authentication include subscription IDs,\\nshared access signature (SAS) tokens, and storage access key and service principal\\ninformation, like client IDs and tenant IDs.\\n２ Warning\\nCross tenant access to storage accounts is not supported. If cross tenant access is\\nneeded for your scenario, please reach out to the Azure Machine Learning Data\\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\\ncode solution.\\nAccess data for training jobs on compute using managed\\nidentity']","question":"What is the recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":26}
{"answer":"To programmatically download data from Azure Blob Storage using the Azure Machine Learning SDK, you can use the `Datastore` and `Dataset` classes. First, you need to register your Azure Blob Storage account as a datastore in your workspace. Then, you can create a dataset from the datastore and download the data programmatically. Would you like a code example to illustrate this process?","documents":"['Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', \"In many cases, you can upload local files. However, Azure S torage Explorer  provides a\\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\\nExplorer as the default way to move files.\\nTo create a dataset from data that's already stored in Blob S torage:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Choose the Dataset type :\\nIf you're using a .csv or .tsv file and each row contains a response, select\\nTabular .\\nIf you're using separate .txt files for each response, select File.\\n4. Select Next .\\n5. Select From Azur e storage , and then select Next .\\n6. Select the datastore, and then select Next .\\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\\nTo include all the files in the subfolders of the selected path, append \/** to\\nthe path.\\nTo include all the data in the current container and its subfolders, append\\n**\/*.* to the path.\\n8. Select Create.\\n9. Select the data asset you created.\\nTo directly upload your data:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Choose the Dataset type :\\nIf you're using a .csv or .tsv file and each row contains a response, select\\nTabular .\\nIf you're using separate .txt files for each response, select File.\\n4. Select Next .\\n5. Select From local files , and then select Next .\\n6. (Optional) Select a datastore. The default uploads to the default blob store\\n(worksp aceblobst ore) for your Machine Learning workspace.\\n7. Select Next .\\nCreate a dataset from uploaded data\", \"Tutorial: Upload, access and explore\\nyour data in Azure Machine Learning\\nArticle •12\/27\/2023\\nAPPLIES T O: \\n  Python SDK azure-ai-ml v2 (curr ent)\\nIn this tutorial you learn how to:\\nThe start of a machine learning project typically involves exploratory data analysis (ED A),\\ndata-preprocessing (cleaning, feature engineering), and the building of Machine\\nLearning model prototypes to validate hypotheses. This prototyping  project phase is\\nhighly interactive. It lends itself to development in an IDE or a Jupyter notebook, with a\\nPython int eractive console. This tutorial describes these ideas.\\nThis video shows how to get started in Azure Machine Learning studio so that you can\\nfollow the steps in the tutorial. The video shows how to create a notebook, clone the\\nnotebook, create a compute instance, and download the data needed for the tutorial.\\nThe steps are also described in the following sections.\\n1. To use Azure Machine Learning, you'll first need a workspace. If you don't have\\none, complete Create resources you need to get started  to create a workspace and\\nlearn more about using it.\\n2. Sign in to studio  and select your workspace if it's not already open.\\n3. Open or create a notebook in your workspace:\\nCreate a new notebook , if you want to copy\/paste code into cells.\\nOr, open tutorials\/get -star ted-not ebooks\/explor e-data.ipynb  from the\\nSamples  section of studio. Then select Clone  to add the notebook to your\\nFiles. (See where to find Samples .)\\nUpload your data to cloud storage＂\\nCreate an Azure Machine Learning data asset＂\\nAccess your data in a notebook for interactive development＂\\nCreate new versions of data assets＂\\nhttps:\/\/learn-video.azurefd.net\/vod\/player?id=514a29e2-0ae7-4a5d-a537-\\n8f10681f5545&locale=en-us&embedUrl=%2F azure%2Fmachine-learning%2Ftutorial-\\nexplore-data\\nPrerequisites\", '2. Select your subscription and the workspace you created.\\n3. Select Get star ted.\\n4. In the left pane, select Automat ed ML  under the Author  section.\\n5. Select +New aut omat ed ML job .\\nBefore you configure your experiment, upload your data file to your workspace in the\\nform of an Azure Machine Learning dataset. Doing so, allows you to ensure that your\\ndata is formatted appropriately for your experiment.\\n1. On the Select dataset  form, select From local files  from the +Create dataset  drop-\\ndown.\\na. On the Basic info  form, give your dataset a name and provide an optional\\ndescription. The dataset type should default to Tabular , since automated ML in\\nAzure Machine Learning studio currently only supports tabular datasets.\\nb. Select Next  on the bottom left\\nc. On the Datast ore and file selection  form, select the default datastore that was\\nautomatically set up during your workspace creation, worksp aceblobst ore\\n(Azur e Blob S torage) . This is the storage location where you upload your data\\nfile.\\nd. Select Uplo ad files  from the Uplo ad drop-down.\\ne. Choose the bike-no.csv  file on your local computer. This is the file you\\ndownloaded as a prerequisite .\\nf. Select Next\\nWhen the upload is complete, the Settings and preview form is pre-populated\\nbased on the file type.\\ng. Verify that the Settings and pr eview  form is populated as follows and select\\nNext .Create and load dataset\\nﾉExpand table', \"In many cases, you can upload local files. However, Azure S torage Explorer  provides a\\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\\nExplorer as the default way to move files.\\nTo create a dataset from data that's already stored in Blob S torage:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\\nimages.\\n4. Select Next .\\n5. Select From Azur e storage , and then select Next .\\n6. Select the datastore, and then select Next .\\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\\nTo include all the files in the subfolders of the selected path, append \/** to\\nthe path.\\nTo include all the data in the current container and its subfolders, append\\n**\/*.* to the path.\\n8. Select Create.\\n9. Select the data asset you created.\\nTo directly upload your data:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\\nimages.\\n4. Select Next .\\n5. Select From local files , and then select Next .\\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\\n7. Select Next .\\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\\nfolders to upload.\\n9. In the browser window, find your files or folders, and then select Open .\\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\\nCreate a dataset from uploaded data\"]","question":"How do you use the Azure Machine Learning SDK to programmatically download data from Azure Blob Storage?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":27}
{"answer":"Azure Data Lake Storage integration with Azure Machine Learning offers benefits such as scalable storage for large datasets, seamless data access from AML workspaces, and the ability to leverage big data processing capabilities for training and inference tasks.","documents":"['Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', 'Azur e Machine Learning designer : Use the designer to train and deploy ML\\nmodels without writing any code. Drag and drop datasets and components to\\ncreate ML pipelines.\\nAutomat ed machine learning UI : Learn how to create automated ML experiments\\nwith an easy-to-use interface.\\nData labeling : Use Machine Learning data labeling to efficiently coordinate image\\nlabeling  or text labeling  projects.\\nMachine Learning integrates with the Azure cloud platform to add security to ML\\nprojects.\\nSecurity integrations include:\\nAzure Virtual Networks with network security groups.\\nAzure K ey Vault, where you can save security secrets, such as access information\\nfor storage accounts.\\nAzure Container R egistry set up behind a virtual network.\\nFor more information, see Tutorial: Set up a secure workspace .\\nOther integrations with Azure services support an ML project from end to end. They\\ninclude:Enterprise-readiness and security\\nAzure integrations for complete solutio ns', \"Data concepts in Azure Machine\\nLearning\\nArticle •07\/13\/2023\\nWith Azure Machine Learning, you can import data from a local machine or an existing\\ncloud-based storage resource. This article describes key Azure Machine Learning data\\nconcepts.\\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\\naccount. An Azure Machine Learning datastore offers these benefits:\\nA common, easy-to-use API that interacts with different storage types\\n(Blob\/Files\/ADLS).\\nEasier discovery of useful datastores in team operations.\\nFor credential-based access (service principal\/SAS\/key), Azure Machine Learning\\ndatastore secures connection information. This way, you won't need to place that\\ninformation in your scripts.\\nWhen you create a datastore with an existing Azure storage account, you can choose\\nbetween two different authentication methods:\\nCredential-b ased  - authenticate data access with a service principal, shared access\\nsignature (SAS) token, or account key. Users with Reader  workspace access can\\naccess the credentials.\\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\\nauthenticate data access.\\nThe following table summarizes the Azure cloud-based storage services that an Azure\\nMachine Learning datastore can create. Additionally, the table summarizes the\\nauthentication types that can access those services:\\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\\nAzure Blob Container ✓ ✓\\nAzure File Share ✓\\nAzure Data Lake Gen1 ✓ ✓\\nAzure Data Lake Gen2 ✓ ✓Datastore\", \"What is Azure Machine Learning?\\nArticle •01\/29\/2024\\nAzure Machine Learning is a cloud service for accelerating and managing the machine\\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\\nin their day-to-day workflows to train and deploy models and manage machine learning\\noperations (ML Ops).\\nYou can create a model in Machine Learning or use a model built from an open-source\\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\\nretrain, and redeploy models.\\nMachine Learning is for individuals and teams implementing ML Ops within their\\norganization to bring ML models into production in a secure and auditable production\\nenvironment.\\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\\nday workflows. Application developers can use tools for integrating models into\\napplications or services. Platform developers can use a robust set of tools, backed by\\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\\nbased access control for infrastructure. Y ou can set up a project to deny access to\\nprotected data and select operations.\\nML projects often require a team with a varied skill set to build and maintain. Machine\\nLearning has tools that help enable you to:\\uea80 Tip\\nFree trial!  If you don't have an Azure subscription, create a free account before you\\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\\nto spend on Azure services. After they're used up, you can keep the account and\\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\\nchange your settings and ask to be charged.\\nWho is Azure Machine Learning for?\\nProductivity for everyone on the team\", 'Learn more about enterprise security in Azure Machine Learning\\nLearn about data administration\\nLearn about managed identities on compute cluster .']","question":"What are the benefits of integrating Azure Data Lake Storage with Azure Machine Learning for big data scenarios?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":28}
{"answer":"To secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces, you can leverage the encryption at rest feature provided by Azure Blob Storage. All data stored in Azure Blob Storage is encrypted at rest with Microsoft-managed keys. Additionally, you can use your own keys for data stored in Azure Blob Storage by following the guidelines for Azure Storage encryption with customer-managed keys in Azure Key Vault.","documents":"[\"Data encryption with Azure Machine\\nLearning\\nArticle •01\/31\/2024\\nAzure Machine Learning relies on various Azure data storage services and compute\\nresources when you're training models and performing inferences. In this article, learn\\nabout the data encryption for each service both at rest and in transit.\\nFor production-grade encryption during training, we recommend that you use an Azure\\nMachine Learning compute cluster. For production-grade encryption during inference,\\nwe recommend that you use Azure Kubernetes Service (AKS).\\nAn Azure Machine Learning compute instance is a dev\/test environment. When you use\\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\\nStore your data in a datastore.\\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\\nmethods for such services.\\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\\naccount (default storage account) that's tied to the Azure Machine Learning workspace\\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\\nMicrosoft-managed keys.\\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\\nTraining data is typically also stored in Azure Blob S torage so that training compute\\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\\nis mounted to compute targets as a remote file system.\\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\\nAzure Blob Storage\", 'This article is part of a series on securing an Azure Machine Learning workflow. See the\\nother articles in this series:\\nSecure the workspace resources\\nSecure machine learning registries\\nSecure the training environment\\nSecure the inference environment\\nEnable studio functionality\\nUse custom DNS\\nUse a firewall\\nAPI platform network isolationNext steps', \"Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\\ninformation, see Data encryption with Azure Machine Learning .\\nWhen you access your workspace from the studio, the network traffic flows are as\\nfollows:\\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\\nFor management and deployment operations, the configuration uses Azure\\nResource Manager.\\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\\nAzure Machine Learning service.\\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\\nDoor.\\nFor most storage operations, traffic flows through the private endpoint of the\\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\\nand the datastore from the studio  section of this article discusses exceptions.\\nYou also need to configure a DNS solution that allows you to resolve the names of\\nthe resources within the virtual network. For more information, see Use your\\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\\n７ Note\\nThe information in this section is specific to using the workspace from Azure\\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\\nto you.\", \"When you use an Azure S torage Account from Azure Machine Learning studio, you must\\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\\naccount:\\nBlob Data R eader\\nIf the storage account uses a private endpoint to connect to the VNet, you must\\ngrant the Reader  role for the storage account private endpoint to the managed\\nidentity.\\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\\nNetwork .\\nThe following sections explain the limitations of using an Azure S torage Account, with\\nyour workspace, in a VNet.\\nTo secure communication between Azure Machine Learning and Azure S torage\\nAccounts, configure the storage to Grant access to trusted Azure services .\\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\\ncan normally be used to allow your client to directly connect over the internet. However,\\nwhen using studio, your client doesn't connect to the storage account. The Azure\\nMachine Learning service that makes the request connects to the storage account. The\\nIP address of the service isn't documented, and it changes frequently. Enabling the\\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\\nconfiguration .\\uea80 Tip\\nTo access data from outside Azure Machine Learning, for example with Azure\\nStorage Explorer, that access probably relies on the user identity. For specific\\ninformation, review the documentation for the tool or service you're using. For\\nmore information about how Azure Machine Learning works with data, see Setup\\nauthentication betw een Azur e Machine Learning and other ser vices .\\nAzure Storage Account\\nSecure communication with Azure Storage Account\\nAzure Storage firewall\", 'You can connect to storage services via identity-based data access with Azure Machine\\nLearning datastores .\\nWhen you use identity-based data access, Azure Machine Learning prompts you for\\nyour Microsoft Entra token for data access authentication instead of keeping your\\ncredentials in the datastore. That approach allows for data access management at the\\nstorage level and keeps credentials confidential.\\nThe same behavior applies when you work with data interactively via a Jupyter\\nNotebook on your local computer or compute instance .\\nTo help ensure that you securely connect to your storage service on Azure, Azure\\nMachine Learning requires that you have permission to access the corresponding data\\nstorage.\\nIdentity-based data access supports connections to only the following storage services.\\nAzure Blob S torage\\nAzure Data Lake S torage Gen1\\nAzure Data Lake S torage Gen2\\nTo access these storage services, you must have at least Storage Blob Data R eader\\naccess to the storage account. Only storage account owners can change your access\\nlevel via the Azure portal .７ Note\\nCredentials stored via credential-based authentication include subscription IDs,\\nshared access signature (SAS) tokens, and storage access key and service principal\\ninformation, like client IDs and tenant IDs.\\n２ Warning\\nCross tenant access to storage accounts is not supported. If cross tenant access is\\nneeded for your scenario, please reach out to the Azure Machine Learning Data\\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\\ncode solution.\\nAccess data for training jobs on compute using managed\\nidentity']","question":"How can you secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":29}
{"answer":"Azure Storage accounts play a crucial role in deploying Azure Machine Learning models as web services by storing model artifacts, deployment configurations, and other resources required for the deployment process.","documents":"[\"Scenar io Requir ed\\ninboundRequir ed outbound Additional\\nconfiguration\\nUse Azure\\nKubernetes\\nServiceNot applicable For information on the outbound\\nconfiguration for AKS, see Secure\\nAzure Kubernetes Service\\ninferencing environment .\\nUse Docker\\nimages that\\nAzure\\nMachine\\nLearning\\nmanagesNot applicable Microsoft Artifact R egistry\\nviennaglobal.azurecr.io\\nglobal container registryIf the container registry\\nfor your workspace is\\nbehind the virtual\\nnetwork, configure the\\nworkspace to use a\\ncompute cluster to build\\nimages. For more\\ninformation, see Secure\\nan Azure Machine\\nLearning workspace with\\nvirtual networks .\\nAzure Machine Learning uses multiple storage accounts. Each stores different data and\\nhas a different purpose:\\nYour st orage : The storage accounts in your Azure subscription store your data and\\nartifacts, such as models, training data, training logs, and Python scripts. For\\nexample, the default  storage account for your workspace is in your subscription.\\nThe Azure Machine Learning compute instance and compute cluster access file and\\nblob data in this storage over ports 445 (SMB) and 443 (HT TPS).\\nWhen you're using a compute instance or compute cluster, your storage account is\\nmounted as a file share via the SMB protocol. The compute instance and cluster\\nuse this file share to store items like the data, models, Jupyter notebooks, and\\ndatasets. The compute instance and cluster use the private endpoint when they\\naccess the storage account.\\nMicrosoft storage : The Azure Machine Learning compute instance and compute\\ncluster rely on Azure Batch. They access storage located in a Microsoft\\nsubscription. This storage is used only for the management of the compute\\ninstance or cluster. None of your data is stored here. The compute instance and\\ncompute cluster access the blob, table, and queue data in this storage, by using\\nport 443 (HT TPS).Purposes of storage accounts\", 'Azure Machine Learning supports storage accounts configured to use either a private\\nendpoint or service endpoint.\\n1. In the Azure portal, select the Azure S torage Account.\\n2. Use the information in Use private endpoints for Azure S torage  to add private\\nendpoints for the following storage resources:\\nBlob\\nFile\\nQueue  - Only needed if you plan to use Batch endpoints  or the\\nParallelRunS tep in an Azure Machine Learning pipeline.\\nTable - Only needed if you plan to use Batch endpoints  or the\\nParallelRunS tep in an Azure Machine Learning pipeline.Securing a workspace with private endpoints does not ensure end-to-end security\\nby itself. Y ou must follow the steps in the rest of this article, and the VNet series, to\\nsecure individual components of your solution. For example, if you use a private\\nendpoint for the workspace, but your Azure S torage Account is not behind the\\nVNet, traffic between the workspace and storage does not use the VNet for\\nsecurity.\\nSecure Azure storage accounts\\nPrivate endpoint', 'What are Azure Machine Learning endpoints?\\nDeploy and score a model with an online endpointNext steps', \"Azure Machine Learning offers many ways to deploy MLflow models to online and batch\\nendpoints. Y ou can deploy models, using the following tools:\\nEach workflow has different capabilities, particularly around which type of compute they\\ncan target. The following table shows the different capabilities.\\nScenar io MLflow SDK Azur e Machine\\nLearning CLI\/SDKAzur e Machine\\nLearning studio\\nDeploy to managed online\\nendpointsSee example See example See example\\nDeploy to managed online\\nendpoints (with a scoring script)Not\\nsupportedSee example See example\\nDeploy to batch endpoints Not\\nsupportedSee example See example\\nDeploy to batch endpoints (with\\na scoring script)Not\\nsupportedSee example See example\\nDeploy to web services\\n(ACI\/AKS)Legacy\\nsupportNot supported Not supported\\nDeploy to web services (A CI\/AKS\\n- with a scoring script)Not\\nsupportedLegacy support Legacy support\\n Deployment to online endpoints that are in workspaces with private link enabled\\nrequires you to package models before deployment (preview) .\\n We recommend switching to managed online endpoints  instead.\\n MLflow (OSS) doesn't have the concept of a scoring script and doesn't support batch\\nexecution currently.\\nUse the MLflow SDK if both of these conditions apply:Deployment tools\\nMLflow SDK＂\\nAzure Machine Learning CLI＂\\nAzure Machine Learning SDK for Python＂\\nAzure Machine Learning studio＂\\nﾉExpand table\\n1 1 1\\n3\\n3\\n3\\n22 2\\n32 2\\n1\\n2\\n3\\nWhich deployment tool to use?\", 'region basis. When you sign up for Azure Machine Learning studio, you receive a default\\nVM quota for several VM families available in the region. Y ou can continue to create\\ndeployments until you reach your quota limit. Once you reach this limit, you can request\\na quota increase.\\nModels deployed as a service with pay-as-you-go are protected by Azure AI content\\nsafety. When deployed to real-time endpoints, you can opt out of this capability. With\\nAzure AI content safety enabled, both the prompt and completion pass through an\\nensemble of classification models aimed at detecting and preventing the output of\\nharmful content. The content filtering system detects and takes action on specific\\ncategories of potentially harmful content in both input prompts and output\\ncompletions. Learn more about Azure AI Content Safety .\\nModel Catalog and Collections\\nDeploy and score a machine learning model by using an online endpoint\\nPlan and manage costs for Azure AI S tudioContent filtering\\nRelated content']","question":"What role do Azure Storage accounts play in deploying Azure Machine Learning models as web services?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":30}
{"answer":"To monitor data access and usage when integrating Azure Machine Learning with Azure Storage, you can leverage Azure Monitor to track metrics, logs, and activity. Additionally, Azure Storage provides detailed logging and monitoring capabilities, including Storage Analytics, which can be used to monitor access patterns and usage.","documents":"['To learn how to monitor resources and logs from Azure Machine Learning, see\\nMonitoring Azure Machine Learning .', 'Monitor Azure Machine Learning\\nArticle •11\/06\/2023\\nWhen you have critical applications and business processes relying on Azure resources, you\\nwant to monitor those resources for their availability, performance, and operation. This\\narticle describes the monitoring data generated by Azure Machine Learning and how to\\nanalyze and alert on this data with Azure Monitor.\\nAzure Machine Learning creates monitoring data using Azure Monitor , which is a full stack\\nmonitoring service in Azure. Azure Monitor provides a complete set of features to monitor\\nyour Azure resources. It can also monitor resources in other clouds and on-premises.\\nStart with the article Monitoring Azure resources with Azure Monitor , which describes the\\nfollowing concepts:\\nWhat is Azure Monitor?\\nCosts associated with monitoring\\nMonitoring data collected in Azure\\nConfiguring data collection\\nStandard tools in Azure for analyzing and alerting on monitoring data\\nThe following sections build on this article by describing the specific data gathered for\\nAzure Machine Learning. These sections also provide examples for configuring data\\ncollection and analyzing this data with Azure tools.\\uea80 Tip\\nThe information in this document is primarily for administrat ors, as it describes\\nmonitoring for the Azure Machine Learning service and associated Azure services. If\\nyou are a data scientist  or developer , and want to monitor information specific to your\\nmodel tr aining r uns, see the following documents:\\nStart, monit or, and cancel training runs\\nLog metrics for training runs\\nTrack experiments with MLflow\\nIf you want to monitor information generated by models deployed to online\\nendpoints, see Monit or online endpoints .\\nWhat is Azure Monitor?', \"Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\\ninformation, see Data encryption with Azure Machine Learning .\\nWhen you access your workspace from the studio, the network traffic flows are as\\nfollows:\\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\\nFor management and deployment operations, the configuration uses Azure\\nResource Manager.\\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\\nAzure Machine Learning service.\\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\\nDoor.\\nFor most storage operations, traffic flows through the private endpoint of the\\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\\nand the datastore from the studio  section of this article discusses exceptions.\\nYou also need to configure a DNS solution that allows you to resolve the names of\\nthe resources within the virtual network. For more information, see Use your\\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\\n７ Note\\nThe information in this section is specific to using the workspace from Azure\\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\\nto you.\", 'Monitor online endpoints\\nArticle •10\/24\/2023\\nAzure Machine Learning uses integration with Azure Monitor to track and monitor\\nmetrics and logs for online endpoints . You can view metrics in charts, compare between\\nendpoints and deployments, pin to Azure portal dashboards, configure alerts, query\\nfrom log tables, and push logs to supported targets. Y ou can also use Application\\nInsights to analyze events from user containers.\\nMetrics : For endpoint-level metrics such as request latency, requests per minute,\\nnew connections per second, and network bytes, you can drill down to see details\\nat the deployment level or status level. Deployment-level metrics such as CPU\/GPU\\nutilization and memory or disk utilization can also be drilled down to instance\\nlevel. Azure Monitor allows tracking these metrics in charts and setting up\\ndashboards and alerts for further analysis.\\nLogs : You can send metrics to the Log Analytics workspace where you can query\\nthe logs using Kusto query syntax. Y ou can also send metrics to Azure S torage\\naccounts and\/or Event Hubs for further processing. In addition, you can use\\ndedicated log tables for online endpoint related events, traffic, and console\\n(container) logs. Kusto query allows complex analysis and joining of multiple\\ntables.\\nApplication insights : Curated environments include integration with Application\\nInsights, and you can enable or disable this integration when you create an online\\ndeployment. Built-in metrics and logs are sent to Application Insights, and you can\\nuse the built-in features of Application Insights (such as Live metrics, T ransaction\\nsearch, F ailures, and P erformance) for further analysis.\\nIn this article you learn how to:\\nDeploy an Azure Machine Learning online endpoint.Choose the right method to view and track metrics and logs＂\\nView metrics for your online endpoint＂\\nCreate a dashboard for your metrics＂\\nCreate a metric alert＂\\nView logs for your online endpoint＂\\nUse Application Insights to track metrics and logs＂\\nPrerequisites', \"Use monitoring view in the Azure Machine Learning studio  to check the CPU and memory utilization of\\nyour job. If you're low on CPU and memory, consider increasing the compute target VM size.\\nConsider increasing RSLEX_DOWNLOADER_THREADS if you're downloading and you don't utilize the CPU and\\nmemory. If you use mount, you should increase DATASET_MOUNT_READ_BUFFER_BLOCK_COUNT to do more\\nprefetching, and increase DATASET_MOUNT_READ_THREADS for more read threads.\\nIf the metrics show low SuccessE2ELat ency and low SuccessSer verLat ency, but the client experiences high\\nlatency , you have a delay in the storage request that reaches the service. Y ou should check:\\nWhether the number of threads used for mount\/download\\n(DATASET_MOUNT_READ_THREADS\/RSLEX_DOWNLOADER_THREADS) is set too low, relative to the number of cores\\navailable on the compute target. If the setting is too low, increase the number of threads.\\nWhether the number of retries for downloading ( AZUREML_DATASET_HTTP_RETRY_COUNT) is set too high. If\\nso, decrease the number of retries.\\nFrom the Azure Machine Learning studio, you can also monitor the compute target disk IO and usage during\\nyour job execution. Navigate to your job, and select the Monit oring  tab. This tab provides insights on the\\nresources of your job, on a 30 day rolling basis. For example:\\nAzure Machine Learning data runtime doesn't use the last RESERVED_FREE_DISK_SPACE bytes of disk space, to\\nkeep the compute healthy (the default value is 150MB). If your disk is full, your code is writing files to disk\\nwithout declaring the files as an output. Therefore, check your code to make sure that data isn't being written\\nerroneously to temporary disk. If you must write files to temporary disk, and that resource is becoming full,\\nconsider:\\nIncreasing the VM Size to one that has a larger temporary disk\\nSetting a T TL on the cached data ( DATASET_MOUNT_ATTRIBUTE_CACHE_TTL), to purge your data from diskMonitor disk usage during a job\\n\\uf80a\\n７ Note\\nJob monitoring supports only compute resources that Azure Machine Learning manages. Jobs with a\\nruntime of less than 5 minutes will not have enough data to populate this view.\"]","question":"How do you monitor data access and usage when integrating Azure Machine Learning with Azure Storage?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":31}
{"answer":"Yes, Azure Machine Learning can use data stored in Azure File Storage. This is useful for scenarios where you need to access shared data across multiple compute targets, such as training clusters or inference endpoints. For example, you can use Azure File Storage to store training data, model checkpoints, and other artifacts that need to be accessed by different compute resources.","documents":"['Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', \"Data encryption with Azure Machine\\nLearning\\nArticle •01\/31\/2024\\nAzure Machine Learning relies on various Azure data storage services and compute\\nresources when you're training models and performing inferences. In this article, learn\\nabout the data encryption for each service both at rest and in transit.\\nFor production-grade encryption during training, we recommend that you use an Azure\\nMachine Learning compute cluster. For production-grade encryption during inference,\\nwe recommend that you use Azure Kubernetes Service (AKS).\\nAn Azure Machine Learning compute instance is a dev\/test environment. When you use\\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\\nStore your data in a datastore.\\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\\nmethods for such services.\\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\\naccount (default storage account) that's tied to the Azure Machine Learning workspace\\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\\nMicrosoft-managed keys.\\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\\nTraining data is typically also stored in Azure Blob S torage so that training compute\\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\\nis mounted to compute targets as a remote file system.\\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\\nAzure Blob Storage\", \"Data concepts in Azure Machine\\nLearning\\nArticle •07\/13\/2023\\nWith Azure Machine Learning, you can import data from a local machine or an existing\\ncloud-based storage resource. This article describes key Azure Machine Learning data\\nconcepts.\\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\\naccount. An Azure Machine Learning datastore offers these benefits:\\nA common, easy-to-use API that interacts with different storage types\\n(Blob\/Files\/ADLS).\\nEasier discovery of useful datastores in team operations.\\nFor credential-based access (service principal\/SAS\/key), Azure Machine Learning\\ndatastore secures connection information. This way, you won't need to place that\\ninformation in your scripts.\\nWhen you create a datastore with an existing Azure storage account, you can choose\\nbetween two different authentication methods:\\nCredential-b ased  - authenticate data access with a service principal, shared access\\nsignature (SAS) token, or account key. Users with Reader  workspace access can\\naccess the credentials.\\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\\nauthenticate data access.\\nThe following table summarizes the Azure cloud-based storage services that an Azure\\nMachine Learning datastore can create. Additionally, the table summarizes the\\nauthentication types that can access those services:\\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\\nAzure Blob Container ✓ ✓\\nAzure File Share ✓\\nAzure Data Lake Gen1 ✓ ✓\\nAzure Data Lake Gen2 ✓ ✓Datastore\", \"Attached comput e: You can attach your own compute resources to your\\nworkspace and use them for training and inference.\\nMachine Learning allows you to work with different types of data:\\nURIs (a location in local or cloud storage):\\nuri_folder\\nuri_file\\nTables (a tabular data abstraction):\\nmltable\\nPrimitives:\\nstring\\nboolean\\nnumber\\nFor most scenarios, you use URIs ( uri_folder and uri_file) to identify a location in\\nstorage that can be easily mapped to the file system of a compute node in a job by\\neither mounting or downloading the storage to the node.\\nThe mltable parameter is an abstraction for tabular data that's used for automated\\nmachine learning (AutoML) jobs, parallel jobs, and some advanced scenarios. If you're\\nstarting to use Machine Learning and aren't using AutoML, we strongly encourage you\\nto begin with URIs.\\nMachine Learning datastores securely keep the connection information to your data\\nstorage on Azure so that you don't have to code it in your scripts. Y ou can register and\\ncreate a datastore to easily connect to your storage account and access the data in your\\nunderlying storage service. The Azure Machine Learning CLI v2 and SDK v2 support the\\nfollowing types of cloud-based storage services:\\nAzure Blob S torage container\\nAzure Files share\\nAzure Data Lake S torage\\nAzure Data Lake S torage Gen2Data\\nDatastore\\nEnvironment\", \"Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\\ninformation, see Data encryption with Azure Machine Learning .\\nWhen you access your workspace from the studio, the network traffic flows are as\\nfollows:\\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\\nFor management and deployment operations, the configuration uses Azure\\nResource Manager.\\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\\nAzure Machine Learning service.\\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\\nDoor.\\nFor most storage operations, traffic flows through the private endpoint of the\\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\\nand the datastore from the studio  section of this article discusses exceptions.\\nYou also need to configure a DNS solution that allows you to resolve the names of\\nthe resources within the virtual network. For more information, see Use your\\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\\n７ Note\\nThe information in this section is specific to using the workspace from Azure\\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\\nto you.\"]","question":"Can Azure Machine Learning use data stored in Azure File Storage? If so, what are the use cases?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":32}
{"answer":"When managing input and output data for Azure Machine Learning pipelines using Azure Storage, it's important to consider the following best practices:\n\n1. **Data Store Compatibility**: Ensure that your input data is stored in Azure Machine Learning data stores or Azure Storage Accounts (Azure Blob Storage, Azure Data Lake Storage Gen1, Azure Data Lake Storage Gen2).\n\n2. **Flattening Folder Structures**: Batch endpoint jobs don't support nested folder structures, so flatten the structure if your data is distributed across multiple folders.\n\n3. **Scoring Script Compatibility**: Ensure that your scoring script provided in the deployment can handle the data as expected to be fed into the job.\n\n4. **Data Store Registration**: Register the storage account your Azure Data Factory is using as a Data Store in Azure Machine Learning to enable writing back to the same storage account from where you are reading.\n\nThese practices will help ensure smooth data management within Azure Machine Learning pipelines using Azure Storage.","documents":"[\"7. Your pipeline is ready to be used.\\nWhen calling Azure Machine Learning batch deployments consider the following limitations:\\nOnly Azure Machine Learning data stores or Azure S torage Accounts (Azure Blob S torage, Azure\\nData Lake S torage Gen1, Azure Data Lake S torage Gen2) are supported as inputs. If your input\\ndata is in another source, use the Azure Data F actory Copy activity before the execution of the\\nbatch job to sink the data to a compatible store.\\nBatch endpoint jobs don't explore nested folders and hence can't work with nested folder\\nstructures. If your data is distributed in multiple folders, notice that you will have to flatten the\\nstructure.\\nMake sure that your scoring script provided in the deployment can handle the data as it is\\nexpected to be fed into the job. If the model is MLflow, read the limitation in terms of the file\\ntype supported by the moment at Using MLflow models in batch deployments .\\nOnly registered Azure Machine Learning data stores are supported by the moment. W e\\nrecommend you to register the storage account your Azure Data F actory is using as a Data S tore\\nin Azure Machine Learning. In that way, you will be able to write back to the same storage\\naccount from where you are reading.\\nLimitations\\nData inputs\\nData outputs\", 'Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', 'Data administration\\nArticle •09\/26\/2023\\nLearn how to manage data access and how to authenticate in Azure Machine Learning\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (current)  \\n  Python SDK azure-ai-ml v2\\n(current)\\nIn general, data access from studio involves these checks:\\nWhich user wants to access the resources?\\nDepending on the storage type, different types of authentication are available,\\nfor example\\naccount key\\ntoken\\nservice principal\\nmanaged identity\\nuser identity\\nFor authentication based on a user identity, you must know which  specific user\\ntried to access the storage resource. For more information about user\\nauthentication, see authentication for Azure Machine Learning . For more\\ninformation about service-level authentication, see authentication between\\nAzure Machine Learning and other services .\\nDoes this user have permission?\\nDoes the user have the correct credentials? If yes, does the service principal,\\nmanaged identity, etc., have the necessary permissions for that storage\\nresource? P ermissions are granted using Azure role-based access controls\\n(Azure RBA C).\\nThe storage account Reader  reads the storage metadata.\\nThe Storage Blob Data R eader  reads data within a blob container.\\nThe Contributor  allows write access to a storage account.\\nMore roles may be required, depending on the type of storage.\\nWhere does the access come from?\\nUser: Is the client IP address in the VNet\/subnet range?\\n） Impor tant\\nThis article is intended for Azure administrators who want to create the required\\ninfrastructure for an Azure Machine Learning solution.', \"original notebook or Python code, changes the training input from local data to\\nparameterized values, split the training code into multiple steps as needed, perform unit\\ntest of each step, and finally wraps all steps into a pipeline.\\nOnce the teams get familiar with pipelines and want to do more machine learning\\nprojects using pipelines, they'll find the first approach is hard to scale. The second\\napproach is set up a few pipeline templates, each try to solve one specific machine\\nlearning problem. The template predefines the pipeline structure including how many\\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\\nlearning project, the team first forks one template repo. The team leader then assigns\\nmembers which step they need to work on. The data scientists and data engineers do\\ntheir regular work. When they're happy with their result, they structure their code to fit\\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\\nexecuted or automated. If there's any change, each member only needs to work on their\\npiece of code without touching the rest of the pipeline code.\\nOnce a team has built a collection of machine learnings pipelines and reusable\\ncomponents, they could start to build the machine learning pipeline from cloning\\nprevious pipeline or tie existing reusable component together. At this stage, the team's\\noverall productivity will be improved significantly.\\nAzure Machine Learning offers different methods to build a pipeline. For users who are\\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\\npipelines by using registered components .\\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\\nfollowing table lists the different pipelines and what they're used for:\\nScenar io Primar y\\npersonaAzur e\\nofferingOSS\\nofferingCanonical\\npipeStrengths\\nModel\\norchestration\\n(Machine\\nlearning)Data\\nscientistAzure\\nMachine\\nLearning\\nPipelinesKubeflow\\nPipelinesData ->\\nModelDistribution,\\ncaching, code-first,\\nreuse\\nData\\norchestrationData\\nengineerAzure Data\\nFactoryApache\\nAirflowData -> Data Strongly typed\\nmovement, data-Which Azure pipeline technology should I use?\\nﾉExpand table\", \"original notebook or Python code, changes the training input from local data to\\nparameterized values, split the training code into multiple steps as needed, perform unit\\ntest of each step, and finally wraps all steps into a pipeline.\\nOnce the teams get familiar with pipelines and want to do more machine learning\\nprojects using pipelines, they'll find the first approach is hard to scale. The second\\napproach is set up a few pipeline templates, each try to solve one specific machine\\nlearning problem. The template predefines the pipeline structure including how many\\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\\nlearning project, the team first forks one template repo. The team leader then assigns\\nmembers which step they need to work on. The data scientists and data engineers do\\ntheir regular work. When they're happy with their result, they structure their code to fit\\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\\nexecuted or automated. If there's any change, each member only needs to work on their\\npiece of code without touching the rest of the pipeline code.\\nOnce a team has built a collection of machine learnings pipelines and reusable\\ncomponents, they could start to build the machine learning pipeline from cloning\\nprevious pipeline or tie existing reusable component together. At this stage, the team's\\noverall productivity will be improved significantly.\\nAzure Machine Learning offers different methods to build a pipeline. For users who are\\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\\npipelines by using registered components .\\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\\nfollowing table lists the different pipelines and what they're used for:\\nScenar io Primar y\\npersonaAzur e\\nofferingOSS\\nofferingCanonical\\npipeStrengths\\nModel\\norchestration\\n(Machine\\nlearning)Data\\nscientistAzure\\nMachine\\nLearning\\nPipelinesKubeflow\\nPipelinesData ->\\nModelDistribution,\\ncaching, code-first,\\nreuse\\nData\\norchestrationData\\nengineerAzure Data\\nFactoryApache\\nAirflowData -> Data Strongly typed\\nmovement, data-Which Azure pipeline technology should I use?\\nﾉExpand table\"]","question":"What are the best practices for managing input and output data for Azure Machine Learning pipelines using Azure Storage?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":33}
{"answer":"To optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training, you can consider the following best practices:\n\n1. **Datastore Configuration**: Use Azure Blob Storage as a datastore for your Azure Machine Learning workspace. This allows you to efficiently access and manage your data.\n\n2. **Data Preparation**: Preprocess and optimize your data before transferring it to Azure Storage. This can include compression, data partitioning, and using efficient file formats.\n\n3. **Data Transfer Optimization**: Leverage Azure's high-speed data transfer options, such as AzCopy or Azure Data Box, to efficiently move large volumes of data to and from Azure Storage.\n\n4. **Compute and Storage Co-location**: Whenever possible, choose compute resources that are geographically close to your Azure Storage account to minimize data transfer latency.\n\n5. **Network Bandwidth**: Consider the network bandwidth available to your compute resources and optimize data transfer accordingly.\n\nBy following these best practices, you can ensure efficient data transfer and storage management for model training in Azure Machine Learning.","documents":"['Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', 'setting up policies to manage your data throughout its lifecycle to archive and delete\\nyour datasets. For more information, see optimize costs by automating Azure Blob\\nStorage access tiers .\\nComputes located in different regions may experience network latency and increased\\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\\nAzure data centers. T o help reduce network costs, deploy all your resources in the\\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\\nin the same region as your data can help lower cost and improve performance.\\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\\neffective to move all resources to Azure to optimize network costs and latency.\\nPlan to manage costs for Azure Machine Learning\\nManage budgets, costs, and quota for Azure Machine Learning at organizational\\nscaleDeploy resources to the same region\\nNext steps', \"12,000 Mbps) would load the same data in ~33 secs (assuming you saturate the NIC at 1500-MB\/s\\nthroughput).\\nStorage tier : For most scenarios - including Large Language Models (LLM) - standard storage provides\\nthe best cost\/performance profile. However, if you have many small files , premium  storage offers a\\nbetter cost\/performance profile. For more information, read Azure S torage options .\\nStorage lo ad: If the storage account is under high load - for example, many GPU nodes in a cluster\\nrequesting data - then you risk hitting the egress capacity of storage. For more information, read\\nStorage load . If you have many small files that need access in parallel, you might hit the request limits\\nof storage. R ead up-to-date information on the limits for both egress capacity and storage requests in\\nScale targets for standard storage accounts .\\nData access p attern in user code : When you use mount mode, data is fetched based on the open\/read\\nactions in your code. For example, when reading random sections of a large file, the default data\\nprefetching settings of mounts can lead to downloads of blocks that won't be read. Y ou might need to\\ntune some settings to reach maximum throughput. For more information, read Optimum mount\\nsettings for common scenarios .\\nTo access the logs of the data runtime from your job:\\n1. Select Outputs+Logs  tab from the job page.\\n2. Select the system_logs  folder, followed by data_cap ability  folder.\\n3. You should see two log files:\\nThe log file data-cap ability .log shows the high-level information about the time spent on key data loading\\ntasks. For example, when you download data, the runtime logs the download activity start and finish times:\\nlog\\nIf the download throughput is a fraction of the expected network bandwidth for the VM size, you can inspect\\nthe log file rslex.log.<TIMEST AMP> . This file contains all the fine-grain logging from the Rust-based\\nruntime; for example, parallelization:\\nlogUsing logs to diagnose issues\\n\\uf80a\\nINFO 2023-05-18 17:14:47,790 sdk_logger.py:44 [28] - ActivityStarted, download\\nINFO 2023-05-18 17:14:50,295 sdk_logger.py:44 [28] - ActivityCompleted: Activity=download,  \\nHowEnded=Success, Duration=2504.39 [ms]\", \"Manage and optimize Azure Machine\\nLearning costs\\nArticle •08\/01\/2023\\nLearn how to manage and optimize costs when training and deploying machine learning\\nmodels to Azure Machine Learning.\\nUse the following tips to help you manage and optimize your compute resource costs.\\nConfigure your training clusters for autoscaling\\nSet quotas on your subscription and workspaces\\nSet termination policies on your training job\\nUse low-priority virtual machines (VM)\\nSchedule compute instances to shut down and start up automatically\\nUse an Azure R eserved VM Instance\\nTrain locally\\nParallelize training\\nSet data retention and deletion policies\\nDeploy resources to the same region\\nFor information on planning and monitoring costs, see the plan to manage costs for\\nAzure Machine Learning  guide.\\nWith constantly changing data, you need fast and streamlined model training and\\nretraining to maintain accurate models. However, continuous training comes at a cost,\\nespecially for deep learning models on GPUs.） Impor tant\\nItems marked (preview) in this article are currently in public preview. The preview\\nversion is provided without a service level agreement, and it's not recommended\\nfor production workloads. Certain features might not be supported or might have\\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\\nMicrosoft Azur e Previews .\\nUse Azure Machine Learning compute cluster\\n(AmlCompute)\", \"Data encryption with Azure Machine\\nLearning\\nArticle •01\/31\/2024\\nAzure Machine Learning relies on various Azure data storage services and compute\\nresources when you're training models and performing inferences. In this article, learn\\nabout the data encryption for each service both at rest and in transit.\\nFor production-grade encryption during training, we recommend that you use an Azure\\nMachine Learning compute cluster. For production-grade encryption during inference,\\nwe recommend that you use Azure Kubernetes Service (AKS).\\nAn Azure Machine Learning compute instance is a dev\/test environment. When you use\\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\\nStore your data in a datastore.\\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\\nmethods for such services.\\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\\naccount (default storage account) that's tied to the Azure Machine Learning workspace\\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\\nMicrosoft-managed keys.\\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\\nTraining data is typically also stored in Azure Blob S torage so that training compute\\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\\nis mounted to compute targets as a remote file system.\\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\\nAzure Blob Storage\"]","question":"How can you optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":34}
{"answer":"When integrating Azure Machine Learning with Azure Storage, several considerations for data privacy and compliance should be taken into account. These include encryption at rest and in transit, access control, and compliance with industry-specific regulations such as GDPR or HIPAA. Azure Machine Learning provides encryption at rest for data stored in Azure Blob Storage, and it's recommended to use Azure Key Vault for managing customer-managed keys. Additionally, access control mechanisms should be implemented to ensure that only authorized users and services can access the data. Finally, compliance with industry-specific regulations should be ensured by following best practices and leveraging Azure's compliance offerings.","documents":"[\"Data encryption with Azure Machine\\nLearning\\nArticle •01\/31\/2024\\nAzure Machine Learning relies on various Azure data storage services and compute\\nresources when you're training models and performing inferences. In this article, learn\\nabout the data encryption for each service both at rest and in transit.\\nFor production-grade encryption during training, we recommend that you use an Azure\\nMachine Learning compute cluster. For production-grade encryption during inference,\\nwe recommend that you use Azure Kubernetes Service (AKS).\\nAn Azure Machine Learning compute instance is a dev\/test environment. When you use\\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\\nStore your data in a datastore.\\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\\nmethods for such services.\\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\\naccount (default storage account) that's tied to the Azure Machine Learning workspace\\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\\nMicrosoft-managed keys.\\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\\nTraining data is typically also stored in Azure Blob S torage so that training compute\\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\\nis mounted to compute targets as a remote file system.\\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\\nAzure Blob Storage\", 'Data administration\\nArticle •09\/26\/2023\\nLearn how to manage data access and how to authenticate in Azure Machine Learning\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (current)  \\n  Python SDK azure-ai-ml v2\\n(current)\\nIn general, data access from studio involves these checks:\\nWhich user wants to access the resources?\\nDepending on the storage type, different types of authentication are available,\\nfor example\\naccount key\\ntoken\\nservice principal\\nmanaged identity\\nuser identity\\nFor authentication based on a user identity, you must know which  specific user\\ntried to access the storage resource. For more information about user\\nauthentication, see authentication for Azure Machine Learning . For more\\ninformation about service-level authentication, see authentication between\\nAzure Machine Learning and other services .\\nDoes this user have permission?\\nDoes the user have the correct credentials? If yes, does the service principal,\\nmanaged identity, etc., have the necessary permissions for that storage\\nresource? P ermissions are granted using Azure role-based access controls\\n(Azure RBA C).\\nThe storage account Reader  reads the storage metadata.\\nThe Storage Blob Data R eader  reads data within a blob container.\\nThe Contributor  allows write access to a storage account.\\nMore roles may be required, depending on the type of storage.\\nWhere does the access come from?\\nUser: Is the client IP address in the VNet\/subnet range?\\n） Impor tant\\nThis article is intended for Azure administrators who want to create the required\\ninfrastructure for an Azure Machine Learning solution.', 'Azure Machine Learning uses various compute resources and datastores on the Azure\\nplatform. T o learn more about how each of these resources supports data encryption at\\nrest and in transit, see Data encryption with Azure Machine Learning .\\nAzure Machine Learning has several inbound and outbound network dependencies.\\nSome of these dependencies can expose a data exfiltration risk by malicious agents\\nwithin your organization. These risks are associated with the outbound requirements to\\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\\nMicrosoft Defender for Cloud  provides unified security management and advanced\\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\\nFor more information, see Introduction to Microsoft Defender for container registries\\nand Introduction to Microsoft Defender for Kubernetes .\\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\\nas whether your Azure Machine Learning workspace uses a private endpoint.\\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\\nmanage Azure Machine Learning .\\nAzure Machine Learning best practices for enterprise security\\nUse Azure Machine Learning with Azure Firewall\\nUse Azure Machine Learning with Azure Virtual Network\\nEncrypt data at rest and in transit\\nBuild a real-time recommendation API on AzurePrevent data exfiltration\\nScan for vulnerabilities\\nAudit and manage compliance\\nNext steps', 'Learn more about enterprise security in Azure Machine Learning\\nLearn about data administration\\nLearn about managed identities on compute cluster .', \"Data concepts in Azure Machine\\nLearning\\nArticle •07\/13\/2023\\nWith Azure Machine Learning, you can import data from a local machine or an existing\\ncloud-based storage resource. This article describes key Azure Machine Learning data\\nconcepts.\\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\\naccount. An Azure Machine Learning datastore offers these benefits:\\nA common, easy-to-use API that interacts with different storage types\\n(Blob\/Files\/ADLS).\\nEasier discovery of useful datastores in team operations.\\nFor credential-based access (service principal\/SAS\/key), Azure Machine Learning\\ndatastore secures connection information. This way, you won't need to place that\\ninformation in your scripts.\\nWhen you create a datastore with an existing Azure storage account, you can choose\\nbetween two different authentication methods:\\nCredential-b ased  - authenticate data access with a service principal, shared access\\nsignature (SAS) token, or account key. Users with Reader  workspace access can\\naccess the credentials.\\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\\nauthenticate data access.\\nThe following table summarizes the Azure cloud-based storage services that an Azure\\nMachine Learning datastore can create. Additionally, the table summarizes the\\nauthentication types that can access those services:\\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\\nAzure Blob Container ✓ ✓\\nAzure File Share ✓\\nAzure Data Lake Gen1 ✓ ✓\\nAzure Data Lake Gen2 ✓ ✓Datastore\"]","question":"What considerations should be taken into account for data privacy and compliance when integrating Azure Machine Learning with Azure Storage?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":35}
{"answer":"To securely access data in Azure Storage from Azure Machine Learning using managed identities, you can configure your compute resources to use a managed identity. This allows the compute to access the storage without needing to manage credentials.","documents":"['You can connect to storage services via identity-based data access with Azure Machine\\nLearning datastores .\\nWhen you use identity-based data access, Azure Machine Learning prompts you for\\nyour Microsoft Entra token for data access authentication instead of keeping your\\ncredentials in the datastore. That approach allows for data access management at the\\nstorage level and keeps credentials confidential.\\nThe same behavior applies when you work with data interactively via a Jupyter\\nNotebook on your local computer or compute instance .\\nTo help ensure that you securely connect to your storage service on Azure, Azure\\nMachine Learning requires that you have permission to access the corresponding data\\nstorage.\\nIdentity-based data access supports connections to only the following storage services.\\nAzure Blob S torage\\nAzure Data Lake S torage Gen1\\nAzure Data Lake S torage Gen2\\nTo access these storage services, you must have at least Storage Blob Data R eader\\naccess to the storage account. Only storage account owners can change your access\\nlevel via the Azure portal .７ Note\\nCredentials stored via credential-based authentication include subscription IDs,\\nshared access signature (SAS) tokens, and storage access key and service principal\\ninformation, like client IDs and tenant IDs.\\n２ Warning\\nCross tenant access to storage accounts is not supported. If cross tenant access is\\nneeded for your scenario, please reach out to the Azure Machine Learning Data\\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\\ncode solution.\\nAccess data for training jobs on compute using managed\\nidentity', \"Certain machine learning scenarios involve working with private data. In such cases, data\\nscientists may not have direct access to data as Microsoft Entra users. In this scenario,\\nthe managed identity of a compute can be used for data access authentication. In this\\nscenario, the data can only be accessed from a compute instance or a machine learning\\ncompute cluster executing a training job. With this approach, the admin grants the\\ncompute instance or compute cluster managed identity S torage Blob Data R eader\\npermissions on the storage. The individual data scientists don't need to be granted\\naccess.\\nTo enable authentication with compute managed identity:\\nCreate compute with managed identity enabled. See the compute cluster  section,\\nor for compute instance, the Assign managed identity  section.\\nGrant compute managed identity at least S torage Blob Data R eader role on the\\nstorage account.\\nCreate any datastores with identity-based authentication enabled. See Create\\ndatastores .\\nOnce the identity-based authentication is enabled, the compute managed identity is\\nused by default when accessing data within your training jobs. Optionally, you can\\nauthenticate with user identity using the steps described in next section.\\nFor information on using configuring Azure RBA C for the storage, see role-based access\\ncontrols .\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (curr ent)\\nWhen training on Azure Machine Learning compute clusters , you can authenticate to\\nstorage with your user Microsoft Entra token.\\nThis authentication mode allows you to:７ Note\\nThe name of the created system managed identity for compute instance or cluster\\nwill be in the format \/workspace-name\/computes\/compute-name in your Microsoft\\nEntra ID.\\nAccess data for training jobs on compute clusters using\\nuser identity\", 'means that in order to successfully read data from storage, the managed identity of the\\ncompute cluster where the deployment is running must have at least Storage Blob Data\\nReader  access to the storage account.\\nTo configure the compute cluster for data access, follow these steps:\\n1. Go to Azure Machine Learning studio .\\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\\ndeployment is using.\\n3. Assign a managed identity to the compute cluster:\\na. In the Managed identity  section, verify if the compute has a managed identity\\nassigned. If not, select the option Edit.\\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\\nusing a S ystem-Assigned Managed Identity, it is named as \"[workspace\\nname]\/computes\/[compute cluster name]\".\\nc. Save the changes.\\n4. Go to the Azure portal  and navigate to the associated storage account where the\\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\\nstorage account where those assets are placed.', 'Learn more about enterprise security in Azure Machine Learning\\nLearn about data administration\\nLearn about managed identities on compute cluster .', 'Data administration\\nArticle •09\/26\/2023\\nLearn how to manage data access and how to authenticate in Azure Machine Learning\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (current)  \\n  Python SDK azure-ai-ml v2\\n(current)\\nIn general, data access from studio involves these checks:\\nWhich user wants to access the resources?\\nDepending on the storage type, different types of authentication are available,\\nfor example\\naccount key\\ntoken\\nservice principal\\nmanaged identity\\nuser identity\\nFor authentication based on a user identity, you must know which  specific user\\ntried to access the storage resource. For more information about user\\nauthentication, see authentication for Azure Machine Learning . For more\\ninformation about service-level authentication, see authentication between\\nAzure Machine Learning and other services .\\nDoes this user have permission?\\nDoes the user have the correct credentials? If yes, does the service principal,\\nmanaged identity, etc., have the necessary permissions for that storage\\nresource? P ermissions are granted using Azure role-based access controls\\n(Azure RBA C).\\nThe storage account Reader  reads the storage metadata.\\nThe Storage Blob Data R eader  reads data within a blob container.\\nThe Contributor  allows write access to a storage account.\\nMore roles may be required, depending on the type of storage.\\nWhere does the access come from?\\nUser: Is the client IP address in the VNet\/subnet range?\\n） Impor tant\\nThis article is intended for Azure administrators who want to create the required\\ninfrastructure for an Azure Machine Learning solution.']","question":"How can you use managed identities to securely access data in Azure Storage from Azure Machine Learning?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":36}
{"answer":"The storage redundancy options in Azure Storage can impact the availability and durability of data used in Azure Machine Learning experiments. By choosing the appropriate redundancy level, you can ensure that your data is highly available and resilient to failures.","documents":"['setting up policies to manage your data throughout its lifecycle to archive and delete\\nyour datasets. For more information, see optimize costs by automating Azure Blob\\nStorage access tiers .\\nComputes located in different regions may experience network latency and increased\\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\\nAzure data centers. T o help reduce network costs, deploy all your resources in the\\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\\nin the same region as your data can help lower cost and improve performance.\\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\\neffective to move all resources to Azure to optimize network costs and latency.\\nPlan to manage costs for Azure Machine Learning\\nManage budgets, costs, and quota for Azure Machine Learning at organizational\\nscaleDeploy resources to the same region\\nNext steps', 'Azure Machine Learning uses various compute resources and datastores on the Azure\\nplatform. T o learn more about how each of these resources supports data encryption at\\nrest and in transit, see Data encryption with Azure Machine Learning .\\nAzure Machine Learning has several inbound and outbound network dependencies.\\nSome of these dependencies can expose a data exfiltration risk by malicious agents\\nwithin your organization. These risks are associated with the outbound requirements to\\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\\nMicrosoft Defender for Cloud  provides unified security management and advanced\\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\\nFor more information, see Introduction to Microsoft Defender for container registries\\nand Introduction to Microsoft Defender for Kubernetes .\\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\\nas whether your Azure Machine Learning workspace uses a private endpoint.\\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\\nmanage Azure Machine Learning .\\nAzure Machine Learning best practices for enterprise security\\nUse Azure Machine Learning with Azure Firewall\\nUse Azure Machine Learning with Azure Virtual Network\\nEncrypt data at rest and in transit\\nBuild a real-time recommendation API on AzurePrevent data exfiltration\\nScan for vulnerabilities\\nAudit and manage compliance\\nNext steps', \"Data encryption with Azure Machine\\nLearning\\nArticle •01\/31\/2024\\nAzure Machine Learning relies on various Azure data storage services and compute\\nresources when you're training models and performing inferences. In this article, learn\\nabout the data encryption for each service both at rest and in transit.\\nFor production-grade encryption during training, we recommend that you use an Azure\\nMachine Learning compute cluster. For production-grade encryption during inference,\\nwe recommend that you use Azure Kubernetes Service (AKS).\\nAn Azure Machine Learning compute instance is a dev\/test environment. When you use\\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\\nStore your data in a datastore.\\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\\nmethods for such services.\\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\\naccount (default storage account) that's tied to the Azure Machine Learning workspace\\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\\nMicrosoft-managed keys.\\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\\nTraining data is typically also stored in Azure Blob S torage so that training compute\\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\\nis mounted to compute targets as a remote file system.\\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\\nAzure Blob Storage\", \"Service Usage Example datanumber of nodes; datastore names and tags, and\\ndescriptions on assets like models; and data label names.\\nAzure AI\\nSearchStores indexes that\\nhelp with querying\\nyour machine learning\\ncontent.These indexes are built on top of the data stored in Azure\\nCosmos DB.\\nAzure\\nStorageStores metadata\\nrelated to Azure\\nMachine Learning\\npipeline data.Data can include designer pipeline names, pipeline layout,\\nand execution properties.\\nFrom the perspective of data lifecycle management, data in the preceding resources is\\ncreated and deleted as you create and delete corresponding objects in Azure Machine\\nLearning.\\nYour Azure Machine Learning workspace reads and writes data by using its managed\\nidentity. This identity is granted access to the resources through a role assignment\\n(Azure role-based access control) on the data resources. The encryption key that you\\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\\nused to create indexes for Azure AI Search at runtime.\\nExtra networking controls are configured when you create a private link endpoint on\\nyour workspace to allow for inbound connectivity. This configuration includes the\\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\\nNetwork access is restricted to only trusted Microsoft services.\\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\\nthe data.\\nWhen you use a customer-managed key, the resources are in your Azure subscription\\nand encrypted with your key. While these resources exist in your subscription, Microsoft\\nmanages them. They're automatically created and configured when you create your\\nAzure Machine Learning workspace.\\nThese Microsoft-managed resources are located in a new Azure resource group that's\\ncreated in your subscription. This resource group is separate from the resource group\\nfor your workspace. It contains the Microsoft-managed resources that your key is used\\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\\nresource group name><GUID>.Customer-managed keys\", 'Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.']","question":"What are the implications of storage redundancy options in Azure Storage on data availability for Azure Machine Learning experiments?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":37}
{"answer":"To automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage, you can leverage Azure's built-in capabilities for data protection and recovery. Azure Blob Storage, which is commonly used for storing datasets in Azure Machine Learning, provides features for data backup and recovery, including versioning, soft delete, and the ability to configure lifecycle management policies to move data to cooler storage tiers or delete it as it ages.\n\nAdditionally, you can use Azure Data Factory to create data pipelines that automate the movement and transformation of data between different storage services, including Azure Blob Storage. By defining data-driven workflows, you can orchestrate the backup and recovery processes for your datasets, ensuring that they are protected and available when needed.\n\nWould you like to explore any specific Azure services or configurations for automating backup and recovery further?","documents":"[\"Data encryption with Azure Machine\\nLearning\\nArticle •01\/31\/2024\\nAzure Machine Learning relies on various Azure data storage services and compute\\nresources when you're training models and performing inferences. In this article, learn\\nabout the data encryption for each service both at rest and in transit.\\nFor production-grade encryption during training, we recommend that you use an Azure\\nMachine Learning compute cluster. For production-grade encryption during inference,\\nwe recommend that you use Azure Kubernetes Service (AKS).\\nAn Azure Machine Learning compute instance is a dev\/test environment. When you use\\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\\nStore your data in a datastore.\\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\\nmethods for such services.\\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\\naccount (default storage account) that's tied to the Azure Machine Learning workspace\\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\\nMicrosoft-managed keys.\\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\\nTraining data is typically also stored in Azure Blob S torage so that training compute\\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\\nis mounted to compute targets as a remote file system.\\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\\nAzure Blob Storage\", 'Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', 'Azure Machine Learning uses various compute resources and datastores on the Azure\\nplatform. T o learn more about how each of these resources supports data encryption at\\nrest and in transit, see Data encryption with Azure Machine Learning .\\nAzure Machine Learning has several inbound and outbound network dependencies.\\nSome of these dependencies can expose a data exfiltration risk by malicious agents\\nwithin your organization. These risks are associated with the outbound requirements to\\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\\nMicrosoft Defender for Cloud  provides unified security management and advanced\\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\\nFor more information, see Introduction to Microsoft Defender for container registries\\nand Introduction to Microsoft Defender for Kubernetes .\\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\\nas whether your Azure Machine Learning workspace uses a private endpoint.\\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\\nmanage Azure Machine Learning .\\nAzure Machine Learning best practices for enterprise security\\nUse Azure Machine Learning with Azure Firewall\\nUse Azure Machine Learning with Azure Virtual Network\\nEncrypt data at rest and in transit\\nBuild a real-time recommendation API on AzurePrevent data exfiltration\\nScan for vulnerabilities\\nAudit and manage compliance\\nNext steps', 'setting up policies to manage your data throughout its lifecycle to archive and delete\\nyour datasets. For more information, see optimize costs by automating Azure Blob\\nStorage access tiers .\\nComputes located in different regions may experience network latency and increased\\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\\nAzure data centers. T o help reduce network costs, deploy all your resources in the\\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\\nin the same region as your data can help lower cost and improve performance.\\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\\neffective to move all resources to Azure to optimize network costs and latency.\\nPlan to manage costs for Azure Machine Learning\\nManage budgets, costs, and quota for Azure Machine Learning at organizational\\nscaleDeploy resources to the same region\\nNext steps', '2. Select your subscription and the workspace you created.\\n3. Select Get star ted.\\n4. In the left pane, select Automat ed ML  under the Author  section.\\n5. Select +New aut omat ed ML job .\\nBefore you configure your experiment, upload your data file to your workspace in the\\nform of an Azure Machine Learning dataset. Doing so, allows you to ensure that your\\ndata is formatted appropriately for your experiment.\\n1. On the Select dataset  form, select From local files  from the +Create dataset  drop-\\ndown.\\na. On the Basic info  form, give your dataset a name and provide an optional\\ndescription. The dataset type should default to Tabular , since automated ML in\\nAzure Machine Learning studio currently only supports tabular datasets.\\nb. Select Next  on the bottom left\\nc. On the Datast ore and file selection  form, select the default datastore that was\\nautomatically set up during your workspace creation, worksp aceblobst ore\\n(Azur e Blob S torage) . This is the storage location where you upload your data\\nfile.\\nd. Select Uplo ad files  from the Uplo ad drop-down.\\ne. Choose the bike-no.csv  file on your local computer. This is the file you\\ndownloaded as a prerequisite .\\nf. Select Next\\nWhen the upload is complete, the Settings and preview form is pre-populated\\nbased on the file type.\\ng. Verify that the Settings and pr eview  form is populated as follows and select\\nNext .Create and load dataset\\nﾉExpand table']","question":"How do you automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":38}
{"answer":"Azure Machine Learning provides cost management features to help you analyze and manage costs. You can use the Azure pricing calculator to plan for costs before adding resources, review estimated costs as you add resources, set budgets, and monitor costs. Additionally, you can review forecasted costs and identify spending trends to manage costs effectively.","documents":"[\"Plan to manage costs for Azure Machine\\nLearning\\nArticle •03\/31\/2023\\nThis article describes how to plan and manage costs for Azure Machine Learning. First,\\nyou use the Azure pricing calculator to help plan for costs before you add any resources.\\nNext, as you add the Azure resources, review the estimated costs.\\nAfter you've started using Azure Machine Learning resources, use the cost management\\nfeatures to set budgets and monitor costs. Also review the forecasted costs and identify\\nspending trends to identify areas where you might want to act.\\nUnderstand that the costs for Azure Machine Learning are only a portion of the monthly\\ncosts in your Azure bill. If you are using other Azure services, you're billed for all the\\nAzure services and resources used in your Azure subscription, including the third-party\\nservices. This article explains how to plan for and manage costs for Azure Machine\\nLearning. After you're familiar with managing costs for Azure Machine Learning, apply\\nsimilar methods to manage costs for all the Azure services used in your subscription.\\nFor more information on optimizing costs, see how to manage and optimize cost in\\nAzure Machine Learning .\\nCost analysis in Cost Management supports most Azure account types, but not all of\\nthem. T o view the full list of supported account types, see Understand Cost\\nManagement data .\\nTo view cost data, you need at least read access for an Azure account. For information\\nabout assigning access to Azure Cost Management data, see Assign access to data .） Impor tant\\nItems marked (preview) in this article are currently in public preview. The preview\\nversion is provided without a service level agreement, and it's not recommended\\nfor production workloads. Certain features might not be supported or might have\\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\\nMicrosoft Azur e Previews .\\nPrerequisites\", \"Use the Azure pricing calculator  to estimate costs before you create the\\nresources in an Azure Machine Learning workspace. On the left, select AI +\\nMachine Learning , then select Azur e Machine Learning  to begin.\\nThe following screenshot shows the cost estimation by using the calculator:\\nAs you add new resources to your workspace, return to this calculator and add the same\\nresource here to update your cost estimates.\\nFor more information, see Azure Machine Learning pricing .\\nAzure Machine Learning runs on Azure infrastructure that accrues costs along with\\nAzure Machine Learning when you deploy the new resource. It's important toEstimate costs before using Azure Machine\\nLearning\\nUnderstand the full billing  model for Azure\\nMachine Learning\", \"Manage and optimize Azure Machine\\nLearning costs\\nArticle •08\/01\/2023\\nLearn how to manage and optimize costs when training and deploying machine learning\\nmodels to Azure Machine Learning.\\nUse the following tips to help you manage and optimize your compute resource costs.\\nConfigure your training clusters for autoscaling\\nSet quotas on your subscription and workspaces\\nSet termination policies on your training job\\nUse low-priority virtual machines (VM)\\nSchedule compute instances to shut down and start up automatically\\nUse an Azure R eserved VM Instance\\nTrain locally\\nParallelize training\\nSet data retention and deletion policies\\nDeploy resources to the same region\\nFor information on planning and monitoring costs, see the plan to manage costs for\\nAzure Machine Learning  guide.\\nWith constantly changing data, you need fast and streamlined model training and\\nretraining to maintain accurate models. However, continuous training comes at a cost,\\nespecially for deep learning models on GPUs.） Impor tant\\nItems marked (preview) in this article are currently in public preview. The preview\\nversion is provided without a service level agreement, and it's not recommended\\nfor production workloads. Certain features might not be supported or might have\\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\\nMicrosoft Azur e Previews .\\nUse Azure Machine Learning compute cluster\\n(AmlCompute)\", 'monthly schedule and set a custom date range. Exporting cost data is the\\nrecommended way to retrieve cost datasets.\\nUse the following tips to help you manage and optimize your compute resource costs.\\nConfigure your training clusters for autoscaling\\nSet quotas on your subscription and workspaces\\nSet termination policies on your training job\\nUse low-priority virtual machines (VM)\\nSchedule compute instances to shut down and start up automatically\\nUse an Azure R eserved VM Instance\\nTrain locally\\nParallelize training\\nSet data retention and deletion policies\\nDeploy resources to the same region\\nDelete instances and clusters if you do not plan on using them in the near future.\\nFor more information, see manage and optimize costs in Azure Machine Learning .\\nManage and optimize costs in Azure Machine Learning .\\nManage budgets, costs, and quota for Azure Machine Learning at organizational\\nscale\\nLearn how to optimize your cloud investment with Azure Cost Management .\\nLearn more about managing costs with cost analysis .\\nLearn about how to prevent unexpected costs .\\nTake the Cost Management  guided learning course.Other ways to manage and reduce costs for\\nAzure Machine Learning\\nNext steps', \"To view Azure Machine Learning costs in cost analysis:\\n1. Sign in to the Azure portal.\\n2. Open the scope in the Azure portal and select Cost analysis  in the menu. For\\nexample, go to Subscriptions , select a subscription from the list, and then select\\nCost analysis  in the menu. Select Scope  to switch to a different scope in cost\\nanalysis.\\n3. By default, cost for services are shown in the first donut chart. Select the area in the\\nchart labeled Azure Machine Learning.\\nActual monthly costs are shown when you initially open cost analysis. Here's an example\\nshowing all monthly usage costs.\\nTo narrow costs for a single service, like Azure Machine Learning, select Add filt er and\\nthen select Service name . Then, select virtual machines .\\nHere's an example showing costs for just Azure Machine Learning.\\n\\uf80a\"]","question":"What tools and techniques are available for analyzing storage costs associated with Azure Machine Learning projects?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":39}
{"answer":"To streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics, data engineers can use Azure Data Factory to orchestrate the movement of data from Azure Storage to Azure Machine Learning. Additionally, they can leverage Azure Event Grid to trigger real-time analytics processes in Azure Machine Learning when new data is ingested into Azure Storage.","documents":"[\"Interactive Data Wrangling with Apache\\nSpark in Azure Machine Learning\\nArticle •10\/05\/2023\\nData wrangling becomes one of the most important steps in machine learning projects.\\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides access\\nto an Apache Spark pool - backed by Azure S ynapse - for interactive data wrangling\\nusing Azure Machine Learning Notebooks.\\nIn this article, you'll learn how to perform data wrangling using\\nServerless Spark compute\\nAttached S ynapse Spark pool\\nAn Azure subscription; if you don't have an Azure subscription, create a free\\naccount  before you begin.\\nAn Azure Machine Learning workspace. See Create workspace resources .\\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an Azure\\nData Lake S torage (ADLS) Gen 2 storage account .\\n(Optional): An Azure K ey Vault. See Create an Azure K ey Vault.\\n(Optional): A Service Principal. See Create a Service Principal .\\n(Optional): An attached S ynapse Spark pool in the Azure Machine Learning\\nworkspace .\\nBefore you start your data wrangling tasks, learn about the process of storing secrets\\nAzure Blob storage account access key\\nShared Access Signature (SAS) token\\nAzure Data Lake S torage (ADLS) Gen 2 service principal information\\nin the Azure K ey Vault. Y ou also need to know how to handle role assignments in the\\nAzure storage accounts. The following sections review these concepts. Then, we'll\\nexplore the details of interactive data wrangling using the Spark pools in Azure Machine\\nLearning Notebooks.Prerequisites\\n\\uea80 Tip\", 'Tutorial Descr iption\\nUpload, access and explore your data in\\nAzure Machine LearningStore large data in the cloud and retrieve it from\\nnotebooks and scripts\\nModel development on a cloud workstation Start prototyping and developing machine\\nlearning models\\nTrain a model in Azure Machine Learning Dive in to the details of training a model\\nDeploy a model as an online endpoint Dive in to the details of deploying a model\\nCreate production machine learning pipelines Split a complete machine learning task into a\\nmultistep workflow.', \"runs from experimentation in the studio user interface. Or they can use versioned assets\\nfor jobs like environments and storage references.\\nFor more information, see Manage Azure Machine Learning workspaces .\\nWhen a project is ready for operationalization, users' work can be automated in an ML\\npipeline and triggered on a schedule or HT TPS request.\\nYou can deploy models to the managed inferencing solution, for both real-time and\\nbatch deployments, abstracting away the infrastructure management typically required\\nfor deploying models.\\nIn Azure Machine Learning, you can run your training script in the cloud or build a\\nmodel from scratch. Customers often bring models they've built and trained in open-\\nsource frameworks so that they can operationalize them in the cloud.\\nData scientists can use models in Azure Machine Learning that they've created in\\ncommon Python frameworks, such as:\\nPyTorch\\nTensorFlow\\nscikit-learn\\nXGBoost\\nLightGBM\\nOther languages and frameworks are also supported:\\nR\\n.NET\\nFor more information, see Open-source integration with Azure Machine Learning .\\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\\nexperience and intuition to select the right data featurization and algorithm for training.\\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\\nLearning studio UI or the Python SDK.Train models\\nOpen and interoperable\\nAutomated featurization and algorithm selection\", \"training and evaluation, the machine learning engineers focus on model deployment\\nand automation of the entire workflow. By leveraging machine learning pipeline, each\\nteam only needs to work on building their own steps. The best way of building steps is\\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\\none step in a machine learning pipeline. All these steps built by different users are finally\\nintegrated into one workflow through the pipeline definition. The pipeline is a\\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\\nits steps can be standardized by each company's preferred DevOps practice. The\\npipeline can be further versioned and automated. If the ML projects are described as a\\npipeline, then the best ML Ops practice is already applied.\\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\\nimproves large model training's efficiency and reduces cost. T aking modern natural\\nlanguage model training as an example. It requires pre-processing large amounts of\\ndata and GPU intensive transformer model training. It takes hours to days to train a\\nmodel each time. When the model is being built, the data scientist wants to test\\ndifferent training code or hyperparameters and run the training many times to get the\\nbest model performance. For most of these trainings, there's usually small changes from\\none training to another one. It will be a significant waste if every time the full training\\nfrom data processing to model training takes place. By using machine learning pipeline,\\nit can automatically calculate which steps result is unchanged and reuse outputs from\\nprevious training. Additionally, the machine learning pipeline supports running each\\nstep on different computation resources. Such that, the memory heavy data processing\\nwork and run-on high memory CPU machines, and the computation intensive training\\ncan run on expensive GPU machines. By properly choosing which step to run on which\\ntype of machines, the training cost can be significantly reduced.\\nDepending on what a machine learning project already has, the starting point of\\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\\nbuilding a pipeline.\\nThe first approach usually applies to the team that hasn't used pipeline before and\\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\\ntypically have developed some machine learning models on their local environment\\nusing their favorite tools. Machine learning engineers need to take data scientists'\\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\\nGetting  started best practices\", \"training and evaluation, the machine learning engineers focus on model deployment\\nand automation of the entire workflow. By leveraging machine learning pipeline, each\\nteam only needs to work on building their own steps. The best way of building steps is\\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\\none step in a machine learning pipeline. All these steps built by different users are finally\\nintegrated into one workflow through the pipeline definition. The pipeline is a\\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\\nits steps can be standardized by each company's preferred DevOps practice. The\\npipeline can be further versioned and automated. If the ML projects are described as a\\npipeline, then the best ML Ops practice is already applied.\\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\\nimproves large model training's efficiency and reduces cost. T aking modern natural\\nlanguage model training as an example. It requires pre-processing large amounts of\\ndata and GPU intensive transformer model training. It takes hours to days to train a\\nmodel each time. When the model is being built, the data scientist wants to test\\ndifferent training code or hyperparameters and run the training many times to get the\\nbest model performance. For most of these trainings, there's usually small changes from\\none training to another one. It will be a significant waste if every time the full training\\nfrom data processing to model training takes place. By using machine learning pipeline,\\nit can automatically calculate which steps result is unchanged and reuse outputs from\\nprevious training. Additionally, the machine learning pipeline supports running each\\nstep on different computation resources. Such that, the memory heavy data processing\\nwork and run-on high memory CPU machines, and the computation intensive training\\ncan run on expensive GPU machines. By properly choosing which step to run on which\\ntype of machines, the training cost can be significantly reduced.\\nDepending on what a machine learning project already has, the starting point of\\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\\nbuilding a pipeline.\\nThe first approach usually applies to the team that hasn't used pipeline before and\\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\\ntypically have developed some machine learning models on their local environment\\nusing their favorite tools. Machine learning engineers need to take data scientists'\\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\\nGetting  started best practices\"]","question":"How can data engineers streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":40}
{"answer":"To use Managed Service Identity (MSI) with Azure Storage, you can assign the necessary permissions to the MSI and then access the storage resources using the MSI's credentials. This allows you to avoid storing credentials in your code or configuration files.","documents":"[\"In many cases, you can upload local files. However, Azure S torage Explorer  provides a\\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\\nExplorer as the default way to move files.\\nTo create a dataset from data that's already stored in Blob S torage:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Choose the Dataset type :\\nIf you're using a .csv or .tsv file and each row contains a response, select\\nTabular .\\nIf you're using separate .txt files for each response, select File.\\n4. Select Next .\\n5. Select From Azur e storage , and then select Next .\\n6. Select the datastore, and then select Next .\\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\\nTo include all the files in the subfolders of the selected path, append \/** to\\nthe path.\\nTo include all the data in the current container and its subfolders, append\\n**\/*.* to the path.\\n8. Select Create.\\n9. Select the data asset you created.\\nTo directly upload your data:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Choose the Dataset type :\\nIf you're using a .csv or .tsv file and each row contains a response, select\\nTabular .\\nIf you're using separate .txt files for each response, select File.\\n4. Select Next .\\n5. Select From local files , and then select Next .\\n6. (Optional) Select a datastore. The default uploads to the default blob store\\n(worksp aceblobst ore) for your Machine Learning workspace.\\n7. Select Next .\\nCreate a dataset from uploaded data\", 'Service tag Protocol Port\\nStorage.<region> TCP 443\\nFor more information, see How to secure training environments  and Configure inbound\\nand outbound network traffic .\\nUse the following steps to enable a storage endpoint for the subnet that contains your\\nAzure Machine Learning compute clusters and compute instances:\\n1. From the Azure portal , select the Azur e Vir tual Netw ork for your Azure Machine\\nLearning workspace.\\n2. From the left of the page, select Subnets  and then select the subnet that contains\\nyour compute cluster and compute instance.\\n3. In the form that appears, expand the Services  dropdown and then enable\\nMicrosoft.Storage . Select Save to save these changes.\\n4. Apply the service endpoint policy to your workspace subnet.７ Note\\nFor the storage outbound, a Service Endpoint P olicy will be applied in a later\\nstep to limit outbound traffic.\\n3. Enable storage endpoint for the subnet\\n\\uf80a', \"In many cases, you can upload local files. However, Azure S torage Explorer  provides a\\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\\nExplorer as the default way to move files.\\nTo create a dataset from data that's already stored in Blob S torage:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\\nimages.\\n4. Select Next .\\n5. Select From Azur e storage , and then select Next .\\n6. Select the datastore, and then select Next .\\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\\nTo include all the files in the subfolders of the selected path, append \/** to\\nthe path.\\nTo include all the data in the current container and its subfolders, append\\n**\/*.* to the path.\\n8. Select Create.\\n9. Select the data asset you created.\\nTo directly upload your data:\\n1. Select Create.\\n2. For Name , enter a name for your dataset. Optionally, enter a description.\\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\\nimages.\\n4. Select Next .\\n5. Select From local files , and then select Next .\\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\\n7. Select Next .\\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\\nfolders to upload.\\n9. In the browser window, find your files or folders, and then select Open .\\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\\nCreate a dataset from uploaded data\", 'b. For Connection name , keep the default populated value.\\nc. For Authentication kind , select Organizational account  to use the credentials\\nof the connected user via O Auth 2.0.\\nd. Select Sign in  to sign in.\\n8. Select Next .\\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\\nthis setting to configure the folder that the shortcut will point to.\\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\\nIn this example, name the shortcut datasets .\\n11. Save the changes.\\n1. Open the Azure Machine Learning studio .\\n2. Go to your Azure Machine Learning workspace.\\n3. Go to the Data  section.\\n4. Select the Datast ores tab.\\n5. Select Create.\\n6. Configure the datastore as follows:\\na. For Datast ore name , enter trust ed_blob .\\nb. For Datast ore type  select Azur e Blob S torage .\\nc. Select the storage account from the wizard, using the Subscription ID , Storage\\naccount , and Blob container  (file system).Create a datastore that points to the storage account\\n\\uea80 Tip\\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\\nGen2 ? Batch endpoints can only write predictions to Blob S torage\\naccounts. However, every Azure Data Lake Gen2 storage account is also a\\nblob storage account; therefore, they can be used interchangeably.', 'b. For Connection name , keep the default populated value.\\nc. For Authentication kind , select Organizational account  to use the credentials\\nof the connected user via O Auth 2.0.\\nd. Select Sign in  to sign in.\\n8. Select Next .\\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\\nthis setting to configure the folder that the shortcut will point to.\\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\\nIn this example, name the shortcut datasets .\\n11. Save the changes.\\n1. Open the Azure Machine Learning studio .\\n2. Go to your Azure Machine Learning workspace.\\n3. Go to the Data  section.\\n4. Select the Datast ores tab.\\n5. Select Create.\\n6. Configure the datastore as follows:\\na. For Datast ore name , enter trust ed_blob .\\nb. For Datast ore type  select Azur e Blob S torage .\\nc. Select the storage account from the wizard, using the Subscription ID , Storage\\naccount , and Blob container  (file system).Create a datastore that points to the storage account\\n\\uea80 Tip\\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\\nGen2 ? Batch endpoints can only write predictions to Blob S torage\\naccounts. However, every Azure Data Lake Gen2 storage account is also a\\nblob storage account; therefore, they can be used interchangeably.']","question":"How can I using MSI with Storage?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":41}
{"answer":"To browse data in your Azure Machine Learning workspace, you can use the Datastores feature. Datastores are references to Azure Storage services or other supported storage services. You can register datastores to your workspace and then use them to access and manage your data.","documents":"[\"You'll need the following items:\\nTo use Azure Machine Learning, you'll first need a workspace. If you don't have\\none, complete Create resources you need to get started  to create a workspace and\\nlearn more about using it.\\nRun one or more jobs in your workspace to have results available in the\\ndashboard. Complete Tutorial: T rain a model in Azure Machine Learning  if you\\ndon't have any jobs yet.\\nEnable this preview feature via the preview panel.\\nNext, let's view your jobs in the studio:\\nSelect Jobs on the left side navigation panel.\\nSelect either All experiments  to view all the jobs in an experiment or select All jobs\\nto view all the jobs submitted in the workspace.\\nYou are now on the default dashboard view where you will find your job list\\nconsolidated into the left side bar and dashboard content on the right.Items marked (preview) in this article are currently in public preview. The preview\\nversion is provided without a service level agreement, and it's not recommended\\nfor production workloads. Certain features might not be supported or might have\\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\\nMicrosoft Azur e Previews .\\nPrerequisites\\nExplore the dashboard view\", \"The following features of Azure Machine Learning studio use data profiling:\\nDataset: Explore the dataset from the studio.\\nDesigner: Visualize module output data.\\nAutoML: View a data preview or profile and choose a target column.\\nLabeling: Use labels to prepare data for a machine learning project.\\nData profiling depends on the ability of the Azure Machine Learning managed service to\\naccess the default Azure storage account for your workspace. The managed service\\ndoesn 't exist in y our vir tual netw ork, so it can't directly access the storage account in the\\nvirtual network. Instead, the workspace uses a service principal to access storage.\\nTo allow access to the storage account, configure the storage account to allow a\\nresource instance for your workspace or select Allow Azur e ser vices on the trust edScenario: Use AutoML, the designer, the\\ndataset, and the datastore from the studio\\n\\uea80 Tip\\nYou can provide a service principal when you're creating the workspace. If you\\ndon't, one is created for you and has the same name as your workspace.\", \"You'll need the following items:\\nTo use Azure Machine Learning, you'll first need a workspace. If you don't have\\none, complete Create resources you need to get started  to create a workspace and\\nlearn more about using it.\\nRun one or more jobs in your workspace to have results available in the\\ndashboard. Complete Tutorial: T rain a model in Azure Machine Learning  if you\\ndon't have any jobs yet.\\nEnable this preview feature via the preview panel.\\nSelect Jobs on the left side navigation panel.\\nSelect either All experiments  to view all the jobs in an experiment or select All jobs\\nto view all the jobs submitted in the workspace.\\nSelect List view  at the top to switch into List view .\\nThe job display name is an optional and customizable name that you can provide for\\nyour job. Y ou can edit this directly in your jobs list view by selecting the pencil icon\\nwhen you move your mouse over a job name.\\nCustomizing the name may help you organize and label your training jobs easily.Prerequisites\\nView jobs list\\nJob display name\\nSelect and pin columns\", 'Azure Machine Learning extension to view, update and create workspace assets like\\ncomputes, data, environments, jobs and more.\\nWhen you use VS Code for the W eb, the latest versions of these extensions are\\nautomatically available to you. If you use the desktop application , you may need to\\ninstall them.\\nWhen you launch VS Code connected to a compute instance  for the first time, make\\nsure you follow these steps and take a few moments to orient yourself to the tools in\\nyour integrated development environment.\\n1. Locate the Azure extension and sign in\\n2. Once your subscriptions are listed, you can filter to the ones you use frequently.\\nYou can also pin workspaces you use most often within the subscriptions.\\n3. The workspace you launched the VS Code remote connection from (the workspace\\nthe compute instance is in) should be automatically set as the default. Y ou can\\nupdate the default workspace from the VS Code status bar.\\n4. If you plan to use the Azure Machine Learning CLI, open a terminal from the menu,\\nand sign in to the Azure Machine Learning CLI using az login --identity.', \"2. Expand your workspace node.\\n3. Expand the Envir onments  node.\\n4. Right-click the environment you want to view and select View Envir onment .\\nAlternatively, use the > Azure ML: View Environment command in the command palette.\\nThe quickest way to create a job is by clicking the Create Job  icon in the extension's\\nactivity bar.\\nUsing the resource nodes in the Azure Machine Learning view:\\n1. Expand the subscription node that contains your workspace.\\n2. Expand your workspace node.\\n3. Right-click the Experiments  node in your workspace and select Create Job .\\n4. Choose your job type.\\n5. A specification file appears. Configure the specification file.\\n6. Right-click the specification file and select Azur eML: Ex ecute YAML .\\nAlternatively, use the > Azure ML: Create Job command in the command palette.\\nTo view your job in Azure Machine Learning studio:\\n1. Expand the subscription node that contains your workspace.\\n2. Expand the Experiments  node inside your workspace.\\n3. Right-click the experiment you want to view and select View Experiment in S tudio .\\n4. A prompt appears asking you to open the experiment URL in Azure Machine\\nLearning studio. Select Open .\\nAlternatively, use the > Azure ML: View Experiment in Studio command respectively in\\nthe command palette.\\nAs you're running your job, you may want to see its progress. T o track the progress of a\\njob in Azure Machine Learning studio from the extension:\\n1. Expand the subscription node that contains your workspace.\\n2. Expand the Experiments  node inside your workspace.\\n3. Expand the job node you want to track progress for.Create job\\nView job\\nTrack job progress\"]","question":"How can I browse data in the AML workspace?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":42}
{"answer":"To run Spark jobs and integrate with ADLS Gen2 in Azure Machine Learning, you need to follow these steps:\n\n1. **Set Up Prerequisites:**\n   - Create an Azure Machine Learning workspace.\n   - Create an Azure Data Lake Storage Gen2 (ADLS Gen2) storage account.\n   - Create an Azure Machine Learning compute instance.\n\n2. **Ensure Access to Data Paths:**\n   - Assign Contributor and Storage Blob Data Contributor roles to the user identity for read and write access.\n\n3. **Submit a Spark Job:**\n   - Use Azure Machine Learning serverless Spark compute.\n   - Specify the ADLS Gen2 storage account as the data source and destination for the job.\n\n4. **Monitor and Manage Jobs:**\n   - Track job status and logs through the Azure Machine Learning workspace.\n\nLet me know if you need more detailed information on any of these steps.","documents":"[\"Quickstart: Apache Spark jobs in Azure\\nMachine Lea rning\\nArticle •05\/23\/2023\\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides easy\\naccess to distributed computing capability - backed by Azure S ynapse - for scaling\\nApache Spark jobs on Azure Machine Learning.\\nIn this quickstart guide, you learn how to submit a Spark job using Azure Machine\\nLearning serverless Spark compute, Azure Data Lake S torage (ADLS) Gen 2 storage\\naccount, and user identity passthrough in a few simple steps.\\nFor more information about Apache Sp ark in Azur e Machine Learning  concepts, see\\nthis resource .\\nAPPLIES T O: \\n  Azure CLI ml extension v2 (curr ent)\\nAn Azure subscription; if you don't have an Azure subscription, create a free\\naccount  before you begin.\\nAn Azure Machine Learning workspace. See Create workspace resources .\\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an\\nAzure Data Lake S torage (ADLS) Gen 2 storage account .\\nCreate an Azure Machine Learning compute instance .\\nInstall Azure Machine Learning CLI .\\nBefore we submit an Apache Spark job, we must ensure that input, and output, data\\npaths are accessible. Assign Contribut or and Storage Blob Data Contribut or roles to\\nthe user identity of the logged-in user to enable read and write access.\\nTo assign appropriate roles to the user identity:Prerequisites\\nCLI\\nAdd role assignments in Azure storage\\naccounts\", '16. Repeat steps 2-13 for Storage Blob Contribut or role assignment.\\nData in the Azure Data Lake S torage (ADLS) Gen 2 storage account should become\\naccessible once the user identity has appropriate roles assigned.\\nA Spark job requires a Python script that takes arguments, which can be developed by\\nmodifying the Python code developed from interactive data wrangling . A sample Python\\nscript is shown here.\\nPython\\n\\uf80a\\nCreate parametrized Python code\\n# titanic.py  \\nimport argparse  \\nfrom operator import add \\nimport pyspark.pandas as pd \\nfrom pyspark.ml.feature import Imputer  \\nparser = argparse.ArgumentParser()  \\nparser.add_argument( \"--titanic_data\" ) \\nparser.add_argument( \"--wrangled_data\" ) \\nargs = parser.parse_args()  \\nprint(args.wrangled_data)  \\nprint(args.titanic_data)  \\ndf = pd.read_csv(args.titanic_data, index_col= \"PassengerId\" ) \\nimputer = Imputer(inputCols=[ \"Age\"], outputCol= \"Age\").setStrategy(\\n    \"mean\" \\n)  # Replace missing values in Age column with the mean value  \\ndf.fillna(  \\n    value={ \"Cabin\": \"None\"}, inplace= True \\n)  # Fill Cabin column with value \"None\" if missing', \"Interactive Data Wrangling with Apache\\nSpark in Azure Machine Learning\\nArticle •10\/05\/2023\\nData wrangling becomes one of the most important steps in machine learning projects.\\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides access\\nto an Apache Spark pool - backed by Azure S ynapse - for interactive data wrangling\\nusing Azure Machine Learning Notebooks.\\nIn this article, you'll learn how to perform data wrangling using\\nServerless Spark compute\\nAttached S ynapse Spark pool\\nAn Azure subscription; if you don't have an Azure subscription, create a free\\naccount  before you begin.\\nAn Azure Machine Learning workspace. See Create workspace resources .\\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an Azure\\nData Lake S torage (ADLS) Gen 2 storage account .\\n(Optional): An Azure K ey Vault. See Create an Azure K ey Vault.\\n(Optional): A Service Principal. See Create a Service Principal .\\n(Optional): An attached S ynapse Spark pool in the Azure Machine Learning\\nworkspace .\\nBefore you start your data wrangling tasks, learn about the process of storing secrets\\nAzure Blob storage account access key\\nShared Access Signature (SAS) token\\nAzure Data Lake S torage (ADLS) Gen 2 service principal information\\nin the Azure K ey Vault. Y ou also need to know how to handle role assignments in the\\nAzure storage accounts. The following sections review these concepts. Then, we'll\\nexplore the details of interactive data wrangling using the Spark pools in Azure Machine\\nLearning Notebooks.Prerequisites\\n\\uea80 Tip\", 'Standard_E4s_v3\\nStandard_E8s_v3\\nStandard_E16s_v3\\nStandard_E32s_v3\\nStandard_E64s_v3\\n4. Input a Spark Session timeout  value, in minutes.\\n5. Select whether to Dynamically allocat e executors\\n6. Select the number of Executors for the Spark session.\\n7. Select Executor size  from the dropdown menu.\\n8. Select Driver size  from the dropdown menu.\\n9. To use a Conda file to configure a Spark session, check the Uplo ad conda file\\ncheckbox. Then, select Browse , and choose the Conda file with the Spark session\\nconfiguration you want.\\n10. Add Configuration settings  properties, input values in the Proper ty and Value\\ntextboxes, and select Add.\\n11. Select Apply .\\n12. Select Stop session  in the Configur e new session?  pop-up.\\nThe session configuration changes persist and become available to another notebook\\nsession that is started using the serverless Spark compute.\\nYou can access and wrangle data stored in Azure Data Lake S torage (ADLS) Gen 2\\nstorage accounts with abfss:\/\/ data URIs following one of the two data access\\nmechanisms:\\nUser identity passthrough\\nService principal-based data access\\uea80 Tip\\nIf you use session-level Conda packages, you can impr ove the Spark session cold\\nstart time if you set the configuration variable spark.hadoop.aml.enable_cache to\\ntrue.\\nImport and wrangle data from Azure Data Lake Storage\\n(ADLS) Gen 2\\n\\uea80 Tip', 'Install and use the CLI (v2)\\nSubmit Spark jobs in Azure Machine Learning    outputs:  \\n      wrangled_data:  \\n        type:  uri_folder  \\n        path:  azureml:\/\/datastores\/workspaceblobstore\/paths\/data\/wrangled\/  \\n        mode:  direct \\n    identity:  \\n      type:  user_identity  \\n    compute:  mysparkcompute  \\nNext steps']","question":"How to use run spark jobs and integrate with ADLS Gen2?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":43}
{"answer":"Batch inferencing in parallel mode involves running inference on multiple mini-batches simultaneously across multiple workers. This can be achieved by distributing the workload across multiple nodes or instances, with each worker processing a different mini-batch. The number of workers and the mini-batch size are key factors to consider when configuring parallel batch inferencing.","documents":"[\"Your deployment configuration controls the size of each mini-batch and the number of\\nworkers on each node. T ake into account them when deciding if you want to read the\\nentire mini-batch to perform inference, or if you want to run inference file by file, or row\\nby row (for tabular). See Running inference at the mini-batch, file or the row level  to see\\nthe different approaches.\\nWhen running multiple workers on the same instance, take into account that memory is\\nshared across all the workers. Usually, increasing the number of workers per node\\nshould be accompanied by a decrease in the mini-batch size or by a change in the\\nscoring strategy (if data size and compute SKU remains the same).\\nBatch endpoints will call the run() function in your scoring script once per mini-batch.\\nHowever, you will have the power to decide if you want to run the inference over the\\nentire batch, over one file at a time, or over one row at a time (if your data happens to\\nbe tabular).\\nYou will typically want to run inference over the batch all at once when you want to\\nachieve high throughput in your batch scoring process. This is the case for instance if\\nyou run inference over a GPU where you want to achieve saturation of the inference\\ndevice. Y ou may also be relying on a data loader that can handle the batching itself if\\ndata doesn't fit on memory, like TensorFlow or PyTorch data loaders. On those cases,\\nyou may want to consider running inference on the entire batch.\\nFor an example about how to achieve it, see High throughput deployments . This\\nexample processes an entire batch of files at a time.Running inference at the mini-batch, file or the row level\\nMini-batch level\\n２ Warning\\nRunning inference at the batch level may require having high control over the input\\ndata size to be able to correctly account for the memory requirements and avoid\\nout of memory exceptions. Whether you are able or not of loading the entire mini-\\nbatch in memory will depend on the size of the mini-batch, the size of the instances\\nin the cluster, the number of workers on each node, and the size of the mini-batch.\\nFile level\", 'One of the easiest ways to perform inference is by iterating over all the files in the mini-\\nbatch and run your model over it. In some cases, like image processing, this may be a\\ngood idea. If your data is tabular, you may need to make a good estimation about the\\nnumber of rows on each file to estimate if your model is able to handle the memory\\nrequirements to not just load the entire data into memory but also to perform inference\\nover it. R emember that some models (specially those based on recurrent neural\\nnetworks) will unfold and present a memory footprint that may not be linear with the\\nnumber of rows. If your model is expensive in terms of memory, please consider running\\ninference at the row level.\\nFor an example about how to achieve it see Image processing with batch deployments .\\nThis example processes a file at a time.\\nFor models that present challenges in the size of their inputs, you may want to consider\\nrunning inference at the row level. Y our batch deployment will still provide your scoring\\nscript with a mini-batch of files, however, you will read one file, one row at a time. This\\nmay look inefficient but for some deep learning models may be the only way to perform\\ninference without scaling up your hardware requirements.\\nFor an example about how to achieve it see Text processing with batch deployments .\\nThis example processes a row at a time.\\nThe environment variable AZUREML_MODEL_DIR contains the path to where the selected\\nmodel is located and it is typically used in the init() function to load the model into\\nmemory. However, some models may contain their files inside of a folder and you may\\nneed to account for that when loading them. Y ou can identify the folder structure of\\nyour model as follows:\\n1. Go to Azure Machine Learning portal .\\n2. Go to the section Models .\\uea80 Tip\\nIf file sizes are too big to be readed even at once, please consider breaking down\\nfiles into multiple smaller files to account for better parallelization.\\nRow level (tabular)\\nUsing models that are folders', \"Batch scoring, or batch infer encing , involves invoking an endpoint with a reference to\\ndata. The batch endpoint runs jobs asynchronously to process data in parallel on\\ncompute clusters and store the data for further analysis.\\nReal-time s coring, or online infer encing , involves invoking an endpoint with one or more\\nmodel deployments and receiving a response in near real time via HT TPS. T raffic can be\\nsplit across multiple deployments, allowing for testing new model versions by diverting\\nsome amount of traffic initially and increasing after confidence in the new model is\\nestablished.\\nFor more information, see:\\nDeploy a model with a real-time managed endpoint\\nUse batch endpoints for scoring\\nDevOps for ML models, often called ML Ops, is a process for developing models for\\nproduction. A model's lifecycle from training to deployment must be auditable if not\\nreproducible.\\nLearn more about MLOps in Azure Machine Learning .\\nMachine Learning is built with the model lifecycle in mind. Y ou can audit the model\\nlifecycle down to a specific commit and environment.Real-time and batch scoring (inferencing)\\nMLOps: DevOps for machine learning\\nML model lifecycle\\nIntegrations enabling MLOPs\", \"There are two types of deployments in batch endpoints:\\nModel deployments\\nPipeline component deployment\\nModel deployment allows operationalizing model inference at scale, processing big\\namounts of data in a low latency and asynchronous way. Scalability is automatically\\ninstrumented by Azure Machine Learning by providing parallelization of the inferencing\\nprocesses across multiple nodes in a compute cluster.\\nUse Model deployments  when:\\nThe main benefit of this kind of deployments is that you can use the very same assets\\ndeployed in the online world (Online Endpoints) but now to run at scale in batch. If your\\nmodel requires simple pre or pos processing, you can author an scoring script  that\\nperforms the data transformations required.\\nTo create a model deployment in a batch endpoint, you need to specify the following\\nelements:Model deployments\\nYou have expensive models that requires a longer time to run inference.＂\\nYou need to perform inference over large amounts of data, distributed in multiple\\nfiles.＂\\nYou don't have low latency requirements.＂\\nYou can take advantage of parallelization.＂\", 'You use the updated settings to save the Y AML file.\\nPython\\nThe batch inference has these steps:\\n1. You use the same built-in feature retrieval component for feature retrieval that you\\nused in the training pipeline (covered in the third tutorial). For pipeline training,\\nyou provided a feature retrieval specification as a component input. For batch\\ninference, you pass the registered model as the input. The component looks for\\nthe feature retrieval specification in the model artifact.\\nAdditionally, for training, the observation data had the target variable. However,\\nthe batch inference observation data doesn\\'t have the target variable. The feature\\nretrieval step joins the observation data with the features and outputs the data for\\nbatch inference.\\n2. The pipeline uses the batch inference input data from previous step, runs inference\\non the model, and appends the predicted value as output.fs_client.feature_sets.begin_create_or_update(transactions_fset_config)\\nprint(fs_poller.result())\\n(Optional) Save the YAML file for the feature\\nset asset\\nPython SDK\\n## uncomment and run\\n# transactions_fset_config.dump(root_dir +  \\n\"\/featurestore\/featuresets\/transactions\/featureset_asset_offline_enabled\\n_with_schedule.yaml\")\\nRun the batch inference pipeline\\n７ Note\\nYou use a job for batch inference in this example. Y ou can also use batch\\nendpoints in Azure Machine Learning.']","question":"How does batch inferencing work in parallel mode?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":44}
{"answer":"To deploy a Custom Vision model to a real-time endpoint, you can use Azure Machine Learning service. First, register the model, then create an inference configuration and deploy it as a web service.","documents":"['Deploy a trained model\\nDeploy a pipeline with batch endpoints', 'Deploy a trained model', 'With the trained model registered in the Machine learning workspace, you are ready to\\ndeploy the model for scoring.\\nThis scenario includes prebuilt workflows for two approaches to deploying a trained\\nmodel, batch scoring or a deploying a model to an endpoint for real-time scoring. Y ou\\nmay run either or both of these workflows to test the performance of the model in your\\nAzure Machine Learning workspace.\\n1. In your GitHub project repository (ex: taxi-fare-regression), select Actions\\n2. Select the deploy-online-endpoint -pipeline  from the workflows listed on the left\\nand click Run w orkflow  to execute the online endpoint deployment pipeline\\nworkflow. The steps in this pipeline will create an online endpoint in your Machine\\nLearning workspace, create a deployment of your model to this endpoint, then\\nallocate traffic to the endpoint.７ Note\\nIf you want to check the output of each individual step, for example to view output\\nof a failed run, click a job output, and then click each step in the job to view any\\noutput of that step.\\nDeploying the Trained Model\\nOnline Endpoint', \"Alternatively Y ou can deploy the model from the Azure Machine Learning studio UI .\\nNavigate to the model you wish to deploy in the Models  tab of the automated ML job\\nand select on Deploy  and select Deploy t o real-time endpoint  .\\n.\\nthis is how your review page looks like. we can select instance type, instance count and\\nset traffic percentage for the current deployment.\\n.az ml online-endpoint update --name 'od-fridge-items-endpoint'  --traffic  \\n'od-fridge-items-mlflow-deploy=100'  --workspace-name  \\n[YOUR_AZURE_ WORKSPACE]  --resource-group  [YOUR_AZURE_RESOURCE_ GROUP] --\\nsubscription  [YOUR_AZURE_ SUBSCRIPTION]\", 'If you directly deploy real-time endpoint from a previous completed real-time inference\\npipeline, it may run into errors.\\nRecommendation : clone the inference pipeline and submit it again, then deploy to real-\\ntime endpoint.\\nTutorial: Build a model in designer to predict auto pricesDeploy real-time endpoint from real-time inference\\npipeline issue\\nNext steps']","question":"How to deploy custom vision model to real time endpoint?","metrics":"gpt_groundedness,gpt_relevance,gpt_retrieval_score","line_number":45}
