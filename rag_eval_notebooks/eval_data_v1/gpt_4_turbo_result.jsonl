{"question": "How do you initiate a new Azure ML experiment using the Azure CLI?", "variant_id": "v1", "line_number": 1, "answer": "To initiate a new Azure ML experiment using the Azure CLI, you would typically follow these steps:\n\n1. Ensure that you have the Azure CLI installed along with the ML extension. You can add the ML extension using the following command:\n\n   ```\n   az extension add -n ml\n   ```\n\n2. Set your Azure subscription context and configure defaults for your workspace, resource group, and location:\n\n   ```\n   az account set --subscription <subscription-id>\n   az configure --defaults workspace=<workspace-name> group=<resource-group-name> location=<location>\n   ```\n\n3. Create or use an existing compute target for running your experiment. If you need to create a new compute target, you can use the following command:\n\n   ```\n   az ml compute create -n <compute-name> --type amlcompute --min-instances 0 --max-instances 5\n   ```\n\n4. To create and start a new experiment, you would use the `az ml job create` command with an appropriate job definition file that specifies the details of the experiment, including the compute target, environment, data inputs, and script to run. Here is a generic example of how you might run an experiment:\n\n   ```\n   az ml job create --file <job-definition-file.yml>\n   ```\n\n   The job definition file (`<job-definition-file.yml>`) should contain the configuration for the experiment, including the script to run, the compute target, environment, inputs, outputs, etc.\n\nPlease note that the exact commands and parameters might vary based on the specifics of your experiment and the version of the Azure CLI and ML extension you are using. Always refer to the latest Azure ML documentation for the most up-to-date instructions.", "context": ["The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\nAzure CLI\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\nso one cluster can host one or many batch deployments (along with other workloads if\ndesired).\nThis article uses a compute created here named batch-cluster. Adjust as needed and\nreference your compute using azureml:<your-compute-name> or create one as shown.\nAzure CLIaz extension add -n ml \nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>  \naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>  \nCreate compute\nAzure CLI\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \n0 --max-instances  5 \n\uff17 Note", "Azure CLI\nIf you're not on the compute instance, omit --identity and follow the prompt to\nopen a browser window to authenticate.\n3. Make sure you have the most recent versions of the CLI and the ml extension:\nAzure CLI\n4. If you have multiple Azure subscriptions, set the active subscription to the one\nyou're using for your workspace. (Y ou can skip this step if you only have access to\na single subscription.) R eplace <SUBSCRIPTION-NAME> with your subscription name.\nAlso remove the brackets <>.\nAzure CLI\n5. Now use CLI to submit the job. If you're doing this on a compute instance in your\nworkspace, you can use environment variables for the workspace name and\nresource group as show in the following code. If you aren't on a compute instance,\nreplace these values with your workspace name and resource group.\nAzure CLI\nOnce you've submitted the job, you can check the status and results in studio:\n1. Sign in to Azure Machine Learning studio .\n2. Select your workspace if it isn't already loaded.\n3. On the left navigation, select Jobs.\n4. Select the Experiment name  that you used to train your model.\n5. Select the Display name  of the job to view details and artifacts of the job,\nincluding metrics, images, child jobs, outputs, logs, and code used in the job.az login --identity\naz upgrade\naz account set --subscription  \"<SUBSCRIPTION-NAME>\"\naz ml job create -f job.yml  --workspace-name  $CI_WORKSPACE  --resource-\ngroup $CI_RESOURCE_GROUP\nRegister model", "Azure CLI\nIf you don't have it, create a cluster called cpu-cluster by running:\nAzure CLI\nNow, create a pipeline job defined in the pipeline.yml file with the following command.\nThe compute target is referenced in the pipeline.yml file as azureml:cpu-cluster. If your\ncompute target uses a different name, remember to update it in the pipeline.yml file.\nAzure CLI\nYou should receive a JSON dictionary with information about the pipeline job including:\nKey Descr iption\nname The GUID-based name of the job.\nexperiment_name The name under which jobs will be organized in studio.\nservices.Studio.endpoint A URL for monitoring and reviewing the pipeline job.\nstatus The status of the job. This will likely be Preparing at this point.\nOpen the services.Studio.endpoint URL to see a graph visualization of the pipeline.az ml compute list\n\uff17 Note\nSkip this step to use serverless comput e.\naz ml compute create -n cpu-cluster  --type amlcompute --min-instances  0 --\nmax-instances  10\naz ml job create --file pipeline.yml\n\uff89Expand table", "The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nIn this example, we are going to create a deployment that can write directly to the\noutput folder of the batch deployment job. The deployment will use this feature to write\ncustom parquet files.Azure CLI\naz extension add -n ml\n\uff17 Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nCreating a batch deployment with a custom\noutput", "The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\ninference over new data:\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\n\uff17 Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nSteps"]}
{"question": "Describe the process of creating a new compute instance via the Azure ML CLI. What parameters are necessary?", "variant_id": "v1", "line_number": 2, "answer": "To create a new compute instance via the Azure Machine Learning CLI, you would typically follow these steps:\n\n1. **Prerequisites**: Ensure you have the Azure CLI installed along with the Azure Machine Learning CLI extension (`ml`), which is referred to as CLI v2 in the context provided. Also, make sure you have an Azure Machine Learning workspace set up, and the \"Allow storage account key access\" option is enabled in the associated storage account.\n\n2. **Login**: Authenticate with Azure using the `az login` command if you haven't already done so.\n\n3. **Set the active subscription**: Use `az account set --subscription \"Your Subscription ID\"` to set the subscription where your workspace resides.\n\n4. **Create the compute instance**: Use the `az ml compute create` command with the necessary parameters. Here are some of the key parameters you might need:\n\n   - `--name` or `-n`: The name of the compute instance.\n   - `--type`: The type of compute resource, which should be set to `computeinstance` for creating a compute instance.\n   - `--size`: The Virtual Machine size for the compute instance.\n   - `--workspace-name` or `-w`: The name of the Azure Machine Learning workspace.\n   - `--resource-group` or `-g`: The name of the resource group where the workspace is located.\n\nAn example command to create a compute instance might look like this:\n\n```sh\naz ml compute create --type computeinstance --name my-compute-instance --size Standard_DS11_v2 --workspace-name my-aml-workspace --resource-group my-resource-group\n```\n\nThis command will create a compute instance named `my-compute-instance` with the specified VM size in the given workspace and resource group.\n\nRemember that the exact parameters and options can vary based on the specific requirements of your compute instance, such as the need for a custom environment or specific networking configurations. Always refer to the latest Azure CLI documentation for the most accurate and up-to-date information.", "context": ["Create an Azure Machine Learning\ncompute instance\nArticle \u202212/08/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nLearn how to create a compute instance  in your Azure Machine Learning workspace.\nUse a compute instance as your fully configured and managed development\nenvironment in the cloud. For development and testing, you can also use the instance as\na training compute target . A compute instance can run multiple jobs in parallel and has\na job queue. As a development environment, a compute instance can't be shared with\nother users in your workspace.\nIn this article, you learn how to create a compute instance. See Manage an Azure\nMachine Learning compute instance  for steps to manage start, stop, restart, delete a\ncompute instance.\nYou can also use a setup script  to create the compute instance with your own custom\nenvironment.\nCompute instances can run jobs securely in a virtual network environment , without\nrequiring enterprises to open up SSH ports. The job executes in a containerized\nenvironment and packages your model dependencies in a Docker container.\nAn Azure Machine Learning workspace. For more information, see Create an Azure\nMachine Learning workspace . In the storage account, the \"Allow storage account\nkey access\" option must be enabled for compute instance creation to be\nsuccessful.\nChoose the tab for the environment you're using for other prerequisites.\n\uff17 Note\nThis article uses CLI v2 in some examples. If you are still using CLI v1, see Create an\nAzur e Machine Learning comput e clust er CLI v1) .\nPrerequisites", "For more information on creating a compute cluster and compute cluster, including how\nto do so with Python and the CLI, see the following articles:\nCreate a compute cluster\nCreate a compute instance\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nWhen Azure Container R egistry is behind the virtual network, Azure Machine Learning\ncan't use it to directly build Docker images (used for training and deployment). Instead,\nconfigure the workspace to use the compute cluster you created earlier. Use the\nfollowing steps to create a compute cluster and configure the workspace to use it to\nbuild images:\n1. Navigate to https://shell.azure.com/  to open the Azure Cloud Shell.\n2. From the Cloud Shell, use the following command to install the 2.0 CLI for Azure\nMachine Learning:\nAzure CLIAllow inbound T CP traffic on ports 29876-29877 from the\nBatchNodeManagement service tag.\nAllow inbound T CP traffic on port 44224 from the AzureMachineLearning\nservice tag.\nThe following screenshot shows an example of these rules:\nConfigure image builds\naz extension add -n ml", "The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\nAzure CLI\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\nso one cluster can host one or many batch deployments (along with other workloads if\ndesired).\nThis article uses a compute created here named batch-cluster. Adjust as needed and\nreference your compute using azureml:<your-compute-name> or create one as shown.\nAzure CLIaz extension add -n ml \nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>  \naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>  \nCreate compute\nAzure CLI\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \n0 --max-instances  5 \n\uff17 Note", "To create a workspace using CLI v2, use the following command:\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nBash\nFor more information, see workspace Y AML schema .\nA compute is a designated compute resource where you run your job or host your\nendpoint. Azure Machine Learning supports the following types of compute:\nComput e clust er - a managed-compute infrastructure that allows you to easily\ncreate a cluster of CPU or GPU compute nodes in the cloud.\nComput e instance  - a fully configured and managed development environment in\nthe cloud. Y ou can use the instance as a training or inference compute for\ndevelopment and testing. It's similar to a virtual machine on the cloud.\nInfer ence clust er - used to deploy trained machine learning models to Azure\nKubernetes Service. Y ou can create an Azure Kubernetes Service (AKS) cluster from\nyour Azure Machine Learning workspace, or attach an existing AKS cluster.\nAttached comput e - You can attach your own compute resources to your\nworkspace and use them for training and inference.\nTo create a compute using CLI v2, use the following command:\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)Azure CLI\naz ml workspace create --file my_workspace.yml  \nCompute\n\uff17 Note\nInstead of creating a compute cluster, use serverless comput e (preview) to\noffload compute lifecycle management to Azure Machine Learning.\nAzure CLI", "Manage an Azure Machine Learning\ncompute instance\nArticle \u202207/06/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nLearn how to manage a compute instance  in your Azure Machine Learning workspace.\nUse a compute instance as your fully configured and managed development\nenvironment in the cloud. For development and testing, you can also use the instance as\na training compute target . A compute instance can run multiple jobs in parallel and has\na job queue. As a development environment, a compute instance can't be shared with\nother users in your workspace.\nIn this article, you learn how to start, stop, restart, delete) a compute instance. See\nCreate an Azure Machine Learning compute instance  to learn how to create a compute\ninstance.\nAn Azure Machine Learning workspace. For more information, see Create an Azure\nMachine Learning workspace . In the storage account, the \"Allow storage account\nkey access\" option must be enabled for compute instance creation to be\nsuccessful.\nThe Azure CLI extension for Machine Learning service (v2) , Azure Machine\nLearning Python SDK (v2) , or the Azure Machine Learning Visual S tudio Code\nextension .\nIf using the Python SDK, set up your development environment with a workspace .\nOnce your environment is set up, attach to the workspace in your Python script:\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\n\uff17 Note\nThis article shows CLI v2 in the sections below. If you are still using CLI v1, see\nCreate an Azur e Machine Learning comput e clust er CLI v1) .\nPrerequisites"]}
{"question": "Explain how to submit a training job using Python and the AML SDK v2. Include an example of specifying a compute target.", "variant_id": "v1", "line_number": 3, "answer": "To submit a training job using Python and the Azure Machine Learning (AML) SDK v2, you need to follow these steps:\n\n1. **Set up your environment**:\n   - Install the Azure Machine Learning SDK v2 if you haven't already.\n   - Import the necessary packages from the SDK.\n   - Create or load an existing Azure Machine Learning workspace.\n\n2. **Create or attach a compute target**:\n   - Define the compute target where the training will occur. This could be an Azure Machine Learning Compute Instance or a Compute Cluster.\n\n3. **Prepare your training script**:\n   - Write a Python script that contains the code to train your model. Save this script in a directory.\n\n4. **Define the command job**:\n   - Create a command job that specifies the script to run, the compute target to use, and any other configurations such as environment dependencies.\n\n5. **Submit the job**:\n   - Submit the command job to the Azure Machine Learning workspace to start the training process.\n\nHere is an example of how you might do this in Python using the AML SDK v2:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import CommandJob, Environment, CodeConfiguration\nfrom azure.identity import DefaultAzureCredential\n\n# Set up the MLClient with your Azure subscription and resource details\nsubscription_id = 'your-subscription-id'\nresource_group = 'your-resource-group'\nworkspace_name = 'your-workspace-name'\n\ncredential = DefaultAzureCredential()\nml_client = MLClient(credential, subscription_id, resource_group, workspace_name)\n\n# Define the compute target\ncompute_target = 'your-compute-target-name'\n\n# Define the environment for the job\nenvironment = Environment(\n    image='mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04',\n    conda_file='path/to/your/conda_file.yml',\n    name='my-environment',\n    version='1'\n)\n\n# Define the code configuration\ncode_configuration = CodeConfiguration(\n    code='path/to/your/training_script_folder',\n    command='python train.py'  # Your training script\n)\n\n# Create the command job\ncommand_job = CommandJob(\n    compute=compute_target,\n    environment=environment,\n    code_configuration=code_configuration,\n    display_name='my-training-job',\n    experiment_name='my-experiment'\n)\n\n# Submit the job\nreturned_job = ml_client.create_or_update(command_job)\n\n# Get the URL to monitor the job\nprint(f\"Monitor your job at {returned_job.studio_url}\")\n```\n\nIn this example, replace the placeholders (like `'your-subscription-id'`, `'your-resource-group'`, `'your-workspace-name'`, `'your-compute-target-name'`, `'path/to/your/conda_file.yml'`, and `'path/to/your/training_script_folder'`) with your actual Azure subscription, resource group, workspace, and file paths. The `train.py` is the Python script that contains the code for training your model.\n\nPlease note that the Azure ML SDK and the platform itself are subject to change, and it's always a good idea to refer to the latest Azure ML documentation for the most current information and best practices.", "context": ["1. On the top bar above your opened notebook, create a compute instance if you\ndon't already have one.\n2. If the compute instance is stopped, select Start comput e and wait until it is\nrunning.\n3. Make sure that the kernel, found on the top right, is Python 3.10 - SDK v2. If not,\nuse the dropdown to select this kernel.\n4. If you see a banner that says you need to be authenticated, select Authenticat e.\nTo train a model, you need to submit a job. The type of job you'll submit in this tutorial\nis a command job . Azure Machine Learning offers several different types of jobs to train\nmodels. Users can select their method of training based on complexity of the model,\ndata size, and training speed requirements. In this tutorial, you'll learn how to submit a\ncommand job  to run a training s cript.\nA command job is a function that allows you to submit a custom training script to train\nyour model. This can also be defined as a custom training job. A command job in Azure\nMachine Learning is a type of job that runs a script or command in a specified\nenvironment. Y ou can use command jobs to train models, process data, or any other\ncustom code you want to execute in the cloud.Set your kernel\n\uf80a\n\uf80a\n\uf80a\n\uff09 Impor tant\nThe rest of this tutorial contains cells of the tutorial notebook. Copy/paste them\ninto your new notebook, or switch to the notebook now if you cloned it.\nUse a command job to train a model in Azure\nMachine Learning", "For detailed instructions on how to complete these steps, see Secure a training\nenvironment .\nIn this section, you learn how Azure Machine Learning securely communicates between\nservices to submit a training job. This example shows you how all your configurations\nwork together to secure communication.\n1. The client uploads training scripts and training data to storage accounts that are\nsecured with a service or private endpoint.\n2. The client submits a training job to the Azure Machine Learning workspace\nthrough the private endpoint.\n3. Azure Batch service receives the job from the workspace. It then submits the\ntraining job to the compute environment through the public load balancer for the\ncompute resource.\n4. The compute resource receives the job and begins training. The compute resource\nuses information stored in key vault to access storage accounts to download\ntraining files and upload output.Example training job submission", "Python\nView more examples of training with serverless compute at:-\nQuick S tart\nTrain Model\nThere's no need to specify compute for AutoML jobs. R esources can be optionally\nspecified. If instance count isn't specified, then it's defaulted based on\nmax_concurrent_trials and max_nodes parameters. If you submit an AutoML image\nclassification or NLP task with no instance type, the GPU VM size is automatically\nselected. It's possible to submit AutoML job through CLIs, SDK, or S tudio. T o submit\nAutoML jobs with serverless compute in studio first enable the submit a training job in\nstudio (preview)  feature in the preview panel.Python SDK\nfrom azure.ai.ml import command\nfrom azure.ai.ml import MLClient      # Handle to the workspace\nfrom azure.identity import DefaultAzureCredential     # Authentication  \npackage\nfrom azure.ai.ml.entities import ResourceConfiguration\nfrom azure.ai.ml.entities import UserIdentityConfiguration \ncredential = DefaultAzureCredential()\n# Get a handle to the workspace. You can find the info on the workspace  \ntab on ml.azure.com\nml_client = MLClient(\n    credential=credential,\n    subscription_id= \"<Azure subscription id>\" , \n    resource_group_name= \"<Azure resource group>\" ,\n    workspace_name= \"<Azure Machine Learning Workspace>\" ,\n)\njob = command(\n    command= \"echo 'hello world'\" ,\n    environment= \"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\" ,\n         identity=UserIdentityConfiguration(),\n    queue_settings={\n      \"job_tier\" : \"Standard\"   \n    }\n)\njob.resources = ResourceConfiguration(instance_type= \"Standard_E4s_v3\" , \ninstance_count= 1)\n# submit the command job\nml_client.create_or_update(job)\nAutoML job", "Designer : Azure Machine Learning designer provides an easy entry-point into\nmachine learning for building proof of concepts, or for users with little coding\nexperience. It allows you to train models using a drag and drop web-based UI. Y ou\ncan use Python code as part of the design, or train models without writing any\ncode.\nAzur e CLI : The machine learning CLI provides commands for common tasks with\nAzure Machine Learning, and is often used for scripting and aut omating tasks . For\nexample, once you've created a training script or pipeline, you might use the Azure\nCLI to start a training job on a schedule or when the data files used for training are\nupdated. For training models, it provides commands that submit training jobs. It\ncan submit jobs using run configurations or pipelines.\nEach of these training methods can use different types of compute resources for\ntraining. Collectively, these resources are referred to as comput e targets. A compute\ntarget can be a local machine or a cloud resource, such as an Azure Machine Learning\nCompute, Azure HDInsight, or a remote virtual machine.\nThe Azure Machine Learning SDK for Python allows you to build and run machine\nlearning workflows with Azure Machine Learning. Y ou can interact with the service from\nan interactive Python session, Jupyter Notebooks, Visual S tudio Code, or other IDE.\nInstall/update the SDK\nConfigure a development environment for Azure Machine Learning\nA generic training job with Azure Machine Learning can be defined using the\ncommand() . The command is then used, along with your training script(s) to train a\nmodel on the specified compute target.\nYou may start with a command for your local computer, and then switch to one for a\ncloud-based compute target as needed. When changing the compute target, you only\nchange the compute parameter in the command that you use. A run also logs\ninformation about the training job, such as the inputs, outputs, and logs.\nTutorial: T rain your first ML model\nExamples: Jupyter Notebook and Python examples of training modelsPython SDK\nSubmit a command\nAutomated Machine Learning", "An Azure Machine Learning compute cluster is a fully managed compute resource that\ncan be used to run the training job. In the following examples, a compute cluster named\ncpu-compute is created.\nPython\nTo run this script, you'll use a command that executes main.py Python script located\nunder ./sdk/python/jobs/single-step/lightgbm/iris/src/. The command will be runworkspace = '<AZUREML_WORKSPACE_NAME>'\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \nresource_group, workspace)\n2. Create a compute resource for training\n\uff17 Note\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\ntraining job .\nPython SDK\nfrom azure.ai.ml.entities import AmlCompute\n# specify aml compute name.\ncpu_compute_target = \"cpu-cluster\"\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(\"Creating a new cpu compute target...\" )\n    compute = AmlCompute(\n        name=cpu_compute_target, size= \"STANDARD_D2_V2\" , min_instances= 0, \nmax_instances= 4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n4. Submit the training job\nPython SDK"]}
{"question": "What is the command to list all the models registered in an Azure ML Workspace using the Azure CLI?", "variant_id": "v1", "line_number": 4, "answer": "The command to list all the models registered in an Azure ML Workspace using the Azure CLI is:\n\n```\naz ml model list\n```", "context": ["then select Regist er.\nThe SDK and CLI (v2) also allow you to manage the lifecycle of your Azure Machine\nLearning model assets.\nList all the models in your workspace:\ncli\nList all the model versions under a given name:\ncli\n\uf80a\nManage models\nList\nAzure CLI\naz ml model list  \nAzure CLI\naz ml model list --name run-model-example", "Install the Azure CLI and the ml extension to the Azure CLI. For more\ninformation on installing the CLI, see Install and set up the CLI (v2) .\nFirst, connect to the Azure Machine Learning workspace where you'll work.\nAzure CLI\nYou can deploy only registered models to online endpoints. In this case, you already\nhave a local copy of the model in the repository, so you only need to publish the model\nto the registry in the workspace. Y ou can skip this step if the model you're trying to\ndeploy is already registered.\nAzure CLI\nIf your model was logged inside of a run, you can register it directly.\nTo register the model, you need to know the location where it is stored. If you're using\nMLflow's autolog feature, the path to the model depends on the model type and\nframework. Y ou should check the jobs output to identify the name of the model's folder.\nThis folder contains a file named MLModel.Connect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nRegister the model\nAzure CLI\nMODEL_NAME= 'sklearn-diabetes'\naz ml model create --name $MODEL_NAME  --type \"mlflow_model\"  --path \n\"endpoints/online/ncd/sklearn-diabetes/model\"\nWhat if your model was logged inside of a run?", "Install the Azure CLI and the ml extension to the Azure CLI. For more\ninformation on installing the CLI, see Install and set up the CLI (v2) .\nFirst, connect to the Azure Machine Learning workspace where you'll work.\nAzure CLI\nYou can deploy only registered models to online endpoints. In this case, you already\nhave a local copy of the model in the repository, so you only need to publish the model\nto the registry in the workspace. Y ou can skip this step if the model you're trying to\ndeploy is already registered.\nAzure CLI\nIf your model was logged inside of a run, you can register it directly.\nTo register the model, you need to know the location where it is stored. If you're using\nMLflow's autolog feature, the path to the model depends on the model type and\nframework. Y ou should check the jobs output to identify the name of the model's folder.\nThis folder contains a file named MLModel.Connect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nRegister the model\nAzure CLI\nMODEL_NAME= 'sklearn-diabetes'\naz ml model create --name $MODEL_NAME  --type \"mlflow_model\"  --path \n\"endpoints/online/ncd/sklearn-diabetes/model\"\nWhat if your model was logged inside of a run?", "The az ml model command can be used for managing Azure Machine Learning models.\nExamples are available in the examples GitHub repository . Several are shown below.\nYAML\nYAML\nInstall and use the CLI (v2)Remarks\nExamples\nYAML: local file\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-file-example  \npath: mlflow-model/model.pkl  \ndescription:  Model created from local file. \nYAML: local folder in MLflow format\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-mlflow-example  \npath: mlflow-model  \ntype: mlflow_model  \ndescription:  Model created from local MLflow model directory.", "The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\ninference over new data:\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\n\uff17 Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nSteps"]}
{"question": "Provide an example of how to use the AML SDK v2 to stream logs from a running experiment in Python.", "variant_id": "v1", "line_number": 5, "answer": "To stream logs from a running experiment using Azure Machine Learning SDK v2 in Python, you would typically start by submitting a job and then use the `stream` method to follow the logs. Here's an example of how you might do this:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import CommandJob\nfrom azure.identity import DefaultAzureCredential\n\n# Connect to the workspace\nml_client = MLClient.from_config(DefaultAzureCredential())\n\n# Define the command job\ncommand_job = CommandJob(\n    code=\"./src\",\n    command=\"python train.py\",\n    environment=\"pytorch-env\",  # Assuming the environment is already registered\n    compute=\"cpu-cluster\",\n)\n\n# Submit the job\nreturned_job = ml_client.jobs.create_or_update(command_job)\n\n# Get the job ID from the returned job\njob_id = returned_job.id\n\n# Stream the logs\nml_client.jobs.stream(job_id)\n```\n\nIn this example, after submitting the job with `create_or_update`, we use the `stream` method of the `jobs` client to follow the logs of the job in real-time. The `stream` method requires the job ID, which you can obtain from the `id` attribute of the job object returned by the `create_or_update` method.\n\nPlease note that the actual code may vary depending on the specifics of your Azure Machine Learning workspace and the job you are running. Make sure that the environment and compute target specified in the `CommandJob` are correctly configured in your workspace.", "context": ["SDK v2\nPython\nFunctionality in SDK v1 Rough mapping in SDK v2\nexperiment.submit MLCLient.jobs.create_or_update\nScriptRunConfig() command()\nFor more information, see:\nV1 - Experiment\nV2 - Command Jobaml_url = run.get_portal_url()  \nprint(aml_url)  \n#import required libraries  \nfrom azure.ai.ml import MLClient, command  \nfrom azure.ai.ml.entities import Environment  \nfrom azure.identity import DefaultAzureCredential  \n#connect to the workspace  \nml_client = MLClient.from_config(DefaultAzureCredential())  \n# set up pytorch environment  \nenv = Environment(  \n    image= \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\" , \n    conda_file= \"pytorch-env.yml\" , \n    name= \"pytorch-env\"  \n) \n# define the command  \ncommand_job = command(  \n    code= \"./src\", \n    command= \"train.py\" , \n    environment=env,  \n    compute= \"cpu-cluster\" , \n) \nreturned_job = ml_client.jobs.create_or_update(command_job)  \nreturned_job  \nMapping of key functionality in v1 and v2\nNext steps", "Bash\nIf you're doing remote tracking (tracking experiments that run outside Azure\nMachine Learning), configure MLflow to track experiments. For more information,\nsee Configure MLflow for Azure Machine Learning .\nTo log metrics, parameters, artifacts, and models in your experiments in Azure\nMachine Learning using MLflow, just import MLflow into your script:\nPython\nMLflow organizes the information in experiments and runs (in Azure Machine Learning,\nruns are called jobs). There are some differences in how to configure them depending\non how you run your code:\nWhen training interactively, such as in a Jupyter Notebook, use the following\npattern:\n1. Create or set the active experiment.\n2. Start the job.\n3. Use logging methods to log metrics and other information.\n4. End the job.\nFor example, the following code snippet configures the experiment, and then logs\nduring a job:\nPythonpip install mlflow azureml-mlflow\nimport mlflow\nConfigure experiments\nTraining interactively\nimport mlflow\n# Set the experiment\nmlflow.set_experiment( \"mlflow-experiment\" )\n# Start the run\nmlflow_run = mlflow.start_run()\n# Log metrics or other information\nmlflow.log_metric( 'mymetric' , 1)", "Log metr ics, parameters, and files with\nMLflow\nArticle \u202201/31/2024\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAzure Machine Learning supports logging and tracking experiments using MLflow\nTracking . You can log models, metrics, parameters, and artifacts with MLflow, either\nlocally on your computer or in a cloud environment.\nLogs can help you diagnose errors and warnings, or track performance metrics like\nparameters and model performance. This article explains how to enable logging in the\nfollowing scenarios:\nYou must have an Azure Machine Learning workspace. If you don't have one, see\nCreate workspace resources .\nYou must have the mlflow and azureml-mlflow packages installed. If you don't, use\nthe following command to install them in your development environment:\n\uff09 Impor tant\nUnlike the Azure Machine Learning SDK v1, there's no logging functionality in the\nAzure Machine Learning SDK for Python (v2). If you used Azure Machine Learning\nSDK v1 before, we recommend that you leverage MLflow for tracking experiments.\nSee Migrat e logging fr om SDK v1 t o MLflow  for specific guidance.\nLog metrics, parameters, and models when submitting jobs.\uff02\nTrack runs when training interactively.\uff02\nView diagnostic information about training.\uff02\n\uea80 Tip\nThis article shows you how to monitor the model training process. If you're\ninterested in monitoring resource usage and events from Azure Machine Learning,\nsuch as quotas, completed training jobs, or completed model deployments, see\nMonit oring Azur e Machine Learning .\nPrerequisites", "Query & co mpare experiments and runs\nwith MLflow\nArticle \u202206/26/2023\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\nYou don't need to install any specific SDK to manage what happens inside of a training\njob, creating a more seamless transition between local runs and the cloud by removing\ncloud-specific dependencies. In this article, you'll learn how to query and compare\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\nin Python.\nMLflow allows you to:\nCreate, query, delete and search for experiments in a workspace.\nQuery, delete, and search for runs in a workspace.\nTrack and retrieve metrics, parameters, artifacts and models from runs.\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\ndetailed comparison between MLflow Open-Source and MLflow when connected to\nAzure Machine Learning.\nQuery and searching experiments and runs is also available using the MLflow REST API.\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\nconsume it.\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\nMLflow azureml-mlflow.\uff17 Note\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\ntracking capabilities. This applies not just for logging but also for querying the\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\nexplains how to use MLflow to manage experiments and runs in Azure Machine\nLearning.\nREST API\nPrerequisites", "Query & co mpare experiments and runs\nwith MLflow\nArticle \u202206/26/2023\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\nYou don't need to install any specific SDK to manage what happens inside of a training\njob, creating a more seamless transition between local runs and the cloud by removing\ncloud-specific dependencies. In this article, you'll learn how to query and compare\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\nin Python.\nMLflow allows you to:\nCreate, query, delete and search for experiments in a workspace.\nQuery, delete, and search for runs in a workspace.\nTrack and retrieve metrics, parameters, artifacts and models from runs.\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\ndetailed comparison between MLflow Open-Source and MLflow when connected to\nAzure Machine Learning.\nQuery and searching experiments and runs is also available using the MLflow REST API.\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\nconsume it.\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\nMLflow azureml-mlflow.\uff17 Note\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\ntracking capabilities. This applies not just for logging but also for querying the\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\nexplains how to use MLflow to manage experiments and runs in Azure Machine\nLearning.\nREST API\nPrerequisites"]}
{"question": "How can you configure an AutoML run for a classification task using the AML SDK v2, including specifying primary metric and training data?", "variant_id": "v1", "line_number": 6, "answer": "To configure an AutoML run for a classification task using the Azure Machine Learning (AML) SDK v2, you can follow these steps:\n\n1. **Import necessary modules** from the AML SDK v2:\n   ```python\n   from azure.ai.ml import automl, Input\n   from azure.ai.ml.constants import AssetTypes\n   from azure.ai.ml.automl import classification\n   ```\n\n2. **Create an MLTable** for your training dataset:\n   ```python\n   my_training_data_input = Input(\n       type=AssetTypes.MLTABLE,\n       path=\"./data/training-mltable-folder\"\n   )\n   ```\n\n3. **Create the AutoML classification job** using the `automl.classification` factory function:\n   ```python\n   classification_job = automl.classification(\n       compute=\"<compute_name>\",\n       experiment_name=\"<exp_name>\",\n       training_data=my_training_data_input,\n       target_column_name=\"<name_of_target_column>\",\n       primary_metric=\"accuracy\",  # Specify the primary metric here\n       n_cross_validations=5,\n       enable_model_explainability=True,\n       tags={\"my_custom_tag\": \"My custom value\"},\n   )\n   ```\n\n4. **Optionally set limits** for the AutoML run:\n   ```python\n   classification_job.set_limits(\n       timeout_minutes=600,\n       trial_timeout_minutes=20,\n       max_trials=5,\n       max_concurrent_trials=4\n   )\n   ```\n\n5. **Submit the AutoML run** to the experiment:\n   ```python\n   remote_run = experiment.submit(classification_job, show_output=False)\n   ```\n\n6. **Retrieve the URL** for the run in the Azure portal:\n   ```python\n   azureml_url = remote_run.get_portal_url()\n   print(azureml_url)\n   ```\n\nMake sure to replace `<compute_name>`, `<exp_name>`, and `<name_of_target_column>` with the actual names of your compute target, experiment, and target column for the classification task. The `primary_metric` is set to \"accuracy\" in this example, but you can choose other metrics suitable for classification tasks, such as AUC_weighted, precision_score_weighted, etc., depending on your specific needs.", "context": ["SDK v2: Below is a sample AutoML classification task. For the entire code, check\nout our examples repo .\nPython    debug_log= \"automl_errors.log\" , \n    compute_target=compute_target,  \n    training_data=training_data,  \n    label_column_name=label_column_name,  \n    **automl_settings,  \n) \n# Submit run  \nremote_run = experiment.submit(automl_config, show_output= False) \nazureml_url = remote_run.get_portal_url()  \nprint(azureml_url)  \n# Imports  \nfrom azure.ai.ml import automl, Input, MLClient  \nfrom azure.ai.ml.constants import AssetTypes  \nfrom azure.ai.ml.automl import ( \n    classification,  \n    ClassificationPrimaryMetrics,\n    ClassificationModels,  \n) \n# Create MLTables for training dataset  \n# Note that AutoML Job can also take in tabular data  \nmy_training_data_input = Input(  \n    type=AssetTypes.MLTABLE, path= \"./data/training-mltable-folder\"  \n) \n# Create the AutoML classification job with the related factory-\nfunction.  \nclassification_job = automl.classification(  \n    compute= \"<compute_name>\" , \n    experiment_name= \"<exp_name?\" , \n    training_data=my_training_data_input,  \n    target_column_name= \"<name_of_target_column>\" , \n    primary_metric= \"accuracy\" , \n    n_cross_validations= 5, \n    enable_model_explainability= True, \n    tags={ \"my_custom_tag\" : \"My custom value\" }, \n) \n# Limits are all optional  \nclassification_job.set_limits(  \n    timeout_minutes= 600, \n    trial_timeout_minutes= 20, \n    max_trials= 5, \n    max_concurrent_trials = 4,", "Upgrade AutoML to SDK v2\nArticle \u202204/04/2023\nIn SDK v2, \"experiments\" and \"runs\" are consolidated into jobs.\nIn SDK v1, AutoML was primarily configured and run using the AutoMLConfig class. In\nSDK v2, this class has been converted to an AutoML job. Although there are some\ndifferences in the configuration options, by and large, naming & functionality has been\npreserved in V2.\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\nSDK v1: Below is a sample AutoML classification task. For the entire code, check\nout our examples repo .\nPythonSubmit AutoML run\n# Imports  \nimport azureml.core  \nfrom azureml.core.experiment import Experiment  \nfrom azureml.core.workspace import Workspace  \nfrom azureml.core.dataset import Dataset  \nfrom azureml.train.automl import AutoMLConfig  \nfrom azureml.train.automl.run import AutoMLRun    \n# Load tabular dataset  \ndata = \"<url_to_data>\"  \ndataset = Dataset.Tabular.from_delimited_files(data)  \ntraining_data, validation_data = dataset.random_split(percentage= 0.8, \nseed=223) \nlabel_column_name = \"Class\" \n# Configure Auto ML settings  \nautoml_settings = {  \n    \"n_cross_validations\" : 3, \n    \"primary_metric\" : \"average_precision_score_weighted\" , \n    \"enable_early_stopping\" : True, \n    \"max_concurrent_iterations\" : 2,   \n    \"experiment_timeout_hours\" : 0.25,   \n    \"verbosity\" : logging.INFO,  \n} \n# Put together an AutoML job constructor  \nautoml_config = AutoMLConfig(  \n    task= \"classification\" ,", "3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\nMachine Learning in many different ways .\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\niterations over different models, hyperparameter settings, advanced\npreprocessing/featurization, and what metrics to look at when determining the\nbest model.\n5. Submit the training job.\n6. Review the r esults\nThe following diagram illustrates this process. \nYou can also inspect the logged job information, which contains metrics  gathered\nduring the job. The training job produces a Python serialized object ( .pkl file) that\ncontains the model and data preprocessing.\nWhile model building is automated, you can also learn how important or relevant\nfeatures are  to the generated models.\nApply automated ML when you want Azure Machine Learning to train and tune a model\nfor you using the target metric you specify. Automated ML democratizes the machine\nlearning model development process, and empowers its users, no matter their data\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\nML professionals and developers across industries can use automated ML to:\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\nforecasting, computer vision & NLP", "iii. Select Create to create your compute target.\nThis tak es a couple minut es to complet e.\niv. After creation, select your new compute target from the drop-down list.\ne. Select Next .\n3. On the Select task and settings  form, complete the setup for your automated ML\nexperiment by specifying the machine learning task type and configuration\nsettings.\na. Select Classification  as the machine learning task type.\nb. Select View additional configuration settings  and populate the fields as\nfollows. These settings are to better control the training job. Otherwise, defaults\nare applied based on experiment selection and data.\nAdditional\u00a0configurations Descr iption Value\u00a0for\u00a0tut orial\nPrimary metric Evaluation metric that the\nmachine learning algorithm\nwill be measured by.AUC_weighted\nExplain best model Automatically shows\nexplainability on the bestEnable", "An Azure subscription. (If you don't have an Azure subscription, create a free\naccount  before you begin)\nAn Azure Machine Learning experiment created with either:\nThe Azure Machine Learning studio  (no code required)\nThe Azure Machine Learning Python SDK\nAfter your automated ML experiment completes, a history of the jobs can be found via:\nA browser with Azure Machine Learning studio\nA Jupyter notebook using the JobDetails Jupyter widget\nThe following steps and video, show you how to view the run history and model\nevaluation metrics and charts in the studio:\n1. Sign into the studio  and navigate to your workspace.\n2. In the left menu, select Jobs.\n3. Select your experiment from the list of experiments.\n4. In the table at the bottom of the page, select an automated ML job.\n5. In the Models  tab, select the Algorithm name  for the model you want to evaluate.\n6. In the Metrics  tab, use the checkboxes on the left to view metrics and charts.\nAutomated ML calculates performance metrics for each classification model generated\nfor your experiment. These metrics are based on the scikit learn implementation.\nMany classification metrics are defined for binary classification on two classes, and\nrequire averaging over classes to produce one score for multi-class classification. Scikit-\nlearn provides several averaging methods, three of which automated ML exposes:\nmacr o, micro, and weight ed.\nMacr o - Calculate the metric for each class and take the unweighted average\nMicro - Calculate the metric globally by counting the total true positives, false\nnegatives, and false positives (independent of classes).\nWeight ed - Calculate the metric for each class and take the weighted average\nbased on the number of samples per class.\nWhile each averaging method has its benefits, one common consideration when\nselecting the appropriate method is class imbalance. If classes have different numbers of\nsamples, it might be more informative to use a macro average where minority classes\nView job results\nClassification metrics"]}
{"question": "Describe the steps to retrieve the best model from an AutoML run using the AML SDK v2.", "variant_id": "v1", "line_number": 7, "answer": "To retrieve the best model from an AutoML run using the Azure Machine Learning SDK v2, you would typically follow these steps:\n\n1. **Initialize Workspace**: Import the necessary packages and classes, and create an instance of the `Workspace` class to connect to your Azure Machine Learning workspace.\n\n    ```python\n    from azure.ai.ml import MLClient\n    from azure.identity import DefaultAzureCredential\n    from azure.ai.ml.entities import Workspace\n\n    # Load the workspace\n    ws = Workspace.from_config()\n    ml_client = MLClient(DefaultAzureCredential(), ws)\n    ```\n\n2. **Retrieve AutoML Run**: Access the specific AutoML run by its ID. You can find the run ID in the Azure Machine Learning studio under the Experiments tab.\n\n    ```python\n    run_id = 'your-automl-run-id'\n    automl_run = ml_client.jobs.get(name=run_id)\n    ```\n\n3. **Get Best Model**: Once you have the AutoML run, you can retrieve the best model from that run. The best model is determined by the primary metric that was specified when the AutoML run was submitted.\n\n    ```python\n    best_child_run = automl_run.get_best_child()\n    best_model = best_child_run.model\n    ```\n\n4. **Download Model**: If you want to download the model to your local environment, you can use the `download` method.\n\n    ```python\n    best_model.download(target_dir='path/to/download/model')\n    ```\n\n5. **Register Model**: Optionally, you can register the model to your workspace for future deployment or reference.\n\n    ```python\n    from azure.ai.ml.entities import Model\n\n    model = Model(\n        path=\"path/to/download/model\",\n        name=\"best_automl_model\",\n        type=\"mlflow_model\",\n        description=\"Best model from AutoML run\"\n    )\n\n    registered_model = ml_client.models.create_or_update(model)\n    ```\n\nPlease note that the code snippets provided are for illustrative purposes and may require adjustments to fit the specifics of your environment and the version of the SDK you are using. Always refer to the latest Azure Machine Learning SDK v2 documentation for the most accurate and up-to-date information.", "context": ["Learn more about how and where to deploy a model .\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps", "Navigate to the Models  tab to see the algorithms (models) tested. By default, the\nmodels are ordered by metric score as they complete. For this tutorial, the model that\nscores the highest based on the chosen Normalized r oot mean squar ed err or metric is\nat the top of the list.\nWhile you wait for all of the experiment models to finish, select the Algorithm name  of\na completed model to explore its performance details.\nThe following example navigates to select a model from the list of models that the job\ncreated. Then, you select the Overview  and the Metrics  tabs to view the selected\nmodel's properties, metrics and performance charts.\nAutomated machine learning in Azure Machine Learning studio allows you to deploy the\nbest model as a web service in a few steps. Deployment is the integration of the model\nso it can predict on new data and identify potential areas of opportunity.\nFor this experiment, deployment to a web service means that the bike share company\nnow has an iterative and scalable web solution for forecasting bike share rental demand.\nOnce the job is complete, navigate back to parent job page by selecting Job 1  at the top\nof your screen.\nIn the Best model summar y section, the best model in the context of this experiment, is\nselected based on the Normalized r oot mean squar ed err or metric.\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\nThe deployment process entails several steps including registering the model,Deploy the model", "To run a fine-tuned model job from Machine Learning, in order to deploy an Azure\nOpenAI model:\n1. After you've finished fine-tuning an Azure OpenAI model, find the registered\nmodel in the Models  list with the name provided during fine-tuning and select the\nmodel you want to deploy.\n2. Select Deploy  and name the deployment. The model is deployed to the default\nAzure OpenAI resource linked to your workspace.\nTo enable users to quickly get started with code-based fine-tuning, we've published\nsamples (both Python notebooks and Azure CLI examples) to the azureml-ex amples\nGitHub repo:\nSDK example\nCLI exampleDeploy fine-tuned models\nFine-tuning by using code-based samples", "Option 2: T o deploy a specific model iteration from this experiment.\na. Select the desired model from the Models  tab\nb. Select Deploy  on the top left of the window.\n2. Populate the Deploy model  pane.\nField Value\nName Enter a unique name for your deployment.\nDescription Enter a description to better identify what this deployment is for.\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\nService (AKS)  or Azure Container Inst ance (ACI).\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\ndeploy to.\nEnable\nauthenticationSelect to allow for token-based or key-based authentication.\nUse custom\ndeployment assetsEnable this feature if you want to upload your own scoring script and\nenvironment file. Otherwise, automated ML provides these assets for\nyou by default. Learn more about scoring scripts .\nThe Advanced menu offers default deployment features such as data collection  and\nresource utilization settings. If you wish to override these defaults do so in this\nmenu.\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\ndeployment begins, the Model summar y tab appears. See the deployment\nprogress under the Deploy status  section.\nNow you have an operational web service to generate predictions! Y ou can test the\npredictions by querying the service from Power BI's built in Azure Machine Learning\nsupport .\uff89Expand table\n\uff09 Impor tant\nFile names must be under 32 characters and must begin and end with\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\nbetween. Spaces are not allowed.", "The automated machine learning interface allows you to deploy the best model as a\nweb service in a few steps. Deployment is the integration of the model so it can predict\non new data and identify potential areas of opportunity.\nFor this experiment, deployment to a web service means that the financial institution\nnow has an iterative and scalable web solution for identifying potential fixed term\ndeposit customers.\nCheck to see if your experiment run is complete. T o do so, navigate back to the parent\njob page by selecting Job 1  at the top of your screen. A Complet ed status is shown on\nthe top left of the screen.\nOnce the experiment run is complete, the Details  page is populated with a Best model\nsummar y section. In this experiment context, VotingEnsemble  is considered the best\nmodel, based on the AUC_w eight ed metric.\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\nThe deployment process entails several steps including registering the model,\ngenerating resources, and configuring them for the web service.\n1. Select VotingEnsemble  to open the model-specific page.\n2. Select the Deploy  menu in the top-left and select Deploy t o web ser vice.\n3. Populate the Deploy a model  pane as follows:\nField Value\nDeployment name my-automl-deploy\nDeployment\ndescriptionMy first automated machine learning experiment deployment\nCompute type Select Azure Container Instance (A CI)\nEnable\nauthenticationDisable.\nUse custom\ndeploymentsDisable. Allows for the default driver file (scoring script) and\nenvironment file to be auto-generated.\nFor this example, we use the defaults provided in the Advanced menu.\n4. Select Deploy .Deploy the best model"]}
{"question": "What are the AutoML capabilities for handling imbalanced data in Azure ML?", "variant_id": "v1", "line_number": 8, "answer": "Azure Machine Learning's Automated ML (AutoML) includes several capabilities to handle imbalanced data, which is a common issue in machine learning where one class is significantly more prevalent than others. Here are some of the ways AutoML addresses this challenge:\n\n1. **Data Preprocessing**: AutoML automatically preprocesses the data before training models. This can include techniques specifically designed to handle imbalanced datasets, such as oversampling the minority class or undersampling the majority class.\n\n2. **Cost-Sensitive Learning**: AutoML can apply cost-sensitive learning, which assigns a higher cost to misclassifying the minority class. This encourages the model to pay more attention to the minority class during training.\n\n3. **Primary Metric Selection**: When setting up an AutoML run, you can select a primary metric that is suitable for imbalanced datasets, such as AUC_weighted, which takes class imbalance into account.\n\n4. **Model Selection**: AutoML evaluates a variety of models and algorithms, some of which may be inherently better at handling imbalanced data, such as decision trees and ensemble methods like random forests or gradient boosting.\n\n5. **Hyperparameter Tuning**: AutoML includes hyperparameter tuning, which can optimize model parameters to better handle imbalanced data, such as adjusting the class_weight parameter in certain algorithms.\n\n6. **Customization**: Users have the option to customize the AutoML process by providing their own training scripts or using the SDK to programmatically adjust settings that can influence how the model deals with imbalanced data.\n\nBy leveraging these capabilities, AutoML in Azure Machine Learning helps to mitigate the effects of imbalanced data and improve model performance on such datasets.", "context": ["Prevent overfitting and imbalanced data\nwith Automated ML\nArticle \u202206/16/2023\nOverfitting and imbalanced data are common pitfalls when you build machine learning\nmodels. By default, Azure Machine Learning's Automated ML provides charts and\nmetrics to help you identify these risks, and implements best practices to help mitigate\nthem.\nOverfitting in machine learning occurs when a model fits the training data too well, and\nas a result can't accurately predict on unseen test data. In other words, the model has\nmemorized specific patterns and noise in the training data, but is not flexible enough to\nmake predictions on real data.\nConsider the following trained models and their corresponding train and test accuracies.\nModel Train accuracy Test accuracy\nA 99.9% 95%\nB 87% 87%\nC 99.9% 45%\nConsider model A, there is a common misconception that if test accuracy on unseen\ndata is lower than training accuracy, the model is overfitted. However, test accuracy\nshould always be less than training accuracy, and the distinction for overfit vs.\nappropriately fit comes down to how much  less accurate.\nCompare models A and B, model A is a better model because it has higher test\naccuracy, and although the test accuracy is slightly lower at 95%, it is not a significant\ndifference that suggests overfitting is present. Y ou wouldn't choose model B because\nthe train and test accuracies are closer together.\nModel C represents a clear case of overfitting; the training accuracy is high but the test\naccuracy isn't anywhere near as high. This distinction is subjective, but comes from\nknowledge of your problem and data, and what magnitudes of error are acceptable.Identify overfitting\nPrevent overfitting", "If it ascertains a better performance through this experiment, then this remedy is\napplied.\nUse a performance metric that deals better with imbalanced data. For example, the\nAUC_weighted is a primary metric that calculates the contribution of every class\nbased on the relative number of samples representing that class, hence is more\nrobust against imbalance.\nThe following techniques are additional options to handle imbalanced data outside of\nAutomated ML.\nResampling to even the class imbalance, either by up-sampling the smaller classes\nor down-sampling the larger classes. These methods require expertise to process\nand analyze.\nReview performance metrics for imbalanced data. For example, the F1 score is the\nharmonic mean of precision and recall. Precision measures a classifier's exactness,\nwhere higher precision indicates fewer false positives, while recall measures a\nclassifier's completeness, where higher recall indicates fewer false negatives.\nSee examples and learn how to build models using Automated ML:\nFollow the Tutorial: T rain an object detection model with automated machine\nlearning and Python .\nConfigure the settings for automatic training experiment:\nIn Azure Machine Learning studio, use these steps .\nWith the Python SDK, use these steps .Next steps", "Imbalanced data is commonly found in data for machine learning classification\nscenarios, and refers to data that contains a disproportionate ratio of observations in\neach class. This imbalance can lead to a falsely perceived positive effect of a model's\naccuracy, because the input data has bias towards one class, which results in the trained\nmodel to mimic that bias.\nIn addition, Automated ML jobs generate the following charts automatically. These\ncharts help you understand the correctness of the classifications of your model, and\nidentify models potentially impacted by imbalanced data.\nChar t Descr iption\nConfusion\nMatrixEvaluates the correctly classified labels against the actual labels of the data.\nPrecision-recall Evaluates the ratio of correct labels against the ratio of found label instances of\nthe data\nROC Curves Evaluates the ratio of correct labels against the ratio of false-positive labels.\nAs part of its goal of simplifying the machine learning workflow, Automated ML has built\nin capabilities to help deal with imbalanced data such as,\nA weight column: Automated ML creates a column of weights as input to cause\nrows in the data to be weighted up or down, which can be used to make a class\nmore or less \"important.\"\nThe algorithms used by Automated ML detect imbalance when the number of\nsamples in the minority class is equal to or fewer than 20% of the number of\nsamples in the majority class, where minority class refers to the one with fewest\nsamples and majority class refers to the one with most samples. Subsequently,\nautomated machine learning will run an experiment with subsampled data to\ncheck if using class weights would remedy this problem and improve performance.Cross-validation isn't enabled by default; it must be configured in Automated\nmachine learning settings. However, after cross-validation is configured and a\nvalidation data set has been provided, the process is automated for you.\nIdentify models with imbalanced data\nHandle imbalanced data", "What is automated machine learning\n(AutoML)?\nArticle \u202204/13/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAutomated machine learning, also referred to as automated ML or AutoML, is the\nprocess of automating the time-consuming, iterative tasks of machine learning model\ndevelopment. It allows data scientists, analysts, and developers to build ML models with\nhigh scale, efficiency, and productivity all while sustaining model quality. Automated ML\nin Azure Machine Learning is based on a breakthrough from our Microsoft R esearch\ndivision .\nFor code-experienced customers, Azure Machine Learning Python SDK . Get\nstarted with Tutorial: T rain an object detection model (preview) with AutoML and\nPython .\nDuring training, Azure Machine Learning creates a number of pipelines in parallel that\ntry different algorithms and parameters for you. The service iterates through ML\nalgorithms paired with feature selections, where each iteration produces a model with a\ntraining score. The better the score for the metric you want to optimize for, the better\nthe model is considered to \"fit\" your data. It will stop once it hits the exit criteria defined\nin the experiment.\nUsing Azur e Machine Learning , you can design and run your automated ML training\nexperiments with these steps:\n1. Identif y the ML pr oblem  to be solved: classification, forecasting, regression,\ncomputer vision or NLP.\n2. Choose whether y ou want a code-fir st experience or a no-code studio w eb\nexperience : Users who prefer a code-first experience can use the Azure Machine\nLearning SDK v2 or the Azure Machine Learning CLIv2 . Get started with Tutorial:\nTrain an object detection model with AutoML and Python . Users who prefer a\nlimited/no-code experience can use the web interface  in Azure Machine Learning\nstudio at https://ml.azure.com . Get started with Tutorial: Create a classification\nmodel with automated ML in Azure Machine Learning .\nHow does AutoML work?", "Automated ML in Azure Machine Learning supports model training for computer vision\ntasks like image classification and object detection. T o debug AutoML vision models and\nexplain model predictions, AutoML models for computer vision are integrated with\nResponsible AI dashboard. T o generate R esponsible AI insights for AutoML computer\nvision models, register your best AutoML model in the Azure Machine Learning\nworkspace and run it through the R esponsible AI vision insights pipeline. T o learn, see\nhow to set up AutoML to train computer vision models .\nNotebooks related to the AutoML supported computer vision tasks can be found in\nazureml-examples  repository.\nThe R esponsible AI vision Insights pipeline could be submitted through one of the\nfollowing methods\nPython SDK: T o learn how to submit the pipeline through Python, see the AutoML\nImage Classification scenario with RAI Dashboard sample notebook . For\nconstructing the pipeline, refer to section 5.1 in the notebook.\nAzure CLI: T o submit the pipeline via Azure-CLI, see the component Y AML in\nsection 5.2 of the example notebook linked above.\nUI (via Azure Machine Learning studio): From the Designer in Azure Machine\nLearning studio, the RAI-vision insights component can be used to create and\nsubmit a pipeline.\nIn addition to the list of R esponsible AI vision insights parameters provided in the\nprevious section, the following are parameters to set specifically for AutoML models.Integration with AutoML Image\nMode of submitting the Responsible AI vision insights\npipeline\nResponsible AI vision insights component parameter\n(AutoML specific)\n\uff17 Note\nA few parameters are specific to the XAI algorithm chosen and are optional for\nother algorithms."]}
{"question": "Explain how to use AutoML for forecasting time series data. What specific settings should be adjusted for time series problems?", "variant_id": "v1", "line_number": 9, "answer": "To use Azure AutoML for forecasting time series data, you should follow these general steps:\n\n1. **Set Up Your Environment**:\n   - Install the Azure Machine Learning Python SDK v2.\n   - Create an Azure Machine Learning workspace if you don't already have one.\n   - Set up your development environment, which can be a local machine or a cloud-based compute instance.\n\n2. **Prepare Your Data**:\n   - Format your data appropriately for time series forecasting. This typically involves having a time column, one or more feature columns, and a target column that you want to forecast.\n   - Ensure your data is split into training and testing datasets.\n\n3. **Configure AutoML**:\n   - Define the settings and configuration for your AutoML run. For time series forecasting, you should adjust specific settings such as:\n     - `time_column_name`: The name of the column that holds the time information.\n     - `grain_column_names`: The names of the columns by which data should be grouped. This is important for models that handle multiple time series.\n     - `max_horizon`: The number of periods to forecast.\n     - `target_lags`: The number of past periods to use to predict future periods.\n     - `target_rolling_window_size`: The window size for creating rolling features.\n     - `forecasting_parameters`: Additional parameters specific to forecasting tasks, such as seasonality, holiday features, etc.\n\n4. **Train the Model**:\n   - Submit the AutoML run with your data and configuration. AutoML will automatically try different models and preprocessing steps to find the best solution for your forecasting problem.\n\n5. **Evaluate and Deploy**:\n   - Once the run is complete, evaluate the performance of the best model using metrics such as MAE, RMSE, etc.\n   - If satisfied with the model's performance, deploy it as a web service for real-time predictions or for batch scoring.\n\nFor more detailed guidance, you can refer to the Azure documentation, specifically the articles on setting up AutoML for time-series forecasting, and the forecasting best practices. Azure also provides Jupyter notebooks with hands-on examples that can be very helpful in understanding the process.\n\nRemember that the performance of AutoML can be influenced by the complexity of the data, the number of time series, and the chosen configurations. For large numbers of time series, consider using the Many Models feature, which can distribute training jobs across a compute cluster for better scalability.", "context": ["Frequently asked questions about\nforecasting in AutoML\nArticle \u202208/01/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nThis article answers common questions about forecasting in automatic machine learning\n(AutoML). For general information about forecasting methodology in AutoML, see the\nOverview of forecasting methods in AutoML  article.\nYou can start by reading the Set up AutoML to train a time-series forecasting model\narticle. Y ou can also find hands-on examples in several Jupyter notebooks:\nBike share example\nForecasting using deep learning\nMany Models solution\nForecasting recipes\nAdvanced forecasting scenarios\nWe're always working to make AutoML faster and more scalable. T o work as a general\nforecasting platform, AutoML does extensive data validations and complex feature\nengineering, and it searches over a large model space. This complexity can require a lot\nof time, depending on the data and the configuration.\nOne common source of slow runtime is training AutoML with default settings on data\nthat contains numerous time series. The cost of many forecasting methods scales with\nthe number of series. For example, methods like Exponential Smoothing and Prophet\ntrain a model for each time series  in the training data.\nThe Many Models feature of AutoML scales to these scenarios by distributing training\njobs across a compute cluster. It has been successfully applied to data with millions of\ntime series. For more information, see the many models  article section. Y ou can also\nread about the success of Many Models  on a high-profile competition dataset.\nHow do I start building forecasting models in\nAutoML?\nWhy is AutoML slow on my data?", "Learn more about how and where to deploy a model .\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps", "If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about calendar features for time series forecasting in AutoML .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about AutoML forecasting for lagged features .Next steps", "Set up AutoML to train a time-series\nforecasting model with SDK and CLI\nArticle \u202208/02/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn this article, you'll learn how to set up AutoML for time-series forecasting with Azure\nMachine Learning automated ML in the Azure Machine Learning Python SDK .\nTo do so, you:\nFor a low code experience, see the Tutorial: Forecast demand with automated machine\nlearning  for a time-series forecasting example using automated ML in the Azure\nMachine Learning studio .\nAutoML uses standard machine learning models along with well-known time series\nmodels to create forecasts. Our approach incorporates historical information about the\ntarget variable, user-provided features in the input data, and automatically engineered\nfeatures. Model search algorithms then work to find a model with the best predictive\naccuracy. For more details, see our articles on forecasting methodology  and model\nsearch .\nFor this article you need,\nAn Azure Machine Learning workspace. T o create the workspace, see Create\nworkspace resources .\nThe ability to launch AutoML training jobs. Follow the how-to guide for setting up\nAutoML  for details.\nPrepare data for training.\uff02\nConfigure specific time-series parameters in a Forecasting Job . \uff02\nOrchestrate training, inference, and model evaluation using components and\npipelines.\uff02\nPrerequisites\nTraining  and validation data", "See the Why is AutoML slow on my data?  answer to understand why AutoML might be\nslow in your case.\nConsider the following configuration changes that might speed up your job:\nBlock time series models  like ARIMA and Prophet.\nTurn off look-back features like lags and rolling windows.\nReduce:\nThe number of trials/iterations.\nTrial/iteration timeout.\nExperiment timeout.\nThe number of cross-validation folds.\nEnsure that early termination is enabled.\nAutoML forecasting supports four basic configurations:\nConfiguration Scenar io Pros Cons\nDefault\nAutoMLRecommended if the dataset\nhas a small number of time\nseries that have roughly\nsimilar historical behavior.- Simple to configure\nfrom code/SDK or\nAzure Machine\nLearning studio.\n- AutoML can learn\nacross different time\nseries because the\nregression models\npool all series together\nin training. For more\ninformation, see\nModel grouping .- Regression models\nmight be less\naccurate if the time\nseries in the training\ndata have divergent\nbehavior.\n- Time series models\nmight take a long\ntime to train if the\ntraining data has a\nlarge number of\nseries. For more\ninformation, see the\nWhy is AutoML slow\non my data?  answer.\nAutoML with\ndeep learningRecommended for datasets\nwith more than 1,000\nobservations and, potentially,\nnumerous time series that\nexhibit complex patterns.\nWhen it's enabled, AutoML\nwill sweep over temporal- Simple to configure\nfrom code/SDK or\nAzure Machine\nLearning studio.\n- Cross-learning\nopportunities, because- Training can take\nmuch longer\nbecause of the\ncomplexity of DNN\nmodels.\n- Series with smallHow can I make AutoML faster?\nWhat modeling configuration should I use?"]}
{"question": "How do you create a custom deep learning environment with specific Python packages using the AML SDK v2?", "variant_id": "v1", "line_number": 10, "answer": "To create a custom deep learning environment with specific Python packages using the Azure Machine Learning (AML) SDK v2, you can follow these steps:\n\n1. **Define the Environment**: Use the `Environment` class from the `azure.ai.ml.entities` namespace to define your custom environment. You will need to specify the name, description, the path to your Conda environment file (which lists the Python packages and versions you want to include), and optionally, a custom Docker image if you need one.\n\n2. **Create a Conda Environment File**: Write a `conda.yaml` file that lists all the Python packages and versions you require for your deep learning tasks. This file will be used to create the environment.\n\n3. **Register the Environment**: Use the `create_or_update` method of the `ml_client.environments` to create a new environment or update an existing one with the specifications you have defined.\n\nHere is an example of how you might do this in code:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import Environment\nfrom azure.identity import DefaultAzureCredential\nimport os\n\n# Set up the MLClient with your Azure subscription and workspace information\nsubscription_id = 'your-subscription-id'\nresource_group = 'your-resource-group'\nworkspace_name = 'your-workspace-name'\nml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace_name)\n\n# Define the custom environment\ncustom_env_name = \"deep-learning-env\"\ndependencies_dir = \"path_to_your_dependencies_directory\"  # Directory where conda.yaml is located\nconda_file_path = os.path.join(dependencies_dir, \"conda.yaml\")\n\njob_env = Environment(\n    name=custom_env_name,\n    description=\"Custom environment for deep learning\",\n    conda_file=conda_file_path,\n    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",  # Optional: specify a base image\n)\n\n# Register the environment to your workspace\njob_env = ml_client.environments.create_or_update(job_env)\n\n# Print out the environment details\nprint(\n    f\"Environment with name {job_env.name} is registered to workspace, \"\n    f\"the environment version is {job_env.version}\"\n)\n```\n\nIn the `conda.yaml` file, you would specify your deep learning and other Python packages like so:\n\n```yaml\nname: deep-learning-env\ndependencies:\n  - python=3.8  # Specify the version of Python you want to use\n  - numpy\n  - pandas\n  - scikit-learn\n  - tensorflow  # Or any other deep learning framework like pytorch, keras, etc.\n  - pip:\n    - somepippackage==1.0.0\n```\n\nMake sure to replace placeholders like `your-subscription-id`, `your-resource-group`, `your-workspace-name`, and `path_to_your_dependencies_directory` with your actual Azure subscription, resource group, workspace name, and local directory path where your `conda.yaml` is located.", "context": ["The specification contains some usual packages (such as numpy and pip) that you'll use\nin your job.\nNext, use the Y AML file to create and register this custom environment in your\nworkspace. The environment will be packaged into a Docker container at runtime.\nPython\nFor more information on creating and using environments, see Create and use software\nenvironments in Azure Machine Learning .\nIn this section, we'll begin by introducing the data for training. W e'll then cover how to\nrun a training job, using a training script that we've provided. Y ou'll learn to build the\ntraining job by configuring the command for running the training script. Then, you'll\nsubmit the training job to run in Azure Machine Learning.\nYou'll use data from the Modified National Institute of S tandards and T echnology\n(MNIST) database of handwritten digits. This data is sourced from Y an LeCun's website\nand stored in an Azure storage account.\nPythonfrom azure.ai.ml.entities import Environment  \ncustom_env_name = \"keras-env\"  \njob_env = Environment(  \n    name=custom_env_name,  \n    description= \"Custom environment for keras image classification\" , \n    conda_file=os.path.join(dependencies_dir, \"conda.yaml\" ), \n    image= \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\" , \n) \njob_env = ml_client.environments.create_or_update(job_env)  \nprint( \n    f\"Environment with name {job_env.name}  is registered to workspace, the  \nenvironment version is {job_env.version} \" \n)\nConfigure and submit your training  job\nObtain the training data\nweb_path = \"wasbs://datasets@azuremlexamples.blob.core.windows.net/mnist/\"", "In the samples deep learning folder on the notebook server, find a\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\ndeploy-with-k eras.\nYour Jupyter notebook server\nInstall the Azure Machine Learning SDK (v2) .\nDownload the training scripts keras_mnist.py  and utils.py .\nYou can also find a completed Jupyter Notebook version  of this guide on the GitHub\nsamples page.\nBefore you can run the code in this article to create a GPU cluster, you'll need to request\na quota increase  for your workspace.\nThis section sets up the job for training by loading the required Python packages,\nconnecting to a workspace, creating a compute resource to run a command job, and\ncreating an environment to run the job.\nFirst, you'll need to connect to your Azure Machine Learning workspace. The Azure\nMachine Learning workspace  is the top-level resource for the service. It provides you\nwith a centralized place to work with all the artifacts you create when you use Azure\nMachine Learning.\nWe're using DefaultAzureCredential to get access to the workspace. This credential\nshould be capable of handling most Azure SDK authentication scenarios.\nIf DefaultAzureCredential doesn't work for you, see azure-identity reference\ndocumentation  or Set up authentication  for more available credentials.\nPython\nSet up the job\nConnect to the workspace\n# Handle to the workspace  \nfrom azure.ai.ml import MLClient  \n# Authentication package  \nfrom azure.identity import DefaultAzureCredential  \ncredential = DefaultAzureCredential()", "Azure Machine Learning allows you to either use a curated (or ready-made)\nenvironment or create a custom environment using a Docker image or a Conda\nconfiguration. In this article, you reuse the curated Azure Machine Learning environment\nAzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu. Use the latest version of this\nenvironment using the @latest directive.\nPython\nIn this section, we begin by introducing the data for training. W e then cover how to run\na training job, using a training script that we've provided. Y ou'll learn to build the\ntraining job by configuring the command for running the training script. Then, you'll\nsubmit the training job to run in Azure Machine Learning.\nYou can use the dataset in this zipped file . This dataset consists of about 120 training\nimages each for two classes (turkeys and chickens), with 100 validation images for each\nclass. The images are a subset of the Open Images v5 Dataset . The training script\npytorch_tr ain.p y downloads and extracts the dataset.\nIn the prerequisites section, we provided the training script pytorch_tr ain.p y. In practice,\nyou should be able to take any custom training script as is and run it with Azure\nMachine Learning without having to modify your code.\nThe provided training script downloads the data, trains a model, and registers the\nmodel.\nNow that you have all the assets required to run your job, it's time to build it using the\nAzure Machine Learning Python SDK v2. For this example, we create a command.\nAn Azure Machine Learning command is a resource that specifies all the details needed to\nexecute your training code in the cloud. These details include the inputs and outputs,curated_env_name = \"AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu@latest\"\nConfigure and submit your training  job\nObtain the training data\nPrepare the training script\nBuild the training job", "2. Create an Azure Machine Learning workspace .\n3. Clone the AzureML-Examples repository .\nBash\n4. Start the notebook server from the directory containing your clone.\nBash\nThese instructions install the base SDK packages necessary for the quickstart and tutorial\nnotebooks. Other sample notebooks might require you to install extra components. For\nmore information, see Install the Azure Machine Learning SDK for Python .\nThe Data Science Virtual Machine (DSVM) is a customized VM image built specifically for\ndoing data science. If you create a DSVM , the SDK and notebook server are installed and\nconfigured for you. However, you'll still need to create a workspace and clone the\nsample repository.\n1. Create an Azure Machine Learning workspace .\n2. Clone the the AzureML-Examples repository .\nBash\n3. Start the notebook server from the directory that contains the clone.\nBash\ngit clone https://github.com/Azure/azureml-examples.git --depth 1\njupyter notebook\nOption 3: Access on a DSVM\ngit clone https://github.com/Azure/azureml-examples.git --depth 1\njupyter notebook\nConnect to a workspace", "Manage Azu re Machine Learning\nenvironments with the CLI & SDK (v2)\nArticle \u202201/03/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAzure Machine Learning environments define the execution environments for your jobs\nor deployments and encapsulate the dependencies for your code. Azure Machine\nLearning uses the environment specification to create the Docker container that your\ntraining or scoring code runs in on the specified compute target. Y ou can define an\nenvironment from a conda specification, Docker image, or Docker build context.\nIn this article, learn how to create and manage Azure Machine Learning environments\nusing the SDK & CLI (v2).\nBefore following the steps in this article, make sure you have the following prerequisites:\nAn Azure Machine Learning workspace. If you don't have one, use the steps in the\nQuickstart: Create workspace resources  article to create one.\nThe Azure CLI and the ml extension or the Azure Machine Learning Python SDK v2:\nTo install the Azure CLI and extension, see Install, set up, and use the CLI (v2) .\nTo install the Python SDK v2, use the following command:\nBash\nPrerequisites\n\uff09 Impor tant\nThe CLI examples in this article assume that you are using the Bash (or\ncompatible) shell. For example, from a Linux system or Windows\nSubsyst em for Linux .\npip install azure-ai-ml azure-identity"]}
{"question": "Describe the process of using Azure ML to train a deep learning model on a GPU compute cluster.", "variant_id": "v1", "line_number": 11, "answer": "To train a deep learning model on a GPU compute cluster using Azure Machine Learning, you would follow these general steps:\n\n1. **Set Up Your Azure ML Workspace**:\n   - Connect to your Azure ML workspace using the `MLClient` from the Azure ML Python SDK.\n   ```python\n   from azure.ai.ml import MLClient\n   from azure.identity import DefaultAzureCredential\n\n   workspace = '<AZUREML_WORKSPACE_NAME>'\n   subscription_id = '<YOUR_SUBSCRIPTION_ID>'\n   resource_group = '<YOUR_RESOURCE_GROUP>'\n\n   ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)\n   ```\n\n2. **Create or Access a GPU Compute Cluster**:\n   - Check if the GPU compute cluster already exists.\n   - If it doesn't exist, create a new GPU compute cluster using the `AmlCompute` class, specifying the GPU VM size (e.g., `STANDARD_NC6` for a VM with one NVIDIA Tesla K80 GPU).\n   ```python\n   from azure.ai.ml.entities import AmlCompute\n\n   gpu_compute_target = \"gpu-cluster\"\n   try:\n       ml_client.compute.get(gpu_compute_target)\n   except Exception:\n       print(\"Creating a new GPU compute target...\")\n       compute = AmlCompute(\n           name=gpu_compute_target,\n           size=\"STANDARD_NC6\",\n           min_instances=0,\n           max_instances=4\n       )\n       ml_client.compute.begin_create_or_update(compute).result()\n   ```\n\n3. **Prepare Your Training Script**:\n   - Write a Python script (e.g., `train.py`) that defines your deep learning model, training process, and any other necessary code.\n\n4. **Configure the Training Job**:\n   - Create a job configuration that specifies the training script, the compute target, and any other parameters such as environment dependencies, data inputs, and outputs.\n   ```python\n   from azure.ai.ml import command, Input, Output\n\n   job = command(\n       code=\"./path_to_your_code\",\n       command=\"python train.py\",\n       compute=gpu_compute_target,\n       environment=\"<YOUR_ENVIRONMENT_NAME>\",\n       inputs={\"training_data\": Input(path=\"<PATH_TO_YOUR_DATA>\")}, \n       outputs={\"model_output\": Output()},\n       distribution={\"type\": \"pytorch\", \"process_count_per_instance\": 1}\n   )\n   ```\n\n5. **Submit the Training Job**:\n   - Submit the job to the Azure ML workspace to run on the GPU compute cluster.\n   ```python\n   returned_job = ml_client.create_or_update(job)\n   ```\n\n6. **Monitor the Training Job**:\n   - Use Azure ML Studio or the SDK to monitor the progress of your training job, view logs, and track metrics.\n\n7. **Retrieve the Trained Model**:\n   - Once the job is complete, retrieve the output which includes the trained model.\n\nThis is a high-level overview, and the actual implementation may require additional steps or modifications based on the specifics of the deep learning framework you're using (e.g., TensorFlow, PyTorch), the complexity of the model, and the dataset.", "context": ["An Azure Machine Learning compute cluster is a fully managed compute resource that\ncan be used to run the training job. In the following examples, a compute cluster named\ncpu-compute is created.\nPython\nTo run this script, you'll use a command that executes main.py Python script located\nunder ./sdk/python/jobs/single-step/lightgbm/iris/src/. The command will be runworkspace = '<AZUREML_WORKSPACE_NAME>'\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \nresource_group, workspace)\n2. Create a compute resource for training\n\uff17 Note\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\ntraining job .\nPython SDK\nfrom azure.ai.ml.entities import AmlCompute\n# specify aml compute name.\ncpu_compute_target = \"cpu-cluster\"\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(\"Creating a new cpu compute target...\" )\n    compute = AmlCompute(\n        name=cpu_compute_target, size= \"STANDARD_D2_V2\" , min_instances= 0, \nmax_instances= 4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n4. Submit the training job\nPython SDK", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Manage and optimize Azure Machine\nLearning costs\nArticle \u202208/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.\uff09 Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)", "For detailed instructions on how to complete these steps, see Secure a training\nenvironment .\nIn this section, you learn how Azure Machine Learning securely communicates between\nservices to submit a training job. This example shows you how all your configurations\nwork together to secure communication.\n1. The client uploads training scripts and training data to storage accounts that are\nsecured with a service or private endpoint.\n2. The client submits a training job to the Azure Machine Learning workspace\nthrough the private endpoint.\n3. Azure Batch service receives the job from the workspace. It then submits the\ntraining job to the compute environment through the public load balancer for the\ncompute resource.\n4. The compute resource receives the job and begins training. The compute resource\nuses information stored in key vault to access storage accounts to download\ntraining files and upload output.Example training job submission", "like Spark.\nIn the following example script, we provision a Linux compute cluster . You can see the\nAzure Machine Learning pricing  page for the full list of VM sizes and prices. Since we\nneed a GPU cluster for this example, let's pick a STAND ARD_NC6  model and create an\nAzure Machine Learning compute.\nPython\nfrom azure.ai.ml.entities import AmlCompute  \ngpu_compute_target = \"gpu-cluster\"  \ntry: \n    # let's see if the compute target already exists  \n    gpu_cluster = ml_client.compute.get(gpu_compute_target)  \n    print(  \n        f\"You already have a cluster named {gpu_compute_target} , we'll reuse  \nit as is.\"  \n    ) \nexcept Exception:  \n    print( \"Creating a new gpu compute target...\" ) \n    # Let's create the Azure ML compute object with the intended parameters  \n    gpu_cluster = AmlCompute(  \n        # Name assigned to the compute cluster  \n        name= \"gpu-cluster\" , \n        # Azure ML Compute is the on-demand VM service  \n        type= \"amlcompute\" , \n        # VM Family  \n        size= \"STANDARD_NC6\" , \n        # Minimum running nodes when there is no job running  \n        min_instances= 0, \n        # Nodes in cluster  \n        max_instances= 4, \n        # How many seconds will the node running after the job termination  \n        idle_time_before_scale_down= 180, \n        # Dedicated or LowPriority. The latter is cheaper but there is a  \nchance of job termination  \n        tier= \"Dedicated\" , \n    ) \n    # Now, we pass the object to MLClient's create_or_update method  \n    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()  \nprint( \n    f\"AMLCompute with name {gpu_cluster.name}  is created, the compute size  \nis {gpu_cluster.size} \" \n)"]}
{"question": "Explain how to deploy a TensorFlow model as a web service in Azure ML. What are the key considerations for deployment?", "variant_id": "v1", "line_number": 12, "answer": "To deploy a TensorFlow model as a web service in Azure Machine Learning, you need to follow several steps. Here are the key considerations and steps for deployment:\n\n1. **Environment Setup**: Ensure that you have an Azure subscription and have set up an Azure Machine Learning workspace.\n\n2. **Model Registration**: Before deployment, you need to register your TensorFlow model in your Azure Machine Learning workspace. This involves uploading the model files and creating a model record in the workspace.\n\n3. **Define the Scoring Script**: Create a scoring script (`score.py`) that will be used to load the model and make predictions. This script must have two required functions:\n   - `init()`: This function is called when the service is initialized and typically loads the model into a global object.\n   - `run(raw_data)`: This function is called when new data is submitted for prediction. It receives input data as a JSON string and returns the prediction results.\n\n4. **Create an Inference Environment**: Define an environment that includes all the necessary packages and dependencies required by your TensorFlow model. You can specify this in a Conda environment file.\n\n5. **Configure the Deployment**: Choose between an Azure Container Instance (ACI) for testing and low-scale CPU-based workloads, or an Azure Kubernetes Service (AKS) for high-scale production deployments. Configure the compute target, and set other deployment properties such as CPU/GPU resources, memory, and instance count.\n\n6. **Deploy the Model**: Use the Azure Machine Learning SDK to deploy the model as a web service. This involves creating an `InferenceConfig` object with the scoring script and environment, and then deploying the model using the `Model.deploy()` method.\n\n7. **Test the Web Service**: After deployment, test the web service endpoint by sending HTTP requests with input data and verifying that it returns the expected predictions.\n\n8. **Monitor and Manage the Endpoint**: Once the service is deployed, monitor its health and usage, and manage it as needed. Azure Machine Learning provides tools for logging, monitoring, and managing deployed services.\n\nKey considerations for deployment include:\n\n- **Scalability**: Choose the right compute target (ACI or AKS) based on the expected load and scalability requirements.\n- **Security**: Secure the endpoint using authentication and ensure that data is transmitted securely using SSL.\n- **Cost**: Be aware of the costs associated with the compute resources you choose for deployment.\n- **Model Performance**: Optimize your TensorFlow model for inference to ensure low latency and high throughput.\n- **Compliance and Regulations**: Ensure that your deployment complies with any relevant regulations and standards.\n\nRemember to follow best practices for production deployments, such as setting up continuous integration and delivery (CI/CD) pipelines, performing load testing, and implementing application insights for monitoring.", "context": ["What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps", "Tutorial Descr iption\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "To learn more, review these articles:\nDeploy models with REST\nCreate and use managed online endpoints in the studio\nSafe rollout for online endpoints\nHow to autoscale managed online endpoints\nView costs for an Azure Machine Learning managed online endpoint\nAccess Azure resources with a managed online endpoint and managed identity\nTroubleshoot managed online endpoints deployment", "Option 2: T o deploy a specific model iteration from this experiment.\na. Select the desired model from the Models  tab\nb. Select Deploy  on the top left of the window.\n2. Populate the Deploy model  pane.\nField Value\nName Enter a unique name for your deployment.\nDescription Enter a description to better identify what this deployment is for.\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\nService (AKS)  or Azure Container Inst ance (ACI).\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\ndeploy to.\nEnable\nauthenticationSelect to allow for token-based or key-based authentication.\nUse custom\ndeployment assetsEnable this feature if you want to upload your own scoring script and\nenvironment file. Otherwise, automated ML provides these assets for\nyou by default. Learn more about scoring scripts .\nThe Advanced menu offers default deployment features such as data collection  and\nresource utilization settings. If you wish to override these defaults do so in this\nmenu.\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\ndeployment begins, the Model summar y tab appears. See the deployment\nprogress under the Deploy status  section.\nNow you have an operational web service to generate predictions! Y ou can test the\npredictions by querying the service from Power BI's built in Azure Machine Learning\nsupport .\uff89Expand table\n\uff09 Impor tant\nFile names must be under 32 characters and must begin and end with\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\nbetween. Spaces are not allowed.", "In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\nmodel to an online endpoint. See these other articles to learn more about Azure\nMachine Learning.\nTrack run metrics during training\nTune hyperparameters\nReference architecture for distributed deep learning training in AzureNext steps"]}
{"question": "Provide an example of how to use the AML SDK v2 for distributed deep learning training. What configurations are necessary for multi-node training?", "variant_id": "v1", "line_number": 13, "answer": "To use Azure Machine Learning SDK v2 for distributed deep learning training, you would typically follow these steps and include necessary configurations for multi-node training:\n\n1. **Set up your environment**: Install the Azure Machine Learning SDK v2 and configure your environment with the necessary dependencies for your deep learning framework (PyTorch, TensorFlow, etc.).\n\n2. **Create or attach a compute target**: Define a multi-node compute cluster in Azure Machine Learning where your training will run. This involves specifying the size and number of nodes.\n\n3. **Prepare your training script**: Write a training script that is compatible with distributed training. For frameworks like PyTorch and TensorFlow, you would use their respective distributed training APIs.\n\n4. **Configure the distributed training job**: Use the Azure Machine Learning SDK to configure the distributed training job. This involves specifying the type of distribution (data parallelism), the number of worker nodes, and other configurations.\n\nHere's an example of how you might set up a distributed training job using the Azure Machine Learning SDK v2 for a PyTorch model:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import Job, Compute\nfrom azure.identity import DefaultAzureCredential\n\n# Authenticate to the Azure ML workspace\ncredential = DefaultAzureCredential()\nml_client = MLClient(credential, \"<subscription_id>\", \"<resource_group>\", \"<workspace_name>\")\n\n# Define the compute target with multiple nodes\ncompute_target = Compute(\n    name=\"my-compute-cluster\",\n    size=\"STANDARD_NC6\",\n    min_instances=0,\n    max_instances=4,\n    idle_time_before_scale_down=120\n)\n\n# Create or attach the compute target\nml_client.compute.begin_create_or_update(compute_target)\n\n# Define the distributed training settings\ndistributed_settings = PyTorch(\n    process_count_per_instance=1,  # Number of processes per node\n    node_count=4,  # Total number of nodes in the cluster\n    distributed_backend=\"nccl\"  # Backend for PyTorch distributed training\n)\n\n# Define the training job\ntraining_job = Job(\n    compute=compute_target,\n    experiment_name=\"my-experiment\",\n    code=\"./src\",  # Directory containing your training script and any other necessary files\n    command=\"python train.py\",  # Command to run your training script\n    environment=\"<environment_name>\",  # Azure ML environment for training\n    distribution=distributed_settings\n)\n\n# Submit the job\nml_client.jobs.begin_create_or_update(training_job)\n```\n\nIn this example, replace `<subscription_id>`, `<resource_group>`, `<workspace_name>`, and `<environment_name>` with your Azure subscription ID, resource group name, workspace name, and the name of the Azure ML environment you've set up for your training, respectively.\n\nPlease note that the code snippet provided is a simplified example. The actual implementation may vary based on the specifics of your training script, the deep learning framework you're using, and the Azure Machine Learning features you want to leverage. Always refer to the latest Azure Machine Learning SDK v2 documentation for the most up-to-date information and APIs.", "context": ["Distributed training with Azure Machine\nLearning\nArticle \u202203/27/2023\nIn this article, you learn about distributed training and how Azure Machine Learning\nsupports it for deep learning models.\nIn distributed training the workload to train a model is split up and shared among\nmultiple mini processors, called worker nodes. These worker nodes work in parallel to\nspeed up model training. Distributed training can be used for traditional ML models, but\nis better suited for compute and time intensive tasks, like deep learning  for training\ndeep neural networks.\nThere are two main types of distributed training: data parallelism  and model parallelism .\nFor distributed training on deep learning models, the Azure Machine Learning SDK in\nPython  supports integrations with popular frameworks, PyT orch and T ensorFlow. Both\nframeworks employ data parallelism for distributed training, and can leverage\nhorovod  for optimizing compute speeds.\nDistributed training with PyT orch\nDistributed training with T ensorFlow\nFor ML models that don't require distributed training, see train models with Azure\nMachine Learning  for the different ways to train models using the Python SDK.\nData parallelism is the easiest to implement of the two distributed training approaches,\nand is sufficient for most use cases.\nIn this approach, the data is divided into partitions, where the number of partitions is\nequal to the total number of available nodes, in the compute cluster or serverless\ncompute . The model is copied in each of these worker nodes, and each worker operates\non its own subset of the data. K eep in mind that each node has to have the capacity to\nsupport the model that's being trained, that is the model has to entirely fit on each\nnode. The following diagram provides a visual demonstration of this approach.Deep learning and distributed training\nData parallelism", "Distributed GPU training guide (SDK v2)\nArticle \u202202/02/2024\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nLearn more about using distributed GPU training code in Azure Machine Learning. This\narticle helps you run your existing distributed training code, and offers tips and\nexamples for you to follow for each framework:\nMessage P assing Interface (MPI)\nHorovod\nEnvironment variables from Open MPI\nPyTorch\nTensorFlow\nAccelerate GPU training with InfiniBand\nReview the basic concepts of distributed GPU training , such as data parallelism ,\ndistributed dat a parallelism , and model p arallelism .\nAzure Machine Learning offers an MPI job  to launch a given number of processes in\neach node. Azure Machine Learning constructs the full MPI launch command ( mpirun)\nbehind the scenes. Y ou can't provide your own full head-node-launcher commands like\nmpirun or DeepSpeed launcher.\nPrerequisites\n\uea80 Tip\nIf you don't know which type of parallelism to use, more than 90% of the time you\nshould use distribut ed data p arallelism .\nMPI\n\uea80 Tip\nThe base Docker image used by an Azure Machine Learning MPI job needs to have\nan MPI library installed. Open MPI  is included in all the Azur e Machine Learning\nGPU b ase images . When you use a custom Docker image, you are responsible\nfor making sure the image includes an MPI library. Open MPI is recommended, but", "Distributed DNN training achieves scalability using a data partitioning algorithm that\nrespects time series boundaries. The following diagram illustrates a simple example with\ntwo partitions:\nDuring training, the DNN data loaders on each compute load just what they need to\ncomplete an iteration of back-propagation; the whole dataset is nev er read int o\nmemor y. The partitions are further distributed across multiple compute cores (usually\nGPUs) on possibly multiple nodes to accelerate training. Coordination across computes\nis provided by the Horovod  framework.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about deep learning models  for forecasting in AutoML\nNext steps", "For more information, see What is automated machine learning? .\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\nLearning can automate this task for arbitrary parameterized commands with little\nmodification to your job definition. R esults are visualized in the studio.\nFor more information, see Tune hyperparameters .\nEfficiency of training for deep learning and sometimes classical machine learning\ntraining jobs can be drastically improved via multinode distributed training. Azure\nMachine Learning compute clusters and serverless compute  offer the latest GPU\noptions.\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\nclusters, and serverless compute :\nPyTorch\nTensorFlow\nMPI\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\nAzure S ynapse Analytics Spark clusters.\nFor more information, see Distributed training with Azure Machine Learning .\nScaling an ML project might require scaling embarrassingly parallel model training. This\npattern is common for scenarios like forecasting demand, where a model might be\ntrained for many stores.\nTo bring a model into production, you deploy the model. The Azure Machine Learning\nmanaged endpoints abstract the required infrastructure for both batch or real-time\n(online) model scoring (inferencing).Hyperparameter optimization\nMultinode distributed training\nEmbarrassingly parallel training\nDeploy models", "Train TensorFlow models at scale with\nAzure Machine Lea rning\nArticle \u202204/04/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this article, learn how to run your TensorFlow  training scripts at scale using Azure\nMachine Learning Python SDK v2.\nThe example code in this article train a T ensorFlow model to classify handwritten digits,\nusing a deep neural network (DNN); register the model; and deploy it to an online\nendpoint.\nWhether you're developing a T ensorFlow model from the ground-up or you're bringing\nan existing model into the cloud, you can use Azure Machine Learning to scale out\nopen-source training jobs using elastic cloud compute resources. Y ou can build, deploy,\nversion, and monitor production-grade models with Azure Machine Learning.\nTo benefit from this article, you'll need to:\nAccess an Azure subscription. If you don't have one already, create a free\naccount .\nRun the code in this article using either an Azure Machine Learning compute\ninstance or your own Jupyter notebook.\nAzure Machine Learning compute instance\u2014no downloads or installation\nnecessary\nComplete the Create resources to get started  to create a dedicated notebook\nserver pre-loaded with the SDK and the sample repository.\nIn the samples deep learning folder on the notebook server, find a\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\ndeploy-with-t ensor flow.\nYour Jupyter notebook server\nInstall the Azure Machine Learning SDK (v2) .\nDownload the following files:\ntraining script tf_mnist.py\nscoring script score.py\nsample request file sample-request.json\nPrerequisites"]}
{"question": "How can Azure ML's HyperDrive be used to tune hyperparameters of a deep learning model? Give an example of specifying the search space.", "variant_id": "v1", "line_number": 14, "answer": "Azure ML's HyperDrive can be used to tune hyperparameters of a deep learning model by automating the process of selecting the best hyperparameters for your model. HyperDrive runs multiple training experiments in parallel, each with different hyperparameter configurations. It then compares the performance of each configuration and provides the best performing hyperparameters based on the defined primary metric.\n\nHere's an example of how you can specify the search space for a deep learning model using HyperDrive in Azure Machine Learning:\n\n```python\nfrom azureml.train.hyperdrive import RandomParameterSampling\nfrom azureml.train.hyperdrive import choice, loguniform\n\n# Define the parameter search space\nparam_sampling = RandomParameterSampling( {\n    \"learning_rate\": loguniform(-6, -1),  # e.g., 10^-6 to 10^-1\n    \"batch_size\": choice(16, 32, 64, 128),  # discrete choices for batch size\n    \"momentum\": uniform(0.85, 0.95),  # a range for momentum values\n    # Add other hyperparameters you want to tune\n})\n\n# Note: The above distributions are examples. You can use other distributions\n# like `uniform`, `normal`, `qnormal`, etc., as per your model's requirements.\n```\n\nIn this example, `RandomParameterSampling` is used to define a random search over the hyperparameter space. The `learning_rate` is searched over a log-uniform distribution between 10^-6 and 10^-1, `batch_size` is chosen from a set of discrete values, and `momentum` is searched over a uniform distribution between 0.85 and 0.95.\n\nOnce you have defined the search space, you can create a HyperDriveConfig and submit a HyperDrive run in the Azure ML workspace. The service will then automatically distribute the experiments and manage the process of finding the best hyperparameters for your deep learning model.", "context": ["For more information, see What is automated machine learning? .\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\nLearning can automate this task for arbitrary parameterized commands with little\nmodification to your job definition. R esults are visualized in the studio.\nFor more information, see Tune hyperparameters .\nEfficiency of training for deep learning and sometimes classical machine learning\ntraining jobs can be drastically improved via multinode distributed training. Azure\nMachine Learning compute clusters and serverless compute  offer the latest GPU\noptions.\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\nclusters, and serverless compute :\nPyTorch\nTensorFlow\nMPI\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\nAzure S ynapse Analytics Spark clusters.\nFor more information, see Distributed training with Azure Machine Learning .\nScaling an ML project might require scaling embarrassingly parallel model training. This\npattern is common for scenarios like forecasting demand, where a model might be\ntrained for many stores.\nTo bring a model into production, you deploy the model. The Azure Machine Learning\nmanaged endpoints abstract the required infrastructure for both batch or real-time\n(online) model scoring (inferencing).Hyperparameter optimization\nMultinode distributed training\nEmbarrassingly parallel training\nDeploy models", "Hyperparameter tuning a model (v2)\nArticle \u202204/04/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAutomate efficient hyperparameter tuning using Azure Machine Learning SDK v2 and\nCLI v2 by way of the S weepJob type.\n1. Define the parameter search space for your trial\n2. Specify the sampling algorithm for your sweep job\n3. Specify the objective to optimize\n4. Specify early termination policy for low-performing jobs\n5. Define limits for the sweep job\n6. Launch an experiment with the defined configuration\n7. Visualize the training jobs\n8. Select the best configuration for your model\nHyperp aramet ers are adjustable parameters that let you control the model training\nprocess. For example, with neural networks, you decide the number of hidden layers and\nthe number of nodes in each layer. Model performance depends heavily on\nhyperparameters.\nHyperp aramet er tuning , also called hyperp aramet er optimization , is the process of\nfinding the configuration of hyperparameters that results in the best performance. The\nprocess is typically computationally expensive and manual.\nAzure Machine Learning lets you automate hyperparameter tuning and run experiments\nin parallel to efficiently optimize hyperparameters.\nTune hyperparameters by exploring the range of values defined for each\nhyperparameter.\nHyperparameters can be discrete or continuous, and has a distribution of values\ndescribed by a parameter expression .\nWhat is hyperparameter tuning ?\nDefine the search space", "When training computer vision models, model performance depends heavily on the\nhyperparameter values selected. Often, you might want to tune the hyperparameters to\nget optimal performance. For computer vision tasks, you can sweep hyperparameters to\nfind the optimal settings for your model. This feature applies the hyperparameter tuning\ncapabilities in Azure Machine Learning. Learn how to tune hyperparameters .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nYAML\nYou can define the model architectures and hyperparameters to sweep in the parameter\nspace. Y ou can either specify a single model architecture or multiple ones.\nSee Individual trials  for the list of supported model architectures for each task type.\nSee Hyperparameters for computer vision tasks  hyperparameters for each\ncomputer vision task type.Azure CLI\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value:  0.0001\n      max_value:  0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value:  0.0001\n      max_value:  0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\nDefine the parameter search space", "To tune the model's hyperparameters, define the parameter space in which to search\nduring training. Y ou'll do this by replacing some of the parameters ( batch_size,\nfirst_layer_neurons, second_layer_neurons, and learning_rate) passed to the training\njob with special inputs from the azure.ml.sweep package.\nPython\nThen, you'll configure sweep on the command job, using some sweep-specific\nparameters, such as the primary metric to watch and the sampling algorithm to use.\nIn the following code, we use random sampling to try different configuration sets of\nhyperparameters in an attempt to maximize our primary metric, validation_acc.\nWe also define an early termination policy\u2014the BanditPolicy. This policy operates by\nchecking the job every two iterations. If the primary metric, validation_acc, falls outside\nthe top ten percent range, Azure Machine Learning will terminate the job. This saves the\nmodel from continuing to explore hyperparameters that show no promise of helping to\nreach the target metric.\nPythonfrom azure.ai.ml.sweep import Choice, LogUniform  \n# we will reuse the command_job created before. we call it as a function so  \nthat we can apply inputs  \n# we do not apply the 'iris_csv' input again -- we will just use what was  \nalready defined earlier  \njob_for_sweep = job(  \n    batch_size=Choice(values=[ 25, 50, 100]), \n    first_layer_neurons=Choice(values=[ 10, 50, 200, 300, 500]), \n    second_layer_neurons=Choice(values=[ 10, 50, 200, 500]), \n    learning_rate=LogUniform(min_value= -6, max_value= -1), \n)\nfrom azure.ai.ml.sweep import BanditPolicy  \nsweep_job = job_for_sweep.sweep(  \n    compute=gpu_compute_target,  \n    sampling_algorithm= \"random\" , \n    primary_metric= \"Accuracy\" , \n    goal= \"Maximize\" , \n    max_total_trials= 20, \n    max_concurrent_trials= 4, \n    early_termination_policy=BanditPolicy(slack_factor= 0.1, \nevaluation_interval= 2), \n)", "You trained the model with one set of parameters, let's now see if you can further\nimprove the accuracy of your model. Y ou can tune and optimize your model's\nhyperparameters using Azure Machine Learning's sweep  capabilities.\nTo tune the model's hyperparameters, define the parameter space in which to search\nduring training. Y ou do this by replacing some of the parameters passed to the training\njob with special inputs from the azure.ml.sweep package.\nSince the training script uses a learning rate schedule to decay the learning rate every\nseveral epochs, you can tune the initial learning rate and the momentum parameters.\nPython\nThen, you can configure sweep on the command job, using some sweep-specific\nparameters, such as the primary metric to watch and the sampling algorithm to use.\nIn the following code, we use random sampling to try different configuration sets of\nhyperparameters in an attempt to maximize our primary metric, best_val_acc.\nWe also define an early termination policy, the BanditPolicy, to terminate poorly\nperforming runs early. The BanditPolicy terminates any run that doesn't fall within the\nslack factor of our primary evaluation metric. Y ou apply this policy every epoch (since we\nreport our best_val_acc metric every epoch and evaluation_interval=1). Notice we\ndelay the first policy evaluation until after the first 10 epochs ( delay_evaluation=10).\nPythonfrom azure.ai.ml.sweep import Uniform\n# we will reuse the command_job created before. we call it as a function so  \nthat we can apply inputs\njob_for_sweep = job(\n    learning_rate=Uniform(min_value= 0.0005, max_value= 0.005),\n    momentum=Uniform(min_value= 0.9, max_value= 0.99),\n)\nfrom azure.ai.ml.sweep import BanditPolicy\nsweep_job = job_for_sweep.sweep(\n    compute= \"gpu-cluster\" ,\n    sampling_algorithm= \"random\" ,\n    primary_metric= \"best_val_acc\" ,\n    goal=\"Maximize\" ,\n    max_total_trials= 8,\n    max_concurrent_trials= 4,\n    early_termination_policy=BanditPolicy(\n        slack_factor= 0.15, evaluation_interval= 1, delay_evaluation= 10"]}
{"question": "What is the command to update an existing model's properties, such as description or tags, using the Azure CLI?", "variant_id": "v1", "line_number": 15, "answer": "The command to update an existing model's properties, such as description or tags, using the Azure CLI is:\n\n```azurecli\naz ml model update --name run-model-example --version 1 --set description=\"This is an updated description.\" --set tags.stage=\"Prod\"\n```\n\nReplace `run-model-example` with the name of your model and `1` with the version of the model you want to update. Adjust the `description` and `tags.stage` values to the desired description and tag for your model.", "context": ["Get the details of a specific model:\ncli\nUpdate mutable properties of a specific model:\ncli\nArchiving a model will hide it by default from list queries ( az ml model list). You can\nstill continue to reference and use an archived model in your workflows. Y ou can archive\neither all versions of a model or only a specific version.\nIf you don't specify a version, all versions of the model under that given name will be\narchived. If you create a new model version under an archived model container, that\nnew version will automatically be set as archived as well.\nArchive all versions of a model:Show\nAzure CLI\naz ml model show --name run-model-example --version 1  \nUpdate\nAzure CLI\naz ml model update --name  run-model-example --version 1 --set  \ndescription=\"This is an updated description.\" --set tags.stage=\"Prod\"  \n\uff09 Impor tant\nFor model, only description and tags can be updated. All other properties are\nimmutable; if you need to change any of those properties you should create a new\nversion of the model.\nArchive", "Execute the following command in the Azure CLI, and update the <> placeholders\nwith your data asset name, version and key-value pair for the tag.\nAzure CLI\nTypically, your ETL processes organize your folder structure on Azure storage by time,\nfor example:\ntext\nThe combination of time/version structured folders and Azure Machine Learning T ables\n(MLTable) allow you to construct versioned datasets. T o show how to achieve versioned\ndata with Azure Machine Learning T ables, we use a hypothetical ex ample . Suppose you\nhave a process that uploads camera images to Azure Blob storage every week, in the\nfollowing structure:\ntextaz ml data update --name <DATA ASSET NAME>  --version  <VERSION>  --set \ntags.<KEY>=<VALUE> \nVersioning best practices\n/ \n\u2514\u2500\u2500 \ud83d\udcc1 mydata \n    \u251c\u2500\u2500 \ud83d\udcc1  year=2022  \n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1  month=11  \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4  file1 \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4  file2 \n    \u2502   \u2514\u2500\u2500 \ud83d\udcc1  month=12  \n    \u2502       \u2514\u2500\u2500 \ud83d\udcc4  file1 \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4  file2 \n    \u2514\u2500\u2500 \ud83d\udcc1  year=2023  \n        \u2514\u2500\u2500 \ud83d\udcc1  month=1  \n            \u2514\u2500\u2500 \ud83d\udcc4  file1 \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4  file2 \n/myimages  \n\u2514\u2500\u2500 \ud83d\udcc1 year=2022  \n    \u251c\u2500\u2500 \ud83d\udcc1  week52 \n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1  camera1  \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\uddbc  file1.jpeg  \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\uddbc  file2.jpeg  \n    \u2502   \u2514\u2500\u2500 \ud83d\udcc1  camera2  \n    \u2502       \u2514\u2500\u2500 \ud83d\uddbc  file1.jpeg  \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\uddbc  file2.jpeg", "The az ml model command can be used for managing Azure Machine Learning models.\nExamples are available in the examples GitHub repository . Several are shown below.\nYAML\nYAML\nInstall and use the CLI (v2)Remarks\nExamples\nYAML: local file\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-file-example  \npath: mlflow-model/model.pkl  \ndescription:  Model created from local file. \nYAML: local folder in MLflow format\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-mlflow-example  \npath: mlflow-model  \ntype: mlflow_model  \ndescription:  Model created from local MLflow model directory.", "If you want to update the code, model, or environment, update the Y AML file, and\nthen run the az ml online-endpoint update command.\nTo understand how update works:\n1. Open the file online/model-1/onlines coring/s core.py.\n2. Change the last line of the init() function: After logging.info(\"Init\ncomplete\"), add logging.info(\"Updated successfully\").\n3. Save the file.\n4. Run this command:\nAzure CLI(Optional) Update the deployment\nAzure CLI\n\uff17 Note\nIf you update instance count (to scale your deployment) along with other\nmodel settings (such as code, model, or environment) in a single update\ncommand, the scaling operation will be performed first, then the other updates\nwill be applied. It's a good practice to perform these operations separately in a\nproduction environment.\naz ml online-deployment update -n blue --endpoint  $ENDPOINT_NAME  -f \nendpoints/online/managed/sample/blue -deployment .yml\n\uff17 Note\nUpdating by using Y AML is declarative. That is, changes in the Y AML are\nreflected in the underlying Azure R esource Manager resources (endpoints\nand deployments). A declarative approach facilitates GitOps : All\nchanges to endpoints and deployments (even instance_count) go\nthrough the Y AML.\n\uea80 Tip", "Create a Y AML file, and copy-and-paste the following code. Y ou must update the\n<> placeholders with the name of your data asset, the version, description, tags\n(key-value pairs) and the path to a single file on a supported location.\nYAML\nNext, execute the following command in the CLI (update the <filename>\nplaceholder to the Y AML filename):\ncliAzure CLI\n$schema:  https://azuremlschemas.azureedge.net/latest/data.schema.json  \n# Supported paths include:  \n# local: './<path>/<file>' (this will be automatically uploaded to cloud  \nstorage)  \n# blob:  \n'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<f\nile>' \n# ADLS gen2:  \n'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<file>\n' \n# Datastore:  \n'azureml://datastores/<data_store_name>/paths/<path>/<file>'  \n# Data asset types, use one of:  \n# uri_file, uri_folder, mltable  \ntype: uri_file  \nname: <NAME OF DATA ASSET> \nversion:  <VERSION>  \ndescription:  <DESCRIPTION>  \ntags: \n    <KEY1>: <VALUE> \n    <KEY2>: <VALUE> \npath: <SUPPORTED  PATH> \naz ml data create -f <filename>.yml  \nAdd tags to an existing data asset\nAzure CLI"]}
{"question": "How does Azure ML integrate with Azure DevOps for implementing CI/CD pipelines for ML models? Describe the steps involved.", "variant_id": "v1", "line_number": 16, "answer": "Azure Machine Learning integrates with Azure DevOps to implement CI/CD pipelines for ML models by allowing you to automate the entire lifecycle of machine learning models from training to deployment and management. Here are the general steps involved in setting up CI/CD pipelines for ML models with Azure ML and Azure DevOps:\n\n1. **Source Control Integration**:\n   - Store your machine learning code and configuration files in a version control system supported by Azure DevOps (e.g., Azure Repos or GitHub).\n   - Use Azure Repos for Git hosting or integrate with existing repositories on GitHub or other services.\n\n2. **Azure Pipelines Setup**:\n   - Create a new pipeline in Azure DevOps.\n   - Use the Azure Pipelines service to define the continuous integration (CI) and continuous deployment (CD) workflows.\n   - Define the pipeline using a YAML file or through the Azure DevOps user interface.\n\n3. **Continuous Integration (CI)**:\n   - Trigger the CI pipeline on code changes (e.g., pull requests, merges).\n   - The CI pipeline typically includes steps to:\n     - Install dependencies.\n     - Run tests.\n     - Train machine learning models using Azure ML.\n     - Register the trained models with Azure ML Model Registry.\n\n4. **Continuous Deployment (CD)**:\n   - Define the CD pipeline to automate the deployment of the trained model to various environments (e.g., testing, staging, production).\n   - Use Azure ML to deploy models as web services on Azure Container Instances (ACI), Azure Kubernetes Service (AKS), or other supported compute targets.\n   - Include steps for model validation, evaluation, and possibly A/B testing.\n\n5. **Model Monitoring and Management**:\n   - After deployment, monitor the model's performance and data drift using Azure ML's monitoring capabilities.\n   - Set up automated retraining pipelines if necessary, based on performance metrics or data drift.\n\n6. **Artifact Tracking**:\n   - Track artifacts like datasets, models, and evaluation metrics using Azure ML's tracking and versioning features.\n\n7. **Security and Compliance**:\n   - Ensure that the pipelines adhere to security and compliance requirements by using features like Azure Key Vault for secrets management and role-based access control (RBAC) in Azure ML.\n\n8. **Feedback Loop**:\n   - Implement a feedback loop to continuously improve the model based on new data and user feedback.\n\nBy integrating Azure ML with Azure DevOps, teams can automate the end-to-end process of training, deploying, and managing machine learning models, ensuring that the models are always up-to-date and meet performance standards.", "context": ["Continuous integration and deployment of machine learning models with Azure\nPipelines\nMachine Learning ML Ops  repository\nLearn more by reading and exploring the following resources:\nSet up ML Ops with Azure DevOps\nLearning path: End-to-end ML Ops with Azure Machine Learning\nHow to deploy a model to an online endpoint  with Machine Learning\nTutorial: T rain and deploy a model\nCI/CD of machine learning models with Azure Pipelines\nMachine learning at scale\nAzure AI reference architectures and best practices repo\nNext steps", "Some key features enabling ML Ops include:\ngit integration.\nMLflow integration.\nMachine learning pipeline scheduling.\nAzure Event Grid integration for custom triggers.\nEase of use with CI/CD tools like GitHub Actions or Azure DevOps.\nMachine Learning also includes features for monitoring and auditing:\nJob artifacts, such as code snapshots, logs, and other outputs.\nLineage between jobs and assets, such as containers, data, and compute resources.\nIf you use Apache Airflow, the airflow-provider-azure-machinelearning  package is a\nprovider that enables you to submit workflows to Azure Machine Learning from Apache\nAirFlow.\nStart using Azure Machine Learning:\nSet up an Azure Machine Learning workspace\nTutorial: Build a first machine learning project\nRun training jobs\nNext steps", "What are Azu re Machine Learning\npipelines?\nArticle \u202201/31/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAn Azure Machine Learning pipeline is an independently executable workflow of a\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\nstandardize the best practices of producing a machine learning model, enables the team\nto execute at scale, and improves the model building efficiency.\nThe core of a machine learning pipeline is to split a complete machine learning task into\na multistep workflow. Each step is a manageable component that can be developed,\noptimized, configured, and automated individually. S teps are connected through well-\ndefined interfaces. The Azure Machine Learning pipeline service automatically\norchestrates all the dependencies between pipeline steps. This modular approach brings\ntwo key benefits:\nStandardize the Machine learning operation (ML Ops) practice and support scalable\nteam collaboration\nTraining efficiency and cost reduction\nMachine learning operation (ML Ops) automates the process of building machine\nlearning models and taking the model to production. This is a complex process. It\nusually requires collaboration from different teams with different skills. A well-defined\nmachine learning pipeline can abstract this complex process into a multiple steps\nworkflow, mapping each step to a specific task such that each team can work\nindependently.\nFor example, a typical machine learning project includes the steps of data collection,\ndata preparation, model training, model evaluation, and model deployment. Usually, the\ndata engineers concentrate on data steps, data scientists spend most time on model\nWhy are Azure Machine Learning pipelines\nneeded?\nStandardize the MLOps practice and support scalable\nteam collaboration", "What are Azu re Machine Learning\npipelines?\nArticle \u202201/31/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAn Azure Machine Learning pipeline is an independently executable workflow of a\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\nstandardize the best practices of producing a machine learning model, enables the team\nto execute at scale, and improves the model building efficiency.\nThe core of a machine learning pipeline is to split a complete machine learning task into\na multistep workflow. Each step is a manageable component that can be developed,\noptimized, configured, and automated individually. S teps are connected through well-\ndefined interfaces. The Azure Machine Learning pipeline service automatically\norchestrates all the dependencies between pipeline steps. This modular approach brings\ntwo key benefits:\nStandardize the Machine learning operation (ML Ops) practice and support scalable\nteam collaboration\nTraining efficiency and cost reduction\nMachine learning operation (ML Ops) automates the process of building machine\nlearning models and taking the model to production. This is a complex process. It\nusually requires collaboration from different teams with different skills. A well-defined\nmachine learning pipeline can abstract this complex process into a multiple steps\nworkflow, mapping each step to a specific task such that each team can work\nindependently.\nFor example, a typical machine learning project includes the steps of data collection,\ndata preparation, model training, model evaluation, and model deployment. Usually, the\ndata engineers concentrate on data steps, data scientists spend most time on model\nWhy are Azure Machine Learning pipelines\nneeded?\nStandardize the MLOps practice and support scalable\nteam collaboration", "Use Azu re Pipelines with Azure Machine\nLearning\nArticle \u202209/29/2023\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\nof the operations you can automate are:\nData preparation (extract, transform, load operations)\nTraining machine learning models with on-demand scale-out and scale-up\nDeployment of machine learning models as public or private web services\nMonitoring deployed machine learning models (such as for performance or data-\ndrift analysis)\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\nmachine learning model to Azure Machine Learning .\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\nv2.\nComplete the Create resources to get started  to:\nCreate a workspace\nCreate a cloud-based compute cluster  to use for training your model\nAzure Machine Learning extension for Azure Pipelines. This extension can be\ninstalled from the Visual S tudio marketplace at\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-\nv2.\nFork the following repo at GitHub:Prerequisites\nStep 1: Get the code\nhttps://github.com/azure/azureml-examples"]}
{"question": "Explain the process of setting up a real-time inferencing endpoint using a deep learning model in Azure ML. What are the performance tuning options available?", "variant_id": "v1", "line_number": 17, "answer": "To set up a real-time inferencing endpoint using a deep learning model in Azure Machine Learning, you would typically follow these steps:\n\n1. **Register the Model**:\n   - First, you need to register your deep learning model in Azure ML. This can be done using the Azure ML Python SDK or the Azure CLI. You'll need to provide the model name, path to the model file, and any other relevant metadata.\n\n2. **Create Scoring Script**:\n   - Create a scoring script (often named `score.py`). This script is executed by the web service and uses the model to predict new data. It must include two functions: `init()` to load the model and `run(data)` to use the model for inferencing.\n\n3. **Define an Inference Configuration**:\n   - Define an inference configuration that specifies the environment needed for the model and the scoring script. This includes the machine learning framework, any necessary packages, and the scoring script itself.\n\n4. **Define an Endpoint**:\n   - Specify the endpoint name and choose the authentication mode (key-based or token-based).\n\n5. **Deploy the Model to an Online Endpoint**:\n   - Deploy the model to an online endpoint. You can choose between a managed online endpoint (recommended for ease of use and scalability) or a private endpoint if you need more control over the environment.\n\n6. **Test the Endpoint**:\n   - Once the endpoint is deployed, test it by sending HTTP requests with input data and verifying that you receive the expected predictions in response.\n\nFor performance tuning of real-time inferencing endpoints in Azure ML, you have several options:\n\n1. **Choose the Right Compute Type**:\n   - Select the appropriate compute type (CPU or GPU) based on the requirements of your deep learning model.\n\n2. **Scale Up or Scale Out**:\n   - You can scale up by choosing a more powerful machine or scale out by increasing the number of instances to handle more requests in parallel.\n\n3. **Optimize the Model**:\n   - Use model optimization techniques like quantization, pruning, or model distillation to reduce the size of the model and improve inference speed without significantly compromising accuracy.\n\n4. **Batch Requests**:\n   - If your use case allows, you can batch requests to utilize the compute resources more efficiently.\n\n5. **Use Efficient Data Formats**:\n   - Opt for efficient data formats for input and output to reduce serialization and deserialization overhead.\n\n6. **Concurrency and Instance Tuning**:\n   - Adjust the concurrency level and instance count based on the load patterns to optimize throughput and latency.\n\n7. **Monitoring and Diagnostics**:\n   - Use Azure ML's monitoring and diagnostics features to identify bottlenecks and performance issues, which can then be addressed through tuning.\n\nRemember to follow best practices for security, monitoring, and management to maintain a robust and efficient real-time inferencing service.", "context": ["Online endpoints and deployments for\nreal-time inference\nArticle \u202210/24/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAzure Machine Learning allows you to perform real-time inferencing on data by using\nmodels that are deployed to online endpoints . Inferencing is the process of applying new\ninput data to a machine learning model to generate outputs. While these outputs are\ntypically referred to as \"predictions,\" inferencing can be used to generate outputs for\nother machine learning tasks, such as classification and clustering.\nOnline endpoints  deploy models to a web server that can return predictions under the\nHTTP protocol. Use online endpoints to operationalize models for real-time inference in\nsynchronous low-latency requests. W e recommend using them when:\nTo define an endpoint, you need to specify:\nEndpoint name : This name must be unique in the Azure region. For more\ninformation on the naming rules, see endpoint limits .\nAuthentication mode : You can choose between key-based authentication mode\nand Azure Machine Learning token-based authentication mode for the endpoint. A\nkey doesn't expire, but a token does expire. For more information on\nauthenticating, see Authenticate to an online endpoint .\nAzure Machine Learning provides the convenience of using managed online endpoints\nfor deploying your ML models in a turnkey manner. This is the recommended  way to use\nonline endpoints in Azure Machine Learning. Managed online endpoints work with\npowerful CPU and GPU machines in Azure in a scalable, fully managed way. These\nendpoints also take care of serving, scaling, securing, and monitoring your models, to\nfree you from the overhead of setting up and managing the underlying infrastructure. T o\nOnline  endpoints\nYou have low-latency requirements\uff02\nYour model can answer the request in a relatively short amount of time\uff02\nYour model's inputs fit on the HT TP payload of the request \uff02\nYou need to scale up in terms of number of requests\uff02", "In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\nmodel to an online endpoint. See these other articles to learn more about Azure\nMachine Learning.\nTrack run metrics during training\nTune hyperparameters\nReference architecture for distributed deep learning training in AzureNext steps", "Batch scoring, or batch infer encing , involves invoking an endpoint with a reference to\ndata. The batch endpoint runs jobs asynchronously to process data in parallel on\ncompute clusters and store the data for further analysis.\nReal-time s coring, or online infer encing , involves invoking an endpoint with one or more\nmodel deployments and receiving a response in near real time via HT TPS. T raffic can be\nsplit across multiple deployments, allowing for testing new model versions by diverting\nsome amount of traffic initially and increasing after confidence in the new model is\nestablished.\nFor more information, see:\nDeploy a model with a real-time managed endpoint\nUse batch endpoints for scoring\nDevOps for ML models, often called ML Ops, is a process for developing models for\nproduction. A model's lifecycle from training to deployment must be auditable if not\nreproducible.\nLearn more about MLOps in Azure Machine Learning .\nMachine Learning is built with the model lifecycle in mind. Y ou can audit the model\nlifecycle down to a specific commit and environment.Real-time and batch scoring (inferencing)\nMLOps: DevOps for machine learning\nML model lifecycle\nIntegrations enabling MLOPs", "What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps", "Currently, Azure Machine Learning supports fine-tuning models for the following\nlanguage tasks:\nText classification\nToken classification\nQuestion answering\nSummarization\nTranslation\nTo enable users to quickly get started with fine-tuning, we have published samples (both\nPython notebooks and CLI examples) for each task in the azureml-examples git repo\nFinetune samples . Each model card also links to fine-tuning samples for supported\nfine-tuning tasks.\nYou can deploy foundation models (both pre-trained models from the model catalog,\nand fine-tuned models, once they're registered to your workspace) to an endpoint that\ncan then be used for inferencing. Deployment to both real time endpoints and batch\nendpoints is supported. Y ou can deploy these models by using either the Deploy UI\nwizard or by using the code based samples linked from the model card.\nYou can invoke the Deploy UI form by selecting the Deploy  button on the model card\nfor any foundation model, and selecting either R eal-time endpoint or Batch endpoint\nDeploying foundation models to endpoints for\ninferencing\nDeploying using the studio\n\uf80a\nDeployment settings"]}
{"question": "How can you automate the process of model retraining and deployment using Azure ML Pipelines?", "variant_id": "v1", "line_number": 18, "answer": "To automate the process of model retraining and deployment using Azure ML Pipelines, you can follow these general steps:\n\n1. **Create an Azure ML Workspace**: This is the foundational block where all the resources are managed. You need to have an Azure ML Workspace set up to start with.\n\n2. **Set Up Compute Resources**: Create a cloud-based compute cluster that will be used for training your model. This can be done within the Azure ML Workspace.\n\n3. **Prepare Your Training Script**: Write a training script in Python that defines how your model should be trained. This script will be used by the pipeline to train the model.\n\n4. **Define a Pipeline**: Use the Azure Machine Learning Python SDK v2 to define a pipeline. A pipeline is a workflow of a series of steps, each of which can perform tasks like data preparation, model training, model evaluation, and more.\n\n5. **Create a Retraining Pipeline**: Within the pipeline, define a step for retraining the model. This step will use the compute resources to run the training script.\n\n6. **Create a Deployment Pipeline**: Define another step in the pipeline for deploying the model. This can be done using Azure ML's model deployment capabilities, which allow you to deploy a model as a web service.\n\n7. **Set Up Automated Triggers**: Configure the pipeline to run on a schedule or to be triggered by a specific event, such as new data arriving or a manual trigger.\n\n8. **Monitor the Pipeline**: Use Azure ML's monitoring capabilities to keep track of the pipeline's performance and the model's accuracy. Set up alerts for any issues that might arise.\n\n9. **Version Control**: Use Azure Repos or GitHub to manage your code and pipeline definitions, ensuring that changes are tracked and the process is reproducible.\n\n10. **Install Azure ML Extension for Azure Pipelines**: To integrate Azure Pipelines with Azure Machine Learning, install the Azure Machine Learning extension for Azure Pipelines from the Visual Studio Marketplace.\n\n11. **Configure the Pipeline in Azure DevOps**: Set up the pipeline in Azure DevOps, using the Azure ML extension to connect to your Azure ML Workspace and manage the execution of your ML workflows.\n\nBy following these steps, you can create a robust CI/CD (Continuous Integration/Continuous Deployment) pipeline that automates the retraining and deployment of machine learning models using Azure ML Pipelines. This ensures that your models are kept up-to-date with the latest data and are deployed efficiently to production environments.", "context": ["Use Azu re Pipelines with Azure Machine\nLearning\nArticle \u202209/29/2023\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\nof the operations you can automate are:\nData preparation (extract, transform, load operations)\nTraining machine learning models with on-demand scale-out and scale-up\nDeployment of machine learning models as public or private web services\nMonitoring deployed machine learning models (such as for performance or data-\ndrift analysis)\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\nmachine learning model to Azure Machine Learning .\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\nv2.\nComplete the Create resources to get started  to:\nCreate a workspace\nCreate a cloud-based compute cluster  to use for training your model\nAzure Machine Learning extension for Azure Pipelines. This extension can be\ninstalled from the Visual S tudio marketplace at\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-\nv2.\nFork the following repo at GitHub:Prerequisites\nStep 1: Get the code\nhttps://github.com/azure/azureml-examples", "Continuous integration and deployment of machine learning models with Azure\nPipelines\nMachine Learning ML Ops  repository\nLearn more by reading and exploring the following resources:\nSet up ML Ops with Azure DevOps\nLearning path: End-to-end ML Ops with Azure Machine Learning\nHow to deploy a model to an online endpoint  with Machine Learning\nTutorial: T rain and deploy a model\nCI/CD of machine learning models with Azure Pipelines\nMachine learning at scale\nAzure AI reference architectures and best practices repo\nNext steps", "6. Select main as a branch and choose /mlops/devops-pipelines/deploy-model-\ntraining-pipeline.yml, then select Continue .\n7. Save and Run  the pipeline\nThis scenario includes prebuilt workflows for two approaches to deploying a trained\nmodel, batch scoring or a deploying a model to an endpoint for real-time scoring. Y ou\nmay run either or both of these workflows to test the performance of the model in your\nAzure ML workspace. IN this example we will be using real-time scoring.\n1. Go to ADO pipelines\n2. Select New Pipeline .\n3. Select Azur e Repos Git .\uff17 Note\nAt this point, the infrastructure is configured and the Prototyping Loop of the\nMLOps Architecture is deployed. you're ready to move to our trained model to\nproduction.\nDeploying the Trained model\nDeploy ML model endpoint", "3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\nMachine Learning in many different ways .\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\niterations over different models, hyperparameter settings, advanced\npreprocessing/featurization, and what metrics to look at when determining the\nbest model.\n5. Submit the training job.\n6. Review the r esults\nThe following diagram illustrates this process. \nYou can also inspect the logged job information, which contains metrics  gathered\nduring the job. The training job produces a Python serialized object ( .pkl file) that\ncontains the model and data preprocessing.\nWhile model building is automated, you can also learn how important or relevant\nfeatures are  to the generated models.\nApply automated ML when you want Azure Machine Learning to train and tune a model\nfor you using the target metric you specify. Automated ML democratizes the machine\nlearning model development process, and empowers its users, no matter their data\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\nML professionals and developers across industries can use automated ML to:\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\nforecasting, computer vision & NLP", "4. When the pipeline has complete successfully, you can find your Azure Machine\nLearning W orkspace and associated resources by logging in to the Azure P ortal.\nNext, a model training and scoring pipelines will be deployed into the new\nMachine Learning environment.\nThe solution accelerator includes code and data for a sample end-to-end machine\nlearning pipeline which runs a linear regression to predict taxi fares in NY C. The pipeline\nis made up of components, each serving different functions, which can be registered\nwith the workspace, versioned, and reused with various inputs and outputs. Sample\npipelines and workflows for the Computer Vision and NLP scenarios will have different\nsteps and deployment steps.\nThis training pipeline contains the following steps:\nPrepare Data\nThis component takes multiple taxi datasets (yellow and green) and merges/filters\nthe data, and prepare the train/val and evaluation datasets.\nInput: Local data under ./data/ (multiple .csv files)\nOutput: Single prepared dataset (.csv) and train/val/test datasets.\nTrain Model\nThis component trains a Linear R egressor with the training set.\nInput: T raining dataset\nOutput: T rained model (pickle format)\nEvaluat e Model\nThis component uses the trained model to predict taxi fares on the test set.\nInput: ML model and T est dataset\nOutput: P erformance of model and a deploy flag whether to deploy or not.Sample Training  and Deployment Scenario"]}
{"question": "Describe the approach to use Azure ML for implementing a recommendation system. What kind of data and algorithms are typically involved?", "variant_id": "v1", "line_number": 19, "answer": "To implement a recommendation system using Azure Machine Learning (Azure ML), you would typically follow these steps:\n\n1. **Data Upload and Access**:\n   - Start by uploading your data to Azure ML. This data could be user-item interactions, user or item features, or a combination of both.\n   - The data is usually stored in Azure Blob Storage or Azure Data Lake and registered as a dataset in Azure ML for easy access and management.\n\n2. **Data Exploration**:\n   - Use Azure ML notebooks or scripts to explore and preprocess your data. This might involve handling missing values, normalizing data, or encoding categorical variables.\n\n3. **Model Development**:\n   - Develop your recommendation model using Azure ML's cloud-based workstations. You can use popular Python libraries like TensorFlow, PyTorch, or scikit-learn, which are supported in Azure ML environments.\n   - Choose an algorithm suitable for recommendation systems, such as collaborative filtering, content-based filtering, or hybrid methods. Azure ML also supports deep learning approaches that can be used for complex recommendation systems.\n\n4. **Model Training**:\n   - Train your model using Azure ML's training capabilities. You can scale out the training process by using Azure ML compute clusters.\n   - During training, you can leverage Azure ML's tracking and logging features to monitor metrics and manage experiments.\n\n5. **Model Deployment**:\n   - Once the model is trained and evaluated, deploy it as an online endpoint using Azure ML.\n   - The deployment can be done on Azure Container Instances (ACI), Azure Kubernetes Service (AKS), or other supported compute targets.\n   - Azure ML handles the infrastructure and provides an HTTP endpoint for your application to consume the recommendation service.\n\n6. **Production Pipelines**:\n   - Create production-grade machine learning pipelines in Azure ML to automate the workflow of data preparation, model training, evaluation, and deployment.\n   - Pipelines ensure that the recommendation system is maintainable and scalable.\n\n**Data Types**:\n- User-item interaction data, which could be explicit (ratings, likes) or implicit (clicks, views).\n- User demographic data or profile information.\n- Item attributes or content information.\n\n**Algorithms**:\n- Collaborative Filtering: Matrix factorization techniques like Singular Value Decomposition (SVD) or algorithms like ALS (Alternating Least Squares).\n- Content-Based Filtering: Using item features to recommend similar items to a user's past preferences.\n- Hybrid Methods: Combining collaborative and content-based filtering to improve recommendations.\n- Deep Learning: Neural networks, such as autoencoders or RNNs, for complex feature learning and prediction tasks.\n\nAzure ML provides the tools and infrastructure to build, train, and deploy recommendation systems at scale, leveraging the cloud's computational resources and services.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Tell us about y our PDF experience.\nAzur e Machine Learning documentation\nLearn how to train and deploy models and manage the ML lifecycle (ML Ops) with Azure\nMachine Learning. Tutorials, code examples, API references, and more.\nOverview\n\uff45OVERVIE W\nWhat is Azure Machine Learning?\nSetup & quickstar t\n\uff46QUICKST ART\nCreate resources\nGet started with Azure Machine Learning\nStart with the b asics\n\uff47TUTORIAL\nPrepare and explore data\nDevelop on a cloud workstation\nTrain a model\nDeploy a model\nSet up a reusable pipeline\nWork with data\n\uff43HOW-T O GUIDE\nUse Apache Spark in Azure Machine Learning\nCreate data assets", "What is Azure Machine Learning?\nArticle \u202201/29/2024\nAzure Machine Learning is a cloud service for accelerating and managing the machine\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\nin their day-to-day workflows to train and deploy models and manage machine learning\noperations (ML Ops).\nYou can create a model in Machine Learning or use a model built from an open-source\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\nretrain, and redeploy models.\nMachine Learning is for individuals and teams implementing ML Ops within their\norganization to bring ML models into production in a secure and auditable production\nenvironment.\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\nday workflows. Application developers can use tools for integrating models into\napplications or services. Platform developers can use a robust set of tools, backed by\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\nbased access control for infrastructure. Y ou can set up a project to deny access to\nprotected data and select operations.\nML projects often require a team with a varied skill set to build and maintain. Machine\nLearning has tools that help enable you to:\uea80 Tip\nFree trial!  If you don't have an Azure subscription, create a free account before you\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\nto spend on Azure services. After they're used up, you can keep the account and\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\nchange your settings and ask to be charged.\nWho is Azure Machine Learning for?\nProductivity for everyone on the team", "If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about calendar features for time series forecasting in AutoML .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about AutoML forecasting for lagged features .Next steps", "For training in developing AI and Machine Learning solutions with Microsoft, see\nMicrosoft Learn training .\nChoose a Microsoft cognitive services technology\nArtificial intelligence (AI) architecture design\nHow Azure Machine Learning works: resources and assetsRelated resources"]}
{"question": "How can you leverage Azure ML's built-in support for ONNX models for interoperability across different frameworks?", "variant_id": "v1", "line_number": 20, "answer": "Azure Machine Learning (Azure ML) provides built-in support for ONNX (Open Neural Network Exchange) models, which facilitates interoperability across different machine learning frameworks. Here's how you can leverage this support:\n\n1. **Model Conversion to ONNX**: You can convert models from various frameworks such as TensorFlow, PyTorch, Scikit-Learn, Keras, and others into the ONNX format. Azure ML supports the conversion process through its integration with the ONNX converters available for these frameworks.\n\n2. **Model Deployment**: Once you have an ONNX model, Azure ML allows you to deploy it as a web service on Azure Container Instances (ACI), Azure Kubernetes Service (AKS), or on IoT devices. The ONNX Runtime is used to serve the model, ensuring efficient inference across different platforms.\n\n3. **Inference Optimization**: ONNX Runtime, which is integrated with Azure ML, provides optimizations for different hardware accelerators. This means that you can deploy your ONNX models and expect optimized performance on CPUs, GPUs, and other specialized hardware.\n\n4. **MLOps**: Azure ML supports MLOps practices, which include versioning, monitoring, and CI/CD for machine learning models. ONNX models can be integrated into this workflow, allowing for a consistent and reproducible way to manage the lifecycle of your models across different environments.\n\n5. **Scalability**: With Azure ML, you can scale your ONNX model deployments to handle large volumes of inference requests, which is essential for production workloads.\n\n6. **Monitoring and Diagnostics**: Azure ML provides tools for monitoring the performance and health of your deployed models, including ONNX models. This helps in maintaining the reliability and efficiency of your machine learning solutions.\n\nBy using Azure ML's support for ONNX, you can train your models in your preferred framework, convert them to ONNX format, and then deploy and manage them efficiently, while also ensuring that they are performant across various platforms and hardware configurations.", "context": ["ONNX and Azure Machine Learning:\nCreate and accelerate ML models\nArticle \u202206/13/2023\nLearn how using the Open Neural Network Exchange  (ONNX) can help optimize the\ninference of your machine learning model. Inference, or model scoring, is the phase\nwhere the deployed model is used for prediction, most commonly on production data.\nOptimizing machine learning models for inference (or model scoring) is difficult since\nyou need to tune the model and the inference library to make the most of the hardware\ncapabilities. The problem becomes extremely hard if you want to get optimal\nperformance on different kinds of platforms (cloud/edge, CPU/GPU, etc.), since each one\nhas different capabilities and characteristics. The complexity increases if you have\nmodels from a variety of frameworks that need to run on a variety of platforms. It's very\ntime consuming to optimize all the different combinations of frameworks and hardware.\nA solution to train once in your preferred framework and run anywhere on the cloud or\nedge is needed. This is where ONNX comes in.\nMicrosoft and a community of partners created ONNX as an open standard for\nrepresenting machine learning models. Models from many frameworks  including\nTensorFlow, PyT orch, SciKit-Learn, K eras, Chainer, MXNet, MA TLAB, and SparkML can be\nexported or converted to the standard ONNX format. Once the models are in the ONNX\nformat, they can be run on a variety of platforms and devices.\nONNX Runtime  is a high-performance inference engine for deploying ONNX models\nto production. It's optimized for both cloud and edge and works on Linux, Windows,\nand Mac. Written in C++, it also has C, Python, C#, Java, and JavaScript (Node.js) APIs\nfor usage in a variety of environments. ONNX Runtime supports both DNN and\ntraditional ML models and integrates with accelerators on different hardware such as\nTensorR T on NVidia GPUs, OpenVINO on Intel processors, DirectML on Windows, and\nmore. By using ONNX Runtime, you can benefit from the extensive production-grade\noptimizations, testing, and ongoing improvements.\nONNX Runtime is used in high-scale Microsoft services such as Bing, Office, and Azure\nAI. Performance gains are dependent on a number of factors, but these Microsoft\nservices have seen an average 2x per formance gain on CPU . In addition to Azure\nMachine Learning services, ONNX Runtime also runs in other products that support\nMachine Learning workloads, including:\nWindows: The runtime is built into Windows as part of Windows Machine Learning\nand runs on hundreds of millions of devices.", "Azure SQL product family: Run native scoring on data in Azure SQL Edge  and\nAzure SQL Managed Instance .\nML.NET: Run ONNX models in ML.NET .\nYou can obtain ONNX models in several ways:\nTrain a new ONNX model in Azure Machine Learning (see examples at the bottom\nof this article) or by using automated Machine Learning capabilities\nConvert existing model from another format to ONNX (see the tutorials )\nGet a pre-trained ONNX model from the ONNX Model Zoo\nGenerate a customized ONNX model from Azure Custom Vision service\nMany models including image classification, object detection, and text processing can\nbe represented as ONNX models. If you run into an issue with a model that cannot be\nconverted successfully, please file an issue in the GitHub of the respective converter that\nyou used. Y ou can continue using your existing format model until the issue is\naddressed.\nWith Azure Machine Learning, you can deploy, manage, and monitor your ONNX\nmodels. Using the standard deployment workflow  and ONNX Runtime, you can create a\nREST endpoint hosted in the cloud. See example Jupyter notebooks at the end of this\narticle to try it out for yourself.\n\uf80a\nGet ONNX models\nDeploy ONNX models in Azure\nInstall and use ONNX Runtime with Python", "See the AutoML package  for changing default ensemble settings in automated machine\nlearning.\nWith Azure Machine Learning, you can use automated ML to build a Python model and\nhave it converted to the ONNX format. Once the models are in the ONNX format, they\ncan be run on a variety of platforms and devices. Learn more about accelerating ML\nmodels with ONNX .\nSee how to convert to ONNX format in this Jupyter notebook example . Learn which\nalgorithms are supported in ONNX .\nThe ONNX runtime also supports C#, so you can use the model built automatically in\nyour C# apps without any need for recoding or any of the network latencies that REST\nendpoints introduce. Learn more about using an AutoML ONNX model in a .NET\napplication with ML.NET  and inferencing ONNX models with the ONNX runtime C#\nAPI .\nThere are multiple resources to get you up and running with AutoML.\nTutorials are end-to-end introductory examples of AutoML scenarios.\nFor a code fir st experience , follow the Tutorial: T rain an object detection model\nwith AutoML and Python\nFor a low or no-code experience , see the Tutorial: T rain a classification model with\nno-code AutoML in Azure Machine Learning studio .\nHow-to articles provide additional detail into what functionality automated ML offers.\nFor example,\nConfigure the settings for automatic training experiments\nWithout code in the Azure Machine Learning studio .\nWith the Python SDK .\nLearn how to train computer vision models with Python .\nLearn how to view the generated code from your automated ML models (SDK v1) .AutoML & ONNX\nNext steps\nTutorials/ how-tos", "Make predictions with ONNX on\ncomputer vision models from AutoML\nArticle \u202204/04/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this article, you will learn how to use Open Neural Network Exchange (ONNX) to\nmake predictions on computer vision models generated from automated machine\nlearning (AutoML) in Azure Machine Learning.\nTo use ONNX for predictions, you need to:\n1. Download ONNX model files from an AutoML training run.\n2. Understand the inputs and outputs of an ONNX model.\n3. Preprocess your data so that it's in the required format for input images.\n4. Perform inference with ONNX Runtime for Python.\n5. Visualize predictions for object detection and instance segmentation tasks.\nONNX  is an open standard for machine learning and deep learning models. It enables\nmodel import and export (interoperability) across the popular AI frameworks. For more\ndetails, explore the ONNX GitHub project .\nONNX Runtime  is an open-source project that supports cross-platform inference.\nONNX Runtime provides APIs across programming languages (including Python, C++,\nC#, C, Java, and JavaScript). Y ou can use these APIs to perform inference on input\nimages. After you have the model that has been exported to ONNX format, you can use\nthese APIs on any programming language that your project needs.\nIn this guide, you'll learn how to use Python APIs for ONNX Runtime  to make\npredictions on images for popular vision tasks. Y ou can use these ONNX exported\nmodels across languages.\nGet an AutoML-trained computer vision model for any of the supported image\ntasks: classification, object detection, or instance segmentation. Learn more about\nAutoML support for computer vision tasks .\nInstall the onnxruntime  package. The methods in this article have been tested\nwith versions 1.3.0 to 1.8.0.\nPrerequisites", "runs from experimentation in the studio user interface. Or they can use versioned assets\nfor jobs like environments and storage references.\nFor more information, see Manage Azure Machine Learning workspaces .\nWhen a project is ready for operationalization, users' work can be automated in an ML\npipeline and triggered on a schedule or HT TPS request.\nYou can deploy models to the managed inferencing solution, for both real-time and\nbatch deployments, abstracting away the infrastructure management typically required\nfor deploying models.\nIn Azure Machine Learning, you can run your training script in the cloud or build a\nmodel from scratch. Customers often bring models they've built and trained in open-\nsource frameworks so that they can operationalize them in the cloud.\nData scientists can use models in Azure Machine Learning that they've created in\ncommon Python frameworks, such as:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nLightGBM\nOther languages and frameworks are also supported:\nR\n.NET\nFor more information, see Open-source integration with Azure Machine Learning .\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\nexperience and intuition to select the right data featurization and algorithm for training.\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\nLearning studio UI or the Python SDK.Train models\nOpen and interoperable\nAutomated featurization and algorithm selection"]}
{"question": "What Azure storage services can be integrated with Azure Machine Learning for storing datasets?", "variant_id": "v1", "line_number": 21, "answer": "Azure Machine Learning can integrate with several Azure storage services for storing datasets, including:\n\n1. Azure Blob Storage: Used for storing large amounts of unstructured data, such as text or binary data.\n2. Azure Data Lake Storage: An enterprise-wide hyper-scale repository for big data analytic workloads, enabling you to store massive amounts of data.\n3. Azure File Storage: Offers shared storage for applications using the standard SMB protocol.\n4. Azure SQL Database: A relational database service that can be used to store structured data.\n\nThese services can be accessed and utilized within Azure Machine Learning for various tasks such as data exploration, model training, and deployment.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Service Usage Example datanumber of nodes; datastore names and tags, and\ndescriptions on assets like models; and data label names.\nAzure AI\nSearchStores indexes that\nhelp with querying\nyour machine learning\ncontent.These indexes are built on top of the data stored in Azure\nCosmos DB.\nAzure\nStorageStores metadata\nrelated to Azure\nMachine Learning\npipeline data.Data can include designer pipeline names, pipeline layout,\nand execution properties.\nFrom the perspective of data lifecycle management, data in the preceding resources is\ncreated and deleted as you create and delete corresponding objects in Azure Machine\nLearning.\nYour Azure Machine Learning workspace reads and writes data by using its managed\nidentity. This identity is granted access to the resources through a role assignment\n(Azure role-based access control) on the data resources. The encryption key that you\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\nused to create indexes for Azure AI Search at runtime.\nExtra networking controls are configured when you create a private link endpoint on\nyour workspace to allow for inbound connectivity. This configuration includes the\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\nNetwork access is restricted to only trusted Microsoft services.\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\nthe data.\nWhen you use a customer-managed key, the resources are in your Azure subscription\nand encrypted with your key. While these resources exist in your subscription, Microsoft\nmanages them. They're automatically created and configured when you create your\nAzure Machine Learning workspace.\nThese Microsoft-managed resources are located in a new Azure resource group that's\ncreated in your subscription. This resource group is separate from the resource group\nfor your workspace. It contains the Microsoft-managed resources that your key is used\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\nresource group name><GUID>.Customer-managed keys"]}
{"question": "How do you authenticate Azure Machine Learning with Azure Blob Storage to access datasets?", "variant_id": "v1", "line_number": 22, "answer": "To authenticate Azure Machine Learning with Azure Blob Storage for accessing datasets, you can use identity-based data access by connecting to an Azure Machine Learning datastore. Here are the steps to set up identity-based authentication:\n\n1. **Create a Managed Identity**:\n   - For an Azure Machine Learning compute instance or a training job, you can use a system-assigned managed identity or a user-assigned managed identity.\n   - Enable the managed identity on the Azure Machine Learning compute resource.\n\n2. **Assign Permissions**:\n   - Ensure that the managed identity has at least \"Storage Blob Data Reader\" access to the Azure Blob Storage account. This permission can be assigned through Azure role-based access control (RBAC) in the Azure portal.\n   - Only the storage account owner or someone with adequate permissions can assign these roles.\n\n3. **Configure Azure Machine Learning Datastore**:\n   - In Azure Machine Learning, create a new datastore or update an existing one to use the managed identity for authentication.\n   - Specify the Azure Blob Storage account details and indicate that you want to use identity-based authentication.\n\n4. **Access Data in Azure Machine Learning**:\n   - When you access data from Azure Blob Storage in your Jupyter Notebooks or training scripts, Azure Machine Learning will use the managed identity to authenticate against the storage account.\n   - You do not need to provide explicit credentials like storage keys or SAS tokens since the authentication is handled through the managed identity.\n\nBy following these steps, you can securely connect Azure Machine Learning to Azure Blob Storage without storing sensitive credentials in the datastore configuration. This approach aligns with best practices for security and access management.", "context": ["You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.", "means that in order to successfully read data from storage, the managed identity of the\ncompute cluster where the deployment is running must have at least Storage Blob Data\nReader  access to the storage account.\nTo configure the compute cluster for data access, follow these steps:\n1. Go to Azure Machine Learning studio .\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\ndeployment is using.\n3. Assign a managed identity to the compute cluster:\na. In the Managed identity  section, verify if the compute has a managed identity\nassigned. If not, select the option Edit.\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\nusing a S ystem-Assigned Managed Identity, it is named as \"[workspace\nname]/computes/[compute cluster name]\".\nc. Save the changes.\n4. Go to the Azure portal  and navigate to the associated storage account where the\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\nstorage account where those assets are placed.", "When you use an Azure S torage Account from Azure Machine Learning studio, you must\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\naccount:\nBlob Data R eader\nIf the storage account uses a private endpoint to connect to the VNet, you must\ngrant the Reader  role for the storage account private endpoint to the managed\nidentity.\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\nNetwork .\nThe following sections explain the limitations of using an Azure S torage Account, with\nyour workspace, in a VNet.\nTo secure communication between Azure Machine Learning and Azure S torage\nAccounts, configure the storage to Grant access to trusted Azure services .\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\ncan normally be used to allow your client to directly connect over the internet. However,\nwhen using studio, your client doesn't connect to the storage account. The Azure\nMachine Learning service that makes the request connects to the storage account. The\nIP address of the service isn't documented, and it changes frequently. Enabling the\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\nconfiguration .\uea80 Tip\nTo access data from outside Azure Machine Learning, for example with Azure\nStorage Explorer, that access probably relies on the user identity. For specific\ninformation, review the documentation for the tool or service you're using. For\nmore information about how Azure Machine Learning works with data, see Setup\nauthentication betw een Azur e Machine Learning and other ser vices .\nAzure Storage Account\nSecure communication with Azure Storage Account\nAzure Storage firewall", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."]}
{"question": "What steps are involved in uploading a dataset to Azure Blob Storage and then accessing it from an Azure Machine Learning experiment?", "variant_id": "v1", "line_number": 23, "answer": "To upload a dataset to Azure Blob Storage and then access it from an Azure Machine Learning experiment, you can follow these steps:\n\n1. **Upload Data to Azure Blob Storage:**\n   - Use Azure Storage Explorer for a more robust transfer, especially for large datasets.\n   - Navigate to your Blob Storage container.\n   - Create a new blob or use an existing one and upload your files (e.g., .csv, .tsv, .txt) to the container.\n\n2. **Create a Dataset in Azure Machine Learning:**\n   - Go to your Azure Machine Learning workspace.\n   - Select \"Create\" to create a new dataset.\n   - Enter a name for your dataset and optionally provide a description.\n   - Choose the Dataset type:\n     - Select \"Tabular\" if you're using a .csv or .tsv file where each row contains a response.\n     - Select \"File\" if you're using separate .txt files for each response.\n   - Select \"Next.\"\n\n3. **Select Datastore and Path:**\n   - Select \"From Azure storage\" and then click \"Next.\"\n   - Choose the datastore that points to your Blob Storage.\n   - If your data is in a subfolder, use \"Browse\" to select the path. Use `/**` to include all files in subfolders or `**/*.*` to include all data in the current container and its subfolders.\n   - Select \"Next.\"\n\n4. **Finalize Dataset Creation:**\n   - Review your settings and then click \"Create\" to create the dataset.\n\n5. **Access the Dataset in an Experiment:**\n   - In your Azure Machine Learning experiment, use the SDK or the studio interface to access the dataset you created.\n   - Reference the dataset by name in your experiment code or through the user interface when building models.\n\nBy following these steps, you will have your data uploaded to Azure Blob Storage and made available as a dataset within your Azure Machine Learning workspace, ready to be used in experiments.", "context": ["In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data", "2. Select your subscription and the workspace you created.\n3. Select Get star ted.\n4. In the left pane, select Automat ed ML  under the Author  section.\n5. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning dataset. Doing so, allows you to ensure that your\ndata is formatted appropriately for your experiment.\n1. On the Select dataset  form, select From local files  from the +Create dataset  drop-\ndown.\na. On the Basic info  form, give your dataset a name and provide an optional\ndescription. The dataset type should default to Tabular , since automated ML in\nAzure Machine Learning studio currently only supports tabular datasets.\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is the storage location where you upload your data\nfile.\nd. Select Uplo ad files  from the Uplo ad drop-down.\ne. Choose the bike-no.csv  file on your local computer. This is the file you\ndownloaded as a prerequisite .\nf. Select Next\nWhen the upload is complete, the Settings and preview form is pre-populated\nbased on the file type.\ng. Verify that the Settings and pr eview  form is populated as follows and select\nNext .Create and load dataset\n\uff89Expand table", "In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data", "Since this is your first automated ML experiment, you'll see an empty list and links\nto documentation.\n3. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning data asset. In the case of this tutorial, you can think\nof a data asset as your dataset for the AutoML job. Doing so, allows you to ensure that\nyour data is formatted appropriately for your experiment.\n1. Create a new data asset by selecting From local files  from the +Create data asset\ndrop-down.\na. On the Basic info  form, give your data asset a name and provide an optional\ndescription. The automated ML interface currently only supports\nTabularDatasets, so the dataset type should default to Tabular .\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is where you'll upload your data file to make it\navailable to your workspace.\nd. Select Uplo ad files  from the Uplo ad drop-down.Create and load a dataset as a data asset", "b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably."]}
{"question": "Can Azure Machine Learning directly access data stored in Azure Data Lake Storage Gen2? If so, how?", "variant_id": "v1", "line_number": 24, "answer": "Yes, Azure Machine Learning can directly access data stored in Azure Data Lake Storage Gen2. Azure Data Lake Storage Gen2 is used as a datastore for Azure Machine Learning, and it is built on top of Azure Blob Storage, which is designed for big data analytics in enterprises.\n\nTo access data from Azure Data Lake Storage Gen2 in Azure Machine Learning, you can follow these general steps:\n\n1. **Set up authentication**: Ensure that your Azure Machine Learning workspace has the appropriate permissions to access the Azure Data Lake Storage Gen2 account. This typically involves configuring an identity (such as a service principal) with the necessary role assignments.\n\n2. **Register a datastore**: In Azure Machine Learning, you can register an Azure Data Lake Storage Gen2 account as a datastore. This involves providing the account name, container name, and the authentication credentials (such as account key or service principal details).\n\n3. **Access data in experiments**: Once the datastore is registered, you can access the data in your Azure Machine Learning experiments by referencing the datastore and the specific paths to the data files or directories you want to use.\n\n4. **Use Azure Machine Learning SDK**: You can use the Azure Machine Learning SDK for Python to interact with the datastore programmatically. The SDK provides methods to upload, download, and manage data within the datastore.\n\nFor detailed instructions and code examples, you can refer to the Azure Machine Learning documentation on how to work with datastores and how to access data from Azure Data Lake Storage Gen2.", "context": ["For information on regenerating the access keys, see Regenerate storage account access\nkeys.\nAzure Data Lake S torage Gen2 is built on top of Azure Blob S torage and is designed for\nbig data analytics in enterprises. Data Lake S torage Gen2 is used as a datastore for\nAzure Machine Learning. Like Azure Blob S torage, the data at rest is encrypted with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Data Lake\nStorage, see Azure S torage encryption with customer-managed keys in Azure K ey Vault.\nThe Azure Machine Learning service supports data from the following data sources.\nTransparent data encryption helps protect Azure SQL Database against the threat of\nmalicious offline activity by encrypting data at rest. By default, transparent data\nencryption is enabled for all newly deployed SQL databases that use Microsoft-\nmanaged keys.\nFor information on how to use customer-managed keys for transparent data encryption,\nsee Azure SQL Database transparent data encryption .Azure Data Lake Storage\n\uff17 Note\nOn Feb 29, 2024 , Azure Data Lake S torage Gen1 will be retired. For more\ninformation, see the official announcement . If you use Azure Data Lake S torage\nGen1, make sure to migrate to Azure Data Lake S torage Gen2 before that date. T o\nlearn how, see Migrat e Azur e Data Lak e Storage fr om Gen1 t o Gen2 by using the\nAzur e por tal.\nUnless you already have an Azure Data Lake S torage Gen1 account, you can't create\nnew ones.\nAzure relational databases\nAzure SQL Database\nAzure Database for PostgreSQL", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore"]}
{"question": "How does Azure Machine Learning support versioning of datasets stored in Azure Blob Storage?", "variant_id": "v1", "line_number": 25, "answer": "Azure Machine Learning supports dataset versioning by allowing users to create and manage versions of datasets that are registered in the Azure Machine Learning workspace. When you register a dataset in Azure Machine Learning, you can specify a version number. If you do not specify a version, Azure Machine Learning will automatically assign a version number starting with 1 and incrementing with each new registration of the dataset.\n\nYou can register datasets that are stored in Azure Blob Storage by providing the storage details and the path to the data. Once registered, you can manage and track different versions of your datasets, which is useful for experiment tracking and model reproducibility. You can retrieve a specific version of a dataset for training or inference by specifying the version number in your code.\n\nThe versioning of datasets in Azure Machine Learning does not directly interact with the data stored in Azure Blob Storage; rather, it is a feature within the Azure Machine Learning service that helps you manage and version your datasets within the workspace. The actual data in Azure Blob Storage remains unchanged unless you update it yourself. Dataset versioning in Azure Machine Learning is more about keeping track of metadata and references to the data in storage rather than versioning the data files themselves.", "context": ["Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore", "Only Azure Blob S torage Accounts are supported for outputs. For instance, Azure Data Lake\nStorage Gen2 isn't supported as output in batch deployment jobs. If you need to output the\ndata to a different location/sink, use the Azure Data F actory Copy activity after the execution of\nthe batch job.\nUse low priority VMs in batch deployments\nAuthorization on batch endpoints\nNetwork isolation in batch endpointsNext steps", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity"]}
{"question": "What is the recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage?", "variant_id": "v1", "line_number": 26, "answer": "The recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage is to use Azure Storage Explorer. It provides a faster and more robust method for transferring a large amount of data compared to uploading local files directly. Azure Storage Explorer should be used as the default method for moving files into Azure Machine Learning.", "context": ["In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity"]}
{"question": "How do you use the Azure Machine Learning SDK to programmatically download data from Azure Blob Storage?", "variant_id": "v1", "line_number": 27, "answer": "To programmatically download data from Azure Blob Storage using the Azure Machine Learning SDK, you would typically follow these steps:\n\n1. **Set up your Azure Machine Learning workspace**: Before you can interact with Azure Blob Storage, you need to have an Azure Machine Learning workspace set up. This workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create.\n\n2. **Install the Azure Machine Learning SDK**: Ensure that you have the Azure Machine Learning SDK installed in your Python environment. You can install it using pip:\n\n   ```bash\n   pip install azureml-sdk\n   ```\n\n3. **Authenticate to Azure**: Use the SDK to authenticate to Azure. This can be done using service principal authentication, interactive login, or other methods provided by the SDK.\n\n4. **Access the workspace**: Access your Azure Machine Learning workspace using the SDK:\n\n   ```python\n   from azureml.core import Workspace\n   ws = Workspace.from_config()\n   ```\n\n   If you don't have a `config.json` file, you can authenticate directly with your subscription ID, resource group, and workspace name:\n\n   ```python\n   ws = Workspace(subscription_id=\"your-subscription-id\",\n                  resource_group=\"your-resource-group\",\n                  workspace_name=\"your-workspace-name\")\n   ```\n\n5. **Access the Datastore**: Azure Machine Learning workspaces have a default datastore (usually backed by an Azure Blob Storage account) that you can use to store data. You can access it as follows:\n\n   ```python\n   from azureml.core import Datastore\n   datastore = Datastore.get(ws, datastore_name='your_datastore_name')\n   ```\n\n   If you want to use the default datastore, you can use:\n\n   ```python\n   datastore = ws.get_default_datastore()\n   ```\n\n6. **Download data from the Datastore**: Once you have a reference to the datastore, you can download files from it using the `download` or `download_to_path` methods:\n\n   ```python\n   datastore.download(target_path='local_folder',\n                      prefix='folder_or_file_prefix_in_datastore',\n                      show_progress=True)\n   ```\n\n   This will download the files from the datastore to a local path on your machine or cloud environment.\n\nRemember to replace placeholders like `'your-subscription-id'`, `'your-resource-group'`, `'your-workspace-name'`, `'your_datastore_name'`, `'local_folder'`, and `'folder_or_file_prefix_in_datastore'` with your actual Azure subscription ID, resource group, workspace name, datastore name, local folder path, and the prefix of the files you want to download.\n\nPlease note that the Azure Machine Learning SDK is continuously updated, and the methods and classes might change. Always refer to the latest Azure Machine Learning SDK documentation for the most current information.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Tutorial: Upload, access and explore\nyour data in Azure Machine Learning\nArticle \u202212/27/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this tutorial you learn how to:\nThe start of a machine learning project typically involves exploratory data analysis (ED A),\ndata-preprocessing (cleaning, feature engineering), and the building of Machine\nLearning model prototypes to validate hypotheses. This prototyping  project phase is\nhighly interactive. It lends itself to development in an IDE or a Jupyter notebook, with a\nPython int eractive console. This tutorial describes these ideas.\nThis video shows how to get started in Azure Machine Learning studio so that you can\nfollow the steps in the tutorial. The video shows how to create a notebook, clone the\nnotebook, create a compute instance, and download the data needed for the tutorial.\nThe steps are also described in the following sections.\n1. To use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\n2. Sign in to studio  and select your workspace if it's not already open.\n3. Open or create a notebook in your workspace:\nCreate a new notebook , if you want to copy/paste code into cells.\nOr, open tutorials/get -star ted-not ebooks/explor e-data.ipynb  from the\nSamples  section of studio. Then select Clone  to add the notebook to your\nFiles. (See where to find Samples .)\nUpload your data to cloud storage\uff02\nCreate an Azure Machine Learning data asset\uff02\nAccess your data in a notebook for interactive development\uff02\nCreate new versions of data assets\uff02\nhttps://learn-video.azurefd.net/vod/player?id=514a29e2-0ae7-4a5d-a537-\n8f10681f5545&locale=en-us&embedUrl=%2F azure%2Fmachine-learning%2Ftutorial-\nexplore-data\nPrerequisites", "Data from Azure Machine Learning registered data stores can be directly referenced by\nbatch deployments jobs. In this example, you first upload some data to the default data\nstore in the Azure Machine Learning workspace and then run a batch deployment on it.\nFollow these steps to run a batch endpoint job using data stored in a data store.\n1. Access the default data store in the Azure Machine Learning workspace. If your\ndata is in a different store, you can use that store instead. Y ou're not required to\nuse the default data store.\nAzure CLI\n2. You need to upload some sample data to the data store. This example assumes\nyou already uploaded the sample data included in the repo in the folder\nsdk/python/endpoints/batch/deploy-models/heart-classifier-mlflow/data in the\nfolder heart-disease-uci-unlabeled in the blob storage account. Ensure you've\ndone that before moving forward.\n3. Create the input or request:\nPlace the file path in the following variable:Azure CLI\nDATASTORE_ID=$(az ml datastore show -n workspaceblobstore | jq -r \n'.id')\n\uff17 Note\nData stores ID would look like\n/subscriptions/<subscription>/resourceGroups/<resource-\ngroup>/providers/Microsoft.MachineLearningServices/workspaces/<worksp\nace>/datastores/<data-store>.\n\uea80 Tip\nThe default blob data store in a workspace is called worksp aceblobst ore. You\ncan skip this step if you already know the resource ID of the default data store\nin your workspace.\nAzure CLI", "Acces s data in a job\nArticle \u202202/06/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2 (current)\nIn this article you learn:\nAn Azure subscription. If you don't have an Azure subscription, create a free account before you begin.\nTry the free or paid version of Azure Machine Learning .\nThe Azure Machine Learning SDK for Python v2 .\nAn Azure Machine Learning workspace\nBefore you explore the detailed options available to you when you access data, we first describe the relevant\ncode snippets for data access.\nIn this example, you submit an Azure Machine Learning job that accesses data from a public  blob storage\naccount. However, you can adapt the snippet to access your own data in a private Azure S torage account.\nUpdate the path as described here. Azure Machine Learning seamlessly handles authentication to cloud\nstorage, with Microsoft Entra passthrough. When you submit a job, you can choose:\nUser identity:  Passthrough your Microsoft Entra identity to access the data\nManaged identity:  Use the managed identity of the compute target to access data\nNone:  Don't specify an identity to access the data. Use None when using credential-based (key/SAS\ntoken) datastores or when accessing public data\nHow to read data from Azure storage in an Azure Machine Learning job.\uff02\nHow to write data from your Azure Machine Learning job to Azure S torage. \uff02\nThe difference between mount  and downlo ad modes. \uff02\nHow to use user identity and managed identity to access data.\uff02\nMount settings available in a job.\uff02\nOptimum mount settings for common scenarios.\uff02\nHow to access V1 data assets.\uff02\nPrerequisites\nQuickstart\nRead data from Azure storage in an Azure Machine Learning job\n\uea80 Tip\nIf you use keys or SAS tokens to authenticate, we suggest that you create an Azur e Machine Learning\ndatast ore, because the runtime will automatically connect to storage without exposure of the key/token.\nPython SDK", "Acces s data from Azure cloud storage during\ninteractive development\nArticle \u202209/13/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nA machine learning project typically starts with exploratory data analysis (ED A), data-preprocessing\n(cleaning, feature engineering), and includes building prototypes of ML models to validate hypotheses.\nThis prototyping  project phase is highly interactive in nature, and it lends itself to development in a\nJupyter notebook, or an IDE with a Python int eractive console. In this article you'll learn how to:\nAn Azure Machine Learning workspace. For more information, see Manage Azure Machine Learning\nworkspaces in the portal or with the Python SDK (v2) .\nAn Azure Machine Learning Datastore. For more information, see Create datastores .\nAn Azure Machine Learning datastore is a reference to an existing  Azure storage account. The benefits of\ndatastore creation and use include:\nAccess data from a Azure Machine Learning Datastores URI as if it were a file system.\uff02\nMaterialize data into P andas using mltable Python library. \uff02\nMaterialize Azure Machine Learning data assets into P andas using mltable Python library. \uff02\nMaterialize data through an explicit download with the azcopy utility. \uff02\nPrerequisites\n\uea80 Tip\nThe guidance in this article describes data access during interactive development. It applies to any\nhost that can run a Python session. This can include your local machine, a cloud VM, a GitHub\nCodespace, etc. W e recommend use of an Azure Machine Learning compute instance - a fully\nmanaged and pre-configured cloud workstation. For more information, see Create an Azur e\nMachine Learning comput e instance .\n\uff09 Impor tant\nEnsure you have the latest azure-fsspec and mltable python libraries installed in your python\nenvironment:\nBash\npip install -U azureml-fsspec mltable\nAccess data from a datastore URI, like a filesystem\nA common, easy-to-use API to interact with different storage types (Blob/Files/ADLS).\uff02"]}
{"question": "What are the benefits of integrating Azure Data Lake Storage with Azure Machine Learning for big data scenarios?", "variant_id": "v1", "line_number": 28, "answer": "Integrating Azure Data Lake Storage with Azure Machine Learning offers several benefits for big data scenarios:\n\n1. Scalability: Azure Data Lake Storage is designed to handle large volumes of data, scaling to petabytes of information. This makes it an ideal choice for big data scenarios where data volumes may grow rapidly.\n\n2. Performance: Azure Data Lake Storage provides high-performance storage, which is crucial when dealing with big data. This ensures that data can be accessed and processed quickly, which is essential for machine learning workloads that require intensive computation.\n\n3. Data Exploration and Preprocessing: Azure Machine Learning allows you to explore and preprocess data directly from Azure Data Lake Storage. This seamless integration simplifies the workflow, as there is no need to move data between services for different stages of the machine learning process.\n\n4. Advanced Analytics: Azure Machine Learning provides advanced analytics capabilities that can be applied to the data stored in Azure Data Lake Storage. This means you can build and train complex machine learning models on large datasets without compromising on the analytical depth.\n\n5. Security and Compliance: Azure Data Lake Storage comes with robust security features, including encryption and access control, which are essential for handling sensitive big data. When integrated with Azure Machine Learning, you can maintain compliance with industry standards and regulations while performing machine learning tasks.\n\n6. Cost-Effectiveness: By using Azure Data Lake Storage, you can take advantage of a cost-effective storage solution for big data. Azure Machine Learning can then process this data efficiently, helping to manage costs associated with large-scale machine learning projects.\n\n7. Streamlined Workflow: The integration allows for a more streamlined workflow from data storage to model training and deployment. This reduces the complexity and time required to manage data and machine learning lifecycles.\n\n8. Collaboration and Sharing: Azure Data Lake Storage supports collaboration and sharing of data across different teams and projects. When used with Azure Machine Learning, it facilitates a collaborative environment for building and deploying machine learning models.\n\nOverall, the integration of Azure Data Lake Storage with Azure Machine Learning provides a robust and efficient environment for handling big data scenarios, from storage and preprocessing to model training and deployment.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Azur e Machine Learning designer : Use the designer to train and deploy ML\nmodels without writing any code. Drag and drop datasets and components to\ncreate ML pipelines.\nAutomat ed machine learning UI : Learn how to create automated ML experiments\nwith an easy-to-use interface.\nData labeling : Use Machine Learning data labeling to efficiently coordinate image\nlabeling  or text labeling  projects.\nMachine Learning integrates with the Azure cloud platform to add security to ML\nprojects.\nSecurity integrations include:\nAzure Virtual Networks with network security groups.\nAzure K ey Vault, where you can save security secrets, such as access information\nfor storage accounts.\nAzure Container R egistry set up behind a virtual network.\nFor more information, see Tutorial: Set up a secure workspace .\nOther integrations with Azure services support an ML project from end to end. They\ninclude:Enterprise-readiness and security\nAzure integrations for complete solutio ns", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore", "What is Azure Machine Learning?\nArticle \u202201/29/2024\nAzure Machine Learning is a cloud service for accelerating and managing the machine\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\nin their day-to-day workflows to train and deploy models and manage machine learning\noperations (ML Ops).\nYou can create a model in Machine Learning or use a model built from an open-source\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\nretrain, and redeploy models.\nMachine Learning is for individuals and teams implementing ML Ops within their\norganization to bring ML models into production in a secure and auditable production\nenvironment.\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\nday workflows. Application developers can use tools for integrating models into\napplications or services. Platform developers can use a robust set of tools, backed by\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\nbased access control for infrastructure. Y ou can set up a project to deny access to\nprotected data and select operations.\nML projects often require a team with a varied skill set to build and maintain. Machine\nLearning has tools that help enable you to:\uea80 Tip\nFree trial!  If you don't have an Azure subscription, create a free account before you\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\nto spend on Azure services. After they're used up, you can keep the account and\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\nchange your settings and ask to be charged.\nWho is Azure Machine Learning for?\nProductivity for everyone on the team", "Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster ."]}
{"question": "How can you secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces?", "variant_id": "v1", "line_number": 29, "answer": "To secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces, you can use the following methods:\n\n1. **Encryption at Rest**: Azure Blob Storage, which is commonly used with Azure Machine Learning, encrypts all data at rest using Microsoft-managed keys by default. This ensures that the data is secure when stored.\n\n2. **Customer-Managed Keys**: For additional control, you can use your own encryption keys for data stored in Azure Blob Storage. This is done through Azure Storage encryption with customer-managed keys in Azure Key Vault. By integrating Azure Key Vault, you have control over the encryption keys and can rotate or revoke them as needed.\n\n3. **Secure Data Transfer**: Ensure that data is encrypted in transit to and from Azure Storage services. Azure Machine Learning and Azure Storage services use secure protocols such as HTTPS to encrypt data during transfer.\n\n4. **Role-Based Access Control (RBAC)**: Configure RBAC to control access to Azure Storage resources. Assign appropriate roles to users and services that need to access the storage account to minimize the risk of unauthorized access.\n\n5. **Virtual Networks and Service Endpoints**: Use Azure Virtual Network service endpoints to secure and isolate the network traffic between Azure Machine Learning workspaces and Azure Storage services.\n\n6. **Private Link**: Azure Private Link provides a private endpoint in your virtual network, ensuring that data accessed by Azure Machine Learning does not traverse the public internet.\n\n7. **Monitoring and Auditing**: Enable Azure Monitor and Azure Storage logging to track access and usage of storage resources. Regularly audit these logs to detect and respond to any suspicious activities.\n\nBy implementing these security measures, you can ensure that data stored in Azure Storage services is protected when accessed by Azure Machine Learning workspaces.", "context": ["Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "This article is part of a series on securing an Azure Machine Learning workflow. See the\nother articles in this series:\nSecure the workspace resources\nSecure machine learning registries\nSecure the training environment\nSecure the inference environment\nEnable studio functionality\nUse custom DNS\nUse a firewall\nAPI platform network isolationNext steps", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.", "When you use an Azure S torage Account from Azure Machine Learning studio, you must\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\naccount:\nBlob Data R eader\nIf the storage account uses a private endpoint to connect to the VNet, you must\ngrant the Reader  role for the storage account private endpoint to the managed\nidentity.\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\nNetwork .\nThe following sections explain the limitations of using an Azure S torage Account, with\nyour workspace, in a VNet.\nTo secure communication between Azure Machine Learning and Azure S torage\nAccounts, configure the storage to Grant access to trusted Azure services .\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\ncan normally be used to allow your client to directly connect over the internet. However,\nwhen using studio, your client doesn't connect to the storage account. The Azure\nMachine Learning service that makes the request connects to the storage account. The\nIP address of the service isn't documented, and it changes frequently. Enabling the\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\nconfiguration .\uea80 Tip\nTo access data from outside Azure Machine Learning, for example with Azure\nStorage Explorer, that access probably relies on the user identity. For specific\ninformation, review the documentation for the tool or service you're using. For\nmore information about how Azure Machine Learning works with data, see Setup\nauthentication betw een Azur e Machine Learning and other ser vices .\nAzure Storage Account\nSecure communication with Azure Storage Account\nAzure Storage firewall", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity"]}
{"question": "What role do Azure Storage accounts play in deploying Azure Machine Learning models as web services?", "variant_id": "v1", "line_number": 30, "answer": "Azure Storage accounts play a crucial role in deploying Azure Machine Learning models as web services in several ways:\n\n1. **Model Storage**: The storage accounts in your Azure subscription are used to store your machine learning models, training data, training logs, and Python scripts. When you deploy a model as a web service, the model itself is often retrieved from this storage.\n\n2. **Operational Data**: When you deploy a model as a web service, the web service may need to access additional data for inference. This data can be stored in Azure Blob Storage and accessed by the web service during operation.\n\n3. **Code Artifacts**: The code artifacts, such as scoring scripts and environment configuration files, are also stored in Azure Storage and are used when setting up the web service.\n\n4. **Endpoints**: When using Azure Kubernetes Service (AKS) for deploying a model as a web service, the AKS cluster may need to access Azure Storage accounts. The storage account could be used to store container images in Azure Container Registry, which AKS uses to pull the image for the web service.\n\n5. **Private Connectivity**: For secure environments, the compute instance and compute cluster can use a private endpoint to access the storage account, ensuring that the data transfer between the service and the storage remains within the Azure network.\n\n6. **Management and Orchestration**: Azure Machine Learning relies on Azure Batch for the management of compute resources. The compute instances and clusters access storage located in a Microsoft subscription, which is used only for the management of these resources and not for storing user data.\n\nIn summary, Azure Storage accounts are integral to the deployment process of Azure Machine Learning models as web services, providing a place to store models, data, and artifacts, and facilitating secure and efficient access to these resources during the operation of the web service.", "context": ["Scenar io Requir ed\ninboundRequir ed outbound Additional\nconfiguration\nUse Azure\nKubernetes\nServiceNot applicable For information on the outbound\nconfiguration for AKS, see Secure\nAzure Kubernetes Service\ninferencing environment .\nUse Docker\nimages that\nAzure\nMachine\nLearning\nmanagesNot applicable Microsoft Artifact R egistry\nviennaglobal.azurecr.io\nglobal container registryIf the container registry\nfor your workspace is\nbehind the virtual\nnetwork, configure the\nworkspace to use a\ncompute cluster to build\nimages. For more\ninformation, see Secure\nan Azure Machine\nLearning workspace with\nvirtual networks .\nAzure Machine Learning uses multiple storage accounts. Each stores different data and\nhas a different purpose:\nYour st orage : The storage accounts in your Azure subscription store your data and\nartifacts, such as models, training data, training logs, and Python scripts. For\nexample, the default  storage account for your workspace is in your subscription.\nThe Azure Machine Learning compute instance and compute cluster access file and\nblob data in this storage over ports 445 (SMB) and 443 (HT TPS).\nWhen you're using a compute instance or compute cluster, your storage account is\nmounted as a file share via the SMB protocol. The compute instance and cluster\nuse this file share to store items like the data, models, Jupyter notebooks, and\ndatasets. The compute instance and cluster use the private endpoint when they\naccess the storage account.\nMicrosoft storage : The Azure Machine Learning compute instance and compute\ncluster rely on Azure Batch. They access storage located in a Microsoft\nsubscription. This storage is used only for the management of the compute\ninstance or cluster. None of your data is stored here. The compute instance and\ncompute cluster access the blob, table, and queue data in this storage, by using\nport 443 (HT TPS).Purposes of storage accounts", "Azure Machine Learning supports storage accounts configured to use either a private\nendpoint or service endpoint.\n1. In the Azure portal, select the Azure S torage Account.\n2. Use the information in Use private endpoints for Azure S torage  to add private\nendpoints for the following storage resources:\nBlob\nFile\nQueue  - Only needed if you plan to use Batch endpoints  or the\nParallelRunS tep in an Azure Machine Learning pipeline.\nTable - Only needed if you plan to use Batch endpoints  or the\nParallelRunS tep in an Azure Machine Learning pipeline.Securing a workspace with private endpoints does not ensure end-to-end security\nby itself. Y ou must follow the steps in the rest of this article, and the VNet series, to\nsecure individual components of your solution. For example, if you use a private\nendpoint for the workspace, but your Azure S torage Account is not behind the\nVNet, traffic between the workspace and storage does not use the VNet for\nsecurity.\nSecure Azure storage accounts\nPrivate endpoint", "What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps", "Azure Machine Learning offers many ways to deploy MLflow models to online and batch\nendpoints. Y ou can deploy models, using the following tools:\nEach workflow has different capabilities, particularly around which type of compute they\ncan target. The following table shows the different capabilities.\nScenar io MLflow SDK Azur e Machine\nLearning CLI/SDKAzur e Machine\nLearning studio\nDeploy to managed online\nendpointsSee example See example See example\nDeploy to managed online\nendpoints (with a scoring script)Not\nsupportedSee example See example\nDeploy to batch endpoints Not\nsupportedSee example See example\nDeploy to batch endpoints (with\na scoring script)Not\nsupportedSee example See example\nDeploy to web services\n(ACI/AKS)Legacy\nsupportNot supported Not supported\nDeploy to web services (A CI/AKS\n- with a scoring script)Not\nsupportedLegacy support Legacy support\n Deployment to online endpoints that are in workspaces with private link enabled\nrequires you to package models before deployment (preview) .\n We recommend switching to managed online endpoints  instead.\n MLflow (OSS) doesn't have the concept of a scoring script and doesn't support batch\nexecution currently.\nUse the MLflow SDK if both of these conditions apply:Deployment tools\nMLflow SDK\uff02\nAzure Machine Learning CLI\uff02\nAzure Machine Learning SDK for Python\uff02\nAzure Machine Learning studio\uff02\n\uff89Expand table\n1 1 1\n3\n3\n3\n22 2\n32 2\n1\n2\n3\nWhich deployment tool to use?", "region basis. When you sign up for Azure Machine Learning studio, you receive a default\nVM quota for several VM families available in the region. Y ou can continue to create\ndeployments until you reach your quota limit. Once you reach this limit, you can request\na quota increase.\nModels deployed as a service with pay-as-you-go are protected by Azure AI content\nsafety. When deployed to real-time endpoints, you can opt out of this capability. With\nAzure AI content safety enabled, both the prompt and completion pass through an\nensemble of classification models aimed at detecting and preventing the output of\nharmful content. The content filtering system detects and takes action on specific\ncategories of potentially harmful content in both input prompts and output\ncompletions. Learn more about Azure AI Content Safety .\nModel Catalog and Collections\nDeploy and score a machine learning model by using an online endpoint\nPlan and manage costs for Azure AI S tudioContent filtering\nRelated content"]}
{"question": "How do you monitor data access and usage when integrating Azure Machine Learning with Azure Storage?", "variant_id": "v1", "line_number": 31, "answer": "To monitor data access and usage when integrating Azure Machine Learning with Azure Storage, you can use Azure Monitor, which provides comprehensive monitoring of your Azure resources and the applications you run on Azure. Here are the steps you can follow:\n\n1. **Enable Diagnostic Settings**: Configure the diagnostic settings for your Azure Storage account to send logs and metrics to Azure Monitor. This can include read, write, and delete operations on your storage blobs or files.\n\n2. **Log Analytics Workspace**: Set up a Log Analytics workspace in Azure Monitor to query and visualize the logs sent from your Azure Storage account.\n\n3. **Metrics**: Use Azure Monitor metrics to get performance metrics in real-time for your Azure Storage account.\n\n4. **Alerts**: Create alert rules in Azure Monitor to get notified about specific conditions in the log data or metrics that may indicate unusual access patterns or potential security concerns.\n\n5. **Azure Storage Analytics**: Use Azure Storage Analytics to track, analyze, and debug your applications that use Azure Storage.\n\n6. **Role-Based Access Control (RBAC)**: Implement RBAC to manage who has access to your Azure Storage data and what they can do with it.\n\n7. **Azure Policy**: Use Azure Policy to enforce rules and effects for your resources, ensuring that your storage account remains compliant with your organization's standards and service level agreements.\n\n8. **Azure Machine Learning Monitoring**: Within Azure Machine Learning, you can monitor data drift, model performance, and other metrics that can indicate how your models are interacting with the data stored in Azure Storage.\n\nRemember to review the Azure documentation for the most up-to-date information on monitoring and securing your Azure resources, including Azure Machine Learning and Azure Storage.", "context": ["To learn how to monitor resources and logs from Azure Machine Learning, see\nMonitoring Azure Machine Learning .", "Monitor Azure Machine Learning\nArticle \u202211/06/2023\nWhen you have critical applications and business processes relying on Azure resources, you\nwant to monitor those resources for their availability, performance, and operation. This\narticle describes the monitoring data generated by Azure Machine Learning and how to\nanalyze and alert on this data with Azure Monitor.\nAzure Machine Learning creates monitoring data using Azure Monitor , which is a full stack\nmonitoring service in Azure. Azure Monitor provides a complete set of features to monitor\nyour Azure resources. It can also monitor resources in other clouds and on-premises.\nStart with the article Monitoring Azure resources with Azure Monitor , which describes the\nfollowing concepts:\nWhat is Azure Monitor?\nCosts associated with monitoring\nMonitoring data collected in Azure\nConfiguring data collection\nStandard tools in Azure for analyzing and alerting on monitoring data\nThe following sections build on this article by describing the specific data gathered for\nAzure Machine Learning. These sections also provide examples for configuring data\ncollection and analyzing this data with Azure tools.\uea80 Tip\nThe information in this document is primarily for administrat ors, as it describes\nmonitoring for the Azure Machine Learning service and associated Azure services. If\nyou are a data scientist  or developer , and want to monitor information specific to your\nmodel tr aining r uns, see the following documents:\nStart, monit or, and cancel training runs\nLog metrics for training runs\nTrack experiments with MLflow\nIf you want to monitor information generated by models deployed to online\nendpoints, see Monit or online endpoints .\nWhat is Azure Monitor?", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.", "Monitor online endpoints\nArticle \u202210/24/2023\nAzure Machine Learning uses integration with Azure Monitor to track and monitor\nmetrics and logs for online endpoints . You can view metrics in charts, compare between\nendpoints and deployments, pin to Azure portal dashboards, configure alerts, query\nfrom log tables, and push logs to supported targets. Y ou can also use Application\nInsights to analyze events from user containers.\nMetrics : For endpoint-level metrics such as request latency, requests per minute,\nnew connections per second, and network bytes, you can drill down to see details\nat the deployment level or status level. Deployment-level metrics such as CPU/GPU\nutilization and memory or disk utilization can also be drilled down to instance\nlevel. Azure Monitor allows tracking these metrics in charts and setting up\ndashboards and alerts for further analysis.\nLogs : You can send metrics to the Log Analytics workspace where you can query\nthe logs using Kusto query syntax. Y ou can also send metrics to Azure S torage\naccounts and/or Event Hubs for further processing. In addition, you can use\ndedicated log tables for online endpoint related events, traffic, and console\n(container) logs. Kusto query allows complex analysis and joining of multiple\ntables.\nApplication insights : Curated environments include integration with Application\nInsights, and you can enable or disable this integration when you create an online\ndeployment. Built-in metrics and logs are sent to Application Insights, and you can\nuse the built-in features of Application Insights (such as Live metrics, T ransaction\nsearch, F ailures, and P erformance) for further analysis.\nIn this article you learn how to:\nDeploy an Azure Machine Learning online endpoint.Choose the right method to view and track metrics and logs\uff02\nView metrics for your online endpoint\uff02\nCreate a dashboard for your metrics\uff02\nCreate a metric alert\uff02\nView logs for your online endpoint\uff02\nUse Application Insights to track metrics and logs\uff02\nPrerequisites", "Use monitoring view in the Azure Machine Learning studio  to check the CPU and memory utilization of\nyour job. If you're low on CPU and memory, consider increasing the compute target VM size.\nConsider increasing RSLEX_DOWNLOADER_THREADS if you're downloading and you don't utilize the CPU and\nmemory. If you use mount, you should increase DATASET_MOUNT_READ_BUFFER_BLOCK_COUNT to do more\nprefetching, and increase DATASET_MOUNT_READ_THREADS for more read threads.\nIf the metrics show low SuccessE2ELat ency and low SuccessSer verLat ency, but the client experiences high\nlatency , you have a delay in the storage request that reaches the service. Y ou should check:\nWhether the number of threads used for mount/download\n(DATASET_MOUNT_READ_THREADS/RSLEX_DOWNLOADER_THREADS) is set too low, relative to the number of cores\navailable on the compute target. If the setting is too low, increase the number of threads.\nWhether the number of retries for downloading ( AZUREML_DATASET_HTTP_RETRY_COUNT) is set too high. If\nso, decrease the number of retries.\nFrom the Azure Machine Learning studio, you can also monitor the compute target disk IO and usage during\nyour job execution. Navigate to your job, and select the Monit oring  tab. This tab provides insights on the\nresources of your job, on a 30 day rolling basis. For example:\nAzure Machine Learning data runtime doesn't use the last RESERVED_FREE_DISK_SPACE bytes of disk space, to\nkeep the compute healthy (the default value is 150MB). If your disk is full, your code is writing files to disk\nwithout declaring the files as an output. Therefore, check your code to make sure that data isn't being written\nerroneously to temporary disk. If you must write files to temporary disk, and that resource is becoming full,\nconsider:\nIncreasing the VM Size to one that has a larger temporary disk\nSetting a T TL on the cached data ( DATASET_MOUNT_ATTRIBUTE_CACHE_TTL), to purge your data from diskMonitor disk usage during a job\n\uf80a\n\uff17 Note\nJob monitoring supports only compute resources that Azure Machine Learning manages. Jobs with a\nruntime of less than 5 minutes will not have enough data to populate this view."]}
{"question": "Can Azure Machine Learning use data stored in Azure File Storage? If so, what are the use cases?", "variant_id": "v1", "line_number": 32, "answer": "Yes, Azure Machine Learning can use data stored in Azure File Storage. Azure File Storage provides shared storage that can be accessed via the SMB protocol, and it can be mounted onto Azure Machine Learning compute instances or pipelines.\n\nUse cases for using Azure File Storage with Azure Machine Learning include:\n\n1. **Data Storage**: You can store large datasets in Azure File Storage and access them during the model training process without the need to copy data to local storage.\n\n2. **Shared Workspace**: Multiple users and compute resources can access the same data set in a shared manner, which is useful for collaborative model development and training.\n\n3. **Persistent Storage**: For scenarios where you need the data to persist beyond the lifecycle of a single training job or experiment, Azure File Storage can be used to ensure data is not lost.\n\n4. **Data Exploration**: You can mount the file share to your development environment, such as a Jupyter notebook running on an Azure Machine Learning compute instance, to explore and visualize the data before and after model training.\n\n5. **Pipeline Input/Output**: In machine learning pipelines, you can use Azure File Storage as a source or destination for data processing steps, making it part of a multi-step workflow.\n\nTo use Azure File Storage with Azure Machine Learning, you would typically mount the file share to the Azure Machine Learning compute target that you are using for training or inference. This allows your code to access the data as if it were on a local file system.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore", "Attached comput e: You can attach your own compute resources to your\nworkspace and use them for training and inference.\nMachine Learning allows you to work with different types of data:\nURIs (a location in local or cloud storage):\nuri_folder\nuri_file\nTables (a tabular data abstraction):\nmltable\nPrimitives:\nstring\nboolean\nnumber\nFor most scenarios, you use URIs ( uri_folder and uri_file) to identify a location in\nstorage that can be easily mapped to the file system of a compute node in a job by\neither mounting or downloading the storage to the node.\nThe mltable parameter is an abstraction for tabular data that's used for automated\nmachine learning (AutoML) jobs, parallel jobs, and some advanced scenarios. If you're\nstarting to use Machine Learning and aren't using AutoML, we strongly encourage you\nto begin with URIs.\nMachine Learning datastores securely keep the connection information to your data\nstorage on Azure so that you don't have to code it in your scripts. Y ou can register and\ncreate a datastore to easily connect to your storage account and access the data in your\nunderlying storage service. The Azure Machine Learning CLI v2 and SDK v2 support the\nfollowing types of cloud-based storage services:\nAzure Blob S torage container\nAzure Files share\nAzure Data Lake S torage\nAzure Data Lake S torage Gen2Data\nDatastore\nEnvironment", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."]}
{"question": "What are the best practices for managing input and output data for Azure Machine Learning pipelines using Azure Storage?", "variant_id": "v1", "line_number": 33, "answer": "When managing input and output data for Azure Machine Learning pipelines using Azure Storage, consider the following best practices:\n\n1. **Use Azure Machine Learning Datastores**: Register your Azure Storage accounts (Azure Blob Storage, Azure Data Lake Storage Gen1, and Azure Data Lake Storage Gen2) as Azure Machine Learning datastores. This allows you to manage and access data within your Azure Machine Learning workspace easily.\n\n2. **Flatten Nested Folder Structures**: Azure Machine Learning batch endpoint jobs do not support nested folder structures. If your data is organized in nested folders, you will need to flatten the structure before using it as input for your pipeline.\n\n3. **Data Movement with Azure Data Factory**: If your data resides in a storage service not directly supported by Azure Machine Learning, use Azure Data Factory to copy the data to a supported Azure Storage account before pipeline execution.\n\n4. **Scoring Script Compatibility**: Ensure that the scoring script used in your pipeline deployment is capable of handling the data in the format it will be provided. This includes being aware of any limitations related to file types, especially when using MLflow models.\n\n5. **Write Back to the Same Storage Account**: To streamline the process, it is recommended to register the storage account used by Azure Data Factory as a datastore in Azure Machine Learning. This allows you to write results back to the same storage account from which you are reading.\n\n6. **Monitor Data Access and Permissions**: Regularly review and manage access permissions to your storage accounts to ensure that only authorized users and services can access or modify the data.\n\n7. **Optimize Data Transfer**: When possible, use datastores that are in the same region as your Azure Machine Learning workspace to minimize latency and data transfer costs.\n\n8. **Data Versioning**: Consider using versioning for your datasets to keep track of changes and to be able to reproduce experiments or retrain models with the same data snapshot.\n\nBy following these best practices, you can effectively manage your data for Azure Machine Learning pipelines and ensure smooth operation of your machine learning workflows.", "context": ["7. Your pipeline is ready to be used.\nWhen calling Azure Machine Learning batch deployments consider the following limitations:\nOnly Azure Machine Learning data stores or Azure S torage Accounts (Azure Blob S torage, Azure\nData Lake S torage Gen1, Azure Data Lake S torage Gen2) are supported as inputs. If your input\ndata is in another source, use the Azure Data F actory Copy activity before the execution of the\nbatch job to sink the data to a compatible store.\nBatch endpoint jobs don't explore nested folders and hence can't work with nested folder\nstructures. If your data is distributed in multiple folders, notice that you will have to flatten the\nstructure.\nMake sure that your scoring script provided in the deployment can handle the data as it is\nexpected to be fed into the job. If the model is MLflow, read the limitation in terms of the file\ntype supported by the moment at Using MLflow models in batch deployments .\nOnly registered Azure Machine Learning data stores are supported by the moment. W e\nrecommend you to register the storage account your Azure Data F actory is using as a Data S tore\nin Azure Machine Learning. In that way, you will be able to write back to the same storage\naccount from where you are reading.\nLimitations\nData inputs\nData outputs", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.", "original notebook or Python code, changes the training input from local data to\nparameterized values, split the training code into multiple steps as needed, perform unit\ntest of each step, and finally wraps all steps into a pipeline.\nOnce the teams get familiar with pipelines and want to do more machine learning\nprojects using pipelines, they'll find the first approach is hard to scale. The second\napproach is set up a few pipeline templates, each try to solve one specific machine\nlearning problem. The template predefines the pipeline structure including how many\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\nlearning project, the team first forks one template repo. The team leader then assigns\nmembers which step they need to work on. The data scientists and data engineers do\ntheir regular work. When they're happy with their result, they structure their code to fit\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\nexecuted or automated. If there's any change, each member only needs to work on their\npiece of code without touching the rest of the pipeline code.\nOnce a team has built a collection of machine learnings pipelines and reusable\ncomponents, they could start to build the machine learning pipeline from cloning\nprevious pipeline or tie existing reusable component together. At this stage, the team's\noverall productivity will be improved significantly.\nAzure Machine Learning offers different methods to build a pipeline. For users who are\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\npipelines by using registered components .\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\nfollowing table lists the different pipelines and what they're used for:\nScenar io Primar y\npersonaAzur e\nofferingOSS\nofferingCanonical\npipeStrengths\nModel\norchestration\n(Machine\nlearning)Data\nscientistAzure\nMachine\nLearning\nPipelinesKubeflow\nPipelinesData ->\nModelDistribution,\ncaching, code-first,\nreuse\nData\norchestrationData\nengineerAzure Data\nFactoryApache\nAirflowData -> Data Strongly typed\nmovement, data-Which Azure pipeline technology should I use?\n\uff89Expand table", "original notebook or Python code, changes the training input from local data to\nparameterized values, split the training code into multiple steps as needed, perform unit\ntest of each step, and finally wraps all steps into a pipeline.\nOnce the teams get familiar with pipelines and want to do more machine learning\nprojects using pipelines, they'll find the first approach is hard to scale. The second\napproach is set up a few pipeline templates, each try to solve one specific machine\nlearning problem. The template predefines the pipeline structure including how many\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\nlearning project, the team first forks one template repo. The team leader then assigns\nmembers which step they need to work on. The data scientists and data engineers do\ntheir regular work. When they're happy with their result, they structure their code to fit\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\nexecuted or automated. If there's any change, each member only needs to work on their\npiece of code without touching the rest of the pipeline code.\nOnce a team has built a collection of machine learnings pipelines and reusable\ncomponents, they could start to build the machine learning pipeline from cloning\nprevious pipeline or tie existing reusable component together. At this stage, the team's\noverall productivity will be improved significantly.\nAzure Machine Learning offers different methods to build a pipeline. For users who are\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\npipelines by using registered components .\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\nfollowing table lists the different pipelines and what they're used for:\nScenar io Primar y\npersonaAzur e\nofferingOSS\nofferingCanonical\npipeStrengths\nModel\norchestration\n(Machine\nlearning)Data\nscientistAzure\nMachine\nLearning\nPipelinesKubeflow\nPipelinesData ->\nModelDistribution,\ncaching, code-first,\nreuse\nData\norchestrationData\nengineerAzure Data\nFactoryApache\nAirflowData -> Data Strongly typed\nmovement, data-Which Azure pipeline technology should I use?\n\uff89Expand table"]}
{"question": "How can you optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training?", "variant_id": "v1", "line_number": 34, "answer": "To optimize data transfer between Azure Machine Learning (Azure ML) and Azure Storage services for efficient model training, you can consider the following strategies:\n\n1. **Datastore Mounting**: Use Azure ML datastores to mount Azure Storage (Blob or File Storage) to your Azure ML compute instances. Mounting allows you to access your data directly without the need to download it, which can save time and network bandwidth.\n\n2. **Data Caching**: For frequently accessed data, consider using Azure ML's dataset caching feature. When you enable caching, the first time a dataset is accessed on a compute target, it's cached on that compute target. Subsequent runs that use the same dataset and compute target can take advantage of the cached data.\n\n3. **Optimized Data Formats**: Store your data in optimized formats such as Apache Parquet or ORC, which are designed for efficient data storage and retrieval. These formats can reduce the amount of data that needs to be transferred and can be read into memory more quickly than other formats like CSV.\n\n4. **Network Optimization**: Use Azure's high-bandwidth networking options if you're transferring large amounts of data. For example, Azure Machine Learning compute clusters can be configured with high-throughput networking.\n\n5. **Data Compression**: Compress your data before transferring it to reduce the volume of data that needs to be moved. Azure ML can handle compressed files, which can be decompressed on the fly during training.\n\n6. **Pipeline Parallelization**: When creating Azure ML pipelines, design them to run steps that can be parallelized concurrently. This can reduce the overall data transfer time as multiple steps can process data simultaneously.\n\n7. **Proximity of Resources**: Ensure that your Azure ML workspace and Azure Storage resources are located in the same Azure region to minimize data transfer latency.\n\n8. **Incremental Data Transfer**: If your data changes incrementally, consider transferring only the new or changed data instead of the entire dataset.\n\n9. **Azure Blob Storage's Hierarchical Namespace**: If you're using Azure Data Lake Storage Gen2, take advantage of the hierarchical namespace feature to efficiently manage and access large datasets.\n\nBy implementing these strategies, you can reduce the time and resources required to transfer data between Azure Machine Learning and Azure Storage services, leading to more efficient model training processes.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps", "12,000 Mbps) would load the same data in ~33 secs (assuming you saturate the NIC at 1500-MB/s\nthroughput).\nStorage tier : For most scenarios - including Large Language Models (LLM) - standard storage provides\nthe best cost/performance profile. However, if you have many small files , premium  storage offers a\nbetter cost/performance profile. For more information, read Azure S torage options .\nStorage lo ad: If the storage account is under high load - for example, many GPU nodes in a cluster\nrequesting data - then you risk hitting the egress capacity of storage. For more information, read\nStorage load . If you have many small files that need access in parallel, you might hit the request limits\nof storage. R ead up-to-date information on the limits for both egress capacity and storage requests in\nScale targets for standard storage accounts .\nData access p attern in user code : When you use mount mode, data is fetched based on the open/read\nactions in your code. For example, when reading random sections of a large file, the default data\nprefetching settings of mounts can lead to downloads of blocks that won't be read. Y ou might need to\ntune some settings to reach maximum throughput. For more information, read Optimum mount\nsettings for common scenarios .\nTo access the logs of the data runtime from your job:\n1. Select Outputs+Logs  tab from the job page.\n2. Select the system_logs  folder, followed by data_cap ability  folder.\n3. You should see two log files:\nThe log file data-cap ability .log shows the high-level information about the time spent on key data loading\ntasks. For example, when you download data, the runtime logs the download activity start and finish times:\nlog\nIf the download throughput is a fraction of the expected network bandwidth for the VM size, you can inspect\nthe log file rslex.log.<TIMEST AMP> . This file contains all the fine-grain logging from the Rust-based\nruntime; for example, parallelization:\nlogUsing logs to diagnose issues\n\uf80a\nINFO 2023-05-18 17:14:47,790 sdk_logger.py:44 [28] - ActivityStarted, download\nINFO 2023-05-18 17:14:50,295 sdk_logger.py:44 [28] - ActivityCompleted: Activity=download,  \nHowEnded=Success, Duration=2504.39 [ms]", "Manage and optimize Azure Machine\nLearning costs\nArticle \u202208/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.\uff09 Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"]}
{"question": "What considerations should be taken into account for data privacy and compliance when integrating Azure Machine Learning with Azure Storage?", "variant_id": "v1", "line_number": 35, "answer": "When integrating Azure Machine Learning with Azure Storage for data privacy and compliance, the following considerations should be taken into account:\n\n1. **Encryption at Rest**: Ensure that all data stored in Azure Blob Storage is encrypted at rest. Azure Storage encrypts data with Microsoft-managed keys by default. For additional control, you can use customer-managed keys in Azure Key Vault.\n\n2. **Encryption in Transit**: Data should be encrypted in transit to and from Azure Storage. Azure Machine Learning and Azure Storage use secure protocols such as HTTPS to protect data as it moves between services.\n\n3. **Access Control**: Implement proper access control to the storage account. Use Azure role-based access control (RBAC) to grant the minimum necessary permissions to users and services that need to access the data.\n\n4. **Datastore Configuration**: When configuring datastores in Azure Machine Learning, ensure that they are set up with security in mind, using secure connections and appropriate authentication methods.\n\n5. **Monitoring and Auditing**: Enable Azure Storage logging and monitoring to track access and usage of the storage account. Regularly audit these logs to detect and respond to any unauthorized access or anomalies.\n\n6. **Compliance Standards**: Adhere to compliance standards relevant to your industry and the type of data you are handling. Azure has many certifications that can help meet various compliance requirements.\n\n7. **Key Management**: Regularly rotate and manage encryption keys. If you are using customer-managed keys, you should have a process in place for key rotation and revocation.\n\n8. **Data Residency**: Be aware of data residency requirements that may apply to your data. Ensure that your data is stored in a geographic location that complies with legal and regulatory requirements.\n\n9. **Data Retention and Deletion**: Define and implement data retention and deletion policies to ensure that data is not kept longer than necessary and is disposed of securely.\n\n10. **Network Security**: Use network security features such as Azure Virtual Network (VNet) service endpoints to secure the network traffic between Azure Machine Learning and Azure Storage.\n\nBy considering these aspects, you can help ensure that your integration of Azure Machine Learning with Azure Storage complies with data privacy and regulatory requirements.", "context": ["Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.", "Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps", "Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore"]}
{"question": "How can you use managed identities to securely access data in Azure Storage from Azure Machine Learning?", "variant_id": "v1", "line_number": 36, "answer": "To securely access data in Azure Storage from Azure Machine Learning using managed identities, you can follow these steps:\n\n1. **Enable Managed Identity on Azure Machine Learning Workspace:**\n   - First, ensure that your Azure Machine Learning workspace has a managed identity enabled. A managed identity is an identity in Azure Active Directory that is automatically managed by Azure. You can enable a system-assigned managed identity directly in the Azure portal under the Azure Machine Learning workspace settings.\n\n2. **Assign the Managed Identity Access to Azure Storage:**\n   - Assign the appropriate role to the managed identity for the Azure Storage account you want to access. For reading data, you would typically assign the \"Storage Blob Data Reader\" role. This can be done in the Azure portal under the \"Access control (IAM)\" settings of the storage account.\n\n3. **Create a Datastore in Azure Machine Learning:**\n   - In Azure Machine Learning, create a datastore that references the Azure Storage account. When creating the datastore, select the option to use the workspace managed identity for authentication.\n\n4. **Access Data in Training Jobs:**\n   - When you submit training jobs on compute targets like Azure Machine Learning Compute Instances or Compute Clusters, the managed identity will be used to access the data in the storage account. You do not need to provide any credentials in your code, as the managed identity will handle the authentication.\n\nBy using managed identities, you avoid the need to store and manage credentials within your application code or Azure Machine Learning datastore, which enhances security by reducing the risk of credential leaks. The managed identity has its permissions managed in Azure Active Directory and can be audited and monitored just like any other user identity.", "context": ["You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity", "Certain machine learning scenarios involve working with private data. In such cases, data\nscientists may not have direct access to data as Microsoft Entra users. In this scenario,\nthe managed identity of a compute can be used for data access authentication. In this\nscenario, the data can only be accessed from a compute instance or a machine learning\ncompute cluster executing a training job. With this approach, the admin grants the\ncompute instance or compute cluster managed identity S torage Blob Data R eader\npermissions on the storage. The individual data scientists don't need to be granted\naccess.\nTo enable authentication with compute managed identity:\nCreate compute with managed identity enabled. See the compute cluster  section,\nor for compute instance, the Assign managed identity  section.\nGrant compute managed identity at least S torage Blob Data R eader role on the\nstorage account.\nCreate any datastores with identity-based authentication enabled. See Create\ndatastores .\nOnce the identity-based authentication is enabled, the compute managed identity is\nused by default when accessing data within your training jobs. Optionally, you can\nauthenticate with user identity using the steps described in next section.\nFor information on using configuring Azure RBA C for the storage, see role-based access\ncontrols .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nWhen training on Azure Machine Learning compute clusters , you can authenticate to\nstorage with your user Microsoft Entra token.\nThis authentication mode allows you to:\uff17 Note\nThe name of the created system managed identity for compute instance or cluster\nwill be in the format /workspace-name/computes/compute-name in your Microsoft\nEntra ID.\nAccess data for training jobs on compute clusters using\nuser identity", "means that in order to successfully read data from storage, the managed identity of the\ncompute cluster where the deployment is running must have at least Storage Blob Data\nReader  access to the storage account.\nTo configure the compute cluster for data access, follow these steps:\n1. Go to Azure Machine Learning studio .\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\ndeployment is using.\n3. Assign a managed identity to the compute cluster:\na. In the Managed identity  section, verify if the compute has a managed identity\nassigned. If not, select the option Edit.\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\nusing a S ystem-Assigned Managed Identity, it is named as \"[workspace\nname]/computes/[compute cluster name]\".\nc. Save the changes.\n4. Go to the Azure portal  and navigate to the associated storage account where the\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\nstorage account where those assets are placed.", "Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution."]}
{"question": "What are the implications of storage redundancy options in Azure Storage on data availability for Azure Machine Learning experiments?", "variant_id": "v1", "line_number": 37, "answer": "Storage redundancy options in Azure Storage can have significant implications on data availability for Azure Machine Learning experiments. Here are some key points to consider:\n\n1. **Locally Redundant Storage (LRS)**: This option replicates your data within a single storage scale unit in a single data center. While it offers the lowest cost, it also has the highest risk in terms of data availability. If the data center faces an outage or a disaster, your Azure Machine Learning experiments could be disrupted due to data unavailability.\n\n2. **Zone-Redundant Storage (ZRS)**: ZRS replicates your data across multiple data centers within or across regions. This increases data availability and fault tolerance. If one zone is compromised, your data is still accessible from another, ensuring that your Azure Machine Learning experiments can continue without significant disruption.\n\n3. **Geo-Redundant Storage (GRS)** and **Geo-Zone-Redundant Storage (GZRS)**: These options provide the highest level of data durability by replicating data to a secondary region far from the primary location. In the event of a regional outage, you can still access your data, which is crucial for maintaining the continuity of Azure Machine Learning experiments.\n\nChoosing the right redundancy option depends on your specific requirements for data availability, fault tolerance, and cost. For Azure Machine Learning, having data readily available and protected against regional outages is critical for ensuring that experiments and model training can proceed without interruption. However, higher redundancy levels come with increased costs, so it's important to balance the need for data availability with budget considerations.", "context": ["setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps", "Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps", "Scenar io Requir ed\ninboundRequir ed outbound Additional\nconfiguration\nUse Azure\nKubernetes\nServiceNot applicable For information on the outbound\nconfiguration for AKS, see Secure\nAzure Kubernetes Service\ninferencing environment .\nUse Docker\nimages that\nAzure\nMachine\nLearning\nmanagesNot applicable Microsoft Artifact R egistry\nviennaglobal.azurecr.io\nglobal container registryIf the container registry\nfor your workspace is\nbehind the virtual\nnetwork, configure the\nworkspace to use a\ncompute cluster to build\nimages. For more\ninformation, see Secure\nan Azure Machine\nLearning workspace with\nvirtual networks .\nAzure Machine Learning uses multiple storage accounts. Each stores different data and\nhas a different purpose:\nYour st orage : The storage accounts in your Azure subscription store your data and\nartifacts, such as models, training data, training logs, and Python scripts. For\nexample, the default  storage account for your workspace is in your subscription.\nThe Azure Machine Learning compute instance and compute cluster access file and\nblob data in this storage over ports 445 (SMB) and 443 (HT TPS).\nWhen you're using a compute instance or compute cluster, your storage account is\nmounted as a file share via the SMB protocol. The compute instance and cluster\nuse this file share to store items like the data, models, Jupyter notebooks, and\ndatasets. The compute instance and cluster use the private endpoint when they\naccess the storage account.\nMicrosoft storage : The Azure Machine Learning compute instance and compute\ncluster rely on Azure Batch. They access storage located in a Microsoft\nsubscription. This storage is used only for the management of the compute\ninstance or cluster. None of your data is stored here. The compute instance and\ncompute cluster access the blob, table, and queue data in this storage, by using\nport 443 (HT TPS).Purposes of storage accounts", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Service Usage Example datanumber of nodes; datastore names and tags, and\ndescriptions on assets like models; and data label names.\nAzure AI\nSearchStores indexes that\nhelp with querying\nyour machine learning\ncontent.These indexes are built on top of the data stored in Azure\nCosmos DB.\nAzure\nStorageStores metadata\nrelated to Azure\nMachine Learning\npipeline data.Data can include designer pipeline names, pipeline layout,\nand execution properties.\nFrom the perspective of data lifecycle management, data in the preceding resources is\ncreated and deleted as you create and delete corresponding objects in Azure Machine\nLearning.\nYour Azure Machine Learning workspace reads and writes data by using its managed\nidentity. This identity is granted access to the resources through a role assignment\n(Azure role-based access control) on the data resources. The encryption key that you\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\nused to create indexes for Azure AI Search at runtime.\nExtra networking controls are configured when you create a private link endpoint on\nyour workspace to allow for inbound connectivity. This configuration includes the\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\nNetwork access is restricted to only trusted Microsoft services.\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\nthe data.\nWhen you use a customer-managed key, the resources are in your Azure subscription\nand encrypted with your key. While these resources exist in your subscription, Microsoft\nmanages them. They're automatically created and configured when you create your\nAzure Machine Learning workspace.\nThese Microsoft-managed resources are located in a new Azure resource group that's\ncreated in your subscription. This resource group is separate from the resource group\nfor your workspace. It contains the Microsoft-managed resources that your key is used\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\nresource group name><GUID>.Customer-managed keys"]}
{"question": "How do you automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage?", "variant_id": "v1", "line_number": 38, "answer": "To automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage, you can use Azure services and features that are designed for backup and disaster recovery. Here are the steps you can follow:\n\n1. **Azure Backup Service**: Use Azure Backup to create backup policies and recover data when needed. Azure Backup can protect Azure Blob Storage by automating the backup process and providing point-in-time restore capabilities.\n\n2. **Azure Blob Storage Snapshots**: Create snapshots of your Azure Blob Storage blobs. Snapshots capture the blob state at that point in time. You can automate snapshot creation using Azure Automation or Azure Functions with a schedule that suits your backup requirements.\n\n3. **Azure Data Factory**: Use Azure Data Factory to create data-driven workflows for orchestrating and automating data movement and data transformation. You can create a pipeline that periodically copies data from your primary storage account to a secondary storage account for backup purposes.\n\n4. **Azure Logic Apps**: Build automated workflows with Azure Logic Apps to manage data backups. You can set up triggers that automatically initiate the backup process at regular intervals.\n\n5. **Azure Storage Account Geo-redundancy**: Configure your Azure Storage account with geo-redundant storage (GRS) or read-access geo-redundant storage (RA-GRS) to maintain backups in a secondary region.\n\n6. **Azure Recovery Services Vault**: Use the Azure Recovery Services Vault to manage and monitor the backup and recovery of your Azure resources, including Azure Storage.\n\n7. **Azure CLI or PowerShell**: Write scripts using Azure CLI or PowerShell to automate the backup process. These scripts can be scheduled to run at specific intervals using Azure Automation or any other scheduler of your choice.\n\n8. **Azure Event Grid**: Use Azure Event Grid to respond to events in Azure Storage, such as the creation of a new blob, and trigger a backup process in response.\n\nRemember to test your backup and recovery process regularly to ensure that it meets your recovery objectives and complies with your organization's policies.", "context": ["Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps", "setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."]}
{"question": "What tools and techniques are available for analyzing storage costs associated with Azure Machine Learning projects?", "variant_id": "v1", "line_number": 39, "answer": "To analyze storage costs associated with Azure Machine Learning projects, you can use the following tools and techniques:\n\n1. **Azure Pricing Calculator**: Before adding any resources, you can use the Azure Pricing Calculator to estimate the costs of the storage resources you plan to use with Azure Machine Learning.\n\n2. **Cost Management Features**: After you've started using Azure Machine Learning resources, you can utilize Azure Cost Management features to monitor and manage your costs. This includes setting budgets, analyzing spending trends, and reviewing forecasted costs.\n\n3. **Cost Analysis in Azure Cost Management**: This tool supports most Azure account types and allows you to view and analyze your Azure spending, including costs associated with storage used by Azure Machine Learning.\n\n4. **Azure Advisor**: Azure Advisor provides personalized recommendations to help you optimize your Azure resources for cost, performance, high availability, and security, including recommendations on how to reduce storage costs.\n\n5. **Azure Storage Analytics**: For detailed insights into your storage usage, Azure Storage Analytics can log detailed information about successful and failed requests to your storage account, which can help you understand your storage patterns and identify areas for cost optimization.\n\nRemember to ensure that you have at least read access to the Azure account to view cost data, and you may need to assign access to Azure Cost Management data to other users as necessary.", "context": ["Plan to manage costs for Azure Machine\nLearning\nArticle \u202203/31/2023\nThis article describes how to plan and manage costs for Azure Machine Learning. First,\nyou use the Azure pricing calculator to help plan for costs before you add any resources.\nNext, as you add the Azure resources, review the estimated costs.\nAfter you've started using Azure Machine Learning resources, use the cost management\nfeatures to set budgets and monitor costs. Also review the forecasted costs and identify\nspending trends to identify areas where you might want to act.\nUnderstand that the costs for Azure Machine Learning are only a portion of the monthly\ncosts in your Azure bill. If you are using other Azure services, you're billed for all the\nAzure services and resources used in your Azure subscription, including the third-party\nservices. This article explains how to plan for and manage costs for Azure Machine\nLearning. After you're familiar with managing costs for Azure Machine Learning, apply\nsimilar methods to manage costs for all the Azure services used in your subscription.\nFor more information on optimizing costs, see how to manage and optimize cost in\nAzure Machine Learning .\nCost analysis in Cost Management supports most Azure account types, but not all of\nthem. T o view the full list of supported account types, see Understand Cost\nManagement data .\nTo view cost data, you need at least read access for an Azure account. For information\nabout assigning access to Azure Cost Management data, see Assign access to data .\uff09 Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nPrerequisites", "Use the Azure pricing calculator  to estimate costs before you create the\nresources in an Azure Machine Learning workspace. On the left, select AI +\nMachine Learning , then select Azur e Machine Learning  to begin.\nThe following screenshot shows the cost estimation by using the calculator:\nAs you add new resources to your workspace, return to this calculator and add the same\nresource here to update your cost estimates.\nFor more information, see Azure Machine Learning pricing .\nAzure Machine Learning runs on Azure infrastructure that accrues costs along with\nAzure Machine Learning when you deploy the new resource. It's important toEstimate costs before using Azure Machine\nLearning\nUnderstand the full billing  model for Azure\nMachine Learning", "Manage and optimize Azure Machine\nLearning costs\nArticle \u202208/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.\uff09 Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)", "monthly schedule and set a custom date range. Exporting cost data is the\nrecommended way to retrieve cost datasets.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nDelete instances and clusters if you do not plan on using them in the near future.\nFor more information, see manage and optimize costs in Azure Machine Learning .\nManage and optimize costs in Azure Machine Learning .\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscale\nLearn how to optimize your cloud investment with Azure Cost Management .\nLearn more about managing costs with cost analysis .\nLearn about how to prevent unexpected costs .\nTake the Cost Management  guided learning course.Other ways to manage and reduce costs for\nAzure Machine Learning\nNext steps", "To view Azure Machine Learning costs in cost analysis:\n1. Sign in to the Azure portal.\n2. Open the scope in the Azure portal and select Cost analysis  in the menu. For\nexample, go to Subscriptions , select a subscription from the list, and then select\nCost analysis  in the menu. Select Scope  to switch to a different scope in cost\nanalysis.\n3. By default, cost for services are shown in the first donut chart. Select the area in the\nchart labeled Azure Machine Learning.\nActual monthly costs are shown when you initially open cost analysis. Here's an example\nshowing all monthly usage costs.\nTo narrow costs for a single service, like Azure Machine Learning, select Add filt er and\nthen select Service name . Then, select virtual machines .\nHere's an example showing costs for just Azure Machine Learning.\n\uf80a"]}
{"question": "How can data engineers streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics?", "variant_id": "v1", "line_number": 40, "answer": "Data engineers can streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics by leveraging several Azure services and features that work together to create an efficient pipeline. Here are some steps and best practices to consider:\n\n1. **Azure Data Lake Storage (ADLS) Gen 2**: Utilize ADLS Gen 2 as the primary data storage, which provides a scalable and secure data lake that supports high-throughput analytics.\n\n2. **Azure Synapse Analytics**: Integrate Azure Machine Learning with Azure Synapse Analytics to access an Apache Spark pool for interactive data wrangling. This allows for processing large volumes of data efficiently.\n\n3. **Azure Machine Learning Notebooks**: Use Azure Machine Learning Notebooks for running data preparation and exploration tasks. These notebooks can be connected to the Synapse Spark pools for scalable processing.\n\n4. **Real-time Ingestion with Azure Event Hubs or Azure IoT Hub**: For real-time analytics, data can be ingested into Azure Machine Learning through Azure Event Hubs or Azure IoT Hub, which can handle high-throughput data streams.\n\n5. **Azure Stream Analytics**: Process the data streams in real-time using Azure Stream Analytics and then send the processed data to Azure Machine Learning for real-time predictions.\n\n6. **Azure Functions**: Use Azure Functions to trigger data processing or model scoring in Azure Machine Learning in response to events, such as new data arriving in Azure Storage.\n\n7. **Azure Key Vault**: Store sensitive information like connection strings, access keys, and secrets securely in Azure Key Vault. This ensures that the data ingestion pipeline is secure and that credentials are managed properly.\n\n8. **Service Principals**: Create a Service Principal for secure, automated access to Azure resources within the pipeline without the need for interactive login credentials.\n\n9. **Role Assignments**: Handle role assignments in Azure storage accounts to ensure that the correct permissions are in place for Azure Machine Learning to access the data.\n\n10. **Monitoring and Logging**: Utilize Azure Monitor and Azure Log Analytics to track the performance and health of the data ingestion pipeline, ensuring that any issues can be quickly identified and resolved.\n\nBy following these steps, data engineers can create a robust and streamlined data ingestion process that facilitates real-time analytics within Azure Machine Learning.", "context": ["Interactive Data Wrangling with Apache\nSpark in Azure Machine Learning\nArticle \u202210/05/2023\nData wrangling becomes one of the most important steps in machine learning projects.\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides access\nto an Apache Spark pool - backed by Azure S ynapse - for interactive data wrangling\nusing Azure Machine Learning Notebooks.\nIn this article, you'll learn how to perform data wrangling using\nServerless Spark compute\nAttached S ynapse Spark pool\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an Azure\nData Lake S torage (ADLS) Gen 2 storage account .\n(Optional): An Azure K ey Vault. See Create an Azure K ey Vault.\n(Optional): A Service Principal. See Create a Service Principal .\n(Optional): An attached S ynapse Spark pool in the Azure Machine Learning\nworkspace .\nBefore you start your data wrangling tasks, learn about the process of storing secrets\nAzure Blob storage account access key\nShared Access Signature (SAS) token\nAzure Data Lake S torage (ADLS) Gen 2 service principal information\nin the Azure K ey Vault. Y ou also need to know how to handle role assignments in the\nAzure storage accounts. The following sections review these concepts. Then, we'll\nexplore the details of interactive data wrangling using the Spark pools in Azure Machine\nLearning Notebooks.Prerequisites\n\uea80 Tip", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "runs from experimentation in the studio user interface. Or they can use versioned assets\nfor jobs like environments and storage references.\nFor more information, see Manage Azure Machine Learning workspaces .\nWhen a project is ready for operationalization, users' work can be automated in an ML\npipeline and triggered on a schedule or HT TPS request.\nYou can deploy models to the managed inferencing solution, for both real-time and\nbatch deployments, abstracting away the infrastructure management typically required\nfor deploying models.\nIn Azure Machine Learning, you can run your training script in the cloud or build a\nmodel from scratch. Customers often bring models they've built and trained in open-\nsource frameworks so that they can operationalize them in the cloud.\nData scientists can use models in Azure Machine Learning that they've created in\ncommon Python frameworks, such as:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nLightGBM\nOther languages and frameworks are also supported:\nR\n.NET\nFor more information, see Open-source integration with Azure Machine Learning .\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\nexperience and intuition to select the right data featurization and algorithm for training.\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\nLearning studio UI or the Python SDK.Train models\nOpen and interoperable\nAutomated featurization and algorithm selection", "training and evaluation, the machine learning engineers focus on model deployment\nand automation of the entire workflow. By leveraging machine learning pipeline, each\nteam only needs to work on building their own steps. The best way of building steps is\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\none step in a machine learning pipeline. All these steps built by different users are finally\nintegrated into one workflow through the pipeline definition. The pipeline is a\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\nits steps can be standardized by each company's preferred DevOps practice. The\npipeline can be further versioned and automated. If the ML projects are described as a\npipeline, then the best ML Ops practice is already applied.\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\nimproves large model training's efficiency and reduces cost. T aking modern natural\nlanguage model training as an example. It requires pre-processing large amounts of\ndata and GPU intensive transformer model training. It takes hours to days to train a\nmodel each time. When the model is being built, the data scientist wants to test\ndifferent training code or hyperparameters and run the training many times to get the\nbest model performance. For most of these trainings, there's usually small changes from\none training to another one. It will be a significant waste if every time the full training\nfrom data processing to model training takes place. By using machine learning pipeline,\nit can automatically calculate which steps result is unchanged and reuse outputs from\nprevious training. Additionally, the machine learning pipeline supports running each\nstep on different computation resources. Such that, the memory heavy data processing\nwork and run-on high memory CPU machines, and the computation intensive training\ncan run on expensive GPU machines. By properly choosing which step to run on which\ntype of machines, the training cost can be significantly reduced.\nDepending on what a machine learning project already has, the starting point of\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\nbuilding a pipeline.\nThe first approach usually applies to the team that hasn't used pipeline before and\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\ntypically have developed some machine learning models on their local environment\nusing their favorite tools. Machine learning engineers need to take data scientists'\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\nGetting  started best practices", "training and evaluation, the machine learning engineers focus on model deployment\nand automation of the entire workflow. By leveraging machine learning pipeline, each\nteam only needs to work on building their own steps. The best way of building steps is\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\none step in a machine learning pipeline. All these steps built by different users are finally\nintegrated into one workflow through the pipeline definition. The pipeline is a\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\nits steps can be standardized by each company's preferred DevOps practice. The\npipeline can be further versioned and automated. If the ML projects are described as a\npipeline, then the best ML Ops practice is already applied.\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\nimproves large model training's efficiency and reduces cost. T aking modern natural\nlanguage model training as an example. It requires pre-processing large amounts of\ndata and GPU intensive transformer model training. It takes hours to days to train a\nmodel each time. When the model is being built, the data scientist wants to test\ndifferent training code or hyperparameters and run the training many times to get the\nbest model performance. For most of these trainings, there's usually small changes from\none training to another one. It will be a significant waste if every time the full training\nfrom data processing to model training takes place. By using machine learning pipeline,\nit can automatically calculate which steps result is unchanged and reuse outputs from\nprevious training. Additionally, the machine learning pipeline supports running each\nstep on different computation resources. Such that, the memory heavy data processing\nwork and run-on high memory CPU machines, and the computation intensive training\ncan run on expensive GPU machines. By properly choosing which step to run on which\ntype of machines, the training cost can be significantly reduced.\nDepending on what a machine learning project already has, the starting point of\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\nbuilding a pipeline.\nThe first approach usually applies to the team that hasn't used pipeline before and\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\ntypically have developed some machine learning models on their local environment\nusing their favorite tools. Machine learning engineers need to take data scientists'\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\nGetting  started best practices"]}
