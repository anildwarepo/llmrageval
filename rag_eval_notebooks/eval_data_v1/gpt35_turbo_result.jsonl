{"question": "How do you initiate a new Azure ML experiment using the Azure CLI?", "variant_id": "v1", "line_number": 1, "answer": "To initiate a new Azure ML experiment using the Azure CLI, you would typically follow these steps:\n\n1. Ensure you have the Azure CLI installed along with the ML extension. If you haven't installed the ML extension, you can add it using the following command:\n\n   ```\n   az extension add -n ml\n   ```\n\n2. Set your Azure subscription context and configure defaults for your workspace, resource group, and location:\n\n   ```\n   az account set --subscription <subscription-id>\n   az configure --defaults workspace=<workspace-name> group=<resource-group-name> location=<location>\n   ```\n\n3. Create an experiment by using the `az ml` command. However, as of my knowledge cutoff in early 2023, the Azure CLI does not have a direct command to create an experiment. Instead, you would typically run an experiment script or submit a job that implicitly creates an experiment if it does not already exist.\n\n   To submit a job as part of an experiment, you would use a command like:\n\n   ```\n   az ml job create --file <job-definition-file.yaml> --experiment-name <experiment-name>\n   ```\n\n   The job definition file (`job-definition-file.yaml`) contains the configuration for the job you want to run, including the compute target, environment, and the script or command to execute.\n\n   If the specified experiment name does not exist, Azure Machine Learning will create a new experiment with that name and associate the submitted job with it.\n\nPlease note that the Azure CLI and Azure Machine Learning services are continuously updated, and the exact commands may change. Always refer to the latest Azure CLI documentation for Azure Machine Learning for the most current information.", "context": ["The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\nAzure CLI\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\nso one cluster can host one or many batch deployments (along with other workloads if\ndesired).\nThis article uses a compute created here named batch-cluster. Adjust as needed and\nreference your compute using azureml:<your-compute-name> or create one as shown.\nAzure CLIaz extension add -n ml \nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>  \naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>  \nCreate compute\nAzure CLI\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \n0 --max-instances  5 \n\uff17 Note", "Azure CLI\nIf you're not on the compute instance, omit --identity and follow the prompt to\nopen a browser window to authenticate.\n3. Make sure you have the most recent versions of the CLI and the ml extension:\nAzure CLI\n4. If you have multiple Azure subscriptions, set the active subscription to the one\nyou're using for your workspace. (Y ou can skip this step if you only have access to\na single subscription.) R eplace <SUBSCRIPTION-NAME> with your subscription name.\nAlso remove the brackets <>.\nAzure CLI\n5. Now use CLI to submit the job. If you're doing this on a compute instance in your\nworkspace, you can use environment variables for the workspace name and\nresource group as show in the following code. If you aren't on a compute instance,\nreplace these values with your workspace name and resource group.\nAzure CLI\nOnce you've submitted the job, you can check the status and results in studio:\n1. Sign in to Azure Machine Learning studio .\n2. Select your workspace if it isn't already loaded.\n3. On the left navigation, select Jobs.\n4. Select the Experiment name  that you used to train your model.\n5. Select the Display name  of the job to view details and artifacts of the job,\nincluding metrics, images, child jobs, outputs, logs, and code used in the job.az login --identity\naz upgrade\naz account set --subscription  \"<SUBSCRIPTION-NAME>\"\naz ml job create -f job.yml  --workspace-name  $CI_WORKSPACE  --resource-\ngroup $CI_RESOURCE_GROUP\nRegister model", "Azure CLI\nIf you don't have it, create a cluster called cpu-cluster by running:\nAzure CLI\nNow, create a pipeline job defined in the pipeline.yml file with the following command.\nThe compute target is referenced in the pipeline.yml file as azureml:cpu-cluster. If your\ncompute target uses a different name, remember to update it in the pipeline.yml file.\nAzure CLI\nYou should receive a JSON dictionary with information about the pipeline job including:\nKey Descr iption\nname The GUID-based name of the job.\nexperiment_name The name under which jobs will be organized in studio.\nservices.Studio.endpoint A URL for monitoring and reviewing the pipeline job.\nstatus The status of the job. This will likely be Preparing at this point.\nOpen the services.Studio.endpoint URL to see a graph visualization of the pipeline.az ml compute list\n\uff17 Note\nSkip this step to use serverless comput e.\naz ml compute create -n cpu-cluster  --type amlcompute --min-instances  0 --\nmax-instances  10\naz ml job create --file pipeline.yml\n\uff89Expand table", "The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nIn this example, we are going to create a deployment that can write directly to the\noutput folder of the batch deployment job. The deployment will use this feature to write\ncustom parquet files.Azure CLI\naz extension add -n ml\n\uff17 Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nCreating a batch deployment with a custom\noutput", "The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\ninference over new data:\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\n\uff17 Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nSteps"]}
{"question": "Describe the process of creating a new compute instance via the Azure ML CLI. What parameters are necessary?", "variant_id": "v1", "line_number": 2, "answer": "To create a new compute instance via the Azure Machine Learning CLI, you would typically follow these steps:\n\n1. **Set up Azure CLI with the ML extension**: Ensure that you have the Azure CLI installed and the ML extension (v2) added to it. You can add the ML extension using the command `az extension add -n ml`.\n\n2. **Log in to Azure**: Use the `az login` command to log in to your Azure account.\n\n3. **Select the appropriate subscription**: If you have access to multiple subscriptions, set the one you want to use with `az account set --subscription \"Your Subscription ID\"`.\n\n4. **Create the compute instance**: Use the `az ml compute create` command with the necessary parameters to create a compute instance. The basic parameters you would need include:\n\n   - `--name` or `-n`: The name of the compute instance.\n   - `--type`: The type of compute resource, which should be set to `computeinstance` for a compute instance.\n   - `--size`: The Virtual Machine size for the compute instance.\n   - `--workspace-name` or `-w`: The name of the Azure Machine Learning workspace.\n   - `--resource-group` or `-g`: The name of the resource group where the workspace is located.\n\nHere is an example command that incorporates these parameters:\n\n```sh\naz ml compute create --name myComputeInstance --type computeinstance --size Standard_DS11_v2 --workspace-name myWorkspace --resource-group myResourceGroup\n```\n\nThis command will create a new compute instance named `myComputeInstance` with the VM size `Standard_DS11_v2` in the workspace `myWorkspace` which is located in the resource group `myResourceGroup`.\n\nRemember that you can also specify additional parameters such as `--location` to define the Azure region for the compute instance, and you can use a setup script with `--setup-script` to customize the environment of the compute instance.\n\nPlease note that the exact parameters and options can change over time, so it's always a good idea to check the latest Azure CLI documentation or use `az ml compute create --help` for the most up-to-date information on creating compute instances.", "context": ["Create an Azure Machine Learning\ncompute instance\nArticle \u202212/08/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nLearn how to create a compute instance  in your Azure Machine Learning workspace.\nUse a compute instance as your fully configured and managed development\nenvironment in the cloud. For development and testing, you can also use the instance as\na training compute target . A compute instance can run multiple jobs in parallel and has\na job queue. As a development environment, a compute instance can't be shared with\nother users in your workspace.\nIn this article, you learn how to create a compute instance. See Manage an Azure\nMachine Learning compute instance  for steps to manage start, stop, restart, delete a\ncompute instance.\nYou can also use a setup script  to create the compute instance with your own custom\nenvironment.\nCompute instances can run jobs securely in a virtual network environment , without\nrequiring enterprises to open up SSH ports. The job executes in a containerized\nenvironment and packages your model dependencies in a Docker container.\nAn Azure Machine Learning workspace. For more information, see Create an Azure\nMachine Learning workspace . In the storage account, the \"Allow storage account\nkey access\" option must be enabled for compute instance creation to be\nsuccessful.\nChoose the tab for the environment you're using for other prerequisites.\n\uff17 Note\nThis article uses CLI v2 in some examples. If you are still using CLI v1, see Create an\nAzur e Machine Learning comput e clust er CLI v1) .\nPrerequisites", "For more information on creating a compute cluster and compute cluster, including how\nto do so with Python and the CLI, see the following articles:\nCreate a compute cluster\nCreate a compute instance\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nWhen Azure Container R egistry is behind the virtual network, Azure Machine Learning\ncan't use it to directly build Docker images (used for training and deployment). Instead,\nconfigure the workspace to use the compute cluster you created earlier. Use the\nfollowing steps to create a compute cluster and configure the workspace to use it to\nbuild images:\n1. Navigate to https://shell.azure.com/  to open the Azure Cloud Shell.\n2. From the Cloud Shell, use the following command to install the 2.0 CLI for Azure\nMachine Learning:\nAzure CLIAllow inbound T CP traffic on ports 29876-29877 from the\nBatchNodeManagement service tag.\nAllow inbound T CP traffic on port 44224 from the AzureMachineLearning\nservice tag.\nThe following screenshot shows an example of these rules:\nConfigure image builds\naz extension add -n ml", "The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\nAzure CLI\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\nso one cluster can host one or many batch deployments (along with other workloads if\ndesired).\nThis article uses a compute created here named batch-cluster. Adjust as needed and\nreference your compute using azureml:<your-compute-name> or create one as shown.\nAzure CLIaz extension add -n ml \nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>  \naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>  \nCreate compute\nAzure CLI\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \n0 --max-instances  5 \n\uff17 Note", "To create a workspace using CLI v2, use the following command:\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nBash\nFor more information, see workspace Y AML schema .\nA compute is a designated compute resource where you run your job or host your\nendpoint. Azure Machine Learning supports the following types of compute:\nComput e clust er - a managed-compute infrastructure that allows you to easily\ncreate a cluster of CPU or GPU compute nodes in the cloud.\nComput e instance  - a fully configured and managed development environment in\nthe cloud. Y ou can use the instance as a training or inference compute for\ndevelopment and testing. It's similar to a virtual machine on the cloud.\nInfer ence clust er - used to deploy trained machine learning models to Azure\nKubernetes Service. Y ou can create an Azure Kubernetes Service (AKS) cluster from\nyour Azure Machine Learning workspace, or attach an existing AKS cluster.\nAttached comput e - You can attach your own compute resources to your\nworkspace and use them for training and inference.\nTo create a compute using CLI v2, use the following command:\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)Azure CLI\naz ml workspace create --file my_workspace.yml  \nCompute\n\uff17 Note\nInstead of creating a compute cluster, use serverless comput e (preview) to\noffload compute lifecycle management to Azure Machine Learning.\nAzure CLI", "Manage an Azure Machine Learning\ncompute instance\nArticle \u202207/06/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nLearn how to manage a compute instance  in your Azure Machine Learning workspace.\nUse a compute instance as your fully configured and managed development\nenvironment in the cloud. For development and testing, you can also use the instance as\na training compute target . A compute instance can run multiple jobs in parallel and has\na job queue. As a development environment, a compute instance can't be shared with\nother users in your workspace.\nIn this article, you learn how to start, stop, restart, delete) a compute instance. See\nCreate an Azure Machine Learning compute instance  to learn how to create a compute\ninstance.\nAn Azure Machine Learning workspace. For more information, see Create an Azure\nMachine Learning workspace . In the storage account, the \"Allow storage account\nkey access\" option must be enabled for compute instance creation to be\nsuccessful.\nThe Azure CLI extension for Machine Learning service (v2) , Azure Machine\nLearning Python SDK (v2) , or the Azure Machine Learning Visual S tudio Code\nextension .\nIf using the Python SDK, set up your development environment with a workspace .\nOnce your environment is set up, attach to the workspace in your Python script:\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\n\uff17 Note\nThis article shows CLI v2 in the sections below. If you are still using CLI v1, see\nCreate an Azur e Machine Learning comput e clust er CLI v1) .\nPrerequisites"]}
{"question": "Explain how to submit a training job using Python and the AML SDK v2. Include an example of specifying a compute target.", "variant_id": "v1", "line_number": 3, "answer": "To submit a training job using Python and the Azure Machine Learning (AML) SDK v2, you need to follow these steps:\n\n1. **Set up your environment**:\n   - Install the Azure Machine Learning SDK v2 if you haven't already.\n   - Import the necessary modules and classes from the SDK.\n   - Create or load an existing Azure Machine Learning workspace.\n\n2. **Create or attach a compute target**:\n   - Define the compute target where the training will occur. This could be an Azure Machine Learning Compute Instance or a Compute Cluster.\n\n3. **Define the training script**:\n   - Write a Python script that contains the code for training your model. Save this script in a directory.\n\n4. **Create a command job**:\n   - Use the `Command` class to define the command job, specifying the script to run, the compute target, and any other necessary parameters such as environment and inputs/outputs.\n\n5. **Submit the job**:\n   - Use the `Job` class to submit the command job to the workspace.\n\nHere's an example of how you might specify a compute target and submit a training job using the AML SDK v2:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import Workspace, Compute, Command, Environment\nfrom azure.identity import DefaultAzureCredential\n\n# Load the workspace\nws = Workspace.from_config()\n\n# Create MLClient using the workspace and DefaultAzureCredential\nml_client = MLClient.from_config(credential=DefaultAzureCredential(), workspace=ws)\n\n# Define the compute target\ncompute_name = \"my-compute-cluster\"\ncompute = Compute(name=compute_name, type=\"amlcompute\")\ncompute.size = \"Standard_DS3_v2\"\ncompute.min_nodes = 0\ncompute.max_nodes = 4\n\n# Register the compute target if it doesn't exist\nif compute_name not in ws.compute_targets:\n    compute = ml_client.compute.begin_create_or_update(compute)\n\n# Define the environment for the job\nenvironment = Environment(\n    name=\"my-environment\",\n    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n    conda_file=\"environment.yml\"  # This should point to your conda environment file\n)\n\n# Define the command job\ncommand_job = Command(\n    code=\"./src\",  # This should point to the directory containing your training script\n    command=\"python train.py\",  # This should be the command to run your training script\n    environment=environment,\n    compute=compute_name,\n    display_name=\"my-training-job\",\n    experiment_name=\"my-experiment\"\n)\n\n# Submit the job\nreturned_job = ml_client.jobs.create_or_update(command_job)\n\n# If you want to monitor the job, you can use the following:\njob_stream = ml_client.jobs.stream(returned_job.name)\n```\n\nIn this example, replace `\"./src\"` with the path to the directory containing your training script, and `\"python train.py\"` with the command to run your training script. The `environment.yml` should specify the conda environment needed for your training job. The `compute_name` should match the name of the compute target you want to use for training.", "context": ["1. On the top bar above your opened notebook, create a compute instance if you\ndon't already have one.\n2. If the compute instance is stopped, select Start comput e and wait until it is\nrunning.\n3. Make sure that the kernel, found on the top right, is Python 3.10 - SDK v2. If not,\nuse the dropdown to select this kernel.\n4. If you see a banner that says you need to be authenticated, select Authenticat e.\nTo train a model, you need to submit a job. The type of job you'll submit in this tutorial\nis a command job . Azure Machine Learning offers several different types of jobs to train\nmodels. Users can select their method of training based on complexity of the model,\ndata size, and training speed requirements. In this tutorial, you'll learn how to submit a\ncommand job  to run a training s cript.\nA command job is a function that allows you to submit a custom training script to train\nyour model. This can also be defined as a custom training job. A command job in Azure\nMachine Learning is a type of job that runs a script or command in a specified\nenvironment. Y ou can use command jobs to train models, process data, or any other\ncustom code you want to execute in the cloud.Set your kernel\n\uf80a\n\uf80a\n\uf80a\n\uff09 Impor tant\nThe rest of this tutorial contains cells of the tutorial notebook. Copy/paste them\ninto your new notebook, or switch to the notebook now if you cloned it.\nUse a command job to train a model in Azure\nMachine Learning", "For detailed instructions on how to complete these steps, see Secure a training\nenvironment .\nIn this section, you learn how Azure Machine Learning securely communicates between\nservices to submit a training job. This example shows you how all your configurations\nwork together to secure communication.\n1. The client uploads training scripts and training data to storage accounts that are\nsecured with a service or private endpoint.\n2. The client submits a training job to the Azure Machine Learning workspace\nthrough the private endpoint.\n3. Azure Batch service receives the job from the workspace. It then submits the\ntraining job to the compute environment through the public load balancer for the\ncompute resource.\n4. The compute resource receives the job and begins training. The compute resource\nuses information stored in key vault to access storage accounts to download\ntraining files and upload output.Example training job submission", "Python\nView more examples of training with serverless compute at:-\nQuick S tart\nTrain Model\nThere's no need to specify compute for AutoML jobs. R esources can be optionally\nspecified. If instance count isn't specified, then it's defaulted based on\nmax_concurrent_trials and max_nodes parameters. If you submit an AutoML image\nclassification or NLP task with no instance type, the GPU VM size is automatically\nselected. It's possible to submit AutoML job through CLIs, SDK, or S tudio. T o submit\nAutoML jobs with serverless compute in studio first enable the submit a training job in\nstudio (preview)  feature in the preview panel.Python SDK\nfrom azure.ai.ml import command\nfrom azure.ai.ml import MLClient      # Handle to the workspace\nfrom azure.identity import DefaultAzureCredential     # Authentication  \npackage\nfrom azure.ai.ml.entities import ResourceConfiguration\nfrom azure.ai.ml.entities import UserIdentityConfiguration \ncredential = DefaultAzureCredential()\n# Get a handle to the workspace. You can find the info on the workspace  \ntab on ml.azure.com\nml_client = MLClient(\n    credential=credential,\n    subscription_id= \"<Azure subscription id>\" , \n    resource_group_name= \"<Azure resource group>\" ,\n    workspace_name= \"<Azure Machine Learning Workspace>\" ,\n)\njob = command(\n    command= \"echo 'hello world'\" ,\n    environment= \"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\" ,\n         identity=UserIdentityConfiguration(),\n    queue_settings={\n      \"job_tier\" : \"Standard\"   \n    }\n)\njob.resources = ResourceConfiguration(instance_type= \"Standard_E4s_v3\" , \ninstance_count= 1)\n# submit the command job\nml_client.create_or_update(job)\nAutoML job", "Designer : Azure Machine Learning designer provides an easy entry-point into\nmachine learning for building proof of concepts, or for users with little coding\nexperience. It allows you to train models using a drag and drop web-based UI. Y ou\ncan use Python code as part of the design, or train models without writing any\ncode.\nAzur e CLI : The machine learning CLI provides commands for common tasks with\nAzure Machine Learning, and is often used for scripting and aut omating tasks . For\nexample, once you've created a training script or pipeline, you might use the Azure\nCLI to start a training job on a schedule or when the data files used for training are\nupdated. For training models, it provides commands that submit training jobs. It\ncan submit jobs using run configurations or pipelines.\nEach of these training methods can use different types of compute resources for\ntraining. Collectively, these resources are referred to as comput e targets. A compute\ntarget can be a local machine or a cloud resource, such as an Azure Machine Learning\nCompute, Azure HDInsight, or a remote virtual machine.\nThe Azure Machine Learning SDK for Python allows you to build and run machine\nlearning workflows with Azure Machine Learning. Y ou can interact with the service from\nan interactive Python session, Jupyter Notebooks, Visual S tudio Code, or other IDE.\nInstall/update the SDK\nConfigure a development environment for Azure Machine Learning\nA generic training job with Azure Machine Learning can be defined using the\ncommand() . The command is then used, along with your training script(s) to train a\nmodel on the specified compute target.\nYou may start with a command for your local computer, and then switch to one for a\ncloud-based compute target as needed. When changing the compute target, you only\nchange the compute parameter in the command that you use. A run also logs\ninformation about the training job, such as the inputs, outputs, and logs.\nTutorial: T rain your first ML model\nExamples: Jupyter Notebook and Python examples of training modelsPython SDK\nSubmit a command\nAutomated Machine Learning", "An Azure Machine Learning compute cluster is a fully managed compute resource that\ncan be used to run the training job. In the following examples, a compute cluster named\ncpu-compute is created.\nPython\nTo run this script, you'll use a command that executes main.py Python script located\nunder ./sdk/python/jobs/single-step/lightgbm/iris/src/. The command will be runworkspace = '<AZUREML_WORKSPACE_NAME>'\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \nresource_group, workspace)\n2. Create a compute resource for training\n\uff17 Note\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\ntraining job .\nPython SDK\nfrom azure.ai.ml.entities import AmlCompute\n# specify aml compute name.\ncpu_compute_target = \"cpu-cluster\"\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(\"Creating a new cpu compute target...\" )\n    compute = AmlCompute(\n        name=cpu_compute_target, size= \"STANDARD_D2_V2\" , min_instances= 0, \nmax_instances= 4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n4. Submit the training job\nPython SDK"]}
{"question": "What is the command to list all the models registered in an Azure ML Workspace using the Azure CLI?", "variant_id": "v1", "line_number": 4, "answer": "The command to list all the models registered in an Azure ML Workspace using the Azure CLI is:\n\n```\naz ml model list\n```", "context": ["then select Regist er.\nThe SDK and CLI (v2) also allow you to manage the lifecycle of your Azure Machine\nLearning model assets.\nList all the models in your workspace:\ncli\nList all the model versions under a given name:\ncli\n\uf80a\nManage models\nList\nAzure CLI\naz ml model list  \nAzure CLI\naz ml model list --name run-model-example", "Install the Azure CLI and the ml extension to the Azure CLI. For more\ninformation on installing the CLI, see Install and set up the CLI (v2) .\nFirst, connect to the Azure Machine Learning workspace where you'll work.\nAzure CLI\nYou can deploy only registered models to online endpoints. In this case, you already\nhave a local copy of the model in the repository, so you only need to publish the model\nto the registry in the workspace. Y ou can skip this step if the model you're trying to\ndeploy is already registered.\nAzure CLI\nIf your model was logged inside of a run, you can register it directly.\nTo register the model, you need to know the location where it is stored. If you're using\nMLflow's autolog feature, the path to the model depends on the model type and\nframework. Y ou should check the jobs output to identify the name of the model's folder.\nThis folder contains a file named MLModel.Connect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nRegister the model\nAzure CLI\nMODEL_NAME= 'sklearn-diabetes'\naz ml model create --name $MODEL_NAME  --type \"mlflow_model\"  --path \n\"endpoints/online/ncd/sklearn-diabetes/model\"\nWhat if your model was logged inside of a run?", "Install the Azure CLI and the ml extension to the Azure CLI. For more\ninformation on installing the CLI, see Install and set up the CLI (v2) .\nFirst, connect to the Azure Machine Learning workspace where you'll work.\nAzure CLI\nYou can deploy only registered models to online endpoints. In this case, you already\nhave a local copy of the model in the repository, so you only need to publish the model\nto the registry in the workspace. Y ou can skip this step if the model you're trying to\ndeploy is already registered.\nAzure CLI\nIf your model was logged inside of a run, you can register it directly.\nTo register the model, you need to know the location where it is stored. If you're using\nMLflow's autolog feature, the path to the model depends on the model type and\nframework. Y ou should check the jobs output to identify the name of the model's folder.\nThis folder contains a file named MLModel.Connect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nRegister the model\nAzure CLI\nMODEL_NAME= 'sklearn-diabetes'\naz ml model create --name $MODEL_NAME  --type \"mlflow_model\"  --path \n\"endpoints/online/ncd/sklearn-diabetes/model\"\nWhat if your model was logged inside of a run?", "The az ml model command can be used for managing Azure Machine Learning models.\nExamples are available in the examples GitHub repository . Several are shown below.\nYAML\nYAML\nInstall and use the CLI (v2)Remarks\nExamples\nYAML: local file\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-file-example  \npath: mlflow-model/model.pkl  \ndescription:  Model created from local file. \nYAML: local folder in MLflow format\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-mlflow-example  \npath: mlflow-model  \ntype: mlflow_model  \ndescription:  Model created from local MLflow model directory.", "The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\ninference over new data:\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\n\uff17 Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nSteps"]}
{"question": "Provide an example of how to use the AML SDK v2 to stream logs from a running experiment in Python.", "variant_id": "v1", "line_number": 5, "answer": "To stream logs from a running experiment using Azure Machine Learning SDK v2 in Python, you can use the `stream` method of the `Job` object. Here's an example of how you might do this:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\n\n# Connect to the workspace\nml_client = MLClient.from_config(DefaultAzureCredential())\n\n# Assuming you have already created and started a command job and have its name\njob_name = \"your_job_name_here\"\n\n# Get the job object\njob = ml_client.jobs.get(name=job_name)\n\n# Stream the logs\nml_client.jobs.stream(name=job_name)\n```\n\nIn this example, `ml_client.jobs.stream(name=job_name)` is used to stream the logs of the job specified by `job_name`. The logs will be streamed to your console, allowing you to monitor the progress and output of your experiment in real-time.\n\nPlease ensure that you replace `\"your_job_name_here\"` with the actual name of the job you want to stream logs from. Also, make sure that you have the necessary permissions and that the job is in a state that produces logs (e.g., running or in progress).", "context": ["SDK v2\nPython\nFunctionality in SDK v1 Rough mapping in SDK v2\nexperiment.submit MLCLient.jobs.create_or_update\nScriptRunConfig() command()\nFor more information, see:\nV1 - Experiment\nV2 - Command Jobaml_url = run.get_portal_url()  \nprint(aml_url)  \n#import required libraries  \nfrom azure.ai.ml import MLClient, command  \nfrom azure.ai.ml.entities import Environment  \nfrom azure.identity import DefaultAzureCredential  \n#connect to the workspace  \nml_client = MLClient.from_config(DefaultAzureCredential())  \n# set up pytorch environment  \nenv = Environment(  \n    image= \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\" , \n    conda_file= \"pytorch-env.yml\" , \n    name= \"pytorch-env\"  \n) \n# define the command  \ncommand_job = command(  \n    code= \"./src\", \n    command= \"train.py\" , \n    environment=env,  \n    compute= \"cpu-cluster\" , \n) \nreturned_job = ml_client.jobs.create_or_update(command_job)  \nreturned_job  \nMapping of key functionality in v1 and v2\nNext steps", "Bash\nIf you're doing remote tracking (tracking experiments that run outside Azure\nMachine Learning), configure MLflow to track experiments. For more information,\nsee Configure MLflow for Azure Machine Learning .\nTo log metrics, parameters, artifacts, and models in your experiments in Azure\nMachine Learning using MLflow, just import MLflow into your script:\nPython\nMLflow organizes the information in experiments and runs (in Azure Machine Learning,\nruns are called jobs). There are some differences in how to configure them depending\non how you run your code:\nWhen training interactively, such as in a Jupyter Notebook, use the following\npattern:\n1. Create or set the active experiment.\n2. Start the job.\n3. Use logging methods to log metrics and other information.\n4. End the job.\nFor example, the following code snippet configures the experiment, and then logs\nduring a job:\nPythonpip install mlflow azureml-mlflow\nimport mlflow\nConfigure experiments\nTraining interactively\nimport mlflow\n# Set the experiment\nmlflow.set_experiment( \"mlflow-experiment\" )\n# Start the run\nmlflow_run = mlflow.start_run()\n# Log metrics or other information\nmlflow.log_metric( 'mymetric' , 1)", "Log metr ics, parameters, and files with\nMLflow\nArticle \u202201/31/2024\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAzure Machine Learning supports logging and tracking experiments using MLflow\nTracking . You can log models, metrics, parameters, and artifacts with MLflow, either\nlocally on your computer or in a cloud environment.\nLogs can help you diagnose errors and warnings, or track performance metrics like\nparameters and model performance. This article explains how to enable logging in the\nfollowing scenarios:\nYou must have an Azure Machine Learning workspace. If you don't have one, see\nCreate workspace resources .\nYou must have the mlflow and azureml-mlflow packages installed. If you don't, use\nthe following command to install them in your development environment:\n\uff09 Impor tant\nUnlike the Azure Machine Learning SDK v1, there's no logging functionality in the\nAzure Machine Learning SDK for Python (v2). If you used Azure Machine Learning\nSDK v1 before, we recommend that you leverage MLflow for tracking experiments.\nSee Migrat e logging fr om SDK v1 t o MLflow  for specific guidance.\nLog metrics, parameters, and models when submitting jobs.\uff02\nTrack runs when training interactively.\uff02\nView diagnostic information about training.\uff02\n\uea80 Tip\nThis article shows you how to monitor the model training process. If you're\ninterested in monitoring resource usage and events from Azure Machine Learning,\nsuch as quotas, completed training jobs, or completed model deployments, see\nMonit oring Azur e Machine Learning .\nPrerequisites", "Query & co mpare experiments and runs\nwith MLflow\nArticle \u202206/26/2023\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\nYou don't need to install any specific SDK to manage what happens inside of a training\njob, creating a more seamless transition between local runs and the cloud by removing\ncloud-specific dependencies. In this article, you'll learn how to query and compare\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\nin Python.\nMLflow allows you to:\nCreate, query, delete and search for experiments in a workspace.\nQuery, delete, and search for runs in a workspace.\nTrack and retrieve metrics, parameters, artifacts and models from runs.\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\ndetailed comparison between MLflow Open-Source and MLflow when connected to\nAzure Machine Learning.\nQuery and searching experiments and runs is also available using the MLflow REST API.\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\nconsume it.\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\nMLflow azureml-mlflow.\uff17 Note\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\ntracking capabilities. This applies not just for logging but also for querying the\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\nexplains how to use MLflow to manage experiments and runs in Azure Machine\nLearning.\nREST API\nPrerequisites", "Query & co mpare experiments and runs\nwith MLflow\nArticle \u202206/26/2023\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\nYou don't need to install any specific SDK to manage what happens inside of a training\njob, creating a more seamless transition between local runs and the cloud by removing\ncloud-specific dependencies. In this article, you'll learn how to query and compare\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\nin Python.\nMLflow allows you to:\nCreate, query, delete and search for experiments in a workspace.\nQuery, delete, and search for runs in a workspace.\nTrack and retrieve metrics, parameters, artifacts and models from runs.\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\ndetailed comparison between MLflow Open-Source and MLflow when connected to\nAzure Machine Learning.\nQuery and searching experiments and runs is also available using the MLflow REST API.\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\nconsume it.\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\nMLflow azureml-mlflow.\uff17 Note\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\ntracking capabilities. This applies not just for logging but also for querying the\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\nexplains how to use MLflow to manage experiments and runs in Azure Machine\nLearning.\nREST API\nPrerequisites"]}
{"question": "How can you configure an AutoML run for a classification task using the AML SDK v2, including specifying primary metric and training data?", "variant_id": "v1", "line_number": 6, "answer": "To configure an AutoML run for a classification task using the Azure Machine Learning (AML) SDK v2, you can follow these steps:\n\n1. **Import Necessary Libraries**: Import the required modules from the Azure ML SDK.\n\n```python\nfrom azure.ai.ml import automl, Input\nfrom azure.ai.ml.constants import AssetTypes\nfrom azure.ai.ml.automl import classification\n```\n\n2. **Create MLTable for Training Data**: Define your training data as an MLTable, which is a structured dataset format used by Azure ML.\n\n```python\nmy_training_data_input = Input(\n    type=AssetTypes.MLTABLE, path=\"./data/training-mltable-folder\"\n)\n```\n\n3. **Define the AutoML Classification Job**: Use the `automl.classification` factory function to create a classification job. Specify the compute target, experiment name, training data, target column name, and primary metric. You can also set other parameters like cross-validation, model explainability, and custom tags.\n\n```python\nclassification_job = automl.classification(\n    compute=\"<compute_name>\",\n    experiment_name=\"<exp_name>\",\n    training_data=my_training_data_input,\n    target_column_name=\"<name_of_target_column>\",\n    primary_metric=\"accuracy\",\n    n_cross_validations=5,\n    enable_model_explainability=True,\n    tags={\"my_custom_tag\": \"My custom value\"},\n)\n```\n\n4. **Set Limits (Optional)**: You can optionally set limits on the AutoML job, such as the overall timeout, per-trial timeout, maximum number of trials, and maximum concurrent trials.\n\n```python\nclassification_job.set_limits(\n    timeout_minutes=600,\n    trial_timeout_minutes=20,\n    max_trials=5,\n    max_concurrent_trials=4\n)\n```\n\n5. **Submit the AutoML Run**: Submit the AutoML job to the experiment and start the run.\n\n```python\nfrom azure.ai.ml import MLClient\n\n# Assuming you have already created an MLClient instance\nremote_run = ml_client.create_or_update(classification_job)\n```\n\n6. **Retrieve the URL for Monitoring**: After submitting the job, you can get the URL to monitor the run in the Azure ML Studio.\n\n```python\nazureml_url = remote_run.get_portal_url()\nprint(azureml_url)\n```\n\nMake sure to replace placeholders like `<compute_name>`, `<exp_name>`, and `<name_of_target_column>` with your actual compute target name, experiment name, and the name of the target column in your dataset, respectively. Also, ensure that the path to the training data is correct and accessible by the compute target.", "context": ["SDK v2: Below is a sample AutoML classification task. For the entire code, check\nout our examples repo .\nPython    debug_log= \"automl_errors.log\" , \n    compute_target=compute_target,  \n    training_data=training_data,  \n    label_column_name=label_column_name,  \n    **automl_settings,  \n) \n# Submit run  \nremote_run = experiment.submit(automl_config, show_output= False) \nazureml_url = remote_run.get_portal_url()  \nprint(azureml_url)  \n# Imports  \nfrom azure.ai.ml import automl, Input, MLClient  \nfrom azure.ai.ml.constants import AssetTypes  \nfrom azure.ai.ml.automl import ( \n    classification,  \n    ClassificationPrimaryMetrics,\n    ClassificationModels,  \n) \n# Create MLTables for training dataset  \n# Note that AutoML Job can also take in tabular data  \nmy_training_data_input = Input(  \n    type=AssetTypes.MLTABLE, path= \"./data/training-mltable-folder\"  \n) \n# Create the AutoML classification job with the related factory-\nfunction.  \nclassification_job = automl.classification(  \n    compute= \"<compute_name>\" , \n    experiment_name= \"<exp_name?\" , \n    training_data=my_training_data_input,  \n    target_column_name= \"<name_of_target_column>\" , \n    primary_metric= \"accuracy\" , \n    n_cross_validations= 5, \n    enable_model_explainability= True, \n    tags={ \"my_custom_tag\" : \"My custom value\" }, \n) \n# Limits are all optional  \nclassification_job.set_limits(  \n    timeout_minutes= 600, \n    trial_timeout_minutes= 20, \n    max_trials= 5, \n    max_concurrent_trials = 4,", "Upgrade AutoML to SDK v2\nArticle \u202204/04/2023\nIn SDK v2, \"experiments\" and \"runs\" are consolidated into jobs.\nIn SDK v1, AutoML was primarily configured and run using the AutoMLConfig class. In\nSDK v2, this class has been converted to an AutoML job. Although there are some\ndifferences in the configuration options, by and large, naming & functionality has been\npreserved in V2.\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\nSDK v1: Below is a sample AutoML classification task. For the entire code, check\nout our examples repo .\nPythonSubmit AutoML run\n# Imports  \nimport azureml.core  \nfrom azureml.core.experiment import Experiment  \nfrom azureml.core.workspace import Workspace  \nfrom azureml.core.dataset import Dataset  \nfrom azureml.train.automl import AutoMLConfig  \nfrom azureml.train.automl.run import AutoMLRun    \n# Load tabular dataset  \ndata = \"<url_to_data>\"  \ndataset = Dataset.Tabular.from_delimited_files(data)  \ntraining_data, validation_data = dataset.random_split(percentage= 0.8, \nseed=223) \nlabel_column_name = \"Class\" \n# Configure Auto ML settings  \nautoml_settings = {  \n    \"n_cross_validations\" : 3, \n    \"primary_metric\" : \"average_precision_score_weighted\" , \n    \"enable_early_stopping\" : True, \n    \"max_concurrent_iterations\" : 2,   \n    \"experiment_timeout_hours\" : 0.25,   \n    \"verbosity\" : logging.INFO,  \n} \n# Put together an AutoML job constructor  \nautoml_config = AutoMLConfig(  \n    task= \"classification\" ,", "3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\nMachine Learning in many different ways .\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\niterations over different models, hyperparameter settings, advanced\npreprocessing/featurization, and what metrics to look at when determining the\nbest model.\n5. Submit the training job.\n6. Review the r esults\nThe following diagram illustrates this process. \nYou can also inspect the logged job information, which contains metrics  gathered\nduring the job. The training job produces a Python serialized object ( .pkl file) that\ncontains the model and data preprocessing.\nWhile model building is automated, you can also learn how important or relevant\nfeatures are  to the generated models.\nApply automated ML when you want Azure Machine Learning to train and tune a model\nfor you using the target metric you specify. Automated ML democratizes the machine\nlearning model development process, and empowers its users, no matter their data\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\nML professionals and developers across industries can use automated ML to:\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\nforecasting, computer vision & NLP", "iii. Select Create to create your compute target.\nThis tak es a couple minut es to complet e.\niv. After creation, select your new compute target from the drop-down list.\ne. Select Next .\n3. On the Select task and settings  form, complete the setup for your automated ML\nexperiment by specifying the machine learning task type and configuration\nsettings.\na. Select Classification  as the machine learning task type.\nb. Select View additional configuration settings  and populate the fields as\nfollows. These settings are to better control the training job. Otherwise, defaults\nare applied based on experiment selection and data.\nAdditional\u00a0configurations Descr iption Value\u00a0for\u00a0tut orial\nPrimary metric Evaluation metric that the\nmachine learning algorithm\nwill be measured by.AUC_weighted\nExplain best model Automatically shows\nexplainability on the bestEnable", "An Azure subscription. (If you don't have an Azure subscription, create a free\naccount  before you begin)\nAn Azure Machine Learning experiment created with either:\nThe Azure Machine Learning studio  (no code required)\nThe Azure Machine Learning Python SDK\nAfter your automated ML experiment completes, a history of the jobs can be found via:\nA browser with Azure Machine Learning studio\nA Jupyter notebook using the JobDetails Jupyter widget\nThe following steps and video, show you how to view the run history and model\nevaluation metrics and charts in the studio:\n1. Sign into the studio  and navigate to your workspace.\n2. In the left menu, select Jobs.\n3. Select your experiment from the list of experiments.\n4. In the table at the bottom of the page, select an automated ML job.\n5. In the Models  tab, select the Algorithm name  for the model you want to evaluate.\n6. In the Metrics  tab, use the checkboxes on the left to view metrics and charts.\nAutomated ML calculates performance metrics for each classification model generated\nfor your experiment. These metrics are based on the scikit learn implementation.\nMany classification metrics are defined for binary classification on two classes, and\nrequire averaging over classes to produce one score for multi-class classification. Scikit-\nlearn provides several averaging methods, three of which automated ML exposes:\nmacr o, micro, and weight ed.\nMacr o - Calculate the metric for each class and take the unweighted average\nMicro - Calculate the metric globally by counting the total true positives, false\nnegatives, and false positives (independent of classes).\nWeight ed - Calculate the metric for each class and take the weighted average\nbased on the number of samples per class.\nWhile each averaging method has its benefits, one common consideration when\nselecting the appropriate method is class imbalance. If classes have different numbers of\nsamples, it might be more informative to use a macro average where minority classes\nView job results\nClassification metrics"]}
{"question": "Describe the steps to retrieve the best model from an AutoML run using the AML SDK v2.", "variant_id": "v1", "line_number": 7, "answer": "To retrieve the best model from an AutoML run using the Azure Machine Learning SDK v2, you would typically follow these steps:\n\n1. **Initialize Workspace**: Import the necessary packages and classes, and create an instance of the `Workspace` class to connect to your Azure Machine Learning workspace.\n\n    ```python\n    from azure.ai.ml import MLClient\n    from azure.identity import DefaultAzureCredential\n    from azure.ai.ml.entities import Workspace\n\n    # Load the workspace\n    ws = Workspace.from_config()\n    ml_client = MLClient(DefaultAzureCredential(), ws)\n    ```\n\n2. **Retrieve AutoML Run**: Access the specific AutoML run by its ID. You can find the run ID in the Azure Machine Learning studio or through the SDK.\n\n    ```python\n    run_id = 'your-automl-run-id'\n    automl_run = ml_client.jobs.get(name=run_id)\n    ```\n\n3. **Get Best Model**: Once you have the AutoML run object, you can retrieve the best model from the run. The best model is typically the one with the best primary metric determined during the AutoML run.\n\n    ```python\n    best_child_run = automl_run.get_best_child()\n    best_model = best_child_run.model\n    ```\n\n4. **Download Model**: If you want to download the model to your local environment, you can use the `download` method.\n\n    ```python\n    best_model.download(target_dir='path/to/download/model', exist_ok=True)\n    ```\n\n5. **Register Model**: Optionally, you can register the model to your workspace for future reference and deployment.\n\n    ```python\n    from azure.ai.ml.entities import Model\n\n    model = Model(\n        path=\"path/to/download/model\",\n        name=\"best-automl-model\",\n        type=\"mlflow_model\",\n        description=\"Best model from AutoML run\"\n    )\n\n    registered_model = ml_client.models.create_or_update(model)\n    ```\n\nPlease note that the code snippets provided are for illustrative purposes and may require adjustments to fit the specifics of your environment and the version of the SDK you are using. Always refer to the latest Azure Machine Learning SDK v2 documentation for the most accurate and up-to-date information.", "context": ["Learn more about how and where to deploy a model .\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps", "Navigate to the Models  tab to see the algorithms (models) tested. By default, the\nmodels are ordered by metric score as they complete. For this tutorial, the model that\nscores the highest based on the chosen Normalized r oot mean squar ed err or metric is\nat the top of the list.\nWhile you wait for all of the experiment models to finish, select the Algorithm name  of\na completed model to explore its performance details.\nThe following example navigates to select a model from the list of models that the job\ncreated. Then, you select the Overview  and the Metrics  tabs to view the selected\nmodel's properties, metrics and performance charts.\nAutomated machine learning in Azure Machine Learning studio allows you to deploy the\nbest model as a web service in a few steps. Deployment is the integration of the model\nso it can predict on new data and identify potential areas of opportunity.\nFor this experiment, deployment to a web service means that the bike share company\nnow has an iterative and scalable web solution for forecasting bike share rental demand.\nOnce the job is complete, navigate back to parent job page by selecting Job 1  at the top\nof your screen.\nIn the Best model summar y section, the best model in the context of this experiment, is\nselected based on the Normalized r oot mean squar ed err or metric.\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\nThe deployment process entails several steps including registering the model,Deploy the model", "To run a fine-tuned model job from Machine Learning, in order to deploy an Azure\nOpenAI model:\n1. After you've finished fine-tuning an Azure OpenAI model, find the registered\nmodel in the Models  list with the name provided during fine-tuning and select the\nmodel you want to deploy.\n2. Select Deploy  and name the deployment. The model is deployed to the default\nAzure OpenAI resource linked to your workspace.\nTo enable users to quickly get started with code-based fine-tuning, we've published\nsamples (both Python notebooks and Azure CLI examples) to the azureml-ex amples\nGitHub repo:\nSDK example\nCLI exampleDeploy fine-tuned models\nFine-tuning by using code-based samples", "Option 2: T o deploy a specific model iteration from this experiment.\na. Select the desired model from the Models  tab\nb. Select Deploy  on the top left of the window.\n2. Populate the Deploy model  pane.\nField Value\nName Enter a unique name for your deployment.\nDescription Enter a description to better identify what this deployment is for.\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\nService (AKS)  or Azure Container Inst ance (ACI).\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\ndeploy to.\nEnable\nauthenticationSelect to allow for token-based or key-based authentication.\nUse custom\ndeployment assetsEnable this feature if you want to upload your own scoring script and\nenvironment file. Otherwise, automated ML provides these assets for\nyou by default. Learn more about scoring scripts .\nThe Advanced menu offers default deployment features such as data collection  and\nresource utilization settings. If you wish to override these defaults do so in this\nmenu.\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\ndeployment begins, the Model summar y tab appears. See the deployment\nprogress under the Deploy status  section.\nNow you have an operational web service to generate predictions! Y ou can test the\npredictions by querying the service from Power BI's built in Azure Machine Learning\nsupport .\uff89Expand table\n\uff09 Impor tant\nFile names must be under 32 characters and must begin and end with\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\nbetween. Spaces are not allowed.", "The automated machine learning interface allows you to deploy the best model as a\nweb service in a few steps. Deployment is the integration of the model so it can predict\non new data and identify potential areas of opportunity.\nFor this experiment, deployment to a web service means that the financial institution\nnow has an iterative and scalable web solution for identifying potential fixed term\ndeposit customers.\nCheck to see if your experiment run is complete. T o do so, navigate back to the parent\njob page by selecting Job 1  at the top of your screen. A Complet ed status is shown on\nthe top left of the screen.\nOnce the experiment run is complete, the Details  page is populated with a Best model\nsummar y section. In this experiment context, VotingEnsemble  is considered the best\nmodel, based on the AUC_w eight ed metric.\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\nThe deployment process entails several steps including registering the model,\ngenerating resources, and configuring them for the web service.\n1. Select VotingEnsemble  to open the model-specific page.\n2. Select the Deploy  menu in the top-left and select Deploy t o web ser vice.\n3. Populate the Deploy a model  pane as follows:\nField Value\nDeployment name my-automl-deploy\nDeployment\ndescriptionMy first automated machine learning experiment deployment\nCompute type Select Azure Container Instance (A CI)\nEnable\nauthenticationDisable.\nUse custom\ndeploymentsDisable. Allows for the default driver file (scoring script) and\nenvironment file to be auto-generated.\nFor this example, we use the defaults provided in the Advanced menu.\n4. Select Deploy .Deploy the best model"]}
{"question": "What are the AutoML capabilities for handling imbalanced data in Azure ML?", "variant_id": "v1", "line_number": 8, "answer": "Azure Machine Learning's Automated ML (AutoML) includes several capabilities to handle imbalanced data, which is a common issue in machine learning where one class is significantly more frequent in the data than others. Here are some of the ways AutoML addresses this challenge:\n\n1. **Data Preprocessing**: AutoML automatically preprocesses the data before training models. This can include techniques such as scaling and normalization, which are important when dealing with imbalanced datasets.\n\n2. **Cost-Sensitive Learning**: AutoML can apply cost-sensitive learning, which assigns a higher cost to misclassifying the minority class. This can help in guiding the learning algorithm to pay more attention to the minority class.\n\n3. **Weight Balancing**: AutoML can automatically balance the dataset by adjusting the weights of the classes. This is often done during the training process where the algorithm assigns higher weights to the minority class to make it more significant during the learning process.\n\n4. **Sampling Techniques**: Automated ML can implement different sampling techniques such as oversampling the minority class or undersampling the majority class to create a more balanced dataset.\n\n5. **Advanced Algorithms**: AutoML includes advanced algorithms that are inherently better at handling imbalanced data, such as decision trees and ensemble methods like Random Forest and Gradient Boosting, which can be more robust to class imbalance.\n\n6. **Evaluation Metrics**: AutoML provides evaluation metrics that are suitable for imbalanced datasets, such as the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), precision, recall, and F1 score, which give a better indication of model performance on imbalanced data than accuracy alone.\n\n7. **Model Tuning**: AutoML allows for hyperparameter tuning that can include parameters specifically designed to address class imbalance. For example, in algorithms like XGBoost, you can tune the `scale_pos_weight` parameter to give more importance to the minority class.\n\n8. **Early Stopping**: To prevent overfitting, which can be exacerbated by imbalanced data, AutoML supports early stopping mechanisms during model training.\n\nThese capabilities are part of the AutoML's suite of tools to help ensure that the models it produces are robust and perform well, even when faced with the challenge of imbalanced datasets.", "context": ["Prevent overfitting and imbalanced data\nwith Automated ML\nArticle \u202206/16/2023\nOverfitting and imbalanced data are common pitfalls when you build machine learning\nmodels. By default, Azure Machine Learning's Automated ML provides charts and\nmetrics to help you identify these risks, and implements best practices to help mitigate\nthem.\nOverfitting in machine learning occurs when a model fits the training data too well, and\nas a result can't accurately predict on unseen test data. In other words, the model has\nmemorized specific patterns and noise in the training data, but is not flexible enough to\nmake predictions on real data.\nConsider the following trained models and their corresponding train and test accuracies.\nModel Train accuracy Test accuracy\nA 99.9% 95%\nB 87% 87%\nC 99.9% 45%\nConsider model A, there is a common misconception that if test accuracy on unseen\ndata is lower than training accuracy, the model is overfitted. However, test accuracy\nshould always be less than training accuracy, and the distinction for overfit vs.\nappropriately fit comes down to how much  less accurate.\nCompare models A and B, model A is a better model because it has higher test\naccuracy, and although the test accuracy is slightly lower at 95%, it is not a significant\ndifference that suggests overfitting is present. Y ou wouldn't choose model B because\nthe train and test accuracies are closer together.\nModel C represents a clear case of overfitting; the training accuracy is high but the test\naccuracy isn't anywhere near as high. This distinction is subjective, but comes from\nknowledge of your problem and data, and what magnitudes of error are acceptable.Identify overfitting\nPrevent overfitting", "If it ascertains a better performance through this experiment, then this remedy is\napplied.\nUse a performance metric that deals better with imbalanced data. For example, the\nAUC_weighted is a primary metric that calculates the contribution of every class\nbased on the relative number of samples representing that class, hence is more\nrobust against imbalance.\nThe following techniques are additional options to handle imbalanced data outside of\nAutomated ML.\nResampling to even the class imbalance, either by up-sampling the smaller classes\nor down-sampling the larger classes. These methods require expertise to process\nand analyze.\nReview performance metrics for imbalanced data. For example, the F1 score is the\nharmonic mean of precision and recall. Precision measures a classifier's exactness,\nwhere higher precision indicates fewer false positives, while recall measures a\nclassifier's completeness, where higher recall indicates fewer false negatives.\nSee examples and learn how to build models using Automated ML:\nFollow the Tutorial: T rain an object detection model with automated machine\nlearning and Python .\nConfigure the settings for automatic training experiment:\nIn Azure Machine Learning studio, use these steps .\nWith the Python SDK, use these steps .Next steps", "Imbalanced data is commonly found in data for machine learning classification\nscenarios, and refers to data that contains a disproportionate ratio of observations in\neach class. This imbalance can lead to a falsely perceived positive effect of a model's\naccuracy, because the input data has bias towards one class, which results in the trained\nmodel to mimic that bias.\nIn addition, Automated ML jobs generate the following charts automatically. These\ncharts help you understand the correctness of the classifications of your model, and\nidentify models potentially impacted by imbalanced data.\nChar t Descr iption\nConfusion\nMatrixEvaluates the correctly classified labels against the actual labels of the data.\nPrecision-recall Evaluates the ratio of correct labels against the ratio of found label instances of\nthe data\nROC Curves Evaluates the ratio of correct labels against the ratio of false-positive labels.\nAs part of its goal of simplifying the machine learning workflow, Automated ML has built\nin capabilities to help deal with imbalanced data such as,\nA weight column: Automated ML creates a column of weights as input to cause\nrows in the data to be weighted up or down, which can be used to make a class\nmore or less \"important.\"\nThe algorithms used by Automated ML detect imbalance when the number of\nsamples in the minority class is equal to or fewer than 20% of the number of\nsamples in the majority class, where minority class refers to the one with fewest\nsamples and majority class refers to the one with most samples. Subsequently,\nautomated machine learning will run an experiment with subsampled data to\ncheck if using class weights would remedy this problem and improve performance.Cross-validation isn't enabled by default; it must be configured in Automated\nmachine learning settings. However, after cross-validation is configured and a\nvalidation data set has been provided, the process is automated for you.\nIdentify models with imbalanced data\nHandle imbalanced data", "What is automated machine learning\n(AutoML)?\nArticle \u202204/13/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAutomated machine learning, also referred to as automated ML or AutoML, is the\nprocess of automating the time-consuming, iterative tasks of machine learning model\ndevelopment. It allows data scientists, analysts, and developers to build ML models with\nhigh scale, efficiency, and productivity all while sustaining model quality. Automated ML\nin Azure Machine Learning is based on a breakthrough from our Microsoft R esearch\ndivision .\nFor code-experienced customers, Azure Machine Learning Python SDK . Get\nstarted with Tutorial: T rain an object detection model (preview) with AutoML and\nPython .\nDuring training, Azure Machine Learning creates a number of pipelines in parallel that\ntry different algorithms and parameters for you. The service iterates through ML\nalgorithms paired with feature selections, where each iteration produces a model with a\ntraining score. The better the score for the metric you want to optimize for, the better\nthe model is considered to \"fit\" your data. It will stop once it hits the exit criteria defined\nin the experiment.\nUsing Azur e Machine Learning , you can design and run your automated ML training\nexperiments with these steps:\n1. Identif y the ML pr oblem  to be solved: classification, forecasting, regression,\ncomputer vision or NLP.\n2. Choose whether y ou want a code-fir st experience or a no-code studio w eb\nexperience : Users who prefer a code-first experience can use the Azure Machine\nLearning SDK v2 or the Azure Machine Learning CLIv2 . Get started with Tutorial:\nTrain an object detection model with AutoML and Python . Users who prefer a\nlimited/no-code experience can use the web interface  in Azure Machine Learning\nstudio at https://ml.azure.com . Get started with Tutorial: Create a classification\nmodel with automated ML in Azure Machine Learning .\nHow does AutoML work?", "Automated ML in Azure Machine Learning supports model training for computer vision\ntasks like image classification and object detection. T o debug AutoML vision models and\nexplain model predictions, AutoML models for computer vision are integrated with\nResponsible AI dashboard. T o generate R esponsible AI insights for AutoML computer\nvision models, register your best AutoML model in the Azure Machine Learning\nworkspace and run it through the R esponsible AI vision insights pipeline. T o learn, see\nhow to set up AutoML to train computer vision models .\nNotebooks related to the AutoML supported computer vision tasks can be found in\nazureml-examples  repository.\nThe R esponsible AI vision Insights pipeline could be submitted through one of the\nfollowing methods\nPython SDK: T o learn how to submit the pipeline through Python, see the AutoML\nImage Classification scenario with RAI Dashboard sample notebook . For\nconstructing the pipeline, refer to section 5.1 in the notebook.\nAzure CLI: T o submit the pipeline via Azure-CLI, see the component Y AML in\nsection 5.2 of the example notebook linked above.\nUI (via Azure Machine Learning studio): From the Designer in Azure Machine\nLearning studio, the RAI-vision insights component can be used to create and\nsubmit a pipeline.\nIn addition to the list of R esponsible AI vision insights parameters provided in the\nprevious section, the following are parameters to set specifically for AutoML models.Integration with AutoML Image\nMode of submitting the Responsible AI vision insights\npipeline\nResponsible AI vision insights component parameter\n(AutoML specific)\n\uff17 Note\nA few parameters are specific to the XAI algorithm chosen and are optional for\nother algorithms."]}
{"question": "Explain how to use AutoML for forecasting time series data. What specific settings should be adjusted for time series problems?", "variant_id": "v1", "line_number": 9, "answer": "To use Azure AutoML for forecasting time series data, you should follow these general steps:\n\n1. **Set Up Your Environment**:\n   - Install the Azure Machine Learning Python SDK v2.\n   - Create an Azure Machine Learning workspace if you don't already have one.\n   - Set up your development environment, which can be a local machine or a cloud-based compute instance.\n\n2. **Prepare Your Data**:\n   - Ensure your data is in a format suitable for time series forecasting. This typically means having a time column, one or more feature columns, and a target column that you want to forecast.\n   - Split your data into training and testing datasets.\n\n3. **Configure Your AutoML Experiment**:\n   - Define the settings and configuration for your AutoML run. For time series forecasting, specific settings you should consider adjusting include:\n     - `time_column_name`: The name of the column that holds the time series information.\n     - `grain_column_names`: The columns that uniquely identify a time series when you have multiple series in your dataset.\n     - `max_horizon`: The number of periods into the future you want to forecast.\n     - `target_lags`: The number of past periods to use to predict future values.\n     - `target_rolling_window_size`: The window size for creating rolling features.\n     - `forecasting_parameters`: Additional parameters specific to forecasting tasks, such as seasonality, holiday features, etc.\n   - Choose a primary metric that is appropriate for evaluating time series forecasting models.\n\n4. **Run the AutoML Experiment**:\n   - Submit the AutoML run with your dataset and configuration. Azure AutoML will automatically preprocess your data, perform feature engineering, select algorithms, and tune hyperparameters.\n\n5. **Evaluate and Deploy the Model**:\n   - Once the run is complete, evaluate the performance of the generated models using the primary metric you selected.\n   - Choose the best model and deploy it to an Azure Container Instance (ACI), Azure Kubernetes Service (AKS), or any other supported deployment target.\n\nFor handling multiple time series efficiently, you can use the Many Models feature of AutoML, which scales by distributing training jobs across a compute cluster. This is particularly useful when your dataset contains numerous time series.\n\nRemember to consult the Azure Machine Learning documentation for the most up-to-date information and detailed instructions on setting up and running your AutoML forecasting experiments.", "context": ["Frequently asked questions about\nforecasting in AutoML\nArticle \u202208/01/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nThis article answers common questions about forecasting in automatic machine learning\n(AutoML). For general information about forecasting methodology in AutoML, see the\nOverview of forecasting methods in AutoML  article.\nYou can start by reading the Set up AutoML to train a time-series forecasting model\narticle. Y ou can also find hands-on examples in several Jupyter notebooks:\nBike share example\nForecasting using deep learning\nMany Models solution\nForecasting recipes\nAdvanced forecasting scenarios\nWe're always working to make AutoML faster and more scalable. T o work as a general\nforecasting platform, AutoML does extensive data validations and complex feature\nengineering, and it searches over a large model space. This complexity can require a lot\nof time, depending on the data and the configuration.\nOne common source of slow runtime is training AutoML with default settings on data\nthat contains numerous time series. The cost of many forecasting methods scales with\nthe number of series. For example, methods like Exponential Smoothing and Prophet\ntrain a model for each time series  in the training data.\nThe Many Models feature of AutoML scales to these scenarios by distributing training\njobs across a compute cluster. It has been successfully applied to data with millions of\ntime series. For more information, see the many models  article section. Y ou can also\nread about the success of Many Models  on a high-profile competition dataset.\nHow do I start building forecasting models in\nAutoML?\nWhy is AutoML slow on my data?", "Learn more about how and where to deploy a model .\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps", "If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about calendar features for time series forecasting in AutoML .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about AutoML forecasting for lagged features .Next steps", "Set up AutoML to train a time-series\nforecasting model with SDK and CLI\nArticle \u202208/02/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn this article, you'll learn how to set up AutoML for time-series forecasting with Azure\nMachine Learning automated ML in the Azure Machine Learning Python SDK .\nTo do so, you:\nFor a low code experience, see the Tutorial: Forecast demand with automated machine\nlearning  for a time-series forecasting example using automated ML in the Azure\nMachine Learning studio .\nAutoML uses standard machine learning models along with well-known time series\nmodels to create forecasts. Our approach incorporates historical information about the\ntarget variable, user-provided features in the input data, and automatically engineered\nfeatures. Model search algorithms then work to find a model with the best predictive\naccuracy. For more details, see our articles on forecasting methodology  and model\nsearch .\nFor this article you need,\nAn Azure Machine Learning workspace. T o create the workspace, see Create\nworkspace resources .\nThe ability to launch AutoML training jobs. Follow the how-to guide for setting up\nAutoML  for details.\nPrepare data for training.\uff02\nConfigure specific time-series parameters in a Forecasting Job . \uff02\nOrchestrate training, inference, and model evaluation using components and\npipelines.\uff02\nPrerequisites\nTraining  and validation data", "See the Why is AutoML slow on my data?  answer to understand why AutoML might be\nslow in your case.\nConsider the following configuration changes that might speed up your job:\nBlock time series models  like ARIMA and Prophet.\nTurn off look-back features like lags and rolling windows.\nReduce:\nThe number of trials/iterations.\nTrial/iteration timeout.\nExperiment timeout.\nThe number of cross-validation folds.\nEnsure that early termination is enabled.\nAutoML forecasting supports four basic configurations:\nConfiguration Scenar io Pros Cons\nDefault\nAutoMLRecommended if the dataset\nhas a small number of time\nseries that have roughly\nsimilar historical behavior.- Simple to configure\nfrom code/SDK or\nAzure Machine\nLearning studio.\n- AutoML can learn\nacross different time\nseries because the\nregression models\npool all series together\nin training. For more\ninformation, see\nModel grouping .- Regression models\nmight be less\naccurate if the time\nseries in the training\ndata have divergent\nbehavior.\n- Time series models\nmight take a long\ntime to train if the\ntraining data has a\nlarge number of\nseries. For more\ninformation, see the\nWhy is AutoML slow\non my data?  answer.\nAutoML with\ndeep learningRecommended for datasets\nwith more than 1,000\nobservations and, potentially,\nnumerous time series that\nexhibit complex patterns.\nWhen it's enabled, AutoML\nwill sweep over temporal- Simple to configure\nfrom code/SDK or\nAzure Machine\nLearning studio.\n- Cross-learning\nopportunities, because- Training can take\nmuch longer\nbecause of the\ncomplexity of DNN\nmodels.\n- Series with smallHow can I make AutoML faster?\nWhat modeling configuration should I use?"]}
{"question": "How do you create a custom deep learning environment with specific Python packages using the AML SDK v2?", "variant_id": "v1", "line_number": 10, "answer": "To create a custom deep learning environment with specific Python packages using the Azure Machine Learning SDK v2, you can follow these steps:\n\n1. **Define the Environment**: Use the `Environment` class from the `azure.ai.ml.entities` namespace to define your custom environment. You will need to specify the name, description, the path to your Conda environment file (which lists the Python packages and versions you want to include), and optionally, a base Docker image.\n\n2. **Create a Conda Environment File**: Write a `conda.yaml` file that lists all the Python packages and versions you need for your deep learning environment. This file will be used to create the environment.\n\n3. **Register the Environment**: Use the `create_or_update` method of the `ml_client.environments` to register the environment with your Azure Machine Learning workspace.\n\nHere's an example of how you might do this in Python using the Azure Machine Learning SDK v2:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import Environment\nfrom azure.identity import DefaultAzureCredential\nimport os\n\n# Set up the MLClient with your Azure subscription and workspace information\nsubscription_id = 'your-subscription-id'\nresource_group = 'your-resource-group'\nworkspace_name = 'your-workspace-name'\n\n# Use DefaultAzureCredential or any other suitable credential class\ncredential = DefaultAzureCredential()\n\n# Create an instance of the MLClient\nml_client = MLClient(credential, subscription_id, resource_group, workspace_name)\n\n# Define the path to your dependencies directory where the conda.yaml file is located\ndependencies_dir = 'path_to_your_dependencies_directory'\n\n# Define the custom environment\ncustom_env_name = \"deep-learning-env\"\njob_env = Environment(\n    name=custom_env_name,\n    description=\"Custom environment for deep learning\",\n    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n)\n\n# Register the environment with your workspace\njob_env = ml_client.environments.create_or_update(job_env)\n\n# Print out the environment details\nprint(\n    f\"Environment with name {job_env.name} is registered to workspace, \"\n    f\"the environment version is {job_env.version}\"\n)\n```\n\nMake sure to replace `'your-subscription-id'`, `'your-resource-group'`, `'your-workspace-name'`, and `'path_to_your_dependencies_directory'` with your actual Azure subscription ID, resource group name, workspace name, and the path to your dependencies directory, respectively.\n\nThe `conda.yaml` file might look something like this:\n\n```yaml\nname: deep-learning-env\ndependencies:\n  - python=3.8\n  - numpy\n  - pandas\n  - scikit-learn\n  - tensorflow\n  - keras\n  - pip:\n    - some-pip-package\n```\n\nThis file specifies a Python 3.8 environment with packages like NumPy, pandas, scikit-learn, TensorFlow, and Keras, as well as any pip packages you might need.", "context": ["The specification contains some usual packages (such as numpy and pip) that you'll use\nin your job.\nNext, use the Y AML file to create and register this custom environment in your\nworkspace. The environment will be packaged into a Docker container at runtime.\nPython\nFor more information on creating and using environments, see Create and use software\nenvironments in Azure Machine Learning .\nIn this section, we'll begin by introducing the data for training. W e'll then cover how to\nrun a training job, using a training script that we've provided. Y ou'll learn to build the\ntraining job by configuring the command for running the training script. Then, you'll\nsubmit the training job to run in Azure Machine Learning.\nYou'll use data from the Modified National Institute of S tandards and T echnology\n(MNIST) database of handwritten digits. This data is sourced from Y an LeCun's website\nand stored in an Azure storage account.\nPythonfrom azure.ai.ml.entities import Environment  \ncustom_env_name = \"keras-env\"  \njob_env = Environment(  \n    name=custom_env_name,  \n    description= \"Custom environment for keras image classification\" , \n    conda_file=os.path.join(dependencies_dir, \"conda.yaml\" ), \n    image= \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\" , \n) \njob_env = ml_client.environments.create_or_update(job_env)  \nprint( \n    f\"Environment with name {job_env.name}  is registered to workspace, the  \nenvironment version is {job_env.version} \" \n)\nConfigure and submit your training  job\nObtain the training data\nweb_path = \"wasbs://datasets@azuremlexamples.blob.core.windows.net/mnist/\"", "In the samples deep learning folder on the notebook server, find a\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\ndeploy-with-k eras.\nYour Jupyter notebook server\nInstall the Azure Machine Learning SDK (v2) .\nDownload the training scripts keras_mnist.py  and utils.py .\nYou can also find a completed Jupyter Notebook version  of this guide on the GitHub\nsamples page.\nBefore you can run the code in this article to create a GPU cluster, you'll need to request\na quota increase  for your workspace.\nThis section sets up the job for training by loading the required Python packages,\nconnecting to a workspace, creating a compute resource to run a command job, and\ncreating an environment to run the job.\nFirst, you'll need to connect to your Azure Machine Learning workspace. The Azure\nMachine Learning workspace  is the top-level resource for the service. It provides you\nwith a centralized place to work with all the artifacts you create when you use Azure\nMachine Learning.\nWe're using DefaultAzureCredential to get access to the workspace. This credential\nshould be capable of handling most Azure SDK authentication scenarios.\nIf DefaultAzureCredential doesn't work for you, see azure-identity reference\ndocumentation  or Set up authentication  for more available credentials.\nPython\nSet up the job\nConnect to the workspace\n# Handle to the workspace  \nfrom azure.ai.ml import MLClient  \n# Authentication package  \nfrom azure.identity import DefaultAzureCredential  \ncredential = DefaultAzureCredential()", "Azure Machine Learning allows you to either use a curated (or ready-made)\nenvironment or create a custom environment using a Docker image or a Conda\nconfiguration. In this article, you reuse the curated Azure Machine Learning environment\nAzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu. Use the latest version of this\nenvironment using the @latest directive.\nPython\nIn this section, we begin by introducing the data for training. W e then cover how to run\na training job, using a training script that we've provided. Y ou'll learn to build the\ntraining job by configuring the command for running the training script. Then, you'll\nsubmit the training job to run in Azure Machine Learning.\nYou can use the dataset in this zipped file . This dataset consists of about 120 training\nimages each for two classes (turkeys and chickens), with 100 validation images for each\nclass. The images are a subset of the Open Images v5 Dataset . The training script\npytorch_tr ain.p y downloads and extracts the dataset.\nIn the prerequisites section, we provided the training script pytorch_tr ain.p y. In practice,\nyou should be able to take any custom training script as is and run it with Azure\nMachine Learning without having to modify your code.\nThe provided training script downloads the data, trains a model, and registers the\nmodel.\nNow that you have all the assets required to run your job, it's time to build it using the\nAzure Machine Learning Python SDK v2. For this example, we create a command.\nAn Azure Machine Learning command is a resource that specifies all the details needed to\nexecute your training code in the cloud. These details include the inputs and outputs,curated_env_name = \"AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu@latest\"\nConfigure and submit your training  job\nObtain the training data\nPrepare the training script\nBuild the training job", "2. Create an Azure Machine Learning workspace .\n3. Clone the AzureML-Examples repository .\nBash\n4. Start the notebook server from the directory containing your clone.\nBash\nThese instructions install the base SDK packages necessary for the quickstart and tutorial\nnotebooks. Other sample notebooks might require you to install extra components. For\nmore information, see Install the Azure Machine Learning SDK for Python .\nThe Data Science Virtual Machine (DSVM) is a customized VM image built specifically for\ndoing data science. If you create a DSVM , the SDK and notebook server are installed and\nconfigured for you. However, you'll still need to create a workspace and clone the\nsample repository.\n1. Create an Azure Machine Learning workspace .\n2. Clone the the AzureML-Examples repository .\nBash\n3. Start the notebook server from the directory that contains the clone.\nBash\ngit clone https://github.com/Azure/azureml-examples.git --depth 1\njupyter notebook\nOption 3: Access on a DSVM\ngit clone https://github.com/Azure/azureml-examples.git --depth 1\njupyter notebook\nConnect to a workspace", "Manage Azu re Machine Learning\nenvironments with the CLI & SDK (v2)\nArticle \u202201/03/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAzure Machine Learning environments define the execution environments for your jobs\nor deployments and encapsulate the dependencies for your code. Azure Machine\nLearning uses the environment specification to create the Docker container that your\ntraining or scoring code runs in on the specified compute target. Y ou can define an\nenvironment from a conda specification, Docker image, or Docker build context.\nIn this article, learn how to create and manage Azure Machine Learning environments\nusing the SDK & CLI (v2).\nBefore following the steps in this article, make sure you have the following prerequisites:\nAn Azure Machine Learning workspace. If you don't have one, use the steps in the\nQuickstart: Create workspace resources  article to create one.\nThe Azure CLI and the ml extension or the Azure Machine Learning Python SDK v2:\nTo install the Azure CLI and extension, see Install, set up, and use the CLI (v2) .\nTo install the Python SDK v2, use the following command:\nBash\nPrerequisites\n\uff09 Impor tant\nThe CLI examples in this article assume that you are using the Bash (or\ncompatible) shell. For example, from a Linux system or Windows\nSubsyst em for Linux .\npip install azure-ai-ml azure-identity"]}
{"question": "Describe the process of using Azure ML to train a deep learning model on a GPU compute cluster.", "variant_id": "v1", "line_number": 11, "answer": "To train a deep learning model on a GPU compute cluster using Azure Machine Learning, you would follow these general steps:\n\n1. **Set Up Your Azure ML Workspace**:\n   - Connect to your Azure ML workspace using the `MLClient` from the Azure ML Python SDK. You'll need your Azure subscription ID, resource group, and workspace name.\n\n   ```python\n   from azure.ai.ml import MLClient\n   from azure.identity import DefaultAzureCredential\n\n   workspace = '<AZUREML_WORKSPACE_NAME>'\n   subscription_id = '<YOUR_SUBSCRIPTION_ID>'\n   resource_group = '<YOUR_RESOURCE_GROUP>'\n\n   ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)\n   ```\n\n2. **Create or Access a GPU Compute Cluster**:\n   - Check if a GPU compute cluster already exists or create a new one if it doesn't. For GPU-based clusters, you would specify a GPU VM size (e.g., `STANDARD_NC6` for a VM with one NVIDIA Tesla K80 GPU).\n\n   ```python\n   from azure.ai.ml.entities import AmlCompute\n\n   gpu_compute_target = \"gpu-cluster\"\n   try:\n       ml_client.compute.get(gpu_compute_target)\n   except Exception:\n       print(\"Creating a new GPU compute target...\")\n       compute = AmlCompute(\n           name=gpu_compute_target,\n           size=\"STANDARD_NC6\",  # Example GPU VM size\n           min_instances=0,\n           max_instances=4\n       )\n       ml_client.compute.begin_create_or_update(compute).result()\n   ```\n\n3. **Prepare Your Training Script**:\n   - Write a Python script (e.g., `train.py`) that defines your deep learning model, the training process, and any other necessary code (data loading, preprocessing, etc.).\n\n4. **Create an Experiment and Submit the Training Job**:\n   - Define the training job configuration, including the source directory containing your training script, the compute target, and any other parameters or environment settings required for the training.\n\n   ```python\n   from azure.ai.ml import command, Input\n\n   # Define the command that will run your training script\n   job = command(\n       code=\"./path_to_your_code_directory\",\n       command=\"python train.py\",\n       compute=gpu_compute_target,\n       environment=\"<YOUR_TRAINING_ENVIRONMENT>\",  # Specify the environment\n       inputs={\"<INPUT_NAME>\": Input(type=\"uri_folder\", path=\"<YOUR_DATA_PATH>\")},  # Specify inputs if needed\n       outputs={\"<OUTPUT_NAME>\": \"./outputs\"},  # Specify outputs if needed\n       distribution={\"type\": \"pytorch\", \"process_count_per_instance\": 4}  # Example for PyTorch distributed training\n   )\n\n   # Submit the job to the Azure ML workspace\n   run = ml_client.jobs.create_or_update(job)\n   ```\n\n5. **Monitor the Training Job**:\n   - Use Azure ML Studio or the SDK to monitor the progress of your training job, view logs, and track metrics.\n\n6. **Retrieve the Trained Model**:\n   - Once the training job is complete, you can download the trained model from the outputs or register it in your Azure ML workspace for later use or deployment.\n\nPlease note that the actual code and parameters will depend on the specifics of your deep learning model and training requirements. The provided code snippets are for illustrative purposes and would need to be adapted to your particular scenario.", "context": ["An Azure Machine Learning compute cluster is a fully managed compute resource that\ncan be used to run the training job. In the following examples, a compute cluster named\ncpu-compute is created.\nPython\nTo run this script, you'll use a command that executes main.py Python script located\nunder ./sdk/python/jobs/single-step/lightgbm/iris/src/. The command will be runworkspace = '<AZUREML_WORKSPACE_NAME>'\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \nresource_group, workspace)\n2. Create a compute resource for training\n\uff17 Note\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\ntraining job .\nPython SDK\nfrom azure.ai.ml.entities import AmlCompute\n# specify aml compute name.\ncpu_compute_target = \"cpu-cluster\"\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(\"Creating a new cpu compute target...\" )\n    compute = AmlCompute(\n        name=cpu_compute_target, size= \"STANDARD_D2_V2\" , min_instances= 0, \nmax_instances= 4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n4. Submit the training job\nPython SDK", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Manage and optimize Azure Machine\nLearning costs\nArticle \u202208/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.\uff09 Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)", "For detailed instructions on how to complete these steps, see Secure a training\nenvironment .\nIn this section, you learn how Azure Machine Learning securely communicates between\nservices to submit a training job. This example shows you how all your configurations\nwork together to secure communication.\n1. The client uploads training scripts and training data to storage accounts that are\nsecured with a service or private endpoint.\n2. The client submits a training job to the Azure Machine Learning workspace\nthrough the private endpoint.\n3. Azure Batch service receives the job from the workspace. It then submits the\ntraining job to the compute environment through the public load balancer for the\ncompute resource.\n4. The compute resource receives the job and begins training. The compute resource\nuses information stored in key vault to access storage accounts to download\ntraining files and upload output.Example training job submission", "like Spark.\nIn the following example script, we provision a Linux compute cluster . You can see the\nAzure Machine Learning pricing  page for the full list of VM sizes and prices. Since we\nneed a GPU cluster for this example, let's pick a STAND ARD_NC6  model and create an\nAzure Machine Learning compute.\nPython\nfrom azure.ai.ml.entities import AmlCompute  \ngpu_compute_target = \"gpu-cluster\"  \ntry: \n    # let's see if the compute target already exists  \n    gpu_cluster = ml_client.compute.get(gpu_compute_target)  \n    print(  \n        f\"You already have a cluster named {gpu_compute_target} , we'll reuse  \nit as is.\"  \n    ) \nexcept Exception:  \n    print( \"Creating a new gpu compute target...\" ) \n    # Let's create the Azure ML compute object with the intended parameters  \n    gpu_cluster = AmlCompute(  \n        # Name assigned to the compute cluster  \n        name= \"gpu-cluster\" , \n        # Azure ML Compute is the on-demand VM service  \n        type= \"amlcompute\" , \n        # VM Family  \n        size= \"STANDARD_NC6\" , \n        # Minimum running nodes when there is no job running  \n        min_instances= 0, \n        # Nodes in cluster  \n        max_instances= 4, \n        # How many seconds will the node running after the job termination  \n        idle_time_before_scale_down= 180, \n        # Dedicated or LowPriority. The latter is cheaper but there is a  \nchance of job termination  \n        tier= \"Dedicated\" , \n    ) \n    # Now, we pass the object to MLClient's create_or_update method  \n    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()  \nprint( \n    f\"AMLCompute with name {gpu_cluster.name}  is created, the compute size  \nis {gpu_cluster.size} \" \n)"]}
{"question": "Explain how to deploy a TensorFlow model as a web service in Azure ML. What are the key considerations for deployment?", "variant_id": "v1", "line_number": 12, "answer": "To deploy a TensorFlow model as a web service in Azure Machine Learning, you can follow these general steps. Please note that the specific details may vary depending on the version of Azure ML and the specifics of your model.\n\n1. **Register the Model**: First, you need to register your trained TensorFlow model in your Azure ML workspace. This can be done by using the `Model.register()` method, providing the model name, description, model path, and workspace details.\n\n2. **Prepare an Inference Configuration**: Create an inference configuration that specifies the environment needed to host the model. This includes the scoring script (`score.py`) and an environment file (`myenv.yml`) that lists all the necessary Python packages.\n\n   - **Scoring Script**: The scoring script should contain two functions: `init()` to load the model and `run(raw_data)` to run the model on input data.\n   - **Environment**: Define an environment with all the dependencies required by your TensorFlow model. Azure ML provides a way to create a Conda environment that encapsulates these dependencies.\n\n3. **Create an Entry Script**: The entry script (often `score.py`) is the Python script that Azure ML will use to score incoming requests. It should include methods for loading your model and processing incoming data to make predictions.\n\n4. **Define a Deployment Configuration**: Depending on whether you want to deploy your model as an online endpoint (real-time inference) or a batch endpoint (batch inference), you will need to create a deployment configuration. For online endpoints, you can use `AciWebservice.deploy_configuration()` or `AksWebservice.deploy_configuration()` for deploying to Azure Container Instances or Azure Kubernetes Service, respectively.\n\n5. **Deploy the Model**: Use the `Model.deploy()` method to deploy your model to the chosen compute target, passing the model, inference configuration, and deployment configuration.\n\n6. **Test the Web Service**: Once deployed, test the web service endpoint by sending HTTP requests with input data and verifying that you receive the expected predictions.\n\nKey considerations for deployment include:\n\n- **Compute Target**: Choose between Azure Container Instances (ACI), Azure Kubernetes Service (AKS), or other supported compute targets based on your load requirements and scalability needs.\n- **Security**: Ensure that the endpoint is secure by enabling authentication and other security measures.\n- **Monitoring and Logging**: Set up monitoring and logging to track the health and usage of your web service.\n- **Cost**: Be aware of the cost implications of the compute resources you choose for hosting your model.\n- **Scalability**: Consider how the endpoint will scale with varying loads, and configure auto-scaling if necessary.\n- **Versioning**: Manage versions of your model to roll out updates or rollback to previous versions if needed.\n\nRemember to consult the Azure ML documentation for the most up-to-date instructions and best practices.", "context": ["What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps", "Tutorial Descr iption\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "To learn more, review these articles:\nDeploy models with REST\nCreate and use managed online endpoints in the studio\nSafe rollout for online endpoints\nHow to autoscale managed online endpoints\nView costs for an Azure Machine Learning managed online endpoint\nAccess Azure resources with a managed online endpoint and managed identity\nTroubleshoot managed online endpoints deployment", "Option 2: T o deploy a specific model iteration from this experiment.\na. Select the desired model from the Models  tab\nb. Select Deploy  on the top left of the window.\n2. Populate the Deploy model  pane.\nField Value\nName Enter a unique name for your deployment.\nDescription Enter a description to better identify what this deployment is for.\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\nService (AKS)  or Azure Container Inst ance (ACI).\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\ndeploy to.\nEnable\nauthenticationSelect to allow for token-based or key-based authentication.\nUse custom\ndeployment assetsEnable this feature if you want to upload your own scoring script and\nenvironment file. Otherwise, automated ML provides these assets for\nyou by default. Learn more about scoring scripts .\nThe Advanced menu offers default deployment features such as data collection  and\nresource utilization settings. If you wish to override these defaults do so in this\nmenu.\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\ndeployment begins, the Model summar y tab appears. See the deployment\nprogress under the Deploy status  section.\nNow you have an operational web service to generate predictions! Y ou can test the\npredictions by querying the service from Power BI's built in Azure Machine Learning\nsupport .\uff89Expand table\n\uff09 Impor tant\nFile names must be under 32 characters and must begin and end with\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\nbetween. Spaces are not allowed.", "In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\nmodel to an online endpoint. See these other articles to learn more about Azure\nMachine Learning.\nTrack run metrics during training\nTune hyperparameters\nReference architecture for distributed deep learning training in AzureNext steps"]}
{"question": "Provide an example of how to use the AML SDK v2 for distributed deep learning training. What configurations are necessary for multi-node training?", "variant_id": "v1", "line_number": 13, "answer": "To use Azure Machine Learning SDK v2 for distributed deep learning training, you would typically follow these steps and include the necessary configurations for multi-node training:\n\n1. **Set up your environment**:\n   - Install the Azure Machine Learning SDK v2 if you haven't already.\n   - Configure your workspace and compute resources.\n\n2. **Define the distributed settings**:\n   - Choose between data parallelism or model parallelism. For most cases, data parallelism is used.\n   - For data parallelism, you can use frameworks like PyTorch or TensorFlow with Horovod or their native distributed training support.\n\n3. **Configure the compute target**:\n   - Define a multi-node compute cluster in Azure Machine Learning. Ensure that each node has enough capacity to fit the model.\n\n4. **Prepare your training script**:\n   - Your training script should be set up to support distributed training. For example, in PyTorch, you would use `torch.distributed.launch` or `torch.nn.parallel.DistributedDataParallel`.\n\n5. **Create an experiment**:\n   - Define an experiment in Azure Machine Learning to track your runs.\n\n6. **Submit the training job**:\n   - Use the Azure Machine Learning SDK to submit the training job with the distributed configuration.\n\nHere is a simplified example of how you might configure and submit a distributed training job using the Azure Machine Learning SDK v2:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import CommandJob, Environment, DistributionConfiguration\nfrom azure.identity import DefaultAzureCredential\n\n# Set up the MLClient with your Azure subscription and resource details\nml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace_name)\n\n# Define the environment for your training job\nenvironment = Environment(\n    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04\",\n    conda_file=\"path_to_conda_file.yml\",\n    name=\"my-distributed-training-env\",\n)\n\n# Define the distribution configuration for data parallelism\n# Here we are using MPI as an example\ndistribution = DistributionConfiguration(\n    type=\"Mpi\",\n    process_count_per_instance=4  # Adjust based on your compute target and needs\n)\n\n# Define the command job for training with the necessary script and arguments\ncommand_job = CommandJob(\n    code=\"path_to_your_code\",\n    command=\"python train.py\",  # Your training script\n    environment=environment,\n    compute=\"name_of_your_compute_cluster\",\n    distribution=distribution,\n    resources=ResourceConfiguration(\n        instance_count=2  # Specify the number of nodes for multi-node training\n    ),\n)\n\n# Submit the job\nrun = ml_client.jobs.create_or_update(command_job)\n\n# Wait for the job to complete\nrun.wait_for_completion(show_output=True)\n```\n\nPlease note that this is a high-level example and the actual code may vary based on the specifics of your training script, the deep learning framework you are using, and the configuration of your Azure Machine Learning workspace and resources. Always refer to the latest Azure Machine Learning SDK v2 documentation for the most accurate and up-to-date information.", "context": ["Distributed training with Azure Machine\nLearning\nArticle \u202203/27/2023\nIn this article, you learn about distributed training and how Azure Machine Learning\nsupports it for deep learning models.\nIn distributed training the workload to train a model is split up and shared among\nmultiple mini processors, called worker nodes. These worker nodes work in parallel to\nspeed up model training. Distributed training can be used for traditional ML models, but\nis better suited for compute and time intensive tasks, like deep learning  for training\ndeep neural networks.\nThere are two main types of distributed training: data parallelism  and model parallelism .\nFor distributed training on deep learning models, the Azure Machine Learning SDK in\nPython  supports integrations with popular frameworks, PyT orch and T ensorFlow. Both\nframeworks employ data parallelism for distributed training, and can leverage\nhorovod  for optimizing compute speeds.\nDistributed training with PyT orch\nDistributed training with T ensorFlow\nFor ML models that don't require distributed training, see train models with Azure\nMachine Learning  for the different ways to train models using the Python SDK.\nData parallelism is the easiest to implement of the two distributed training approaches,\nand is sufficient for most use cases.\nIn this approach, the data is divided into partitions, where the number of partitions is\nequal to the total number of available nodes, in the compute cluster or serverless\ncompute . The model is copied in each of these worker nodes, and each worker operates\non its own subset of the data. K eep in mind that each node has to have the capacity to\nsupport the model that's being trained, that is the model has to entirely fit on each\nnode. The following diagram provides a visual demonstration of this approach.Deep learning and distributed training\nData parallelism", "Distributed GPU training guide (SDK v2)\nArticle \u202202/02/2024\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nLearn more about using distributed GPU training code in Azure Machine Learning. This\narticle helps you run your existing distributed training code, and offers tips and\nexamples for you to follow for each framework:\nMessage P assing Interface (MPI)\nHorovod\nEnvironment variables from Open MPI\nPyTorch\nTensorFlow\nAccelerate GPU training with InfiniBand\nReview the basic concepts of distributed GPU training , such as data parallelism ,\ndistributed dat a parallelism , and model p arallelism .\nAzure Machine Learning offers an MPI job  to launch a given number of processes in\neach node. Azure Machine Learning constructs the full MPI launch command ( mpirun)\nbehind the scenes. Y ou can't provide your own full head-node-launcher commands like\nmpirun or DeepSpeed launcher.\nPrerequisites\n\uea80 Tip\nIf you don't know which type of parallelism to use, more than 90% of the time you\nshould use distribut ed data p arallelism .\nMPI\n\uea80 Tip\nThe base Docker image used by an Azure Machine Learning MPI job needs to have\nan MPI library installed. Open MPI  is included in all the Azur e Machine Learning\nGPU b ase images . When you use a custom Docker image, you are responsible\nfor making sure the image includes an MPI library. Open MPI is recommended, but", "Distributed DNN training achieves scalability using a data partitioning algorithm that\nrespects time series boundaries. The following diagram illustrates a simple example with\ntwo partitions:\nDuring training, the DNN data loaders on each compute load just what they need to\ncomplete an iteration of back-propagation; the whole dataset is nev er read int o\nmemor y. The partitions are further distributed across multiple compute cores (usually\nGPUs) on possibly multiple nodes to accelerate training. Coordination across computes\nis provided by the Horovod  framework.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about deep learning models  for forecasting in AutoML\nNext steps", "For more information, see What is automated machine learning? .\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\nLearning can automate this task for arbitrary parameterized commands with little\nmodification to your job definition. R esults are visualized in the studio.\nFor more information, see Tune hyperparameters .\nEfficiency of training for deep learning and sometimes classical machine learning\ntraining jobs can be drastically improved via multinode distributed training. Azure\nMachine Learning compute clusters and serverless compute  offer the latest GPU\noptions.\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\nclusters, and serverless compute :\nPyTorch\nTensorFlow\nMPI\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\nAzure S ynapse Analytics Spark clusters.\nFor more information, see Distributed training with Azure Machine Learning .\nScaling an ML project might require scaling embarrassingly parallel model training. This\npattern is common for scenarios like forecasting demand, where a model might be\ntrained for many stores.\nTo bring a model into production, you deploy the model. The Azure Machine Learning\nmanaged endpoints abstract the required infrastructure for both batch or real-time\n(online) model scoring (inferencing).Hyperparameter optimization\nMultinode distributed training\nEmbarrassingly parallel training\nDeploy models", "Train TensorFlow models at scale with\nAzure Machine Lea rning\nArticle \u202204/04/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this article, learn how to run your TensorFlow  training scripts at scale using Azure\nMachine Learning Python SDK v2.\nThe example code in this article train a T ensorFlow model to classify handwritten digits,\nusing a deep neural network (DNN); register the model; and deploy it to an online\nendpoint.\nWhether you're developing a T ensorFlow model from the ground-up or you're bringing\nan existing model into the cloud, you can use Azure Machine Learning to scale out\nopen-source training jobs using elastic cloud compute resources. Y ou can build, deploy,\nversion, and monitor production-grade models with Azure Machine Learning.\nTo benefit from this article, you'll need to:\nAccess an Azure subscription. If you don't have one already, create a free\naccount .\nRun the code in this article using either an Azure Machine Learning compute\ninstance or your own Jupyter notebook.\nAzure Machine Learning compute instance\u2014no downloads or installation\nnecessary\nComplete the Create resources to get started  to create a dedicated notebook\nserver pre-loaded with the SDK and the sample repository.\nIn the samples deep learning folder on the notebook server, find a\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\ndeploy-with-t ensor flow.\nYour Jupyter notebook server\nInstall the Azure Machine Learning SDK (v2) .\nDownload the following files:\ntraining script tf_mnist.py\nscoring script score.py\nsample request file sample-request.json\nPrerequisites"]}
{"question": "How can Azure ML's HyperDrive be used to tune hyperparameters of a deep learning model? Give an example of specifying the search space.", "variant_id": "v1", "line_number": 14, "answer": "Azure ML's HyperDrive can be used to tune hyperparameters of a deep learning model by automating the process of selecting the best hyperparameters for your model. HyperDrive runs multiple training experiments in parallel, each with different hyperparameter configurations. It then compares the performance of each configuration and provides the best performing hyperparameters based on the defined primary metric.\n\nHere's an example of how you can specify the search space for a deep learning model using HyperDrive in Azure Machine Learning:\n\n```python\nfrom azureml.train.hyperdrive import RandomParameterSampling, HyperDriveConfig, PrimaryMetricGoal\nfrom azureml.train.hyperdrive import choice, loguniform\n\n# Define the parameter search space\nparam_sampling = RandomParameterSampling({\n    'learning_rate': loguniform(-6, -1),  # Searching in a log space between 10^-6 and 10^-1\n    'batch_size': choice(16, 32, 64, 128),  # Choosing from a discrete set of options\n    'momentum': uniform(0.85, 0.95)  # Sampling uniformly between 0.85 and 0.95\n})\n\n# Specify a primary metric to optimize\nprimary_metric_name = \"accuracy\"\nprimary_metric_goal = PrimaryMetricGoal.MAXIMIZE\n\n# Create a HyperDriveConfig using the estimator (assuming `estimator` is already defined)\nhyperdrive_config = HyperDriveConfig(estimator=estimator,\n                                     hyperparameter_sampling=param_sampling,\n                                     policy=None,\n                                     primary_metric_name=primary_metric_name,\n                                     primary_metric_goal=primary_metric_goal,\n                                     max_total_runs=100,\n                                     max_concurrent_runs=4)\n```\n\nIn this example, `RandomParameterSampling` is used to define a random search over the hyperparameter space. The `learning_rate` is searched over a logarithmic scale between 10^-6 and 10^-1, `batch_size` is chosen from a set of discrete values, and `momentum` is sampled from a uniform distribution between 0.85 and 0.95. The `HyperDriveConfig` is then configured with the estimator, the sampling space, the primary metric, and other settings like the maximum number of runs and the maximum number of concurrent runs.\n\nPlease note that the actual implementation may vary depending on the specific deep learning framework you are using (e.g., TensorFlow, PyTorch) and the details of your experiment setup.", "context": ["For more information, see What is automated machine learning? .\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\nLearning can automate this task for arbitrary parameterized commands with little\nmodification to your job definition. R esults are visualized in the studio.\nFor more information, see Tune hyperparameters .\nEfficiency of training for deep learning and sometimes classical machine learning\ntraining jobs can be drastically improved via multinode distributed training. Azure\nMachine Learning compute clusters and serverless compute  offer the latest GPU\noptions.\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\nclusters, and serverless compute :\nPyTorch\nTensorFlow\nMPI\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\nAzure S ynapse Analytics Spark clusters.\nFor more information, see Distributed training with Azure Machine Learning .\nScaling an ML project might require scaling embarrassingly parallel model training. This\npattern is common for scenarios like forecasting demand, where a model might be\ntrained for many stores.\nTo bring a model into production, you deploy the model. The Azure Machine Learning\nmanaged endpoints abstract the required infrastructure for both batch or real-time\n(online) model scoring (inferencing).Hyperparameter optimization\nMultinode distributed training\nEmbarrassingly parallel training\nDeploy models", "Hyperparameter tuning a model (v2)\nArticle \u202204/04/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAutomate efficient hyperparameter tuning using Azure Machine Learning SDK v2 and\nCLI v2 by way of the S weepJob type.\n1. Define the parameter search space for your trial\n2. Specify the sampling algorithm for your sweep job\n3. Specify the objective to optimize\n4. Specify early termination policy for low-performing jobs\n5. Define limits for the sweep job\n6. Launch an experiment with the defined configuration\n7. Visualize the training jobs\n8. Select the best configuration for your model\nHyperp aramet ers are adjustable parameters that let you control the model training\nprocess. For example, with neural networks, you decide the number of hidden layers and\nthe number of nodes in each layer. Model performance depends heavily on\nhyperparameters.\nHyperp aramet er tuning , also called hyperp aramet er optimization , is the process of\nfinding the configuration of hyperparameters that results in the best performance. The\nprocess is typically computationally expensive and manual.\nAzure Machine Learning lets you automate hyperparameter tuning and run experiments\nin parallel to efficiently optimize hyperparameters.\nTune hyperparameters by exploring the range of values defined for each\nhyperparameter.\nHyperparameters can be discrete or continuous, and has a distribution of values\ndescribed by a parameter expression .\nWhat is hyperparameter tuning ?\nDefine the search space", "When training computer vision models, model performance depends heavily on the\nhyperparameter values selected. Often, you might want to tune the hyperparameters to\nget optimal performance. For computer vision tasks, you can sweep hyperparameters to\nfind the optimal settings for your model. This feature applies the hyperparameter tuning\ncapabilities in Azure Machine Learning. Learn how to tune hyperparameters .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nYAML\nYou can define the model architectures and hyperparameters to sweep in the parameter\nspace. Y ou can either specify a single model architecture or multiple ones.\nSee Individual trials  for the list of supported model architectures for each task type.\nSee Hyperparameters for computer vision tasks  hyperparameters for each\ncomputer vision task type.Azure CLI\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value:  0.0001\n      max_value:  0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value:  0.0001\n      max_value:  0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\nDefine the parameter search space", "To tune the model's hyperparameters, define the parameter space in which to search\nduring training. Y ou'll do this by replacing some of the parameters ( batch_size,\nfirst_layer_neurons, second_layer_neurons, and learning_rate) passed to the training\njob with special inputs from the azure.ml.sweep package.\nPython\nThen, you'll configure sweep on the command job, using some sweep-specific\nparameters, such as the primary metric to watch and the sampling algorithm to use.\nIn the following code, we use random sampling to try different configuration sets of\nhyperparameters in an attempt to maximize our primary metric, validation_acc.\nWe also define an early termination policy\u2014the BanditPolicy. This policy operates by\nchecking the job every two iterations. If the primary metric, validation_acc, falls outside\nthe top ten percent range, Azure Machine Learning will terminate the job. This saves the\nmodel from continuing to explore hyperparameters that show no promise of helping to\nreach the target metric.\nPythonfrom azure.ai.ml.sweep import Choice, LogUniform  \n# we will reuse the command_job created before. we call it as a function so  \nthat we can apply inputs  \n# we do not apply the 'iris_csv' input again -- we will just use what was  \nalready defined earlier  \njob_for_sweep = job(  \n    batch_size=Choice(values=[ 25, 50, 100]), \n    first_layer_neurons=Choice(values=[ 10, 50, 200, 300, 500]), \n    second_layer_neurons=Choice(values=[ 10, 50, 200, 500]), \n    learning_rate=LogUniform(min_value= -6, max_value= -1), \n)\nfrom azure.ai.ml.sweep import BanditPolicy  \nsweep_job = job_for_sweep.sweep(  \n    compute=gpu_compute_target,  \n    sampling_algorithm= \"random\" , \n    primary_metric= \"Accuracy\" , \n    goal= \"Maximize\" , \n    max_total_trials= 20, \n    max_concurrent_trials= 4, \n    early_termination_policy=BanditPolicy(slack_factor= 0.1, \nevaluation_interval= 2), \n)", "You trained the model with one set of parameters, let's now see if you can further\nimprove the accuracy of your model. Y ou can tune and optimize your model's\nhyperparameters using Azure Machine Learning's sweep  capabilities.\nTo tune the model's hyperparameters, define the parameter space in which to search\nduring training. Y ou do this by replacing some of the parameters passed to the training\njob with special inputs from the azure.ml.sweep package.\nSince the training script uses a learning rate schedule to decay the learning rate every\nseveral epochs, you can tune the initial learning rate and the momentum parameters.\nPython\nThen, you can configure sweep on the command job, using some sweep-specific\nparameters, such as the primary metric to watch and the sampling algorithm to use.\nIn the following code, we use random sampling to try different configuration sets of\nhyperparameters in an attempt to maximize our primary metric, best_val_acc.\nWe also define an early termination policy, the BanditPolicy, to terminate poorly\nperforming runs early. The BanditPolicy terminates any run that doesn't fall within the\nslack factor of our primary evaluation metric. Y ou apply this policy every epoch (since we\nreport our best_val_acc metric every epoch and evaluation_interval=1). Notice we\ndelay the first policy evaluation until after the first 10 epochs ( delay_evaluation=10).\nPythonfrom azure.ai.ml.sweep import Uniform\n# we will reuse the command_job created before. we call it as a function so  \nthat we can apply inputs\njob_for_sweep = job(\n    learning_rate=Uniform(min_value= 0.0005, max_value= 0.005),\n    momentum=Uniform(min_value= 0.9, max_value= 0.99),\n)\nfrom azure.ai.ml.sweep import BanditPolicy\nsweep_job = job_for_sweep.sweep(\n    compute= \"gpu-cluster\" ,\n    sampling_algorithm= \"random\" ,\n    primary_metric= \"best_val_acc\" ,\n    goal=\"Maximize\" ,\n    max_total_trials= 8,\n    max_concurrent_trials= 4,\n    early_termination_policy=BanditPolicy(\n        slack_factor= 0.15, evaluation_interval= 1, delay_evaluation= 10"]}
{"question": "What is the command to update an existing model's properties, such as description or tags, using the Azure CLI?", "variant_id": "v1", "line_number": 15, "answer": "The command to update an existing model's properties, such as description or tags, using the Azure CLI is:\n\n```azurecli\naz ml model update --name run-model-example --version 1 --set description=\"This is an updated description.\" --set tags.stage=\"Prod\"\n```\n\nReplace `run-model-example` with the name of your model, `1` with the version of the model you want to update, and the description and tags with the values you wish to set.", "context": ["Get the details of a specific model:\ncli\nUpdate mutable properties of a specific model:\ncli\nArchiving a model will hide it by default from list queries ( az ml model list). You can\nstill continue to reference and use an archived model in your workflows. Y ou can archive\neither all versions of a model or only a specific version.\nIf you don't specify a version, all versions of the model under that given name will be\narchived. If you create a new model version under an archived model container, that\nnew version will automatically be set as archived as well.\nArchive all versions of a model:Show\nAzure CLI\naz ml model show --name run-model-example --version 1  \nUpdate\nAzure CLI\naz ml model update --name  run-model-example --version 1 --set  \ndescription=\"This is an updated description.\" --set tags.stage=\"Prod\"  \n\uff09 Impor tant\nFor model, only description and tags can be updated. All other properties are\nimmutable; if you need to change any of those properties you should create a new\nversion of the model.\nArchive", "Execute the following command in the Azure CLI, and update the <> placeholders\nwith your data asset name, version and key-value pair for the tag.\nAzure CLI\nTypically, your ETL processes organize your folder structure on Azure storage by time,\nfor example:\ntext\nThe combination of time/version structured folders and Azure Machine Learning T ables\n(MLTable) allow you to construct versioned datasets. T o show how to achieve versioned\ndata with Azure Machine Learning T ables, we use a hypothetical ex ample . Suppose you\nhave a process that uploads camera images to Azure Blob storage every week, in the\nfollowing structure:\ntextaz ml data update --name <DATA ASSET NAME>  --version  <VERSION>  --set \ntags.<KEY>=<VALUE> \nVersioning best practices\n/ \n\u2514\u2500\u2500 \ud83d\udcc1 mydata \n    \u251c\u2500\u2500 \ud83d\udcc1  year=2022  \n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1  month=11  \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4  file1 \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4  file2 \n    \u2502   \u2514\u2500\u2500 \ud83d\udcc1  month=12  \n    \u2502       \u2514\u2500\u2500 \ud83d\udcc4  file1 \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4  file2 \n    \u2514\u2500\u2500 \ud83d\udcc1  year=2023  \n        \u2514\u2500\u2500 \ud83d\udcc1  month=1  \n            \u2514\u2500\u2500 \ud83d\udcc4  file1 \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4  file2 \n/myimages  \n\u2514\u2500\u2500 \ud83d\udcc1 year=2022  \n    \u251c\u2500\u2500 \ud83d\udcc1  week52 \n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1  camera1  \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\uddbc  file1.jpeg  \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\uddbc  file2.jpeg  \n    \u2502   \u2514\u2500\u2500 \ud83d\udcc1  camera2  \n    \u2502       \u2514\u2500\u2500 \ud83d\uddbc  file1.jpeg  \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\uddbc  file2.jpeg", "The az ml model command can be used for managing Azure Machine Learning models.\nExamples are available in the examples GitHub repository . Several are shown below.\nYAML\nYAML\nInstall and use the CLI (v2)Remarks\nExamples\nYAML: local file\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-file-example  \npath: mlflow-model/model.pkl  \ndescription:  Model created from local file. \nYAML: local folder in MLflow format\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-mlflow-example  \npath: mlflow-model  \ntype: mlflow_model  \ndescription:  Model created from local MLflow model directory.", "If you want to update the code, model, or environment, update the Y AML file, and\nthen run the az ml online-endpoint update command.\nTo understand how update works:\n1. Open the file online/model-1/onlines coring/s core.py.\n2. Change the last line of the init() function: After logging.info(\"Init\ncomplete\"), add logging.info(\"Updated successfully\").\n3. Save the file.\n4. Run this command:\nAzure CLI(Optional) Update the deployment\nAzure CLI\n\uff17 Note\nIf you update instance count (to scale your deployment) along with other\nmodel settings (such as code, model, or environment) in a single update\ncommand, the scaling operation will be performed first, then the other updates\nwill be applied. It's a good practice to perform these operations separately in a\nproduction environment.\naz ml online-deployment update -n blue --endpoint  $ENDPOINT_NAME  -f \nendpoints/online/managed/sample/blue -deployment .yml\n\uff17 Note\nUpdating by using Y AML is declarative. That is, changes in the Y AML are\nreflected in the underlying Azure R esource Manager resources (endpoints\nand deployments). A declarative approach facilitates GitOps : All\nchanges to endpoints and deployments (even instance_count) go\nthrough the Y AML.\n\uea80 Tip", "Create a Y AML file, and copy-and-paste the following code. Y ou must update the\n<> placeholders with the name of your data asset, the version, description, tags\n(key-value pairs) and the path to a single file on a supported location.\nYAML\nNext, execute the following command in the CLI (update the <filename>\nplaceholder to the Y AML filename):\ncliAzure CLI\n$schema:  https://azuremlschemas.azureedge.net/latest/data.schema.json  \n# Supported paths include:  \n# local: './<path>/<file>' (this will be automatically uploaded to cloud  \nstorage)  \n# blob:  \n'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<f\nile>' \n# ADLS gen2:  \n'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<file>\n' \n# Datastore:  \n'azureml://datastores/<data_store_name>/paths/<path>/<file>'  \n# Data asset types, use one of:  \n# uri_file, uri_folder, mltable  \ntype: uri_file  \nname: <NAME OF DATA ASSET> \nversion:  <VERSION>  \ndescription:  <DESCRIPTION>  \ntags: \n    <KEY1>: <VALUE> \n    <KEY2>: <VALUE> \npath: <SUPPORTED  PATH> \naz ml data create -f <filename>.yml  \nAdd tags to an existing data asset\nAzure CLI"]}
{"question": "How does Azure ML integrate with Azure DevOps for implementing CI/CD pipelines for ML models? Describe the steps involved.", "variant_id": "v1", "line_number": 16, "answer": "Azure Machine Learning integrates with Azure DevOps to implement CI/CD pipelines for machine learning models through a process that typically involves the following steps:\n\n1. **Repository Setup**: Start by setting up a repository for your machine learning project in Azure DevOps. This repository will contain all the code, configuration files, and dependencies required to train and deploy your machine learning models.\n\n2. **Azure Pipelines Configuration**: Use Azure Pipelines within Azure DevOps to define the continuous integration (CI) and continuous deployment (CD) workflows. Pipelines are defined using YAML or through the Azure DevOps user interface.\n\n3. **Continuous Integration (CI)**: Configure the CI pipeline to automatically trigger when changes are pushed to the repository. The CI pipeline typically includes steps to:\n   - Install dependencies.\n   - Run tests to validate code and model changes.\n   - Train the machine learning model using Azure Machine Learning.\n   - Register the trained model in the Azure Machine Learning Model Registry.\n\n4. **Continuous Deployment (CD)**: Set up the CD pipeline to deploy the trained model to a target environment, such as an Azure Kubernetes Service (AKS) cluster or Azure Container Instances (ACI). The CD pipeline can be configured to:\n   - Retrieve the registered model from the Azure Machine Learning Model Registry.\n   - Package the model into a container image.\n   - Deploy the containerized model to the chosen deployment target as an inference endpoint.\n\n5. **Model Monitoring and Management**: After deployment, use Azure Machine Learning to monitor the model's performance and collect data on its predictions. This information can be used to trigger retraining workflows or rollback procedures if necessary.\n\n6. **Version Control and Tracing**: Throughout the CI/CD process, Azure DevOps and Azure Machine Learning provide version control and tracing capabilities to keep track of code, model versions, and pipeline executions. This ensures reproducibility and accountability in the machine learning lifecycle.\n\n7. **Automated Retraining and Testing**: Optionally, set up automated retraining pipelines that can be triggered based on various conditions, such as data drift or a scheduled interval. These pipelines can retrain the model and, if the new model passes validation tests, automatically deploy it to replace the older version.\n\nBy integrating Azure Machine Learning with Azure DevOps, teams can automate the end-to-end process of training, validating, deploying, and monitoring machine learning models, ensuring that ML systems are scalable, reproducible, and maintain high-quality standards.", "context": ["Continuous integration and deployment of machine learning models with Azure\nPipelines\nMachine Learning ML Ops  repository\nLearn more by reading and exploring the following resources:\nSet up ML Ops with Azure DevOps\nLearning path: End-to-end ML Ops with Azure Machine Learning\nHow to deploy a model to an online endpoint  with Machine Learning\nTutorial: T rain and deploy a model\nCI/CD of machine learning models with Azure Pipelines\nMachine learning at scale\nAzure AI reference architectures and best practices repo\nNext steps", "Some key features enabling ML Ops include:\ngit integration.\nMLflow integration.\nMachine learning pipeline scheduling.\nAzure Event Grid integration for custom triggers.\nEase of use with CI/CD tools like GitHub Actions or Azure DevOps.\nMachine Learning also includes features for monitoring and auditing:\nJob artifacts, such as code snapshots, logs, and other outputs.\nLineage between jobs and assets, such as containers, data, and compute resources.\nIf you use Apache Airflow, the airflow-provider-azure-machinelearning  package is a\nprovider that enables you to submit workflows to Azure Machine Learning from Apache\nAirFlow.\nStart using Azure Machine Learning:\nSet up an Azure Machine Learning workspace\nTutorial: Build a first machine learning project\nRun training jobs\nNext steps", "What are Azu re Machine Learning\npipelines?\nArticle \u202201/31/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAn Azure Machine Learning pipeline is an independently executable workflow of a\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\nstandardize the best practices of producing a machine learning model, enables the team\nto execute at scale, and improves the model building efficiency.\nThe core of a machine learning pipeline is to split a complete machine learning task into\na multistep workflow. Each step is a manageable component that can be developed,\noptimized, configured, and automated individually. S teps are connected through well-\ndefined interfaces. The Azure Machine Learning pipeline service automatically\norchestrates all the dependencies between pipeline steps. This modular approach brings\ntwo key benefits:\nStandardize the Machine learning operation (ML Ops) practice and support scalable\nteam collaboration\nTraining efficiency and cost reduction\nMachine learning operation (ML Ops) automates the process of building machine\nlearning models and taking the model to production. This is a complex process. It\nusually requires collaboration from different teams with different skills. A well-defined\nmachine learning pipeline can abstract this complex process into a multiple steps\nworkflow, mapping each step to a specific task such that each team can work\nindependently.\nFor example, a typical machine learning project includes the steps of data collection,\ndata preparation, model training, model evaluation, and model deployment. Usually, the\ndata engineers concentrate on data steps, data scientists spend most time on model\nWhy are Azure Machine Learning pipelines\nneeded?\nStandardize the MLOps practice and support scalable\nteam collaboration", "What are Azu re Machine Learning\npipelines?\nArticle \u202201/31/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAn Azure Machine Learning pipeline is an independently executable workflow of a\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\nstandardize the best practices of producing a machine learning model, enables the team\nto execute at scale, and improves the model building efficiency.\nThe core of a machine learning pipeline is to split a complete machine learning task into\na multistep workflow. Each step is a manageable component that can be developed,\noptimized, configured, and automated individually. S teps are connected through well-\ndefined interfaces. The Azure Machine Learning pipeline service automatically\norchestrates all the dependencies between pipeline steps. This modular approach brings\ntwo key benefits:\nStandardize the Machine learning operation (ML Ops) practice and support scalable\nteam collaboration\nTraining efficiency and cost reduction\nMachine learning operation (ML Ops) automates the process of building machine\nlearning models and taking the model to production. This is a complex process. It\nusually requires collaboration from different teams with different skills. A well-defined\nmachine learning pipeline can abstract this complex process into a multiple steps\nworkflow, mapping each step to a specific task such that each team can work\nindependently.\nFor example, a typical machine learning project includes the steps of data collection,\ndata preparation, model training, model evaluation, and model deployment. Usually, the\ndata engineers concentrate on data steps, data scientists spend most time on model\nWhy are Azure Machine Learning pipelines\nneeded?\nStandardize the MLOps practice and support scalable\nteam collaboration", "Use Azu re Pipelines with Azure Machine\nLearning\nArticle \u202209/29/2023\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\nof the operations you can automate are:\nData preparation (extract, transform, load operations)\nTraining machine learning models with on-demand scale-out and scale-up\nDeployment of machine learning models as public or private web services\nMonitoring deployed machine learning models (such as for performance or data-\ndrift analysis)\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\nmachine learning model to Azure Machine Learning .\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\nv2.\nComplete the Create resources to get started  to:\nCreate a workspace\nCreate a cloud-based compute cluster  to use for training your model\nAzure Machine Learning extension for Azure Pipelines. This extension can be\ninstalled from the Visual S tudio marketplace at\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-\nv2.\nFork the following repo at GitHub:Prerequisites\nStep 1: Get the code\nhttps://github.com/azure/azureml-examples"]}
{"question": "Explain the process of setting up a real-time inferencing endpoint using a deep learning model in Azure ML. What are the performance tuning options available?", "variant_id": "v1", "line_number": 17, "answer": "To set up a real-time inferencing endpoint using a deep learning model in Azure Machine Learning, you would typically follow these steps:\n\n1. **Register the Model**: First, you need to register your deep learning model in Azure ML. This can be done using the Azure ML Python SDK or the Azure CLI.\n\n2. **Define the Scoring Script**: Create a scoring script (often named `score.py`) that will be used to load your model and make predictions. This script must contain two required functions: `init()` to load the model and `run(raw_data)` to handle the requests to the endpoint.\n\n3. **Create an Inference Configuration**: Define an inference configuration that specifies the environment needed for your model. This includes the scoring script, the Python environment (dependencies, packages), and the compute resources required (CPU/GPU).\n\n4. **Define an Endpoint**: Specify the endpoint details such as the endpoint name and the authentication mode (key-based or token-based).\n\n5. **Deploy the Model to an Online Endpoint**: Deploy your model to an online endpoint. You can choose between Azure Managed Online Endpoints and Kubernetes-based endpoints depending on your needs.\n\n6. **Test the Endpoint**: Once the endpoint is deployed, you can test it by sending HTTP requests with the appropriate payload to the endpoint's URL.\n\nFor performance tuning, Azure Machine Learning offers several options:\n\n- **Instance Type Selection**: Choose the right compute instance type that matches the performance characteristics of your deep learning model. For example, you might need a more powerful GPU for intensive computations.\n\n- **Scaling**: Configure the autoscaling settings for your endpoint to handle varying loads. You can set minimum and maximum node counts and the autoscale target metric (like CPU or memory usage).\n\n- **Concurrency**: Adjust the concurrency level to optimize the number of parallel requests that your endpoint can handle. This depends on the complexity of your model and the compute resources.\n\n- **Model Optimization**: Optimize your deep learning model for inference. Techniques like quantization, pruning, and using optimized model formats (like ONNX) can improve performance.\n\n- **Caching**: For scenarios where the same requests are frequently made, you can implement caching mechanisms to store and quickly retrieve predictions without recomputing them.\n\n- **Batching**: If your model can process multiple inputs at once more efficiently than handling them individually, you can enable batching to group incoming requests.\n\n- **Monitoring and Diagnostics**: Use Azure ML's monitoring features to track the performance and health of your endpoints. This can help you identify bottlenecks and areas for improvement.\n\nRemember to consult the Azure Machine Learning documentation for the most up-to-date information and detailed guidance on each step of the process and performance tuning options.", "context": ["Online endpoints and deployments for\nreal-time inference\nArticle \u202210/24/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAzure Machine Learning allows you to perform real-time inferencing on data by using\nmodels that are deployed to online endpoints . Inferencing is the process of applying new\ninput data to a machine learning model to generate outputs. While these outputs are\ntypically referred to as \"predictions,\" inferencing can be used to generate outputs for\nother machine learning tasks, such as classification and clustering.\nOnline endpoints  deploy models to a web server that can return predictions under the\nHTTP protocol. Use online endpoints to operationalize models for real-time inference in\nsynchronous low-latency requests. W e recommend using them when:\nTo define an endpoint, you need to specify:\nEndpoint name : This name must be unique in the Azure region. For more\ninformation on the naming rules, see endpoint limits .\nAuthentication mode : You can choose between key-based authentication mode\nand Azure Machine Learning token-based authentication mode for the endpoint. A\nkey doesn't expire, but a token does expire. For more information on\nauthenticating, see Authenticate to an online endpoint .\nAzure Machine Learning provides the convenience of using managed online endpoints\nfor deploying your ML models in a turnkey manner. This is the recommended  way to use\nonline endpoints in Azure Machine Learning. Managed online endpoints work with\npowerful CPU and GPU machines in Azure in a scalable, fully managed way. These\nendpoints also take care of serving, scaling, securing, and monitoring your models, to\nfree you from the overhead of setting up and managing the underlying infrastructure. T o\nOnline  endpoints\nYou have low-latency requirements\uff02\nYour model can answer the request in a relatively short amount of time\uff02\nYour model's inputs fit on the HT TP payload of the request \uff02\nYou need to scale up in terms of number of requests\uff02", "In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\nmodel to an online endpoint. See these other articles to learn more about Azure\nMachine Learning.\nTrack run metrics during training\nTune hyperparameters\nReference architecture for distributed deep learning training in AzureNext steps", "Batch scoring, or batch infer encing , involves invoking an endpoint with a reference to\ndata. The batch endpoint runs jobs asynchronously to process data in parallel on\ncompute clusters and store the data for further analysis.\nReal-time s coring, or online infer encing , involves invoking an endpoint with one or more\nmodel deployments and receiving a response in near real time via HT TPS. T raffic can be\nsplit across multiple deployments, allowing for testing new model versions by diverting\nsome amount of traffic initially and increasing after confidence in the new model is\nestablished.\nFor more information, see:\nDeploy a model with a real-time managed endpoint\nUse batch endpoints for scoring\nDevOps for ML models, often called ML Ops, is a process for developing models for\nproduction. A model's lifecycle from training to deployment must be auditable if not\nreproducible.\nLearn more about MLOps in Azure Machine Learning .\nMachine Learning is built with the model lifecycle in mind. Y ou can audit the model\nlifecycle down to a specific commit and environment.Real-time and batch scoring (inferencing)\nMLOps: DevOps for machine learning\nML model lifecycle\nIntegrations enabling MLOPs", "What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps", "Currently, Azure Machine Learning supports fine-tuning models for the following\nlanguage tasks:\nText classification\nToken classification\nQuestion answering\nSummarization\nTranslation\nTo enable users to quickly get started with fine-tuning, we have published samples (both\nPython notebooks and CLI examples) for each task in the azureml-examples git repo\nFinetune samples . Each model card also links to fine-tuning samples for supported\nfine-tuning tasks.\nYou can deploy foundation models (both pre-trained models from the model catalog,\nand fine-tuned models, once they're registered to your workspace) to an endpoint that\ncan then be used for inferencing. Deployment to both real time endpoints and batch\nendpoints is supported. Y ou can deploy these models by using either the Deploy UI\nwizard or by using the code based samples linked from the model card.\nYou can invoke the Deploy UI form by selecting the Deploy  button on the model card\nfor any foundation model, and selecting either R eal-time endpoint or Batch endpoint\nDeploying foundation models to endpoints for\ninferencing\nDeploying using the studio\n\uf80a\nDeployment settings"]}
{"question": "How can you automate the process of model retraining and deployment using Azure ML Pipelines?", "variant_id": "v1", "line_number": 18, "answer": "To automate the process of model retraining and deployment using Azure ML Pipelines, you can follow these general steps:\n\n1. **Create an Azure ML Workspace**: This is the foundational block for any machine learning work in Azure ML. It provides a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n\n2. **Set up a Compute Target**: Create a cloud-based compute cluster that can scale out for training your model. This is where your model will be trained.\n\n3. **Prepare Your Training Script**: Write a training script in Python that defines how your model should be trained. This script is run on the compute target you've set up.\n\n4. **Define a Pipeline**: Create an Azure ML Pipeline that specifies the steps to run for retraining your model. This can include data preparation, model training, validation, and any other steps necessary for your workflow.\n\n5. **Create a Pipeline Endpoint**: Once your pipeline is defined, create a pipeline endpoint. This endpoint allows you to trigger pipeline runs programmatically or via a schedule.\n\n6. **Automate Retraining with Triggers**: Set up triggers for your pipeline to run automatically. This can be time-based (e.g., retrain every week), or it can be based on a data-driven event (e.g., when new data is available in a specified blob storage).\n\n7. **Register the Model**: After the model is retrained, use the Azure ML SDK to register the model in your Azure ML Workspace. This keeps track of different versions of your models.\n\n8. **Deploy the Model**: Use the Azure ML SDK to deploy the model as a web service on Azure Container Instances (ACI), Azure Kubernetes Service (AKS), or other supported compute targets.\n\n9. **Monitor the Model**: Set up monitoring for your deployed model to track its performance and data drift. This can be done using Azure ML's monitoring capabilities.\n\n10. **Update the Deployment**: If the retrained model performs better, you can update your deployment with the new model version.\n\nThe Azure Machine Learning Python SDK v2 and Azure CLI ML extension v2 are tools that can be used to interact with Azure ML services and manage these processes programmatically. Additionally, you can integrate Azure DevOps with Azure ML by using the Azure Machine Learning extension for Azure Pipelines, which can be installed from the Visual Studio marketplace. This extension allows you to define CI/CD pipelines that include steps for training and deploying ML models.\n\nRemember to fork the necessary GitHub repository that contains example code and templates to get started with setting up your pipelines for model retraining and deployment.", "context": ["Use Azu re Pipelines with Azure Machine\nLearning\nArticle \u202209/29/2023\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\nof the operations you can automate are:\nData preparation (extract, transform, load operations)\nTraining machine learning models with on-demand scale-out and scale-up\nDeployment of machine learning models as public or private web services\nMonitoring deployed machine learning models (such as for performance or data-\ndrift analysis)\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\nmachine learning model to Azure Machine Learning .\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\nv2.\nComplete the Create resources to get started  to:\nCreate a workspace\nCreate a cloud-based compute cluster  to use for training your model\nAzure Machine Learning extension for Azure Pipelines. This extension can be\ninstalled from the Visual S tudio marketplace at\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-\nv2.\nFork the following repo at GitHub:Prerequisites\nStep 1: Get the code\nhttps://github.com/azure/azureml-examples", "Continuous integration and deployment of machine learning models with Azure\nPipelines\nMachine Learning ML Ops  repository\nLearn more by reading and exploring the following resources:\nSet up ML Ops with Azure DevOps\nLearning path: End-to-end ML Ops with Azure Machine Learning\nHow to deploy a model to an online endpoint  with Machine Learning\nTutorial: T rain and deploy a model\nCI/CD of machine learning models with Azure Pipelines\nMachine learning at scale\nAzure AI reference architectures and best practices repo\nNext steps", "6. Select main as a branch and choose /mlops/devops-pipelines/deploy-model-\ntraining-pipeline.yml, then select Continue .\n7. Save and Run  the pipeline\nThis scenario includes prebuilt workflows for two approaches to deploying a trained\nmodel, batch scoring or a deploying a model to an endpoint for real-time scoring. Y ou\nmay run either or both of these workflows to test the performance of the model in your\nAzure ML workspace. IN this example we will be using real-time scoring.\n1. Go to ADO pipelines\n2. Select New Pipeline .\n3. Select Azur e Repos Git .\uff17 Note\nAt this point, the infrastructure is configured and the Prototyping Loop of the\nMLOps Architecture is deployed. you're ready to move to our trained model to\nproduction.\nDeploying the Trained model\nDeploy ML model endpoint", "3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\nMachine Learning in many different ways .\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\niterations over different models, hyperparameter settings, advanced\npreprocessing/featurization, and what metrics to look at when determining the\nbest model.\n5. Submit the training job.\n6. Review the r esults\nThe following diagram illustrates this process. \nYou can also inspect the logged job information, which contains metrics  gathered\nduring the job. The training job produces a Python serialized object ( .pkl file) that\ncontains the model and data preprocessing.\nWhile model building is automated, you can also learn how important or relevant\nfeatures are  to the generated models.\nApply automated ML when you want Azure Machine Learning to train and tune a model\nfor you using the target metric you specify. Automated ML democratizes the machine\nlearning model development process, and empowers its users, no matter their data\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\nML professionals and developers across industries can use automated ML to:\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\nforecasting, computer vision & NLP", "4. When the pipeline has complete successfully, you can find your Azure Machine\nLearning W orkspace and associated resources by logging in to the Azure P ortal.\nNext, a model training and scoring pipelines will be deployed into the new\nMachine Learning environment.\nThe solution accelerator includes code and data for a sample end-to-end machine\nlearning pipeline which runs a linear regression to predict taxi fares in NY C. The pipeline\nis made up of components, each serving different functions, which can be registered\nwith the workspace, versioned, and reused with various inputs and outputs. Sample\npipelines and workflows for the Computer Vision and NLP scenarios will have different\nsteps and deployment steps.\nThis training pipeline contains the following steps:\nPrepare Data\nThis component takes multiple taxi datasets (yellow and green) and merges/filters\nthe data, and prepare the train/val and evaluation datasets.\nInput: Local data under ./data/ (multiple .csv files)\nOutput: Single prepared dataset (.csv) and train/val/test datasets.\nTrain Model\nThis component trains a Linear R egressor with the training set.\nInput: T raining dataset\nOutput: T rained model (pickle format)\nEvaluat e Model\nThis component uses the trained model to predict taxi fares on the test set.\nInput: ML model and T est dataset\nOutput: P erformance of model and a deploy flag whether to deploy or not.Sample Training  and Deployment Scenario"]}
{"question": "Describe the approach to use Azure ML for implementing a recommendation system. What kind of data and algorithms are typically involved?", "variant_id": "v1", "line_number": 19, "answer": "To implement a recommendation system using Azure Machine Learning (Azure ML), you would typically follow these steps:\n\n1. **Data Upload and Exploration**:\n   - You would start by uploading your data to Azure ML, which could include user data, item data, and interaction data (such as ratings, clicks, or purchases).\n   - You would then explore and preprocess your data using Azure ML notebooks or scripts to ensure it is clean and formatted correctly for model training.\n\n2. **Model Development**:\n   - On a cloud workstation provided by Azure ML, you would begin prototyping your recommendation system. This could involve feature engineering to create meaningful attributes that can help in predicting user preferences.\n\n3. **Model Training**:\n   - You would select an appropriate algorithm for your recommendation system. Common algorithms include collaborative filtering, content-based filtering, and hybrid methods.\n   - Azure ML allows you to train your model at scale, leveraging its compute resources. You can use Automated Machine Learning (AutoML) to find the best model or manually select and tune your model.\n\n4. **Model Deployment**:\n   - Once the model is trained and evaluated, you can deploy it as an online endpoint using Azure ML.\n   - This endpoint can then be used to serve real-time recommendations to users.\n\n5. **Production Pipelines**:\n   - To automate and scale the recommendation system, you can create production pipelines in Azure ML. These pipelines can handle tasks such as data ingestion, model retraining, and batch predictions.\n\n**Data Types**:\n- **User Data**: Information about the users, such as demographics or user profiles.\n- **Item Data**: Information about the items being recommended, such as product descriptions or content features.\n- **Interaction Data**: User-item interactions, which could be explicit (e.g., ratings, reviews) or implicit (e.g., clicks, purchase history).\n\n**Algorithms**:\n- **Collaborative Filtering**: Makes automatic predictions about the interests of a user by collecting preferences from many users (collaborating). The assumption of collaborative filtering is that if a user A has the same opinion as a user B on an issue, A is more likely to have B's opinion on a different issue.\n- **Content-Based Filtering**: Recommends items similar to those a given user has liked in the past, based on the content of the items.\n- **Hybrid Methods**: Combine collaborative and content-based filtering to improve recommendation performance.\n\nAzure ML provides the tools and infrastructure to build, train, and deploy recommendation systems at scale, with support for various data types and machine learning algorithms.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Tell us about y our PDF experience.\nAzur e Machine Learning documentation\nLearn how to train and deploy models and manage the ML lifecycle (ML Ops) with Azure\nMachine Learning. Tutorials, code examples, API references, and more.\nOverview\n\uff45OVERVIE W\nWhat is Azure Machine Learning?\nSetup & quickstar t\n\uff46QUICKST ART\nCreate resources\nGet started with Azure Machine Learning\nStart with the b asics\n\uff47TUTORIAL\nPrepare and explore data\nDevelop on a cloud workstation\nTrain a model\nDeploy a model\nSet up a reusable pipeline\nWork with data\n\uff43HOW-T O GUIDE\nUse Apache Spark in Azure Machine Learning\nCreate data assets", "What is Azure Machine Learning?\nArticle \u202201/29/2024\nAzure Machine Learning is a cloud service for accelerating and managing the machine\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\nin their day-to-day workflows to train and deploy models and manage machine learning\noperations (ML Ops).\nYou can create a model in Machine Learning or use a model built from an open-source\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\nretrain, and redeploy models.\nMachine Learning is for individuals and teams implementing ML Ops within their\norganization to bring ML models into production in a secure and auditable production\nenvironment.\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\nday workflows. Application developers can use tools for integrating models into\napplications or services. Platform developers can use a robust set of tools, backed by\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\nbased access control for infrastructure. Y ou can set up a project to deny access to\nprotected data and select operations.\nML projects often require a team with a varied skill set to build and maintain. Machine\nLearning has tools that help enable you to:\uea80 Tip\nFree trial!  If you don't have an Azure subscription, create a free account before you\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\nto spend on Azure services. After they're used up, you can keep the account and\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\nchange your settings and ask to be charged.\nWho is Azure Machine Learning for?\nProductivity for everyone on the team", "If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about calendar features for time series forecasting in AutoML .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about AutoML forecasting for lagged features .Next steps", "For training in developing AI and Machine Learning solutions with Microsoft, see\nMicrosoft Learn training .\nChoose a Microsoft cognitive services technology\nArtificial intelligence (AI) architecture design\nHow Azure Machine Learning works: resources and assetsRelated resources"]}
{"question": "How can you leverage Azure ML's built-in support for ONNX models for interoperability across different frameworks?", "variant_id": "v1", "line_number": 20, "answer": "Azure Machine Learning (Azure ML) provides built-in support for ONNX (Open Neural Network Exchange) models, which facilitates interoperability across different machine learning frameworks. Here's how you can leverage this support:\n\n1. **Model Conversion**: Convert your machine learning models from various frameworks such as TensorFlow, PyTorch, Scikit-Learn, Keras, etc., into the ONNX format. Azure ML supports the conversion process through interoperability with these frameworks.\n\n2. **Model Deployment**: Once your model is in ONNX format, you can deploy it using Azure ML services. The ONNX model can be deployed to a variety of platforms, including both cloud and edge devices, ensuring that you can run your model wherever you need it.\n\n3. **Inference Optimization**: ONNX Runtime, which is integrated with Azure ML, provides a high-performance inference engine that can be used to run ONNX models. It is optimized for both cloud and edge scenarios and supports various hardware accelerators.\n\n4. **Cross-Platform Execution**: With ONNX Runtime, you can execute the ONNX models on different operating systems such as Linux, Windows, and macOS, and across different hardware platforms, ensuring that your models are portable and versatile.\n\n5. **Hardware Acceleration**: ONNX Runtime integrates with hardware-specific accelerators like NVIDIA TensorRT for GPUs, Intel OpenVINO for Intel processors, and DirectML for Windows, allowing you to take full advantage of the underlying hardware capabilities for improved performance.\n\n6. **Azure ML Pipelines**: You can incorporate ONNX models into Azure ML pipelines, which allows for efficient management of the model lifecycle, including training, evaluation, deployment, and monitoring.\n\n7. **MLOps**: Azure ML supports MLOps practices, which you can use to manage and automate the lifecycle of your ONNX models, including versioning, auditing, and validation of models in production.\n\nBy using Azure ML's support for ONNX, you can create a seamless workflow that allows for model training in your preferred framework, conversion to ONNX for interoperability, and deployment for inference across a wide range of platforms and devices. This approach simplifies the process of managing and scaling machine learning models in production environments.", "context": ["ONNX and Azure Machine Learning:\nCreate and accelerate ML models\nArticle \u202206/13/2023\nLearn how using the Open Neural Network Exchange  (ONNX) can help optimize the\ninference of your machine learning model. Inference, or model scoring, is the phase\nwhere the deployed model is used for prediction, most commonly on production data.\nOptimizing machine learning models for inference (or model scoring) is difficult since\nyou need to tune the model and the inference library to make the most of the hardware\ncapabilities. The problem becomes extremely hard if you want to get optimal\nperformance on different kinds of platforms (cloud/edge, CPU/GPU, etc.), since each one\nhas different capabilities and characteristics. The complexity increases if you have\nmodels from a variety of frameworks that need to run on a variety of platforms. It's very\ntime consuming to optimize all the different combinations of frameworks and hardware.\nA solution to train once in your preferred framework and run anywhere on the cloud or\nedge is needed. This is where ONNX comes in.\nMicrosoft and a community of partners created ONNX as an open standard for\nrepresenting machine learning models. Models from many frameworks  including\nTensorFlow, PyT orch, SciKit-Learn, K eras, Chainer, MXNet, MA TLAB, and SparkML can be\nexported or converted to the standard ONNX format. Once the models are in the ONNX\nformat, they can be run on a variety of platforms and devices.\nONNX Runtime  is a high-performance inference engine for deploying ONNX models\nto production. It's optimized for both cloud and edge and works on Linux, Windows,\nand Mac. Written in C++, it also has C, Python, C#, Java, and JavaScript (Node.js) APIs\nfor usage in a variety of environments. ONNX Runtime supports both DNN and\ntraditional ML models and integrates with accelerators on different hardware such as\nTensorR T on NVidia GPUs, OpenVINO on Intel processors, DirectML on Windows, and\nmore. By using ONNX Runtime, you can benefit from the extensive production-grade\noptimizations, testing, and ongoing improvements.\nONNX Runtime is used in high-scale Microsoft services such as Bing, Office, and Azure\nAI. Performance gains are dependent on a number of factors, but these Microsoft\nservices have seen an average 2x per formance gain on CPU . In addition to Azure\nMachine Learning services, ONNX Runtime also runs in other products that support\nMachine Learning workloads, including:\nWindows: The runtime is built into Windows as part of Windows Machine Learning\nand runs on hundreds of millions of devices.", "Azure SQL product family: Run native scoring on data in Azure SQL Edge  and\nAzure SQL Managed Instance .\nML.NET: Run ONNX models in ML.NET .\nYou can obtain ONNX models in several ways:\nTrain a new ONNX model in Azure Machine Learning (see examples at the bottom\nof this article) or by using automated Machine Learning capabilities\nConvert existing model from another format to ONNX (see the tutorials )\nGet a pre-trained ONNX model from the ONNX Model Zoo\nGenerate a customized ONNX model from Azure Custom Vision service\nMany models including image classification, object detection, and text processing can\nbe represented as ONNX models. If you run into an issue with a model that cannot be\nconverted successfully, please file an issue in the GitHub of the respective converter that\nyou used. Y ou can continue using your existing format model until the issue is\naddressed.\nWith Azure Machine Learning, you can deploy, manage, and monitor your ONNX\nmodels. Using the standard deployment workflow  and ONNX Runtime, you can create a\nREST endpoint hosted in the cloud. See example Jupyter notebooks at the end of this\narticle to try it out for yourself.\n\uf80a\nGet ONNX models\nDeploy ONNX models in Azure\nInstall and use ONNX Runtime with Python", "See the AutoML package  for changing default ensemble settings in automated machine\nlearning.\nWith Azure Machine Learning, you can use automated ML to build a Python model and\nhave it converted to the ONNX format. Once the models are in the ONNX format, they\ncan be run on a variety of platforms and devices. Learn more about accelerating ML\nmodels with ONNX .\nSee how to convert to ONNX format in this Jupyter notebook example . Learn which\nalgorithms are supported in ONNX .\nThe ONNX runtime also supports C#, so you can use the model built automatically in\nyour C# apps without any need for recoding or any of the network latencies that REST\nendpoints introduce. Learn more about using an AutoML ONNX model in a .NET\napplication with ML.NET  and inferencing ONNX models with the ONNX runtime C#\nAPI .\nThere are multiple resources to get you up and running with AutoML.\nTutorials are end-to-end introductory examples of AutoML scenarios.\nFor a code fir st experience , follow the Tutorial: T rain an object detection model\nwith AutoML and Python\nFor a low or no-code experience , see the Tutorial: T rain a classification model with\nno-code AutoML in Azure Machine Learning studio .\nHow-to articles provide additional detail into what functionality automated ML offers.\nFor example,\nConfigure the settings for automatic training experiments\nWithout code in the Azure Machine Learning studio .\nWith the Python SDK .\nLearn how to train computer vision models with Python .\nLearn how to view the generated code from your automated ML models (SDK v1) .AutoML & ONNX\nNext steps\nTutorials/ how-tos", "Make predictions with ONNX on\ncomputer vision models from AutoML\nArticle \u202204/04/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this article, you will learn how to use Open Neural Network Exchange (ONNX) to\nmake predictions on computer vision models generated from automated machine\nlearning (AutoML) in Azure Machine Learning.\nTo use ONNX for predictions, you need to:\n1. Download ONNX model files from an AutoML training run.\n2. Understand the inputs and outputs of an ONNX model.\n3. Preprocess your data so that it's in the required format for input images.\n4. Perform inference with ONNX Runtime for Python.\n5. Visualize predictions for object detection and instance segmentation tasks.\nONNX  is an open standard for machine learning and deep learning models. It enables\nmodel import and export (interoperability) across the popular AI frameworks. For more\ndetails, explore the ONNX GitHub project .\nONNX Runtime  is an open-source project that supports cross-platform inference.\nONNX Runtime provides APIs across programming languages (including Python, C++,\nC#, C, Java, and JavaScript). Y ou can use these APIs to perform inference on input\nimages. After you have the model that has been exported to ONNX format, you can use\nthese APIs on any programming language that your project needs.\nIn this guide, you'll learn how to use Python APIs for ONNX Runtime  to make\npredictions on images for popular vision tasks. Y ou can use these ONNX exported\nmodels across languages.\nGet an AutoML-trained computer vision model for any of the supported image\ntasks: classification, object detection, or instance segmentation. Learn more about\nAutoML support for computer vision tasks .\nInstall the onnxruntime  package. The methods in this article have been tested\nwith versions 1.3.0 to 1.8.0.\nPrerequisites", "runs from experimentation in the studio user interface. Or they can use versioned assets\nfor jobs like environments and storage references.\nFor more information, see Manage Azure Machine Learning workspaces .\nWhen a project is ready for operationalization, users' work can be automated in an ML\npipeline and triggered on a schedule or HT TPS request.\nYou can deploy models to the managed inferencing solution, for both real-time and\nbatch deployments, abstracting away the infrastructure management typically required\nfor deploying models.\nIn Azure Machine Learning, you can run your training script in the cloud or build a\nmodel from scratch. Customers often bring models they've built and trained in open-\nsource frameworks so that they can operationalize them in the cloud.\nData scientists can use models in Azure Machine Learning that they've created in\ncommon Python frameworks, such as:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nLightGBM\nOther languages and frameworks are also supported:\nR\n.NET\nFor more information, see Open-source integration with Azure Machine Learning .\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\nexperience and intuition to select the right data featurization and algorithm for training.\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\nLearning studio UI or the Python SDK.Train models\nOpen and interoperable\nAutomated featurization and algorithm selection"]}
{"question": "What Azure storage services can be integrated with Azure Machine Learning for storing datasets?", "variant_id": "v1", "line_number": 21, "answer": "Azure Machine Learning can be integrated with several Azure storage services for storing datasets, including:\n\n1. Azure Blob Storage: This is a scalable, object storage solution for unstructured data. It is often used to store large amounts of data for machine learning workloads.\n\n2. Azure Data Lake Storage: This is an enterprise-wide hyper-scale repository for big data analytic workloads, which allows you to store data of any size, shape, and speed.\n\n3. Azure File Storage: Offers shared storage for applications using the standard SMB protocol. Machine learning experiments can access data stored in file shares.\n\n4. Azure SQL Database: A relational database service that can be used to store structured data, which can then be accessed by machine learning models for training and inference.\n\nThese services can be accessed from within Azure Machine Learning notebooks, scripts, and pipelines to store and retrieve datasets for machine learning workflows.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Service Usage Example datanumber of nodes; datastore names and tags, and\ndescriptions on assets like models; and data label names.\nAzure AI\nSearchStores indexes that\nhelp with querying\nyour machine learning\ncontent.These indexes are built on top of the data stored in Azure\nCosmos DB.\nAzure\nStorageStores metadata\nrelated to Azure\nMachine Learning\npipeline data.Data can include designer pipeline names, pipeline layout,\nand execution properties.\nFrom the perspective of data lifecycle management, data in the preceding resources is\ncreated and deleted as you create and delete corresponding objects in Azure Machine\nLearning.\nYour Azure Machine Learning workspace reads and writes data by using its managed\nidentity. This identity is granted access to the resources through a role assignment\n(Azure role-based access control) on the data resources. The encryption key that you\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\nused to create indexes for Azure AI Search at runtime.\nExtra networking controls are configured when you create a private link endpoint on\nyour workspace to allow for inbound connectivity. This configuration includes the\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\nNetwork access is restricted to only trusted Microsoft services.\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\nthe data.\nWhen you use a customer-managed key, the resources are in your Azure subscription\nand encrypted with your key. While these resources exist in your subscription, Microsoft\nmanages them. They're automatically created and configured when you create your\nAzure Machine Learning workspace.\nThese Microsoft-managed resources are located in a new Azure resource group that's\ncreated in your subscription. This resource group is separate from the resource group\nfor your workspace. It contains the Microsoft-managed resources that your key is used\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\nresource group name><GUID>.Customer-managed keys"]}
{"question": "How do you authenticate Azure Machine Learning with Azure Blob Storage to access datasets?", "variant_id": "v1", "line_number": 22, "answer": "To authenticate Azure Machine Learning with Azure Blob Storage for accessing datasets, you can use identity-based data access by connecting to an Azure Machine Learning datastore. Here are the steps to set up identity-based authentication:\n\n1. **Managed Identity**: Ensure that the Azure Machine Learning workspace or the compute resource (like a compute instance or compute cluster) has a managed identity assigned to it. A managed identity allows Azure services to authenticate to cloud services without storing credentials in code.\n\n2. **Assign Role**: Assign the appropriate role to the managed identity for the Azure Blob Storage. Typically, you would assign the \"Storage Blob Data Reader\" role to the managed identity to allow read access to the datasets.\n\n3. **Create Datastore**: In Azure Machine Learning, create a datastore that references the Azure Blob Storage. When creating the datastore, select the option to use the managed identity for authentication.\n\n4. **Access Data**: When you access data in your training jobs or Jupyter Notebooks, Azure Machine Learning will use the managed identity to authenticate against the Azure Blob Storage, and you won't need to provide any credentials.\n\nBy following these steps, you can securely connect to Azure Blob Storage and access datasets for your machine learning workflows without storing sensitive credentials.", "context": ["You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.", "means that in order to successfully read data from storage, the managed identity of the\ncompute cluster where the deployment is running must have at least Storage Blob Data\nReader  access to the storage account.\nTo configure the compute cluster for data access, follow these steps:\n1. Go to Azure Machine Learning studio .\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\ndeployment is using.\n3. Assign a managed identity to the compute cluster:\na. In the Managed identity  section, verify if the compute has a managed identity\nassigned. If not, select the option Edit.\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\nusing a S ystem-Assigned Managed Identity, it is named as \"[workspace\nname]/computes/[compute cluster name]\".\nc. Save the changes.\n4. Go to the Azure portal  and navigate to the associated storage account where the\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\nstorage account where those assets are placed.", "When you use an Azure S torage Account from Azure Machine Learning studio, you must\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\naccount:\nBlob Data R eader\nIf the storage account uses a private endpoint to connect to the VNet, you must\ngrant the Reader  role for the storage account private endpoint to the managed\nidentity.\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\nNetwork .\nThe following sections explain the limitations of using an Azure S torage Account, with\nyour workspace, in a VNet.\nTo secure communication between Azure Machine Learning and Azure S torage\nAccounts, configure the storage to Grant access to trusted Azure services .\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\ncan normally be used to allow your client to directly connect over the internet. However,\nwhen using studio, your client doesn't connect to the storage account. The Azure\nMachine Learning service that makes the request connects to the storage account. The\nIP address of the service isn't documented, and it changes frequently. Enabling the\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\nconfiguration .\uea80 Tip\nTo access data from outside Azure Machine Learning, for example with Azure\nStorage Explorer, that access probably relies on the user identity. For specific\ninformation, review the documentation for the tool or service you're using. For\nmore information about how Azure Machine Learning works with data, see Setup\nauthentication betw een Azur e Machine Learning and other ser vices .\nAzure Storage Account\nSecure communication with Azure Storage Account\nAzure Storage firewall", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."]}
{"question": "What steps are involved in uploading a dataset to Azure Blob Storage and then accessing it from an Azure Machine Learning experiment?", "variant_id": "v1", "line_number": 23, "answer": "To upload a dataset to Azure Blob Storage and then access it from an Azure Machine Learning experiment, you can follow these steps:\n\n1. **Upload Data to Azure Blob Storage:**\n   - Use Azure Storage Explorer to transfer your data to Azure Blob Storage. This is recommended for a large amount of data due to its robustness and speed.\n   - Alternatively, you can upload files directly through the Azure portal or by using Azure Storage SDKs.\n\n2. **Create a Dataset in Azure Machine Learning:**\n   - In the Azure Machine Learning Studio, select **Create** to create a new dataset.\n   - Enter a name for your dataset and, optionally, a description.\n   - Choose the **Dataset type**:\n     - Select **Tabular** if you're using a `.csv` or `.tsv` file where each row contains a response.\n     - Select **File** if you're using separate `.txt` files for each response.\n   - Select **Next**.\n\n3. **Specify Datastore and Path:**\n   - Select **From Azure storage**, then select **Next**.\n   - Choose the datastore that points to your Azure Blob Storage.\n   - If your data is in a subfolder within Blob Storage, use **Browse** to select the path. To include all files in the subfolders, append `/**` to the path. To include all data in the current container and its subfolders, append `**/*.*` to the path.\n   - Select **Next**.\n\n4. **Finalize Dataset Creation:**\n   - Review your settings and then select **Create** to create the dataset.\n\n5. **Access the Dataset in an Experiment:**\n   - In your Azure Machine Learning experiment, you can now access the dataset you created by referencing it in your code or through the user interface when setting up an experiment.\n\nBy following these steps, you will have your data uploaded to Azure Blob Storage and made available as a dataset within Azure Machine Learning, ready to be used in your experiments.", "context": ["In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data", "2. Select your subscription and the workspace you created.\n3. Select Get star ted.\n4. In the left pane, select Automat ed ML  under the Author  section.\n5. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning dataset. Doing so, allows you to ensure that your\ndata is formatted appropriately for your experiment.\n1. On the Select dataset  form, select From local files  from the +Create dataset  drop-\ndown.\na. On the Basic info  form, give your dataset a name and provide an optional\ndescription. The dataset type should default to Tabular , since automated ML in\nAzure Machine Learning studio currently only supports tabular datasets.\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is the storage location where you upload your data\nfile.\nd. Select Uplo ad files  from the Uplo ad drop-down.\ne. Choose the bike-no.csv  file on your local computer. This is the file you\ndownloaded as a prerequisite .\nf. Select Next\nWhen the upload is complete, the Settings and preview form is pre-populated\nbased on the file type.\ng. Verify that the Settings and pr eview  form is populated as follows and select\nNext .Create and load dataset\n\uff89Expand table", "In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data", "Since this is your first automated ML experiment, you'll see an empty list and links\nto documentation.\n3. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning data asset. In the case of this tutorial, you can think\nof a data asset as your dataset for the AutoML job. Doing so, allows you to ensure that\nyour data is formatted appropriately for your experiment.\n1. Create a new data asset by selecting From local files  from the +Create data asset\ndrop-down.\na. On the Basic info  form, give your data asset a name and provide an optional\ndescription. The automated ML interface currently only supports\nTabularDatasets, so the dataset type should default to Tabular .\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is where you'll upload your data file to make it\navailable to your workspace.\nd. Select Uplo ad files  from the Uplo ad drop-down.Create and load a dataset as a data asset", "b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably."]}
{"question": "Can Azure Machine Learning directly access data stored in Azure Data Lake Storage Gen2? If so, how?", "variant_id": "v1", "line_number": 24, "answer": "Yes, Azure Machine Learning can directly access data stored in Azure Data Lake Storage Gen2. Azure Data Lake Storage Gen2 is used as a datastore for Azure Machine Learning, and it is built on top of Azure Blob Storage, which is designed for big data analytics in enterprises.\n\nTo access data from Azure Data Lake Storage Gen2 in Azure Machine Learning, you can follow these general steps:\n\n1. **Set up Azure Data Lake Storage Gen2**: Ensure that your Azure Data Lake Storage Gen2 account is properly set up and that you have the necessary permissions to access the data.\n\n2. **Register a datastore**: In Azure Machine Learning, you can register a datastore that references the data in your Azure Data Lake Storage Gen2 account. This involves providing the necessary connection information such as the storage account name, container name, and access credentials.\n\n3. **Access data in your experiments**: Once the datastore is registered, you can access the data from your Azure Machine Learning experiments, training runs, or pipelines by referencing the registered datastore. You can read and write data to the datastore as part of your machine learning workflows.\n\n4. **Use Azure Machine Learning SDK**: You can use the Azure Machine Learning SDK for Python to interact with the datastore programmatically. The SDK provides methods to upload, download, and manage data within the datastore.\n\nRemember to handle access control and security appropriately, such as by using Azure role-based access control (RBAC) and managing access keys or shared access signatures (SAS) securely. If you want to use your own keys for encryption, you can also set up Azure Storage encryption with customer-managed keys in Azure Key Vault.", "context": ["For information on regenerating the access keys, see Regenerate storage account access\nkeys.\nAzure Data Lake S torage Gen2 is built on top of Azure Blob S torage and is designed for\nbig data analytics in enterprises. Data Lake S torage Gen2 is used as a datastore for\nAzure Machine Learning. Like Azure Blob S torage, the data at rest is encrypted with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Data Lake\nStorage, see Azure S torage encryption with customer-managed keys in Azure K ey Vault.\nThe Azure Machine Learning service supports data from the following data sources.\nTransparent data encryption helps protect Azure SQL Database against the threat of\nmalicious offline activity by encrypting data at rest. By default, transparent data\nencryption is enabled for all newly deployed SQL databases that use Microsoft-\nmanaged keys.\nFor information on how to use customer-managed keys for transparent data encryption,\nsee Azure SQL Database transparent data encryption .Azure Data Lake Storage\n\uff17 Note\nOn Feb 29, 2024 , Azure Data Lake S torage Gen1 will be retired. For more\ninformation, see the official announcement . If you use Azure Data Lake S torage\nGen1, make sure to migrate to Azure Data Lake S torage Gen2 before that date. T o\nlearn how, see Migrat e Azur e Data Lak e Storage fr om Gen1 t o Gen2 by using the\nAzur e por tal.\nUnless you already have an Azure Data Lake S torage Gen1 account, you can't create\nnew ones.\nAzure relational databases\nAzure SQL Database\nAzure Database for PostgreSQL", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore"]}
{"question": "How does Azure Machine Learning support versioning of datasets stored in Azure Blob Storage?", "variant_id": "v1", "line_number": 25, "answer": "Azure Machine Learning supports dataset versioning by allowing users to create and manage versions of datasets that are registered in the Azure Machine Learning workspace. When you register a dataset in your workspace, you can specify a version number. If you do not specify a version, Azure Machine Learning will automatically assign a version number starting with 1 and incrementing with each new registration of the dataset.\n\nWhen datasets are stored in Azure Blob Storage, you can track changes to the data by registering a new version of the dataset whenever the underlying data changes. This allows you to maintain a history of the dataset's evolution over time and ensures reproducibility in experiments and model training by referencing specific dataset versions.\n\nTo use a specific version of a dataset in your machine learning experiments, you can retrieve it by specifying the dataset name and version number in your Azure Machine Learning code. This ensures that the exact data used for a particular run can be identified, which is crucial for experiment tracking and model management.\n\nIn summary, Azure Machine Learning supports dataset versioning in Azure Blob Storage by allowing you to register, manage, and retrieve different versions of datasets within your Azure Machine Learning workspace.", "context": ["Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore", "Only Azure Blob S torage Accounts are supported for outputs. For instance, Azure Data Lake\nStorage Gen2 isn't supported as output in batch deployment jobs. If you need to output the\ndata to a different location/sink, use the Azure Data F actory Copy activity after the execution of\nthe batch job.\nUse low priority VMs in batch deployments\nAuthorization on batch endpoints\nNetwork isolation in batch endpointsNext steps", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity"]}
{"question": "What is the recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage?", "variant_id": "v1", "line_number": 26, "answer": "The recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage is to use Azure Storage Explorer. It provides a faster and more robust method for transferring a large amount of data. Azure Storage Explorer should be used as the default method for moving files into Azure Machine Learning.", "context": ["In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity"]}
{"question": "How do you use the Azure Machine Learning SDK to programmatically download data from Azure Blob Storage?", "variant_id": "v1", "line_number": 27, "answer": "To programmatically download data from Azure Blob Storage using the Azure Machine Learning SDK, you would typically follow these steps:\n\n1. **Set up your Azure Machine Learning workspace**: Before you can interact with Azure Blob Storage, you need to have an Azure Machine Learning workspace set up. This workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create.\n\n2. **Install the Azure Machine Learning SDK**: Ensure that you have the Azure Machine Learning SDK installed in your Python environment. You can install it using pip:\n\n   ```bash\n   pip install azureml-sdk\n   ```\n\n3. **Authenticate to Azure**: Use the SDK to authenticate to Azure. This can be done using service principal authentication, interactive login, or other methods provided by the SDK.\n\n4. **Access the workspace**: Access your Azure Machine Learning workspace using the SDK:\n\n   ```python\n   from azureml.core import Workspace\n   ws = Workspace.from_config()\n   ```\n\n   The `from_config()` method assumes you have a `config.json` file in your current directory or its parent containing the workspace details. Alternatively, you can specify the subscription ID, resource group, and workspace name directly.\n\n5. **Access the Datastore**: Azure Machine Learning workspaces come with a default datastore (usually backed by an Azure Blob Storage account), and you can also register additional datastores. Access the datastore where your data is located:\n\n   ```python\n   from azureml.core import Datastore\n   datastore = Datastore.get(ws, datastore_name='your_datastore_name')\n   ```\n\n6. **Download data from the Datastore**: Once you have a reference to the datastore, you can download data from it. You can download files to your local file system using the `download` or `download_to_path` methods:\n\n   ```python\n   datastore.download(target_path='local_folder',\n                      prefix='folder_or_file_in_datastore',\n                      show_progress=True)\n   ```\n\n   Replace `'local_folder'` with the path where you want to download the files, and `'folder_or_file_in_datastore'` with the specific folder or file you want to download from the datastore.\n\nRemember to replace placeholders like `'your_datastore_name'` with the actual names relevant to your Azure environment. Also, ensure that you have the necessary permissions to access the data in the blob storage.\n\nPlease note that the Azure Machine Learning SDK is continuously updated, and the methods and classes might change. Always refer to the latest Azure Machine Learning SDK documentation for the most up-to-date information.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Tutorial: Upload, access and explore\nyour data in Azure Machine Learning\nArticle \u202212/27/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this tutorial you learn how to:\nThe start of a machine learning project typically involves exploratory data analysis (ED A),\ndata-preprocessing (cleaning, feature engineering), and the building of Machine\nLearning model prototypes to validate hypotheses. This prototyping  project phase is\nhighly interactive. It lends itself to development in an IDE or a Jupyter notebook, with a\nPython int eractive console. This tutorial describes these ideas.\nThis video shows how to get started in Azure Machine Learning studio so that you can\nfollow the steps in the tutorial. The video shows how to create a notebook, clone the\nnotebook, create a compute instance, and download the data needed for the tutorial.\nThe steps are also described in the following sections.\n1. To use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\n2. Sign in to studio  and select your workspace if it's not already open.\n3. Open or create a notebook in your workspace:\nCreate a new notebook , if you want to copy/paste code into cells.\nOr, open tutorials/get -star ted-not ebooks/explor e-data.ipynb  from the\nSamples  section of studio. Then select Clone  to add the notebook to your\nFiles. (See where to find Samples .)\nUpload your data to cloud storage\uff02\nCreate an Azure Machine Learning data asset\uff02\nAccess your data in a notebook for interactive development\uff02\nCreate new versions of data assets\uff02\nhttps://learn-video.azurefd.net/vod/player?id=514a29e2-0ae7-4a5d-a537-\n8f10681f5545&locale=en-us&embedUrl=%2F azure%2Fmachine-learning%2Ftutorial-\nexplore-data\nPrerequisites", "Data from Azure Machine Learning registered data stores can be directly referenced by\nbatch deployments jobs. In this example, you first upload some data to the default data\nstore in the Azure Machine Learning workspace and then run a batch deployment on it.\nFollow these steps to run a batch endpoint job using data stored in a data store.\n1. Access the default data store in the Azure Machine Learning workspace. If your\ndata is in a different store, you can use that store instead. Y ou're not required to\nuse the default data store.\nAzure CLI\n2. You need to upload some sample data to the data store. This example assumes\nyou already uploaded the sample data included in the repo in the folder\nsdk/python/endpoints/batch/deploy-models/heart-classifier-mlflow/data in the\nfolder heart-disease-uci-unlabeled in the blob storage account. Ensure you've\ndone that before moving forward.\n3. Create the input or request:\nPlace the file path in the following variable:Azure CLI\nDATASTORE_ID=$(az ml datastore show -n workspaceblobstore | jq -r \n'.id')\n\uff17 Note\nData stores ID would look like\n/subscriptions/<subscription>/resourceGroups/<resource-\ngroup>/providers/Microsoft.MachineLearningServices/workspaces/<worksp\nace>/datastores/<data-store>.\n\uea80 Tip\nThe default blob data store in a workspace is called worksp aceblobst ore. You\ncan skip this step if you already know the resource ID of the default data store\nin your workspace.\nAzure CLI", "Acces s data in a job\nArticle \u202202/06/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2 (current)\nIn this article you learn:\nAn Azure subscription. If you don't have an Azure subscription, create a free account before you begin.\nTry the free or paid version of Azure Machine Learning .\nThe Azure Machine Learning SDK for Python v2 .\nAn Azure Machine Learning workspace\nBefore you explore the detailed options available to you when you access data, we first describe the relevant\ncode snippets for data access.\nIn this example, you submit an Azure Machine Learning job that accesses data from a public  blob storage\naccount. However, you can adapt the snippet to access your own data in a private Azure S torage account.\nUpdate the path as described here. Azure Machine Learning seamlessly handles authentication to cloud\nstorage, with Microsoft Entra passthrough. When you submit a job, you can choose:\nUser identity:  Passthrough your Microsoft Entra identity to access the data\nManaged identity:  Use the managed identity of the compute target to access data\nNone:  Don't specify an identity to access the data. Use None when using credential-based (key/SAS\ntoken) datastores or when accessing public data\nHow to read data from Azure storage in an Azure Machine Learning job.\uff02\nHow to write data from your Azure Machine Learning job to Azure S torage. \uff02\nThe difference between mount  and downlo ad modes. \uff02\nHow to use user identity and managed identity to access data.\uff02\nMount settings available in a job.\uff02\nOptimum mount settings for common scenarios.\uff02\nHow to access V1 data assets.\uff02\nPrerequisites\nQuickstart\nRead data from Azure storage in an Azure Machine Learning job\n\uea80 Tip\nIf you use keys or SAS tokens to authenticate, we suggest that you create an Azur e Machine Learning\ndatast ore, because the runtime will automatically connect to storage without exposure of the key/token.\nPython SDK", "Acces s data from Azure cloud storage during\ninteractive development\nArticle \u202209/13/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nA machine learning project typically starts with exploratory data analysis (ED A), data-preprocessing\n(cleaning, feature engineering), and includes building prototypes of ML models to validate hypotheses.\nThis prototyping  project phase is highly interactive in nature, and it lends itself to development in a\nJupyter notebook, or an IDE with a Python int eractive console. In this article you'll learn how to:\nAn Azure Machine Learning workspace. For more information, see Manage Azure Machine Learning\nworkspaces in the portal or with the Python SDK (v2) .\nAn Azure Machine Learning Datastore. For more information, see Create datastores .\nAn Azure Machine Learning datastore is a reference to an existing  Azure storage account. The benefits of\ndatastore creation and use include:\nAccess data from a Azure Machine Learning Datastores URI as if it were a file system.\uff02\nMaterialize data into P andas using mltable Python library. \uff02\nMaterialize Azure Machine Learning data assets into P andas using mltable Python library. \uff02\nMaterialize data through an explicit download with the azcopy utility. \uff02\nPrerequisites\n\uea80 Tip\nThe guidance in this article describes data access during interactive development. It applies to any\nhost that can run a Python session. This can include your local machine, a cloud VM, a GitHub\nCodespace, etc. W e recommend use of an Azure Machine Learning compute instance - a fully\nmanaged and pre-configured cloud workstation. For more information, see Create an Azur e\nMachine Learning comput e instance .\n\uff09 Impor tant\nEnsure you have the latest azure-fsspec and mltable python libraries installed in your python\nenvironment:\nBash\npip install -U azureml-fsspec mltable\nAccess data from a datastore URI, like a filesystem\nA common, easy-to-use API to interact with different storage types (Blob/Files/ADLS).\uff02"]}
{"question": "What are the benefits of integrating Azure Data Lake Storage with Azure Machine Learning for big data scenarios?", "variant_id": "v1", "line_number": 28, "answer": "Integrating Azure Data Lake Storage with Azure Machine Learning offers several benefits for big data scenarios:\n\n1. **Scalability**: Azure Data Lake Storage is designed to handle large volumes of data in a scalable manner. It can store and analyze petabytes of data, which is essential for big data scenarios.\n\n2. **Performance**: Azure Data Lake Storage provides high-performance processing capabilities that can handle massive amounts of data efficiently. This is crucial when training machine learning models on large datasets.\n\n3. **Data Exploration**: Azure Machine Learning allows you to explore and preprocess data stored in Azure Data Lake Storage directly. This seamless integration simplifies the data exploration process, making it easier to prepare data for machine learning.\n\n4. **Advanced Analytics**: With Azure Machine Learning, you can build and deploy advanced analytics solutions that leverage the data stored in Azure Data Lake Storage, enabling you to extract valuable insights from your big data.\n\n5. **Security**: Azure Data Lake Storage includes security features such as encryption and access control, which are important for protecting sensitive big data. Azure Machine Learning can utilize these features to ensure that data used for machine learning is secure.\n\n6. **Cost-Effectiveness**: Storing large volumes of data in Azure Data Lake Storage can be more cost-effective than traditional data storage solutions. When integrated with Azure Machine Learning, you can optimize the cost of training and deploying machine learning models.\n\n7. **Streamlined Workflow**: By using Azure Data Lake Storage with Azure Machine Learning, you can create end-to-end machine learning pipelines that handle data storage, model training, and deployment within the same ecosystem, streamlining the workflow and reducing complexity.\n\n8. **Collaboration**: Azure Machine Learning workspace allows for collaboration among data scientists and developers. When integrated with Azure Data Lake Storage, teams can work together on the same datasets and machine learning models, improving productivity and innovation.\n\nThese benefits make the integration of Azure Data Lake Storage with Azure Machine Learning a powerful combination for handling big data scenarios in machine learning projects.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Azur e Machine Learning designer : Use the designer to train and deploy ML\nmodels without writing any code. Drag and drop datasets and components to\ncreate ML pipelines.\nAutomat ed machine learning UI : Learn how to create automated ML experiments\nwith an easy-to-use interface.\nData labeling : Use Machine Learning data labeling to efficiently coordinate image\nlabeling  or text labeling  projects.\nMachine Learning integrates with the Azure cloud platform to add security to ML\nprojects.\nSecurity integrations include:\nAzure Virtual Networks with network security groups.\nAzure K ey Vault, where you can save security secrets, such as access information\nfor storage accounts.\nAzure Container R egistry set up behind a virtual network.\nFor more information, see Tutorial: Set up a secure workspace .\nOther integrations with Azure services support an ML project from end to end. They\ninclude:Enterprise-readiness and security\nAzure integrations for complete solutio ns", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore", "What is Azure Machine Learning?\nArticle \u202201/29/2024\nAzure Machine Learning is a cloud service for accelerating and managing the machine\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\nin their day-to-day workflows to train and deploy models and manage machine learning\noperations (ML Ops).\nYou can create a model in Machine Learning or use a model built from an open-source\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\nretrain, and redeploy models.\nMachine Learning is for individuals and teams implementing ML Ops within their\norganization to bring ML models into production in a secure and auditable production\nenvironment.\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\nday workflows. Application developers can use tools for integrating models into\napplications or services. Platform developers can use a robust set of tools, backed by\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\nbased access control for infrastructure. Y ou can set up a project to deny access to\nprotected data and select operations.\nML projects often require a team with a varied skill set to build and maintain. Machine\nLearning has tools that help enable you to:\uea80 Tip\nFree trial!  If you don't have an Azure subscription, create a free account before you\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\nto spend on Azure services. After they're used up, you can keep the account and\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\nchange your settings and ask to be charged.\nWho is Azure Machine Learning for?\nProductivity for everyone on the team", "Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster ."]}
{"question": "How can you secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces?", "variant_id": "v1", "line_number": 29, "answer": "To secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces, you can use the following methods:\n\n1. **Encryption at Rest**: By default, all data stored in Azure Blob Storage is encrypted at rest with Microsoft-managed keys. This ensures that your data is secure when it is not being accessed or moved.\n\n2. **Customer-Managed Keys**: For additional control, you can use your own encryption keys for data stored in Azure Blob Storage. This is done by integrating with Azure Key Vault, where you can manage your encryption keys. You can rotate or revoke your keys as needed, and the storage account will start using the new key for encrypting data at rest or fail requests if a key is revoked.\n\n3. **Secure Data Transfer**: Ensure that data is encrypted in transit by using secure transfer methods. Azure Machine Learning integrates with services that support encryption in transit.\n\n4. **Access Control**: Use Azure role-based access control (RBAC) to manage who has access to your Azure Storage accounts. You can assign roles to users, groups, or applications at a fine-grained level.\n\n5. **Network Security**: Configure network security groups and firewalls to control access to the storage accounts. You can also use Virtual Network (VNet) service endpoints to isolate the network access to your Azure Storage account.\n\n6. **Monitoring and Auditing**: Enable logging and monitoring to track access and usage of your storage accounts. Azure offers tools like Azure Monitor and Azure Security Center to help with monitoring and threat detection.\n\nBy combining these methods, you can ensure that your data is secure both at rest and in transit when being accessed by Azure Machine Learning workspaces.", "context": ["Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "This article is part of a series on securing an Azure Machine Learning workflow. See the\nother articles in this series:\nSecure the workspace resources\nSecure machine learning registries\nSecure the training environment\nSecure the inference environment\nEnable studio functionality\nUse custom DNS\nUse a firewall\nAPI platform network isolationNext steps", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.", "When you use an Azure S torage Account from Azure Machine Learning studio, you must\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\naccount:\nBlob Data R eader\nIf the storage account uses a private endpoint to connect to the VNet, you must\ngrant the Reader  role for the storage account private endpoint to the managed\nidentity.\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\nNetwork .\nThe following sections explain the limitations of using an Azure S torage Account, with\nyour workspace, in a VNet.\nTo secure communication between Azure Machine Learning and Azure S torage\nAccounts, configure the storage to Grant access to trusted Azure services .\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\ncan normally be used to allow your client to directly connect over the internet. However,\nwhen using studio, your client doesn't connect to the storage account. The Azure\nMachine Learning service that makes the request connects to the storage account. The\nIP address of the service isn't documented, and it changes frequently. Enabling the\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\nconfiguration .\uea80 Tip\nTo access data from outside Azure Machine Learning, for example with Azure\nStorage Explorer, that access probably relies on the user identity. For specific\ninformation, review the documentation for the tool or service you're using. For\nmore information about how Azure Machine Learning works with data, see Setup\nauthentication betw een Azur e Machine Learning and other ser vices .\nAzure Storage Account\nSecure communication with Azure Storage Account\nAzure Storage firewall", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity"]}
{"question": "What role do Azure Storage accounts play in deploying Azure Machine Learning models as web services?", "variant_id": "v1", "line_number": 30, "answer": "Azure Storage accounts play a crucial role in deploying Azure Machine Learning models as web services in several ways:\n\n1. **Model Storage**: Azure Storage accounts are used to store the trained machine learning models. When you register a model in Azure Machine Learning, you can store it in an Azure Blob Storage container associated with your workspace.\n\n2. **Data Storage**: They store the datasets used for training and inferencing. These datasets can be mounted or downloaded to the compute targets during model training or scoring.\n\n3. **Code and Artifacts Storage**: They store the code, scripts, and other artifacts needed to train models and create web services.\n\n4. **Web Service Assets**: When deploying a model as a web service, the scoring script, environment configuration, and the model itself are packaged into a Docker image. This image is then stored in a container registry such as Azure Container Registry (ACR).\n\n5. **Operationalization**: For web services, the storage account may also be used to store logs and other operational data that is generated by the web service.\n\n6. **Endpoint Configuration**: When setting up a real-time inferencing endpoint, the storage account can be used to store the deployment configuration and related assets.\n\n7. **Secure Access**: Azure Storage accounts can be configured with private endpoints for secure access from Azure Machine Learning compute instances and compute clusters, ensuring that data and model artifacts are securely accessed during the deployment process.\n\nIn summary, Azure Storage accounts provide a secure and scalable way to store and manage the data, models, and artifacts that are essential for deploying and managing Azure Machine Learning models as web services.", "context": ["Scenar io Requir ed\ninboundRequir ed outbound Additional\nconfiguration\nUse Azure\nKubernetes\nServiceNot applicable For information on the outbound\nconfiguration for AKS, see Secure\nAzure Kubernetes Service\ninferencing environment .\nUse Docker\nimages that\nAzure\nMachine\nLearning\nmanagesNot applicable Microsoft Artifact R egistry\nviennaglobal.azurecr.io\nglobal container registryIf the container registry\nfor your workspace is\nbehind the virtual\nnetwork, configure the\nworkspace to use a\ncompute cluster to build\nimages. For more\ninformation, see Secure\nan Azure Machine\nLearning workspace with\nvirtual networks .\nAzure Machine Learning uses multiple storage accounts. Each stores different data and\nhas a different purpose:\nYour st orage : The storage accounts in your Azure subscription store your data and\nartifacts, such as models, training data, training logs, and Python scripts. For\nexample, the default  storage account for your workspace is in your subscription.\nThe Azure Machine Learning compute instance and compute cluster access file and\nblob data in this storage over ports 445 (SMB) and 443 (HT TPS).\nWhen you're using a compute instance or compute cluster, your storage account is\nmounted as a file share via the SMB protocol. The compute instance and cluster\nuse this file share to store items like the data, models, Jupyter notebooks, and\ndatasets. The compute instance and cluster use the private endpoint when they\naccess the storage account.\nMicrosoft storage : The Azure Machine Learning compute instance and compute\ncluster rely on Azure Batch. They access storage located in a Microsoft\nsubscription. This storage is used only for the management of the compute\ninstance or cluster. None of your data is stored here. The compute instance and\ncompute cluster access the blob, table, and queue data in this storage, by using\nport 443 (HT TPS).Purposes of storage accounts", "Azure Machine Learning supports storage accounts configured to use either a private\nendpoint or service endpoint.\n1. In the Azure portal, select the Azure S torage Account.\n2. Use the information in Use private endpoints for Azure S torage  to add private\nendpoints for the following storage resources:\nBlob\nFile\nQueue  - Only needed if you plan to use Batch endpoints  or the\nParallelRunS tep in an Azure Machine Learning pipeline.\nTable - Only needed if you plan to use Batch endpoints  or the\nParallelRunS tep in an Azure Machine Learning pipeline.Securing a workspace with private endpoints does not ensure end-to-end security\nby itself. Y ou must follow the steps in the rest of this article, and the VNet series, to\nsecure individual components of your solution. For example, if you use a private\nendpoint for the workspace, but your Azure S torage Account is not behind the\nVNet, traffic between the workspace and storage does not use the VNet for\nsecurity.\nSecure Azure storage accounts\nPrivate endpoint", "What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps", "Azure Machine Learning offers many ways to deploy MLflow models to online and batch\nendpoints. Y ou can deploy models, using the following tools:\nEach workflow has different capabilities, particularly around which type of compute they\ncan target. The following table shows the different capabilities.\nScenar io MLflow SDK Azur e Machine\nLearning CLI/SDKAzur e Machine\nLearning studio\nDeploy to managed online\nendpointsSee example See example See example\nDeploy to managed online\nendpoints (with a scoring script)Not\nsupportedSee example See example\nDeploy to batch endpoints Not\nsupportedSee example See example\nDeploy to batch endpoints (with\na scoring script)Not\nsupportedSee example See example\nDeploy to web services\n(ACI/AKS)Legacy\nsupportNot supported Not supported\nDeploy to web services (A CI/AKS\n- with a scoring script)Not\nsupportedLegacy support Legacy support\n Deployment to online endpoints that are in workspaces with private link enabled\nrequires you to package models before deployment (preview) .\n We recommend switching to managed online endpoints  instead.\n MLflow (OSS) doesn't have the concept of a scoring script and doesn't support batch\nexecution currently.\nUse the MLflow SDK if both of these conditions apply:Deployment tools\nMLflow SDK\uff02\nAzure Machine Learning CLI\uff02\nAzure Machine Learning SDK for Python\uff02\nAzure Machine Learning studio\uff02\n\uff89Expand table\n1 1 1\n3\n3\n3\n22 2\n32 2\n1\n2\n3\nWhich deployment tool to use?", "region basis. When you sign up for Azure Machine Learning studio, you receive a default\nVM quota for several VM families available in the region. Y ou can continue to create\ndeployments until you reach your quota limit. Once you reach this limit, you can request\na quota increase.\nModels deployed as a service with pay-as-you-go are protected by Azure AI content\nsafety. When deployed to real-time endpoints, you can opt out of this capability. With\nAzure AI content safety enabled, both the prompt and completion pass through an\nensemble of classification models aimed at detecting and preventing the output of\nharmful content. The content filtering system detects and takes action on specific\ncategories of potentially harmful content in both input prompts and output\ncompletions. Learn more about Azure AI Content Safety .\nModel Catalog and Collections\nDeploy and score a machine learning model by using an online endpoint\nPlan and manage costs for Azure AI S tudioContent filtering\nRelated content"]}
{"question": "How do you monitor data access and usage when integrating Azure Machine Learning with Azure Storage?", "variant_id": "v1", "line_number": 31, "answer": "To monitor data access and usage when integrating Azure Machine Learning with Azure Storage, you can use Azure Monitor, which provides comprehensive monitoring of your Azure resources and the applications you run on Azure. Here are the steps you can follow:\n\n1. **Enable Diagnostic Settings**: Configure the diagnostic settings for your Azure Storage account to send logs and metrics to Azure Monitor. This can include read, write, and delete operations on your storage blobs or files, which can be indicative of data access and usage.\n\n2. **Log Analytics Workspace**: Set up a Log Analytics workspace in Azure Monitor to query and visualize the logs sent from your Azure Storage account. You can write queries to analyze the data access patterns, look for anomalies, or monitor for specific events.\n\n3. **Metrics**: Review the metrics provided by Azure Storage to monitor the performance, availability, and usage. Metrics like Total Requests, Ingress/Egress data, and Success Egress can give insights into how the data is being used.\n\n4. **Alerts**: Create alerts in Azure Monitor based on metrics or log query results. For example, you can set an alert for when there's an unusual spike in data egress, which might indicate a potential data exfiltration attempt.\n\n5. **Azure Storage Analytics Logging**: Use Azure Storage Analytics logging to record details of successful and failed requests to your storage account. This can help you trace any access or usage of the data stored in Azure Storage.\n\n6. **Role-Based Access Control (RBAC)**: Ensure that you have proper RBAC policies in place to control who has access to your Azure Storage resources. Monitor changes to these policies and access patterns to ensure compliance with your organization's data access policies.\n\n7. **Azure Policy**: Use Azure Policy to enforce rules and conventions for your resources in Azure Storage, ensuring that they comply with your organization's standards and service level agreements.\n\nRemember to consult the Azure documentation for the most up-to-date guidance on monitoring Azure Machine Learning and Azure Storage, as features and best practices can evolve over time.", "context": ["To learn how to monitor resources and logs from Azure Machine Learning, see\nMonitoring Azure Machine Learning .", "Monitor Azure Machine Learning\nArticle \u202211/06/2023\nWhen you have critical applications and business processes relying on Azure resources, you\nwant to monitor those resources for their availability, performance, and operation. This\narticle describes the monitoring data generated by Azure Machine Learning and how to\nanalyze and alert on this data with Azure Monitor.\nAzure Machine Learning creates monitoring data using Azure Monitor , which is a full stack\nmonitoring service in Azure. Azure Monitor provides a complete set of features to monitor\nyour Azure resources. It can also monitor resources in other clouds and on-premises.\nStart with the article Monitoring Azure resources with Azure Monitor , which describes the\nfollowing concepts:\nWhat is Azure Monitor?\nCosts associated with monitoring\nMonitoring data collected in Azure\nConfiguring data collection\nStandard tools in Azure for analyzing and alerting on monitoring data\nThe following sections build on this article by describing the specific data gathered for\nAzure Machine Learning. These sections also provide examples for configuring data\ncollection and analyzing this data with Azure tools.\uea80 Tip\nThe information in this document is primarily for administrat ors, as it describes\nmonitoring for the Azure Machine Learning service and associated Azure services. If\nyou are a data scientist  or developer , and want to monitor information specific to your\nmodel tr aining r uns, see the following documents:\nStart, monit or, and cancel training runs\nLog metrics for training runs\nTrack experiments with MLflow\nIf you want to monitor information generated by models deployed to online\nendpoints, see Monit or online endpoints .\nWhat is Azure Monitor?", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.", "Monitor online endpoints\nArticle \u202210/24/2023\nAzure Machine Learning uses integration with Azure Monitor to track and monitor\nmetrics and logs for online endpoints . You can view metrics in charts, compare between\nendpoints and deployments, pin to Azure portal dashboards, configure alerts, query\nfrom log tables, and push logs to supported targets. Y ou can also use Application\nInsights to analyze events from user containers.\nMetrics : For endpoint-level metrics such as request latency, requests per minute,\nnew connections per second, and network bytes, you can drill down to see details\nat the deployment level or status level. Deployment-level metrics such as CPU/GPU\nutilization and memory or disk utilization can also be drilled down to instance\nlevel. Azure Monitor allows tracking these metrics in charts and setting up\ndashboards and alerts for further analysis.\nLogs : You can send metrics to the Log Analytics workspace where you can query\nthe logs using Kusto query syntax. Y ou can also send metrics to Azure S torage\naccounts and/or Event Hubs for further processing. In addition, you can use\ndedicated log tables for online endpoint related events, traffic, and console\n(container) logs. Kusto query allows complex analysis and joining of multiple\ntables.\nApplication insights : Curated environments include integration with Application\nInsights, and you can enable or disable this integration when you create an online\ndeployment. Built-in metrics and logs are sent to Application Insights, and you can\nuse the built-in features of Application Insights (such as Live metrics, T ransaction\nsearch, F ailures, and P erformance) for further analysis.\nIn this article you learn how to:\nDeploy an Azure Machine Learning online endpoint.Choose the right method to view and track metrics and logs\uff02\nView metrics for your online endpoint\uff02\nCreate a dashboard for your metrics\uff02\nCreate a metric alert\uff02\nView logs for your online endpoint\uff02\nUse Application Insights to track metrics and logs\uff02\nPrerequisites", "Use monitoring view in the Azure Machine Learning studio  to check the CPU and memory utilization of\nyour job. If you're low on CPU and memory, consider increasing the compute target VM size.\nConsider increasing RSLEX_DOWNLOADER_THREADS if you're downloading and you don't utilize the CPU and\nmemory. If you use mount, you should increase DATASET_MOUNT_READ_BUFFER_BLOCK_COUNT to do more\nprefetching, and increase DATASET_MOUNT_READ_THREADS for more read threads.\nIf the metrics show low SuccessE2ELat ency and low SuccessSer verLat ency, but the client experiences high\nlatency , you have a delay in the storage request that reaches the service. Y ou should check:\nWhether the number of threads used for mount/download\n(DATASET_MOUNT_READ_THREADS/RSLEX_DOWNLOADER_THREADS) is set too low, relative to the number of cores\navailable on the compute target. If the setting is too low, increase the number of threads.\nWhether the number of retries for downloading ( AZUREML_DATASET_HTTP_RETRY_COUNT) is set too high. If\nso, decrease the number of retries.\nFrom the Azure Machine Learning studio, you can also monitor the compute target disk IO and usage during\nyour job execution. Navigate to your job, and select the Monit oring  tab. This tab provides insights on the\nresources of your job, on a 30 day rolling basis. For example:\nAzure Machine Learning data runtime doesn't use the last RESERVED_FREE_DISK_SPACE bytes of disk space, to\nkeep the compute healthy (the default value is 150MB). If your disk is full, your code is writing files to disk\nwithout declaring the files as an output. Therefore, check your code to make sure that data isn't being written\nerroneously to temporary disk. If you must write files to temporary disk, and that resource is becoming full,\nconsider:\nIncreasing the VM Size to one that has a larger temporary disk\nSetting a T TL on the cached data ( DATASET_MOUNT_ATTRIBUTE_CACHE_TTL), to purge your data from diskMonitor disk usage during a job\n\uf80a\n\uff17 Note\nJob monitoring supports only compute resources that Azure Machine Learning manages. Jobs with a\nruntime of less than 5 minutes will not have enough data to populate this view."]}
{"question": "Can Azure Machine Learning use data stored in Azure File Storage? If so, what are the use cases?", "variant_id": "v1", "line_number": 32, "answer": "Yes, Azure Machine Learning can use data stored in Azure File Storage. Azure File Storage provides shared storage that can be accessed via the SMB protocol, and it can be mounted onto Azure Machine Learning compute instances or pipelines.\n\nUse cases for using Azure File Storage with Azure Machine Learning include:\n\n1. **Data Storage**: You can store large datasets that are used for training machine learning models in Azure File Storage. This is particularly useful when the datasets are too large to store on local compute instances.\n\n2. **Data Sharing**: Azure File Storage can be used to share data between multiple machine learning experiments or between team members working on the same project. Since the storage is shared, it ensures consistency and easy access to the latest data.\n\n3. **Persistent Storage**: When using Azure Machine Learning compute instances, you may want persistent storage that is not tied to the lifecycle of a compute instance. Azure File Storage can be used for this purpose, allowing you to retain data even when compute instances are deprovisioned.\n\n4. **Streamlined Workflows**: You can use Azure File Storage to store intermediate data in multi-step machine learning pipelines. This allows for efficient data handling between different steps of the pipeline without the need to transfer data back and forth between local storage and the cloud.\n\n5. **Collaboration and Versioning**: Teams can collaborate on datasets and maintain versions of datasets in Azure File Storage, which can be beneficial for tracking changes over time and ensuring reproducibility in machine learning experiments.\n\nTo utilize Azure File Storage within Azure Machine Learning, you would typically mount the file share to your Azure Machine Learning workspace or directly to the compute targets where the training or inference runs are executed. This integration allows for seamless access to the data during the machine learning workflow.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore", "Attached comput e: You can attach your own compute resources to your\nworkspace and use them for training and inference.\nMachine Learning allows you to work with different types of data:\nURIs (a location in local or cloud storage):\nuri_folder\nuri_file\nTables (a tabular data abstraction):\nmltable\nPrimitives:\nstring\nboolean\nnumber\nFor most scenarios, you use URIs ( uri_folder and uri_file) to identify a location in\nstorage that can be easily mapped to the file system of a compute node in a job by\neither mounting or downloading the storage to the node.\nThe mltable parameter is an abstraction for tabular data that's used for automated\nmachine learning (AutoML) jobs, parallel jobs, and some advanced scenarios. If you're\nstarting to use Machine Learning and aren't using AutoML, we strongly encourage you\nto begin with URIs.\nMachine Learning datastores securely keep the connection information to your data\nstorage on Azure so that you don't have to code it in your scripts. Y ou can register and\ncreate a datastore to easily connect to your storage account and access the data in your\nunderlying storage service. The Azure Machine Learning CLI v2 and SDK v2 support the\nfollowing types of cloud-based storage services:\nAzure Blob S torage container\nAzure Files share\nAzure Data Lake S torage\nAzure Data Lake S torage Gen2Data\nDatastore\nEnvironment", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."]}
{"question": "What are the best practices for managing input and output data for Azure Machine Learning pipelines using Azure Storage?", "variant_id": "v1", "line_number": 33, "answer": "When managing input and output data for Azure Machine Learning pipelines using Azure Storage, consider the following best practices:\n\n1. **Use Azure Machine Learning Datastores**: Register your Azure Storage accounts (Azure Blob Storage, Azure Data Lake Storage Gen1, and Azure Data Lake Storage Gen2) as Azure Machine Learning datastores. This allows you to manage and access data within your Azure Machine Learning workspace easily.\n\n2. **Flatten Nested Folder Structures**: Azure Machine Learning batch endpoint jobs do not support nested folder structures. If your data is organized in nested folders, you will need to flatten the structure before the batch job can process it.\n\n3. **Data Movement with Azure Data Factory**: If your input data resides in a storage service not directly supported by Azure Machine Learning, use Azure Data Factory to copy the data to a compatible Azure Storage account before executing the batch job.\n\n4. **Scoring Script Compatibility**: Ensure that your scoring script is capable of handling the data format and structure as it will be provided to the batch job. If you are using MLflow models, be aware of the file type limitations and ensure compatibility.\n\n5. **Write Back to the Same Storage Account**: To streamline the process, it is recommended to register the storage account used by Azure Data Factory as a datastore in Azure Machine Learning. This allows you to write results back to the same storage account from which you are reading, maintaining a consistent data flow.\n\n6. **Monitor Data Access and Permissions**: Regularly review and manage access permissions to your Azure Storage accounts to ensure that only authorized users and services can access or modify the data.\n\n7. **Optimize Data Transfer**: When possible, use datastores that are in the same region as your Azure Machine Learning workspace to minimize latency and data transfer costs.\n\nBy following these best practices, you can effectively manage your data for Azure Machine Learning pipelines and ensure smooth operation of your batch deployments.", "context": ["7. Your pipeline is ready to be used.\nWhen calling Azure Machine Learning batch deployments consider the following limitations:\nOnly Azure Machine Learning data stores or Azure S torage Accounts (Azure Blob S torage, Azure\nData Lake S torage Gen1, Azure Data Lake S torage Gen2) are supported as inputs. If your input\ndata is in another source, use the Azure Data F actory Copy activity before the execution of the\nbatch job to sink the data to a compatible store.\nBatch endpoint jobs don't explore nested folders and hence can't work with nested folder\nstructures. If your data is distributed in multiple folders, notice that you will have to flatten the\nstructure.\nMake sure that your scoring script provided in the deployment can handle the data as it is\nexpected to be fed into the job. If the model is MLflow, read the limitation in terms of the file\ntype supported by the moment at Using MLflow models in batch deployments .\nOnly registered Azure Machine Learning data stores are supported by the moment. W e\nrecommend you to register the storage account your Azure Data F actory is using as a Data S tore\nin Azure Machine Learning. In that way, you will be able to write back to the same storage\naccount from where you are reading.\nLimitations\nData inputs\nData outputs", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.", "original notebook or Python code, changes the training input from local data to\nparameterized values, split the training code into multiple steps as needed, perform unit\ntest of each step, and finally wraps all steps into a pipeline.\nOnce the teams get familiar with pipelines and want to do more machine learning\nprojects using pipelines, they'll find the first approach is hard to scale. The second\napproach is set up a few pipeline templates, each try to solve one specific machine\nlearning problem. The template predefines the pipeline structure including how many\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\nlearning project, the team first forks one template repo. The team leader then assigns\nmembers which step they need to work on. The data scientists and data engineers do\ntheir regular work. When they're happy with their result, they structure their code to fit\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\nexecuted or automated. If there's any change, each member only needs to work on their\npiece of code without touching the rest of the pipeline code.\nOnce a team has built a collection of machine learnings pipelines and reusable\ncomponents, they could start to build the machine learning pipeline from cloning\nprevious pipeline or tie existing reusable component together. At this stage, the team's\noverall productivity will be improved significantly.\nAzure Machine Learning offers different methods to build a pipeline. For users who are\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\npipelines by using registered components .\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\nfollowing table lists the different pipelines and what they're used for:\nScenar io Primar y\npersonaAzur e\nofferingOSS\nofferingCanonical\npipeStrengths\nModel\norchestration\n(Machine\nlearning)Data\nscientistAzure\nMachine\nLearning\nPipelinesKubeflow\nPipelinesData ->\nModelDistribution,\ncaching, code-first,\nreuse\nData\norchestrationData\nengineerAzure Data\nFactoryApache\nAirflowData -> Data Strongly typed\nmovement, data-Which Azure pipeline technology should I use?\n\uff89Expand table", "original notebook or Python code, changes the training input from local data to\nparameterized values, split the training code into multiple steps as needed, perform unit\ntest of each step, and finally wraps all steps into a pipeline.\nOnce the teams get familiar with pipelines and want to do more machine learning\nprojects using pipelines, they'll find the first approach is hard to scale. The second\napproach is set up a few pipeline templates, each try to solve one specific machine\nlearning problem. The template predefines the pipeline structure including how many\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\nlearning project, the team first forks one template repo. The team leader then assigns\nmembers which step they need to work on. The data scientists and data engineers do\ntheir regular work. When they're happy with their result, they structure their code to fit\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\nexecuted or automated. If there's any change, each member only needs to work on their\npiece of code without touching the rest of the pipeline code.\nOnce a team has built a collection of machine learnings pipelines and reusable\ncomponents, they could start to build the machine learning pipeline from cloning\nprevious pipeline or tie existing reusable component together. At this stage, the team's\noverall productivity will be improved significantly.\nAzure Machine Learning offers different methods to build a pipeline. For users who are\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\npipelines by using registered components .\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\nfollowing table lists the different pipelines and what they're used for:\nScenar io Primar y\npersonaAzur e\nofferingOSS\nofferingCanonical\npipeStrengths\nModel\norchestration\n(Machine\nlearning)Data\nscientistAzure\nMachine\nLearning\nPipelinesKubeflow\nPipelinesData ->\nModelDistribution,\ncaching, code-first,\nreuse\nData\norchestrationData\nengineerAzure Data\nFactoryApache\nAirflowData -> Data Strongly typed\nmovement, data-Which Azure pipeline technology should I use?\n\uff89Expand table"]}
{"question": "How can you optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training?", "variant_id": "v1", "line_number": 34, "answer": "To optimize data transfer between Azure Machine Learning (Azure ML) and Azure Storage services for efficient model training, you can consider the following strategies:\n\n1. **Datastore Mounting**: Use Azure ML datastores to mount Azure Storage (Blob or File storage) directly to your Azure ML compute instances. This allows you to access your data without the need to copy it over the network, which can save time and reduce data transfer costs.\n\n2. **Data Caching**: When using Azure ML Pipelines, you can cache the intermediate data at various steps of the pipeline. This means that if a part of the pipeline has already been run and the data hasn't changed, the pipeline can use the cached data instead of transferring it again.\n\n3. **Optimized Data Formats**: Store your data in optimized formats such as Apache Parquet or ORC, which are designed for efficient data storage and retrieval. These formats can reduce the amount of data that needs to be transferred and can be read into Azure ML more quickly than other formats.\n\n4. **Incremental Data Transfer**: If your data changes incrementally, consider transferring only the new or changed data instead of the entire dataset. This can be achieved by using Azure Data Factory or Azure Logic Apps to automate the incremental data transfer process.\n\n5. **Network Optimization**: Use Azure's high-bandwidth networking options if you are transferring large amounts of data. For example, Azure ExpressRoute provides a private, high-speed connection between your on-premises infrastructure and Azure services.\n\n6. **Parallelization**: When transferring data, use parallelization to split the data into chunks and transfer them simultaneously. This can significantly speed up the data transfer process.\n\n7. **Proximity of Services**: Ensure that your Azure ML workspace and Azure Storage are in the same Azure region to minimize data transfer latency and costs.\n\n8. **Azure Blob Storage's Hierarchical Namespace**: If you are using Azure Data Lake Storage Gen2, take advantage of the hierarchical namespace feature, which can improve the performance of directory management operations and is beneficial for big data workloads.\n\nBy implementing these strategies, you can optimize the data transfer process and make your model training in Azure ML more efficient.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps", "12,000 Mbps) would load the same data in ~33 secs (assuming you saturate the NIC at 1500-MB/s\nthroughput).\nStorage tier : For most scenarios - including Large Language Models (LLM) - standard storage provides\nthe best cost/performance profile. However, if you have many small files , premium  storage offers a\nbetter cost/performance profile. For more information, read Azure S torage options .\nStorage lo ad: If the storage account is under high load - for example, many GPU nodes in a cluster\nrequesting data - then you risk hitting the egress capacity of storage. For more information, read\nStorage load . If you have many small files that need access in parallel, you might hit the request limits\nof storage. R ead up-to-date information on the limits for both egress capacity and storage requests in\nScale targets for standard storage accounts .\nData access p attern in user code : When you use mount mode, data is fetched based on the open/read\nactions in your code. For example, when reading random sections of a large file, the default data\nprefetching settings of mounts can lead to downloads of blocks that won't be read. Y ou might need to\ntune some settings to reach maximum throughput. For more information, read Optimum mount\nsettings for common scenarios .\nTo access the logs of the data runtime from your job:\n1. Select Outputs+Logs  tab from the job page.\n2. Select the system_logs  folder, followed by data_cap ability  folder.\n3. You should see two log files:\nThe log file data-cap ability .log shows the high-level information about the time spent on key data loading\ntasks. For example, when you download data, the runtime logs the download activity start and finish times:\nlog\nIf the download throughput is a fraction of the expected network bandwidth for the VM size, you can inspect\nthe log file rslex.log.<TIMEST AMP> . This file contains all the fine-grain logging from the Rust-based\nruntime; for example, parallelization:\nlogUsing logs to diagnose issues\n\uf80a\nINFO 2023-05-18 17:14:47,790 sdk_logger.py:44 [28] - ActivityStarted, download\nINFO 2023-05-18 17:14:50,295 sdk_logger.py:44 [28] - ActivityCompleted: Activity=download,  \nHowEnded=Success, Duration=2504.39 [ms]", "Manage and optimize Azure Machine\nLearning costs\nArticle \u202208/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.\uff09 Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"]}
{"question": "What considerations should be taken into account for data privacy and compliance when integrating Azure Machine Learning with Azure Storage?", "variant_id": "v1", "line_number": 35, "answer": "When integrating Azure Machine Learning with Azure Storage for data privacy and compliance, the following considerations should be taken into account:\n\n1. **Encryption at Rest**: Ensure that all data stored in Azure Blob Storage is encrypted at rest. Azure Storage encrypts data with Microsoft-managed keys by default. For additional control, you can use customer-managed keys in Azure Key Vault.\n\n2. **Encryption in Transit**: Data should be encrypted in transit to and from Azure Storage. Azure Machine Learning and Azure Storage use secure protocols such as HTTPS to protect data as it moves between services.\n\n3. **Access Control**: Implement proper access control mechanisms. Use Azure Role-Based Access Control (RBAC) to define who has access to your Azure Storage data. Ensure that only authorized Azure Machine Learning compute resources and personnel can access sensitive data.\n\n4. **Datastore Configuration**: When configuring datastores in Azure Machine Learning, ensure that they are linked to secure and compliant Azure Storage accounts.\n\n5. **Monitoring and Auditing**: Enable Azure Monitor and Azure Storage logging to track access and usage of data. Regularly audit these logs to detect and respond to any unauthorized access or anomalies.\n\n6. **Data Residency and Compliance Standards**: Be aware of data residency requirements and ensure that your data is stored in a region that complies with those requirements. Also, ensure that Azure Storage and Azure Machine Learning configurations comply with industry standards and regulations such as GDPR, HIPAA, or others relevant to your organization.\n\n7. **Key Management**: Regularly rotate and manage encryption keys. If you are using customer-managed keys, you should have a process in place for key rotation and revocation. Azure Key Vault can help manage this process.\n\n8. **Secure Compute Environments**: For production-grade encryption during training, use an Azure Machine Learning compute cluster, and for inference, use Azure Kubernetes Service (AKS) to ensure that the compute environments are secure.\n\n9. **Data Handling Policies**: Establish clear data handling policies for how data is accessed, shared, and disposed of within Azure Machine Learning and Azure Storage.\n\n10. **Backup and Recovery**: Implement backup and recovery strategies to protect against data loss. Ensure that backups are also encrypted and stored securely.\n\nBy considering these points, you can help ensure that your integration of Azure Machine Learning with Azure Storage is secure and compliant with data privacy regulations.", "context": ["Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.", "Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps", "Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore"]}
{"question": "How can you use managed identities to securely access data in Azure Storage from Azure Machine Learning?", "variant_id": "v1", "line_number": 36, "answer": "To securely access data in Azure Storage from Azure Machine Learning using managed identities, you can follow these steps:\n\n1. **Enable Managed Identity on Azure Machine Learning Workspace:**\n   - In the Azure portal, navigate to your Azure Machine Learning workspace.\n   - Under the \"Settings\" section, find \"Identity\" and click on it.\n   - Here, you can enable a system-assigned managed identity for your workspace.\n\n2. **Assign Permissions to Managed Identity:**\n   - Once the managed identity is enabled, you need to grant it access to the Azure Storage account.\n   - Navigate to the Azure Storage account you want to access.\n   - Go to the \"Access control (IAM)\" section.\n   - Click on \"Add role assignment\" to assign the appropriate role to the managed identity of your Azure Machine Learning workspace. For data access, roles like \"Storage Blob Data Reader\" or \"Storage Blob Data Contributor\" are commonly used.\n\n3. **Configure Azure Machine Learning to Use Managed Identity:**\n   - When setting up a datastore in Azure Machine Learning, select the option to use managed identity for authentication.\n   - This can be done through the Azure Machine Learning studio or by using the Azure Machine Learning SDK.\n\n4. **Access Data in Training Jobs:**\n   - When you submit training jobs on compute targets like Azure Machine Learning Compute Instances or Compute Clusters, the managed identity will be used to authenticate and access the data in the Azure Storage account.\n   - Ensure that the compute resources are also configured to use the managed identity.\n\nBy following these steps, you can leverage managed identities to securely access Azure Storage without the need to store credentials in your Azure Machine Learning datastore. This approach enhances security by managing data access at the storage level and keeping credentials confidential.", "context": ["You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity", "Certain machine learning scenarios involve working with private data. In such cases, data\nscientists may not have direct access to data as Microsoft Entra users. In this scenario,\nthe managed identity of a compute can be used for data access authentication. In this\nscenario, the data can only be accessed from a compute instance or a machine learning\ncompute cluster executing a training job. With this approach, the admin grants the\ncompute instance or compute cluster managed identity S torage Blob Data R eader\npermissions on the storage. The individual data scientists don't need to be granted\naccess.\nTo enable authentication with compute managed identity:\nCreate compute with managed identity enabled. See the compute cluster  section,\nor for compute instance, the Assign managed identity  section.\nGrant compute managed identity at least S torage Blob Data R eader role on the\nstorage account.\nCreate any datastores with identity-based authentication enabled. See Create\ndatastores .\nOnce the identity-based authentication is enabled, the compute managed identity is\nused by default when accessing data within your training jobs. Optionally, you can\nauthenticate with user identity using the steps described in next section.\nFor information on using configuring Azure RBA C for the storage, see role-based access\ncontrols .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nWhen training on Azure Machine Learning compute clusters , you can authenticate to\nstorage with your user Microsoft Entra token.\nThis authentication mode allows you to:\uff17 Note\nThe name of the created system managed identity for compute instance or cluster\nwill be in the format /workspace-name/computes/compute-name in your Microsoft\nEntra ID.\nAccess data for training jobs on compute clusters using\nuser identity", "means that in order to successfully read data from storage, the managed identity of the\ncompute cluster where the deployment is running must have at least Storage Blob Data\nReader  access to the storage account.\nTo configure the compute cluster for data access, follow these steps:\n1. Go to Azure Machine Learning studio .\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\ndeployment is using.\n3. Assign a managed identity to the compute cluster:\na. In the Managed identity  section, verify if the compute has a managed identity\nassigned. If not, select the option Edit.\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\nusing a S ystem-Assigned Managed Identity, it is named as \"[workspace\nname]/computes/[compute cluster name]\".\nc. Save the changes.\n4. Go to the Azure portal  and navigate to the associated storage account where the\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\nstorage account where those assets are placed.", "Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution."]}
{"question": "What are the implications of storage redundancy options in Azure Storage on data availability for Azure Machine Learning experiments?", "variant_id": "v1", "line_number": 37, "answer": "Storage redundancy options in Azure Storage can have significant implications on data availability for Azure Machine Learning experiments. Here are some key points to consider:\n\n1. **Locally Redundant Storage (LRS)**: This option replicates your data within a single storage scale unit in a single data center. While it offers the lowest cost, it also has the highest risk in terms of data availability. If the data center faces an outage or a disaster, your Azure Machine Learning experiments could be disrupted due to data unavailability.\n\n2. **Zone-Redundant Storage (ZRS)**: ZRS replicates your data across multiple data centers within or across regions. This increases data availability and fault tolerance for your experiments. If one zone is compromised, your data is still accessible from another zone, ensuring that your Azure Machine Learning experiments can continue to run.\n\n3. **Geo-Redundant Storage (GRS)** and **Geo-Zone-Redundant Storage (GZRS)**: These options provide the highest level of data durability by replicating data to a secondary region that is hundreds of miles away from the primary location. In the event of a regional outage, you can still access your data, ensuring that your Azure Machine Learning experiments can be resumed with minimal disruption.\n\nChoosing the right redundancy option depends on your specific requirements for data availability, durability, and the criticality of your Azure Machine Learning experiments. It's important to balance the cost implications with the need for high availability and disaster recovery capabilities.", "context": ["setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps", "Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps", "Scenar io Requir ed\ninboundRequir ed outbound Additional\nconfiguration\nUse Azure\nKubernetes\nServiceNot applicable For information on the outbound\nconfiguration for AKS, see Secure\nAzure Kubernetes Service\ninferencing environment .\nUse Docker\nimages that\nAzure\nMachine\nLearning\nmanagesNot applicable Microsoft Artifact R egistry\nviennaglobal.azurecr.io\nglobal container registryIf the container registry\nfor your workspace is\nbehind the virtual\nnetwork, configure the\nworkspace to use a\ncompute cluster to build\nimages. For more\ninformation, see Secure\nan Azure Machine\nLearning workspace with\nvirtual networks .\nAzure Machine Learning uses multiple storage accounts. Each stores different data and\nhas a different purpose:\nYour st orage : The storage accounts in your Azure subscription store your data and\nartifacts, such as models, training data, training logs, and Python scripts. For\nexample, the default  storage account for your workspace is in your subscription.\nThe Azure Machine Learning compute instance and compute cluster access file and\nblob data in this storage over ports 445 (SMB) and 443 (HT TPS).\nWhen you're using a compute instance or compute cluster, your storage account is\nmounted as a file share via the SMB protocol. The compute instance and cluster\nuse this file share to store items like the data, models, Jupyter notebooks, and\ndatasets. The compute instance and cluster use the private endpoint when they\naccess the storage account.\nMicrosoft storage : The Azure Machine Learning compute instance and compute\ncluster rely on Azure Batch. They access storage located in a Microsoft\nsubscription. This storage is used only for the management of the compute\ninstance or cluster. None of your data is stored here. The compute instance and\ncompute cluster access the blob, table, and queue data in this storage, by using\nport 443 (HT TPS).Purposes of storage accounts", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Service Usage Example datanumber of nodes; datastore names and tags, and\ndescriptions on assets like models; and data label names.\nAzure AI\nSearchStores indexes that\nhelp with querying\nyour machine learning\ncontent.These indexes are built on top of the data stored in Azure\nCosmos DB.\nAzure\nStorageStores metadata\nrelated to Azure\nMachine Learning\npipeline data.Data can include designer pipeline names, pipeline layout,\nand execution properties.\nFrom the perspective of data lifecycle management, data in the preceding resources is\ncreated and deleted as you create and delete corresponding objects in Azure Machine\nLearning.\nYour Azure Machine Learning workspace reads and writes data by using its managed\nidentity. This identity is granted access to the resources through a role assignment\n(Azure role-based access control) on the data resources. The encryption key that you\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\nused to create indexes for Azure AI Search at runtime.\nExtra networking controls are configured when you create a private link endpoint on\nyour workspace to allow for inbound connectivity. This configuration includes the\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\nNetwork access is restricted to only trusted Microsoft services.\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\nthe data.\nWhen you use a customer-managed key, the resources are in your Azure subscription\nand encrypted with your key. While these resources exist in your subscription, Microsoft\nmanages them. They're automatically created and configured when you create your\nAzure Machine Learning workspace.\nThese Microsoft-managed resources are located in a new Azure resource group that's\ncreated in your subscription. This resource group is separate from the resource group\nfor your workspace. It contains the Microsoft-managed resources that your key is used\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\nresource group name><GUID>.Customer-managed keys"]}
{"question": "How do you automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage?", "variant_id": "v1", "line_number": 38, "answer": "To automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage, you can use Azure services such as Azure Backup or Azure Data Factory, along with Azure Blob Storage lifecycle management policies. Here's a general approach to automate this process:\n\n1. **Azure Backup:**\n   - You can use Azure Backup to create backup policies for Azure Blob Storage. This service allows you to define the frequency and retention period for backups.\n   - Set up a Recovery Services vault in Azure and configure the backup policy for the storage account where your datasets are stored.\n   - Azure Backup will automatically take snapshots based on the policy you set, and you can recover the data from these snapshots if needed.\n\n2. **Azure Data Factory:**\n   - Create a data pipeline in Azure Data Factory that copies your datasets from the primary storage account to a secondary storage account for backup purposes.\n   - Schedule the pipeline to run at regular intervals, ensuring that your datasets are backed up consistently.\n   - In case of data loss or corruption, you can use the copied data in the secondary storage account to restore your datasets.\n\n3. **Azure Blob Storage Lifecycle Management:**\n   - Configure lifecycle management policies on your Azure Blob Storage account to automatically move or delete data based on rules that you define.\n   - For example, you can create a rule to move older versions of your datasets to a cooler storage tier (such as Cool or Archive) for cost-effective long-term storage or to delete outdated backups.\n\n4. **Azure Automation:**\n   - Use Azure Automation to run runbooks that can manage the backup process. You can write scripts that use Azure PowerShell or Azure CLI to copy data to a backup location.\n   - Schedule these runbooks to run at specific intervals to ensure regular backups.\n\n5. **Monitoring and Alerts:**\n   - Set up monitoring and alerts using Azure Monitor to track the status of your backup jobs and get notified in case of failures or issues.\n\n6. **Versioning:**\n   - Enable versioning on your Azure Blob Storage to keep track of different versions of your datasets. This can be useful for recovery in case you need to revert to a previous version of a dataset.\n\nRemember to test your backup and recovery process regularly to ensure that it meets your recovery objectives and complies with your organization's policies. Additionally, consider the cost implications of the backup strategy you choose, as data transfer and storage can incur costs.", "context": ["Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps", "setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."]}
{"question": "What tools and techniques are available for analyzing storage costs associated with Azure Machine Learning projects?", "variant_id": "v1", "line_number": 39, "answer": "To analyze storage costs associated with Azure Machine Learning projects, you can use the following tools and techniques:\n\n1. **Azure Pricing Calculator**: Before adding any resources, you can use the Azure Pricing Calculator to estimate the costs of the storage resources you plan to use with Azure Machine Learning.\n\n2. **Cost Management Features**: After you've started using Azure Machine Learning resources, you can utilize Azure Cost Management features to monitor and manage your costs. This includes setting budgets, analyzing spending trends, and reviewing forecasted costs.\n\n3. **Cost Analysis in Azure Cost Management**: This tool supports most Azure account types and allows you to view and analyze your Azure storage costs. It provides detailed cost insights and the ability to break down costs by various factors, such as resource type or tags.\n\n4. **Azure Advisor**: Azure Advisor provides personalized recommendations to help optimize your Azure resources for cost, performance, availability, and security, including recommendations for reducing storage costs.\n\n5. **Tags and Resource Grouping**: By organizing resources with tags and resource groups, you can more easily track and allocate costs to specific projects or departments.\n\n6. **Monitoring Tools**: Azure Monitor can be used to track the performance and resource usage of your storage, which can help you make informed decisions about scaling and cost optimization.\n\nRemember to review the Azure documentation for the most current tools and best practices for managing and optimizing costs in Azure Machine Learning.", "context": ["Plan to manage costs for Azure Machine\nLearning\nArticle \u202203/31/2023\nThis article describes how to plan and manage costs for Azure Machine Learning. First,\nyou use the Azure pricing calculator to help plan for costs before you add any resources.\nNext, as you add the Azure resources, review the estimated costs.\nAfter you've started using Azure Machine Learning resources, use the cost management\nfeatures to set budgets and monitor costs. Also review the forecasted costs and identify\nspending trends to identify areas where you might want to act.\nUnderstand that the costs for Azure Machine Learning are only a portion of the monthly\ncosts in your Azure bill. If you are using other Azure services, you're billed for all the\nAzure services and resources used in your Azure subscription, including the third-party\nservices. This article explains how to plan for and manage costs for Azure Machine\nLearning. After you're familiar with managing costs for Azure Machine Learning, apply\nsimilar methods to manage costs for all the Azure services used in your subscription.\nFor more information on optimizing costs, see how to manage and optimize cost in\nAzure Machine Learning .\nCost analysis in Cost Management supports most Azure account types, but not all of\nthem. T o view the full list of supported account types, see Understand Cost\nManagement data .\nTo view cost data, you need at least read access for an Azure account. For information\nabout assigning access to Azure Cost Management data, see Assign access to data .\uff09 Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nPrerequisites", "Use the Azure pricing calculator  to estimate costs before you create the\nresources in an Azure Machine Learning workspace. On the left, select AI +\nMachine Learning , then select Azur e Machine Learning  to begin.\nThe following screenshot shows the cost estimation by using the calculator:\nAs you add new resources to your workspace, return to this calculator and add the same\nresource here to update your cost estimates.\nFor more information, see Azure Machine Learning pricing .\nAzure Machine Learning runs on Azure infrastructure that accrues costs along with\nAzure Machine Learning when you deploy the new resource. It's important toEstimate costs before using Azure Machine\nLearning\nUnderstand the full billing  model for Azure\nMachine Learning", "Manage and optimize Azure Machine\nLearning costs\nArticle \u202208/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.\uff09 Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)", "monthly schedule and set a custom date range. Exporting cost data is the\nrecommended way to retrieve cost datasets.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nDelete instances and clusters if you do not plan on using them in the near future.\nFor more information, see manage and optimize costs in Azure Machine Learning .\nManage and optimize costs in Azure Machine Learning .\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscale\nLearn how to optimize your cloud investment with Azure Cost Management .\nLearn more about managing costs with cost analysis .\nLearn about how to prevent unexpected costs .\nTake the Cost Management  guided learning course.Other ways to manage and reduce costs for\nAzure Machine Learning\nNext steps", "To view Azure Machine Learning costs in cost analysis:\n1. Sign in to the Azure portal.\n2. Open the scope in the Azure portal and select Cost analysis  in the menu. For\nexample, go to Subscriptions , select a subscription from the list, and then select\nCost analysis  in the menu. Select Scope  to switch to a different scope in cost\nanalysis.\n3. By default, cost for services are shown in the first donut chart. Select the area in the\nchart labeled Azure Machine Learning.\nActual monthly costs are shown when you initially open cost analysis. Here's an example\nshowing all monthly usage costs.\nTo narrow costs for a single service, like Azure Machine Learning, select Add filt er and\nthen select Service name . Then, select virtual machines .\nHere's an example showing costs for just Azure Machine Learning.\n\uf80a"]}
{"question": "How can data engineers streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics?", "variant_id": "v1", "line_number": 40, "answer": "Data engineers can streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics by leveraging several Azure services and features that work together to create an efficient data pipeline. Here are some steps and best practices to consider:\n\n1. **Azure Data Lake Storage (ADLS) Gen 2**: Utilize ADLS Gen 2 for large-scale analytics. It provides a hierarchical file system, which supports both the Blob storage and Data Lake storage models, making it an optimal choice for machine learning workloads.\n\n2. **Azure Synapse Analytics**: Integrate Azure Machine Learning with Azure Synapse Analytics to access an Apache Spark pool for interactive data wrangling. This allows for the processing of large volumes of data in a scalable manner.\n\n3. **Azure Machine Learning Notebooks**: Use Azure Machine Learning Notebooks for data exploration and transformation. These notebooks can be connected to Synapse Spark pools for interactive data wrangling.\n\n4. **Azure Key Vault**: Store sensitive information such as storage account access keys, Shared Access Signature (SAS) tokens, and service principal credentials securely in Azure Key Vault. This ensures that your data ingestion pipeline is secure and that credentials are managed properly.\n\n5. **Service Principals**: Create a service principal for authentication without interactive login. This allows automated tools and services, like Azure Machine Learning pipelines, to access Azure resources securely.\n\n6. **Role-Based Access Control (RBAC)**: Configure RBAC on Azure Storage accounts to control access to data. Assign appropriate roles to the service principal used by Azure Machine Learning to ensure it has the necessary permissions for data ingestion.\n\n7. **Real-Time Analytics**: For real-time analytics, consider using Azure Stream Analytics to process data in real-time from Azure Blob Storage or Event Hubs and then send the processed data to Azure Machine Learning for predictive analytics.\n\n8. **Azure Event Hubs or Azure IoT Hub**: If the data ingestion is from streaming data sources, use Azure Event Hubs or Azure IoT Hub to ingest real-time data streams.\n\n9. **Azure Machine Learning Pipelines**: Create Azure Machine Learning pipelines to automate the data ingestion and model training processes. Pipelines can be scheduled or triggered by events, ensuring that the latest data is used for real-time analytics.\n\n10. **Monitoring and Logging**: Implement monitoring and logging using Azure Monitor and Azure Log Analytics to track the performance and health of your data ingestion pipeline.\n\nBy following these steps, data engineers can create a robust and streamlined data ingestion process that feeds into Azure Machine Learning for real-time analytics, ensuring that the data is ready for model training and scoring with minimal latency.", "context": ["Interactive Data Wrangling with Apache\nSpark in Azure Machine Learning\nArticle \u202210/05/2023\nData wrangling becomes one of the most important steps in machine learning projects.\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides access\nto an Apache Spark pool - backed by Azure S ynapse - for interactive data wrangling\nusing Azure Machine Learning Notebooks.\nIn this article, you'll learn how to perform data wrangling using\nServerless Spark compute\nAttached S ynapse Spark pool\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an Azure\nData Lake S torage (ADLS) Gen 2 storage account .\n(Optional): An Azure K ey Vault. See Create an Azure K ey Vault.\n(Optional): A Service Principal. See Create a Service Principal .\n(Optional): An attached S ynapse Spark pool in the Azure Machine Learning\nworkspace .\nBefore you start your data wrangling tasks, learn about the process of storing secrets\nAzure Blob storage account access key\nShared Access Signature (SAS) token\nAzure Data Lake S torage (ADLS) Gen 2 service principal information\nin the Azure K ey Vault. Y ou also need to know how to handle role assignments in the\nAzure storage accounts. The following sections review these concepts. Then, we'll\nexplore the details of interactive data wrangling using the Spark pools in Azure Machine\nLearning Notebooks.Prerequisites\n\uea80 Tip", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "runs from experimentation in the studio user interface. Or they can use versioned assets\nfor jobs like environments and storage references.\nFor more information, see Manage Azure Machine Learning workspaces .\nWhen a project is ready for operationalization, users' work can be automated in an ML\npipeline and triggered on a schedule or HT TPS request.\nYou can deploy models to the managed inferencing solution, for both real-time and\nbatch deployments, abstracting away the infrastructure management typically required\nfor deploying models.\nIn Azure Machine Learning, you can run your training script in the cloud or build a\nmodel from scratch. Customers often bring models they've built and trained in open-\nsource frameworks so that they can operationalize them in the cloud.\nData scientists can use models in Azure Machine Learning that they've created in\ncommon Python frameworks, such as:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nLightGBM\nOther languages and frameworks are also supported:\nR\n.NET\nFor more information, see Open-source integration with Azure Machine Learning .\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\nexperience and intuition to select the right data featurization and algorithm for training.\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\nLearning studio UI or the Python SDK.Train models\nOpen and interoperable\nAutomated featurization and algorithm selection", "training and evaluation, the machine learning engineers focus on model deployment\nand automation of the entire workflow. By leveraging machine learning pipeline, each\nteam only needs to work on building their own steps. The best way of building steps is\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\none step in a machine learning pipeline. All these steps built by different users are finally\nintegrated into one workflow through the pipeline definition. The pipeline is a\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\nits steps can be standardized by each company's preferred DevOps practice. The\npipeline can be further versioned and automated. If the ML projects are described as a\npipeline, then the best ML Ops practice is already applied.\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\nimproves large model training's efficiency and reduces cost. T aking modern natural\nlanguage model training as an example. It requires pre-processing large amounts of\ndata and GPU intensive transformer model training. It takes hours to days to train a\nmodel each time. When the model is being built, the data scientist wants to test\ndifferent training code or hyperparameters and run the training many times to get the\nbest model performance. For most of these trainings, there's usually small changes from\none training to another one. It will be a significant waste if every time the full training\nfrom data processing to model training takes place. By using machine learning pipeline,\nit can automatically calculate which steps result is unchanged and reuse outputs from\nprevious training. Additionally, the machine learning pipeline supports running each\nstep on different computation resources. Such that, the memory heavy data processing\nwork and run-on high memory CPU machines, and the computation intensive training\ncan run on expensive GPU machines. By properly choosing which step to run on which\ntype of machines, the training cost can be significantly reduced.\nDepending on what a machine learning project already has, the starting point of\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\nbuilding a pipeline.\nThe first approach usually applies to the team that hasn't used pipeline before and\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\ntypically have developed some machine learning models on their local environment\nusing their favorite tools. Machine learning engineers need to take data scientists'\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\nGetting  started best practices", "training and evaluation, the machine learning engineers focus on model deployment\nand automation of the entire workflow. By leveraging machine learning pipeline, each\nteam only needs to work on building their own steps. The best way of building steps is\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\none step in a machine learning pipeline. All these steps built by different users are finally\nintegrated into one workflow through the pipeline definition. The pipeline is a\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\nits steps can be standardized by each company's preferred DevOps practice. The\npipeline can be further versioned and automated. If the ML projects are described as a\npipeline, then the best ML Ops practice is already applied.\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\nimproves large model training's efficiency and reduces cost. T aking modern natural\nlanguage model training as an example. It requires pre-processing large amounts of\ndata and GPU intensive transformer model training. It takes hours to days to train a\nmodel each time. When the model is being built, the data scientist wants to test\ndifferent training code or hyperparameters and run the training many times to get the\nbest model performance. For most of these trainings, there's usually small changes from\none training to another one. It will be a significant waste if every time the full training\nfrom data processing to model training takes place. By using machine learning pipeline,\nit can automatically calculate which steps result is unchanged and reuse outputs from\nprevious training. Additionally, the machine learning pipeline supports running each\nstep on different computation resources. Such that, the memory heavy data processing\nwork and run-on high memory CPU machines, and the computation intensive training\ncan run on expensive GPU machines. By properly choosing which step to run on which\ntype of machines, the training cost can be significantly reduced.\nDepending on what a machine learning project already has, the starting point of\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\nbuilding a pipeline.\nThe first approach usually applies to the team that hasn't used pipeline before and\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\ntypically have developed some machine learning models on their local environment\nusing their favorite tools. Machine learning engineers need to take data scientists'\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\nGetting  started best practices"]}
