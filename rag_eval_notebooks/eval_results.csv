llm_name,line_number,gpt_relevance,gpt_groundedness,gpt_retrieval_score,question,variant_id,answer,context,total_tokens
gpt-35-turbo,1,,5.0,,How do you initiate a new Azure ML experiment using the Azure CLI?,v1,"To initiate a new Azure ML experiment using the Azure CLI, you would typically follow these steps:

1. Ensure you have the Azure CLI installed along with the ML extension by running the command:
   ```
   az extension add -n ml
   ```

2. Set your Azure subscription context and configure defaults for your workspace, resource group, and location:
   ```
   az account set --subscription <subscription-id>
   az configure --defaults workspace=<workspace-name> group=<resource-group-name> location=<location-name>
   ```

3. Create or select an existing compute target for running your experiment.

4. Start a new experiment by creating a job definition that specifies the details of the experiment, such as the compute target, environment, code, and data inputs.

5. Submit the job to run the experiment:
   ```
   az ml job create --file <job-definition-file.yml>
   ```

The job definition file (`<job-definition-file.yml>`) will contain the configuration for the experiment, including the script to run, the compute target, environment, and any datasets or parameters required.

Please note that the specific commands and parameters might vary based on the Azure ML CLI version and the configuration of your Azure ML workspace. Always refer to the latest Azure ML documentation for the most accurate and up-to-date information.","[""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\nAzure CLI\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\nso one cluster can host one or many batch deployments (along with other workloads if\ndesired).\nThis article uses a compute created here named batch-cluster. Adjust as needed and\nreference your compute using azureml:<your-compute-name> or create one as shown.\nAzure CLIaz extension add -n ml \nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>  \naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>  \nCreate compute\nAzure CLI\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \n0 --max-instances  5 \n７ Note"", 'Azure CLI\nIf you\'re not on the compute instance, omit --identity and follow the prompt to\nopen a browser window to authenticate.\n3. Make sure you have the most recent versions of the CLI and the ml extension:\nAzure CLI\n4. If you have multiple Azure subscriptions, set the active subscription to the one\nyou\'re using for your workspace. (Y ou can skip this step if you only have access to\na single subscription.) R eplace <SUBSCRIPTION-NAME> with your subscription name.\nAlso remove the brackets <>.\nAzure CLI\n5. Now use CLI to submit the job. If you\'re doing this on a compute instance in your\nworkspace, you can use environment variables for the workspace name and\nresource group as show in the following code. If you aren\'t on a compute instance,\nreplace these values with your workspace name and resource group.\nAzure CLI\nOnce you\'ve submitted the job, you can check the status and results in studio:\n1. Sign in to Azure Machine Learning studio .\n2. Select your workspace if it isn\'t already loaded.\n3. On the left navigation, select Jobs.\n4. Select the Experiment name  that you used to train your model.\n5. Select the Display name  of the job to view details and artifacts of the job,\nincluding metrics, images, child jobs, outputs, logs, and code used in the job.az login --identity\naz upgrade\naz account set --subscription  ""<SUBSCRIPTION-NAME>""\naz ml job create -f job.yml  --workspace-name  $CI_WORKSPACE  --resource-\ngroup $CI_RESOURCE_GROUP\nRegister model', ""Azure CLI\nIf you don't have it, create a cluster called cpu-cluster by running:\nAzure CLI\nNow, create a pipeline job defined in the pipeline.yml file with the following command.\nThe compute target is referenced in the pipeline.yml file as azureml:cpu-cluster. If your\ncompute target uses a different name, remember to update it in the pipeline.yml file.\nAzure CLI\nYou should receive a JSON dictionary with information about the pipeline job including:\nKey Descr iption\nname The GUID-based name of the job.\nexperiment_name The name under which jobs will be organized in studio.\nservices.Studio.endpoint A URL for monitoring and reviewing the pipeline job.\nstatus The status of the job. This will likely be Preparing at this point.\nOpen the services.Studio.endpoint URL to see a graph visualization of the pipeline.az ml compute list\n７ Note\nSkip this step to use serverless comput e.\naz ml compute create -n cpu-cluster  --type amlcompute --min-instances  0 --\nmax-instances  10\naz ml job create --file pipeline.yml\nﾉExpand table"", ""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nIn this example, we are going to create a deployment that can write directly to the\noutput folder of the batch deployment job. The deployment will use this feature to write\ncustom parquet files.Azure CLI\naz extension add -n ml\n７ Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nCreating a batch deployment with a custom\noutput"", ""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\ninference over new data:\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\n７ Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nSteps""]",1524
gpt-35-turbo,2,,5.0,,Describe the process of creating a new compute instance via the Azure ML CLI. What parameters are necessary?,v1,"To create a new compute instance via the Azure Machine Learning CLI, you would typically follow these steps and include necessary parameters:

1. **Prerequisites**: Before you begin, ensure you have an Azure Machine Learning workspace and the Azure CLI with the ML extension installed.

2. **Login**: Authenticate with Azure using the CLI by running `az login`.

3. **Set the active subscription**: Use `az account set --subscription ""your-subscription-name""` to set the subscription where your workspace resides.

4. **Create the compute instance**: Use the `az ml compute create` command with the necessary parameters. Here are some of the key parameters you might include:

   - `--name` or `-n`: The name of the compute instance.
   - `--type`: The type of compute resource, which should be set to `computeinstance` for creating a compute instance.
   - `--size`: The Virtual Machine size for the compute instance.
   - `--workspace-name` or `-w`: The name of the Azure Machine Learning workspace.
   - `--resource-group` or `-g`: The name of the resource group where the workspace is located.

An example command might look like this:

```sh
az ml compute create --name my-compute-instance --type computeinstance --size Standard_DS11_v2 --workspace-name my-aml-workspace --resource-group my-resource-group
```

This command will create a new compute instance named `my-compute-instance` with the specified VM size in the given workspace and resource group.

Remember that the exact parameters and their usage might vary depending on the version of the Azure CLI and the ML extension you are using. Always refer to the official Azure CLI documentation for the most accurate and up-to-date information.","['Create an Azure Machine Learning\ncompute instance\nArticle •12/08/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nLearn how to create a compute instance  in your Azure Machine Learning workspace.\nUse a compute instance as your fully configured and managed development\nenvironment in the cloud. For development and testing, you can also use the instance as\na training compute target . A compute instance can run multiple jobs in parallel and has\na job queue. As a development environment, a compute instance can\'t be shared with\nother users in your workspace.\nIn this article, you learn how to create a compute instance. See Manage an Azure\nMachine Learning compute instance  for steps to manage start, stop, restart, delete a\ncompute instance.\nYou can also use a setup script  to create the compute instance with your own custom\nenvironment.\nCompute instances can run jobs securely in a virtual network environment , without\nrequiring enterprises to open up SSH ports. The job executes in a containerized\nenvironment and packages your model dependencies in a Docker container.\nAn Azure Machine Learning workspace. For more information, see Create an Azure\nMachine Learning workspace . In the storage account, the ""Allow storage account\nkey access"" option must be enabled for compute instance creation to be\nsuccessful.\nChoose the tab for the environment you\'re using for other prerequisites.\n７ Note\nThis article uses CLI v2 in some examples. If you are still using CLI v1, see Create an\nAzur e Machine Learning comput e clust er CLI v1) .\nPrerequisites', ""For more information on creating a compute cluster and compute cluster, including how\nto do so with Python and the CLI, see the following articles:\nCreate a compute cluster\nCreate a compute instance\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nWhen Azure Container R egistry is behind the virtual network, Azure Machine Learning\ncan't use it to directly build Docker images (used for training and deployment). Instead,\nconfigure the workspace to use the compute cluster you created earlier. Use the\nfollowing steps to create a compute cluster and configure the workspace to use it to\nbuild images:\n1. Navigate to https://shell.azure.com/  to open the Azure Cloud Shell.\n2. From the Cloud Shell, use the following command to install the 2.0 CLI for Azure\nMachine Learning:\nAzure CLIAllow inbound T CP traffic on ports 29876-29877 from the\nBatchNodeManagement service tag.\nAllow inbound T CP traffic on port 44224 from the AzureMachineLearning\nservice tag.\nThe following screenshot shows an example of these rules:\nConfigure image builds\naz extension add -n ml"", ""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\nAzure CLI\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\nso one cluster can host one or many batch deployments (along with other workloads if\ndesired).\nThis article uses a compute created here named batch-cluster. Adjust as needed and\nreference your compute using azureml:<your-compute-name> or create one as shown.\nAzure CLIaz extension add -n ml \nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>  \naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>  \nCreate compute\nAzure CLI\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \n0 --max-instances  5 \n７ Note"", ""To create a workspace using CLI v2, use the following command:\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nBash\nFor more information, see workspace Y AML schema .\nA compute is a designated compute resource where you run your job or host your\nendpoint. Azure Machine Learning supports the following types of compute:\nComput e clust er - a managed-compute infrastructure that allows you to easily\ncreate a cluster of CPU or GPU compute nodes in the cloud.\nComput e instance  - a fully configured and managed development environment in\nthe cloud. Y ou can use the instance as a training or inference compute for\ndevelopment and testing. It's similar to a virtual machine on the cloud.\nInfer ence clust er - used to deploy trained machine learning models to Azure\nKubernetes Service. Y ou can create an Azure Kubernetes Service (AKS) cluster from\nyour Azure Machine Learning workspace, or attach an existing AKS cluster.\nAttached comput e - You can attach your own compute resources to your\nworkspace and use them for training and inference.\nTo create a compute using CLI v2, use the following command:\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)Azure CLI\naz ml workspace create --file my_workspace.yml  \nCompute\n７ Note\nInstead of creating a compute cluster, use serverless comput e (preview) to\noffload compute lifecycle management to Azure Machine Learning.\nAzure CLI"", 'Manage an Azure Machine Learning\ncompute instance\nArticle •07/06/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nLearn how to manage a compute instance  in your Azure Machine Learning workspace.\nUse a compute instance as your fully configured and managed development\nenvironment in the cloud. For development and testing, you can also use the instance as\na training compute target . A compute instance can run multiple jobs in parallel and has\na job queue. As a development environment, a compute instance can\'t be shared with\nother users in your workspace.\nIn this article, you learn how to start, stop, restart, delete) a compute instance. See\nCreate an Azure Machine Learning compute instance  to learn how to create a compute\ninstance.\nAn Azure Machine Learning workspace. For more information, see Create an Azure\nMachine Learning workspace . In the storage account, the ""Allow storage account\nkey access"" option must be enabled for compute instance creation to be\nsuccessful.\nThe Azure CLI extension for Machine Learning service (v2) , Azure Machine\nLearning Python SDK (v2) , or the Azure Machine Learning Visual S tudio Code\nextension .\nIf using the Python SDK, set up your development environment with a workspace .\nOnce your environment is set up, attach to the workspace in your Python script:\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\n７ Note\nThis article shows CLI v2 in the sections below. If you are still using CLI v1, see\nCreate an Azur e Machine Learning comput e clust er CLI v1) .\nPrerequisites']",1810
gpt-35-turbo,3,,5.0,,Explain how to submit a training job using Python and the AML SDK v2. Include an example of specifying a compute target.,v1,"To submit a training job using Python and the Azure Machine Learning (AML) SDK v2, you need to follow these steps:

1. **Set up your environment**:
   - Install the Azure Machine Learning SDK v2 if you haven't already.
   - Import the necessary packages from the SDK.

2. **Authenticate to Azure**:
   - Use the `DefaultAzureCredential` or `InteractiveBrowserCredential` to authenticate your session.

3. **Create or attach to an existing workspace**:
   - Use the `Workspace` class to create a new workspace or attach to an existing one.

4. **Define a compute target**:
   - Specify the compute target where the training will occur. This could be an Azure Machine Learning compute instance or a compute cluster.

5. **Create a command job**:
   - Define the command job with the necessary parameters, including the training script, environment, and compute target.

6. **Submit the job**:
   - Use the `Job` class to submit the job to the workspace.

Here's an example of how you might do this in code:

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import CommandJob, Environment, CodeConfiguration
from azure.identity import DefaultAzureCredential

# Authenticate to Azure
credential = DefaultAzureCredential()
subscription_id = ""<your-subscription-id>""
resource_group = ""<your-resource-group>""
workspace_name = ""<your-workspace-name>""

# Create MLClient
ml_client = MLClient(credential, subscription_id, resource_group, workspace_name)

# Define the compute target
compute_target = ""your-compute-target-name""

# Define the environment for the job
environment = Environment(
    image=""mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04"",
    conda_file=""path/to/your/conda_file.yml"",
    name=""my-environment"",
    version=""1""
)

# Define the command job
command_job = CommandJob(
    code_configuration=CodeConfiguration(
        code=""path/to/your/training_script_folder"",
        command=""python train.py""
    ),
    environment_id=environment.id,
    compute=compute_target,
    display_name=""my-training-job"",
    experiment_name=""my-experiment""
)

# Submit the job
returned_job = ml_client.jobs.create_or_update(command_job)

# Get the status of the job
job_status = ml_client.jobs.get(returned_job.name)
print(f""Job status: {job_status.status}"")
```

In this example, replace the placeholders (e.g., `<your-subscription-id>`, `<your-resource-group>`, etc.) with your actual Azure subscription ID, resource group, workspace name, and other relevant details. The `compute_target` should be the name of the compute resource you've set up in Azure Machine Learning for training. The `conda_file` should point to a Conda environment file that specifies the dependencies for your training script. The `code` parameter should point to the directory containing your training script, and the `command` parameter specifies the command to run the script (`train.py` in this case).

Please note that the Azure ML SDK and services are continuously updated, and the code provided here is based on the state of the SDK as of the knowledge cutoff date. Always refer to the latest Azure ML documentation for the most current information and APIs.","[""1. On the top bar above your opened notebook, create a compute instance if you\ndon't already have one.\n2. If the compute instance is stopped, select Start comput e and wait until it is\nrunning.\n3. Make sure that the kernel, found on the top right, is Python 3.10 - SDK v2. If not,\nuse the dropdown to select this kernel.\n4. If you see a banner that says you need to be authenticated, select Authenticat e.\nTo train a model, you need to submit a job. The type of job you'll submit in this tutorial\nis a command job . Azure Machine Learning offers several different types of jobs to train\nmodels. Users can select their method of training based on complexity of the model,\ndata size, and training speed requirements. In this tutorial, you'll learn how to submit a\ncommand job  to run a training s cript.\nA command job is a function that allows you to submit a custom training script to train\nyour model. This can also be defined as a custom training job. A command job in Azure\nMachine Learning is a type of job that runs a script or command in a specified\nenvironment. Y ou can use command jobs to train models, process data, or any other\ncustom code you want to execute in the cloud.Set your kernel\n\uf80a\n\uf80a\n\uf80a\n） Impor tant\nThe rest of this tutorial contains cells of the tutorial notebook. Copy/paste them\ninto your new notebook, or switch to the notebook now if you cloned it.\nUse a command job to train a model in Azure\nMachine Learning"", 'For detailed instructions on how to complete these steps, see Secure a training\nenvironment .\nIn this section, you learn how Azure Machine Learning securely communicates between\nservices to submit a training job. This example shows you how all your configurations\nwork together to secure communication.\n1. The client uploads training scripts and training data to storage accounts that are\nsecured with a service or private endpoint.\n2. The client submits a training job to the Azure Machine Learning workspace\nthrough the private endpoint.\n3. Azure Batch service receives the job from the workspace. It then submits the\ntraining job to the compute environment through the public load balancer for the\ncompute resource.\n4. The compute resource receives the job and begins training. The compute resource\nuses information stored in key vault to access storage accounts to download\ntraining files and upload output.Example training job submission', 'Python\nView more examples of training with serverless compute at:-\nQuick S tart\nTrain Model\nThere\'s no need to specify compute for AutoML jobs. R esources can be optionally\nspecified. If instance count isn\'t specified, then it\'s defaulted based on\nmax_concurrent_trials and max_nodes parameters. If you submit an AutoML image\nclassification or NLP task with no instance type, the GPU VM size is automatically\nselected. It\'s possible to submit AutoML job through CLIs, SDK, or S tudio. T o submit\nAutoML jobs with serverless compute in studio first enable the submit a training job in\nstudio (preview)  feature in the preview panel.Python SDK\nfrom azure.ai.ml import command\nfrom azure.ai.ml import MLClient      # Handle to the workspace\nfrom azure.identity import DefaultAzureCredential     # Authentication  \npackage\nfrom azure.ai.ml.entities import ResourceConfiguration\nfrom azure.ai.ml.entities import UserIdentityConfiguration \ncredential = DefaultAzureCredential()\n# Get a handle to the workspace. You can find the info on the workspace  \ntab on ml.azure.com\nml_client = MLClient(\n    credential=credential,\n    subscription_id= ""<Azure subscription id>"" , \n    resource_group_name= ""<Azure resource group>"" ,\n    workspace_name= ""<Azure Machine Learning Workspace>"" ,\n)\njob = command(\n    command= ""echo \'hello world\'"" ,\n    environment= ""AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest"" ,\n         identity=UserIdentityConfiguration(),\n    queue_settings={\n      ""job_tier"" : ""Standard""   \n    }\n)\njob.resources = ResourceConfiguration(instance_type= ""Standard_E4s_v3"" , \ninstance_count= 1)\n# submit the command job\nml_client.create_or_update(job)\nAutoML job', ""Designer : Azure Machine Learning designer provides an easy entry-point into\nmachine learning for building proof of concepts, or for users with little coding\nexperience. It allows you to train models using a drag and drop web-based UI. Y ou\ncan use Python code as part of the design, or train models without writing any\ncode.\nAzur e CLI : The machine learning CLI provides commands for common tasks with\nAzure Machine Learning, and is often used for scripting and aut omating tasks . For\nexample, once you've created a training script or pipeline, you might use the Azure\nCLI to start a training job on a schedule or when the data files used for training are\nupdated. For training models, it provides commands that submit training jobs. It\ncan submit jobs using run configurations or pipelines.\nEach of these training methods can use different types of compute resources for\ntraining. Collectively, these resources are referred to as comput e targets. A compute\ntarget can be a local machine or a cloud resource, such as an Azure Machine Learning\nCompute, Azure HDInsight, or a remote virtual machine.\nThe Azure Machine Learning SDK for Python allows you to build and run machine\nlearning workflows with Azure Machine Learning. Y ou can interact with the service from\nan interactive Python session, Jupyter Notebooks, Visual S tudio Code, or other IDE.\nInstall/update the SDK\nConfigure a development environment for Azure Machine Learning\nA generic training job with Azure Machine Learning can be defined using the\ncommand() . The command is then used, along with your training script(s) to train a\nmodel on the specified compute target.\nYou may start with a command for your local computer, and then switch to one for a\ncloud-based compute target as needed. When changing the compute target, you only\nchange the compute parameter in the command that you use. A run also logs\ninformation about the training job, such as the inputs, outputs, and logs.\nTutorial: T rain your first ML model\nExamples: Jupyter Notebook and Python examples of training modelsPython SDK\nSubmit a command\nAutomated Machine Learning"", 'An Azure Machine Learning compute cluster is a fully managed compute resource that\ncan be used to run the training job. In the following examples, a compute cluster named\ncpu-compute is created.\nPython\nTo run this script, you\'ll use a command that executes main.py Python script located\nunder ./sdk/python/jobs/single-step/lightgbm/iris/src/. The command will be runworkspace = \'<AZUREML_WORKSPACE_NAME>\'\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \nresource_group, workspace)\n2. Create a compute resource for training\n７ Note\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\ntraining job .\nPython SDK\nfrom azure.ai.ml.entities import AmlCompute\n# specify aml compute name.\ncpu_compute_target = ""cpu-cluster""\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(""Creating a new cpu compute target..."" )\n    compute = AmlCompute(\n        name=cpu_compute_target, size= ""STANDARD_D2_V2"" , min_instances= 0, \nmax_instances= 4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n4. Submit the training job\nPython SDK']",2301
gpt-35-turbo,4,,5.0,,What is the command to list all the models registered in an Azure ML Workspace using the Azure CLI?,v1,"The command to list all the models registered in an Azure ML Workspace using the Azure CLI is:

```
az ml model list
```","['then select Regist er.\nThe SDK and CLI (v2) also allow you to manage the lifecycle of your Azure Machine\nLearning model assets.\nList all the models in your workspace:\ncli\nList all the model versions under a given name:\ncli\n\uf80a\nManage models\nList\nAzure CLI\naz ml model list  \nAzure CLI\naz ml model list --name run-model-example', 'Install the Azure CLI and the ml extension to the Azure CLI. For more\ninformation on installing the CLI, see Install and set up the CLI (v2) .\nFirst, connect to the Azure Machine Learning workspace where you\'ll work.\nAzure CLI\nYou can deploy only registered models to online endpoints. In this case, you already\nhave a local copy of the model in the repository, so you only need to publish the model\nto the registry in the workspace. Y ou can skip this step if the model you\'re trying to\ndeploy is already registered.\nAzure CLI\nIf your model was logged inside of a run, you can register it directly.\nTo register the model, you need to know the location where it is stored. If you\'re using\nMLflow\'s autolog feature, the path to the model depends on the model type and\nframework. Y ou should check the jobs output to identify the name of the model\'s folder.\nThis folder contains a file named MLModel.Connect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nRegister the model\nAzure CLI\nMODEL_NAME= \'sklearn-diabetes\'\naz ml model create --name $MODEL_NAME  --type ""mlflow_model""  --path \n""endpoints/online/ncd/sklearn-diabetes/model""\nWhat if your model was logged inside of a run?', 'Install the Azure CLI and the ml extension to the Azure CLI. For more\ninformation on installing the CLI, see Install and set up the CLI (v2) .\nFirst, connect to the Azure Machine Learning workspace where you\'ll work.\nAzure CLI\nYou can deploy only registered models to online endpoints. In this case, you already\nhave a local copy of the model in the repository, so you only need to publish the model\nto the registry in the workspace. Y ou can skip this step if the model you\'re trying to\ndeploy is already registered.\nAzure CLI\nIf your model was logged inside of a run, you can register it directly.\nTo register the model, you need to know the location where it is stored. If you\'re using\nMLflow\'s autolog feature, the path to the model depends on the model type and\nframework. Y ou should check the jobs output to identify the name of the model\'s folder.\nThis folder contains a file named MLModel.Connect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nRegister the model\nAzure CLI\nMODEL_NAME= \'sklearn-diabetes\'\naz ml model create --name $MODEL_NAME  --type ""mlflow_model""  --path \n""endpoints/online/ncd/sklearn-diabetes/model""\nWhat if your model was logged inside of a run?', 'The az ml model command can be used for managing Azure Machine Learning models.\nExamples are available in the examples GitHub repository . Several are shown below.\nYAML\nYAML\nInstall and use the CLI (v2)Remarks\nExamples\nYAML: local file\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-file-example  \npath: mlflow-model/model.pkl  \ndescription:  Model created from local file. \nYAML: local folder in MLflow format\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-mlflow-example  \npath: mlflow-model  \ntype: mlflow_model  \ndescription:  Model created from local MLflow model directory.', ""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\ninference over new data:\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\n７ Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nSteps""]",1129
gpt-35-turbo,5,,3.0,,Provide an example of how to use the AML SDK v2 to stream logs from a running experiment in Python.,v1,"To stream logs from a running experiment using Azure Machine Learning SDK v2 in Python, you can use the `stream` method of the `Job` object. Here's an example of how you might do this:

```python
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the workspace
ml_client = MLClient.from_config(DefaultAzureCredential())

# Assuming you have the job name or ID of the running experiment
job_name = ""your_job_name_here""

# Get the job object
job = ml_client.jobs.get(name=job_name)

# Stream the logs
ml_client.jobs.stream(name=job_name)
```

In this example, replace `""your_job_name_here""` with the actual name or ID of the job you want to stream logs from. The `stream` method will continuously output the logs to your console until the job completes or fails. If you want to stop streaming the logs, you can simply interrupt the process (e.g., by pressing Ctrl+C in the console).","['SDK v2\nPython\nFunctionality in SDK v1 Rough mapping in SDK v2\nexperiment.submit MLCLient.jobs.create_or_update\nScriptRunConfig() command()\nFor more information, see:\nV1 - Experiment\nV2 - Command Jobaml_url = run.get_portal_url()  \nprint(aml_url)  \n#import required libraries  \nfrom azure.ai.ml import MLClient, command  \nfrom azure.ai.ml.entities import Environment  \nfrom azure.identity import DefaultAzureCredential  \n#connect to the workspace  \nml_client = MLClient.from_config(DefaultAzureCredential())  \n# set up pytorch environment  \nenv = Environment(  \n    image= ""mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04"" , \n    conda_file= ""pytorch-env.yml"" , \n    name= ""pytorch-env""  \n) \n# define the command  \ncommand_job = command(  \n    code= ""./src"", \n    command= ""train.py"" , \n    environment=env,  \n    compute= ""cpu-cluster"" , \n) \nreturned_job = ml_client.jobs.create_or_update(command_job)  \nreturned_job  \nMapping of key functionality in v1 and v2\nNext steps', ""Explore Azu re Machine Learning with\nJupyter Notebooks\nArticle •02/06/2024\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nThe AzureML-Examples  repository includes the latest (v2) Azure Machine Learning\nPython CLI and SDK samples. For information on the various example types, see the\nreadme .\nThis article shows you how to access the repository from the following environments:\nAzure Machine Learning compute instance\nYour own compute resource\nData Science Virtual Machine\nThe easiest way to get started with the samples is to complete the Create resources to\nget started . Once completed, you'll have a dedicated notebook server preloaded with\nthe SDK and the Azure Machine Learning Notebooks repository. No downloads or\ninstallation necessary.\nTo view example notebooks:\n1. Sign in to studio  and select your workspace if necessary.\n2. Select Notebooks .\n3. Select the Samples  tab. Use the SDK v2  folder for examples using Python SDK v2.\n4. Open the notebook you want to run. Select Clone this not ebook  to create a copy\nin your workspace file share. This action will copy the notebook along with any\ndependent resources.\nIf you'd like to bring your own notebook server for local development, follow these\nsteps on your computer.\n1. Use the instructions at Azure Machine Learning SDK  to install the Azure Machine\nLearning SDK (v2) for Python\nOption 1: Access on Azure Machine Learning\ncompute instance (recommended)\nOption 2: Access on your own notebook server"", ""Query & co mpare experiments and runs\nwith MLflow\nArticle •06/26/2023\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\nYou don't need to install any specific SDK to manage what happens inside of a training\njob, creating a more seamless transition between local runs and the cloud by removing\ncloud-specific dependencies. In this article, you'll learn how to query and compare\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\nin Python.\nMLflow allows you to:\nCreate, query, delete and search for experiments in a workspace.\nQuery, delete, and search for runs in a workspace.\nTrack and retrieve metrics, parameters, artifacts and models from runs.\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\ndetailed comparison between MLflow Open-Source and MLflow when connected to\nAzure Machine Learning.\nQuery and searching experiments and runs is also available using the MLflow REST API.\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\nconsume it.\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\nMLflow azureml-mlflow.７ Note\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\ntracking capabilities. This applies not just for logging but also for querying the\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\nexplains how to use MLflow to manage experiments and runs in Azure Machine\nLearning.\nREST API\nPrerequisites"", ""Query & co mpare experiments and runs\nwith MLflow\nArticle •06/26/2023\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\nYou don't need to install any specific SDK to manage what happens inside of a training\njob, creating a more seamless transition between local runs and the cloud by removing\ncloud-specific dependencies. In this article, you'll learn how to query and compare\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\nin Python.\nMLflow allows you to:\nCreate, query, delete and search for experiments in a workspace.\nQuery, delete, and search for runs in a workspace.\nTrack and retrieve metrics, parameters, artifacts and models from runs.\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\ndetailed comparison between MLflow Open-Source and MLflow when connected to\nAzure Machine Learning.\nQuery and searching experiments and runs is also available using the MLflow REST API.\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\nconsume it.\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\nMLflow azureml-mlflow.７ Note\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\ntracking capabilities. This applies not just for logging but also for querying the\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\nexplains how to use MLflow to manage experiments and runs in Azure Machine\nLearning.\nREST API\nPrerequisites"", 'SDK v2. Full sample link\nPython\'--learning_rate\' , 0.1,\'--model_output\' , model_output],\n    source_directory=script_folder,\n    compute_target=aml_compute,\n    allow_reuse= True,\n)\nscore_output = OutputFileDatasetConfig( \'score_output\' )\nscore_step = PythonScriptStep(\n    name=""score step"" ,\n    script_name= ""score.py"" ,\n    arguments=[ \'--model_input\' ,model_output.as_input( \'model_input\' ), \'-\n-test_data\' , input_ds.as_named_input( \'test_data\' ).as_mount(), \'--\nscore_output\' , score_output],\n    source_directory=script_folder,\n    compute_target=aml_compute,\n    allow_reuse= True,\n)\neval_output = OutputFileDatasetConfig( \'eval_output\' )\neval_step = PythonScriptStep(\n    name=""eval step"" ,\n    script_name= ""eval.py"" ,\n    arguments=[ \'--\nscoring_result\' ,score_output.as_input( \'scoring_result\' ), \'--\neval_output\' , eval_output],\n    source_directory=script_folder,\n    compute_target=aml_compute,\n    allow_reuse= True,\n)\n# built pipeline\nfrom azureml.pipeline.core import Pipeline\npipeline_steps = [train_step, score_step, eval_step]\npipeline = Pipeline(workspace = workspace, steps=pipeline_steps)\nprint(""Pipeline is built."" )\npipeline_run = experiment.submit(pipeline, regenerate_outputs= False)\nprint(""Pipeline submitted for execution."" )\n# import required libraries\nfrom azure.identity import DefaultAzureCredential,  \nInteractiveBrowserCredential\nfrom azure.ai.ml import MLClient, Input\nfrom azure.ai.ml.dsl import pipeline']",1795
gpt-35-turbo,6,,5.0,,"How can you configure an AutoML run for a classification task using the AML SDK v2, including specifying primary metric and training data?",v1,"To configure an AutoML run for a classification task using the Azure Machine Learning (AML) SDK v2, you can follow these steps:

1. **Import Necessary Libraries**: Import the required modules from the Azure AI ML package.

```python
from azure.ai.ml import automl, Input
from azure.ai.ml.constants import AssetTypes
from azure.ai.ml.automl import classification, ClassificationPrimaryMetrics
```

2. **Create MLTables for Training Dataset**: Define your training data input by specifying the path to your MLTable-formatted data.

```python
my_training_data_input = Input(
    type=AssetTypes.MLTABLE, 
    path=""./data/training-mltable-folder""
)
```

3. **Create the AutoML Classification Job**: Use the `automl.classification` factory function to create the AutoML classification job. Specify the compute target, experiment name, training data, target column name, primary metric, and other settings such as cross-validation, model explainability, and custom tags.

```python
classification_job = automl.classification(
    compute=""<compute_name>"",
    experiment_name=""<exp_name>"",
    training_data=my_training_data_input,
    target_column_name=""<name_of_target_column>"",
    primary_metric=ClassificationPrimaryMetrics.ACCURACY,
    n_cross_validations=5,
    enable_model_explainability=True,
    tags={""my_custom_tag"": ""My custom value""},
)
```

4. **Set Limits (Optional)**: You can optionally set limits on the AutoML job, such as the overall timeout, trial timeout, maximum number of trials, and maximum concurrent trials.

```python
classification_job.set_limits(
    timeout_minutes=600,
    trial_timeout_minutes=20,
    max_trials=5,
    max_concurrent_trials=4
)
```

5. **Submit the AutoML Run**: Submit the AutoML configuration to the experiment and start the run.

```python
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Assuming you have set up the MLClient with the appropriate workspace and subscription details
ml_client = MLClient(DefaultAzureCredential(), ""<subscription_id>"", ""<resource_group>"", ""<workspace_name>"")

# Submit the AutoML run
remote_run = ml_client.create_or_update(classification_job)

# Get the URL to monitor the run in Azure Portal
azureml_url = remote_run.get_portal_url()
print(azureml_url)
```

Make sure to replace `<compute_name>`, `<exp_name>`, `<name_of_target_column>`, `<subscription_id>`, `<resource_group>`, and `<workspace_name>` with your actual compute target name, experiment name, target column name, Azure subscription ID, resource group, and workspace name, respectively.

This is a high-level overview of configuring an AutoML classification task using the AML SDK v2. For a complete example and additional details, you should refer to the Azure Machine Learning examples repository as suggested in the provided context.","['SDK v2: Below is a sample AutoML classification task. For the entire code, check\nout our examples repo .\nPython    debug_log= ""automl_errors.log"" , \n    compute_target=compute_target,  \n    training_data=training_data,  \n    label_column_name=label_column_name,  \n    **automl_settings,  \n) \n# Submit run  \nremote_run = experiment.submit(automl_config, show_output= False) \nazureml_url = remote_run.get_portal_url()  \nprint(azureml_url)  \n# Imports  \nfrom azure.ai.ml import automl, Input, MLClient  \nfrom azure.ai.ml.constants import AssetTypes  \nfrom azure.ai.ml.automl import ( \n    classification,  \n    ClassificationPrimaryMetrics,\n    ClassificationModels,  \n) \n# Create MLTables for training dataset  \n# Note that AutoML Job can also take in tabular data  \nmy_training_data_input = Input(  \n    type=AssetTypes.MLTABLE, path= ""./data/training-mltable-folder""  \n) \n# Create the AutoML classification job with the related factory-\nfunction.  \nclassification_job = automl.classification(  \n    compute= ""<compute_name>"" , \n    experiment_name= ""<exp_name?"" , \n    training_data=my_training_data_input,  \n    target_column_name= ""<name_of_target_column>"" , \n    primary_metric= ""accuracy"" , \n    n_cross_validations= 5, \n    enable_model_explainability= True, \n    tags={ ""my_custom_tag"" : ""My custom value"" }, \n) \n# Limits are all optional  \nclassification_job.set_limits(  \n    timeout_minutes= 600, \n    trial_timeout_minutes= 20, \n    max_trials= 5, \n    max_concurrent_trials = 4,', 'Upgrade AutoML to SDK v2\nArticle •04/04/2023\nIn SDK v2, ""experiments"" and ""runs"" are consolidated into jobs.\nIn SDK v1, AutoML was primarily configured and run using the AutoMLConfig class. In\nSDK v2, this class has been converted to an AutoML job. Although there are some\ndifferences in the configuration options, by and large, naming & functionality has been\npreserved in V2.\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\nSDK v1: Below is a sample AutoML classification task. For the entire code, check\nout our examples repo .\nPythonSubmit AutoML run\n# Imports  \nimport azureml.core  \nfrom azureml.core.experiment import Experiment  \nfrom azureml.core.workspace import Workspace  \nfrom azureml.core.dataset import Dataset  \nfrom azureml.train.automl import AutoMLConfig  \nfrom azureml.train.automl.run import AutoMLRun    \n# Load tabular dataset  \ndata = ""<url_to_data>""  \ndataset = Dataset.Tabular.from_delimited_files(data)  \ntraining_data, validation_data = dataset.random_split(percentage= 0.8, \nseed=223) \nlabel_column_name = ""Class"" \n# Configure Auto ML settings  \nautoml_settings = {  \n    ""n_cross_validations"" : 3, \n    ""primary_metric"" : ""average_precision_score_weighted"" , \n    ""enable_early_stopping"" : True, \n    ""max_concurrent_iterations"" : 2,   \n    ""experiment_timeout_hours"" : 0.25,   \n    ""verbosity"" : logging.INFO,  \n} \n# Put together an AutoML job constructor  \nautoml_config = AutoMLConfig(  \n    task= ""classification"" ,', '3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\nMachine Learning in many different ways .\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\niterations over different models, hyperparameter settings, advanced\npreprocessing/featurization, and what metrics to look at when determining the\nbest model.\n5. Submit the training job.\n6. Review the r esults\nThe following diagram illustrates this process. \nYou can also inspect the logged job information, which contains metrics  gathered\nduring the job. The training job produces a Python serialized object ( .pkl file) that\ncontains the model and data preprocessing.\nWhile model building is automated, you can also learn how important or relevant\nfeatures are  to the generated models.\nApply automated ML when you want Azure Machine Learning to train and tune a model\nfor you using the target metric you specify. Automated ML democratizes the machine\nlearning model development process, and empowers its users, no matter their data\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\nML professionals and developers across industries can use automated ML to:\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\nforecasting, computer vision & NLP', 'iii. Select Create to create your compute target.\nThis tak es a couple minut es to complet e.\niv. After creation, select your new compute target from the drop-down list.\ne. Select Next .\n3. On the Select task and settings  form, complete the setup for your automated ML\nexperiment by specifying the machine learning task type and configuration\nsettings.\na. Select Classification  as the machine learning task type.\nb. Select View additional configuration settings  and populate the fields as\nfollows. These settings are to better control the training job. Otherwise, defaults\nare applied based on experiment selection and data.\nAdditional\xa0configurations Descr iption Value\xa0for\xa0tut orial\nPrimary metric Evaluation metric that the\nmachine learning algorithm\nwill be measured by.AUC_weighted\nExplain best model Automatically shows\nexplainability on the bestEnable', ""An Azure subscription. (If you don't have an Azure subscription, create a free\naccount  before you begin)\nAn Azure Machine Learning experiment created with either:\nThe Azure Machine Learning studio  (no code required)\nThe Azure Machine Learning Python SDK\nAfter your automated ML experiment completes, a history of the jobs can be found via:\nA browser with Azure Machine Learning studio\nA Jupyter notebook using the JobDetails Jupyter widget\nThe following steps and video, show you how to view the run history and model\nevaluation metrics and charts in the studio:\n1. Sign into the studio  and navigate to your workspace.\n2. In the left menu, select Jobs.\n3. Select your experiment from the list of experiments.\n4. In the table at the bottom of the page, select an automated ML job.\n5. In the Models  tab, select the Algorithm name  for the model you want to evaluate.\n6. In the Metrics  tab, use the checkboxes on the left to view metrics and charts.\nAutomated ML calculates performance metrics for each classification model generated\nfor your experiment. These metrics are based on the scikit learn implementation.\nMany classification metrics are defined for binary classification on two classes, and\nrequire averaging over classes to produce one score for multi-class classification. Scikit-\nlearn provides several averaging methods, three of which automated ML exposes:\nmacr o, micro, and weight ed.\nMacr o - Calculate the metric for each class and take the unweighted average\nMicro - Calculate the metric globally by counting the total true positives, false\nnegatives, and false positives (independent of classes).\nWeight ed - Calculate the metric for each class and take the weighted average\nbased on the number of samples per class.\nWhile each averaging method has its benefits, one common consideration when\nselecting the appropriate method is class imbalance. If classes have different numbers of\nsamples, it might be more informative to use a macro average where minority classes\nView job results\nClassification metrics""]",2235
gpt-35-turbo,7,,5.0,,Describe the steps to retrieve the best model from an AutoML run using the AML SDK v2.,v1,"To retrieve the best model from an AutoML run using the Azure Machine Learning SDK v2, you would typically follow these steps:

1. **Initialize Workspace**: Import the necessary packages and classes, and create an instance of the `Workspace` class to connect to your Azure Machine Learning workspace.

    ```python
    from azure.ai.ml import MLClient
    from azure.identity import DefaultAzureCredential
    from azure.ai.ml.entities import Workspace

    # Load the workspace
    ws = Workspace(subscription_id, resource_group, workspace_name)
    ml_client = MLClient(DefaultAzureCredential(), ws)
    ```

2. **Retrieve AutoML Run**: Access the specific AutoML run by its run ID. You can find the run ID in the Azure Machine Learning studio or through the SDK.

    ```python
    from azure.ai.ml.entities import AutoMLJob

    # Replace 'your_run_id' with the actual run ID
    run_id = 'your_run_id'
    automl_run = ml_client.jobs.get(name=run_id)
    ```

3. **Get Best Model**: Once you have the AutoML run, you can retrieve the best model from it. The best model is determined by the primary metric that was specified when the AutoML run was submitted.

    ```python
    # Get the best model (child run) from the AutoML run
    best_child_run = automl_run.get_best_child()
    ```

4. **Download Model**: After identifying the best model, you can download the model files to your local environment if needed.

    ```python
    # Download the best model files
    best_child_run.download_files(prefix='outputs/', output_directory='model_files/')
    ```

5. **Register Model**: Optionally, you can register the model to the Azure Machine Learning workspace for deployment or further tracking.

    ```python
    from azure.ai.ml.entities import Model

    # Register the model
    model = Model(
        name='best_automl_model',
        path='model_files/',  # Path to the downloaded model files
        type='mlflow_model'
    )
    registered_model = ml_client.models.create_or_update(model)
    ```

Please note that the Azure Machine Learning SDK v2 is continuously evolving, and the exact methods and classes might change. Always refer to the latest Azure Machine Learning SDK v2 documentation for the most up-to-date information.","['Learn more about how and where to deploy a model .\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps', ""Navigate to the Models  tab to see the algorithms (models) tested. By default, the\nmodels are ordered by metric score as they complete. For this tutorial, the model that\nscores the highest based on the chosen Normalized r oot mean squar ed err or metric is\nat the top of the list.\nWhile you wait for all of the experiment models to finish, select the Algorithm name  of\na completed model to explore its performance details.\nThe following example navigates to select a model from the list of models that the job\ncreated. Then, you select the Overview  and the Metrics  tabs to view the selected\nmodel's properties, metrics and performance charts.\nAutomated machine learning in Azure Machine Learning studio allows you to deploy the\nbest model as a web service in a few steps. Deployment is the integration of the model\nso it can predict on new data and identify potential areas of opportunity.\nFor this experiment, deployment to a web service means that the bike share company\nnow has an iterative and scalable web solution for forecasting bike share rental demand.\nOnce the job is complete, navigate back to parent job page by selecting Job 1  at the top\nof your screen.\nIn the Best model summar y section, the best model in the context of this experiment, is\nselected based on the Normalized r oot mean squar ed err or metric.\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\nThe deployment process entails several steps including registering the model,Deploy the model"", ""To run a fine-tuned model job from Machine Learning, in order to deploy an Azure\nOpenAI model:\n1. After you've finished fine-tuning an Azure OpenAI model, find the registered\nmodel in the Models  list with the name provided during fine-tuning and select the\nmodel you want to deploy.\n2. Select Deploy  and name the deployment. The model is deployed to the default\nAzure OpenAI resource linked to your workspace.\nTo enable users to quickly get started with code-based fine-tuning, we've published\nsamples (both Python notebooks and Azure CLI examples) to the azureml-ex amples\nGitHub repo:\nSDK example\nCLI exampleDeploy fine-tuned models\nFine-tuning by using code-based samples"", ""Option 2: T o deploy a specific model iteration from this experiment.\na. Select the desired model from the Models  tab\nb. Select Deploy  on the top left of the window.\n2. Populate the Deploy model  pane.\nField Value\nName Enter a unique name for your deployment.\nDescription Enter a description to better identify what this deployment is for.\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\nService (AKS)  or Azure Container Inst ance (ACI).\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\ndeploy to.\nEnable\nauthenticationSelect to allow for token-based or key-based authentication.\nUse custom\ndeployment assetsEnable this feature if you want to upload your own scoring script and\nenvironment file. Otherwise, automated ML provides these assets for\nyou by default. Learn more about scoring scripts .\nThe Advanced menu offers default deployment features such as data collection  and\nresource utilization settings. If you wish to override these defaults do so in this\nmenu.\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\ndeployment begins, the Model summar y tab appears. See the deployment\nprogress under the Deploy status  section.\nNow you have an operational web service to generate predictions! Y ou can test the\npredictions by querying the service from Power BI's built in Azure Machine Learning\nsupport .ﾉExpand table\n） Impor tant\nFile names must be under 32 characters and must begin and end with\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\nbetween. Spaces are not allowed."", 'The automated machine learning interface allows you to deploy the best model as a\nweb service in a few steps. Deployment is the integration of the model so it can predict\non new data and identify potential areas of opportunity.\nFor this experiment, deployment to a web service means that the financial institution\nnow has an iterative and scalable web solution for identifying potential fixed term\ndeposit customers.\nCheck to see if your experiment run is complete. T o do so, navigate back to the parent\njob page by selecting Job 1  at the top of your screen. A Complet ed status is shown on\nthe top left of the screen.\nOnce the experiment run is complete, the Details  page is populated with a Best model\nsummar y section. In this experiment context, VotingEnsemble  is considered the best\nmodel, based on the AUC_w eight ed metric.\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\nThe deployment process entails several steps including registering the model,\ngenerating resources, and configuring them for the web service.\n1. Select VotingEnsemble  to open the model-specific page.\n2. Select the Deploy  menu in the top-left and select Deploy t o web ser vice.\n3. Populate the Deploy a model  pane as follows:\nField Value\nDeployment name my-automl-deploy\nDeployment\ndescriptionMy first automated machine learning experiment deployment\nCompute type Select Azure Container Instance (A CI)\nEnable\nauthenticationDisable.\nUse custom\ndeploymentsDisable. Allows for the default driver file (scoring script) and\nenvironment file to be auto-generated.\nFor this example, we use the defaults provided in the Advanced menu.\n4. Select Deploy .Deploy the best model']",1700
gpt-35-turbo,8,,5.0,,What are the AutoML capabilities for handling imbalanced data in Azure ML?,v1,"Azure Machine Learning's Automated ML (AutoML) includes several capabilities to handle imbalanced data, which is a common issue where the number of instances in one class significantly outnumbers the instances in other classes. Here are some of the key features that AutoML provides to address imbalanced datasets:

1. **Data Preprocessing**: AutoML automatically applies data preprocessing techniques that can help mitigate the impact of imbalanced data. This includes scaling and normalization that ensure the model does not bias towards certain features.

2. **Cost-Sensitive Learning**: AutoML can use cost-sensitive learning where the algorithm assigns a higher cost to misclassifying the minority class. This can help in guiding the model to pay more attention to the minority class.

3. **Sampling Techniques**: Automated ML can employ different sampling techniques such as over-sampling the minority class or under-sampling the majority class to balance the dataset before training.

4. **Advanced Algorithms**: AutoML includes advanced machine learning algorithms that are inherently better at handling imbalanced data, such as decision trees and ensemble methods like Random Forest and Gradient Boosting.

5. **Evaluation Metrics**: AutoML provides a range of evaluation metrics that are suitable for imbalanced datasets, such as the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), precision, recall, and F1 score. These metrics are more informative than accuracy in the context of imbalanced datasets.

6. **Model Tuning**: AutoML can automatically tune hyperparameters of the models to better handle imbalanced data, such as adjusting the class_weight parameter in algorithms that support it.

7. **Early Stopping**: To prevent overfitting, which can be exacerbated by imbalanced data, AutoML supports early stopping mechanisms during model training.

By leveraging these capabilities, users can better manage imbalanced datasets and improve the performance of their machine learning models using Azure Machine Learning's Automated ML.","[""Prevent overfitting and imbalanced data\nwith Automated ML\nArticle •06/16/2023\nOverfitting and imbalanced data are common pitfalls when you build machine learning\nmodels. By default, Azure Machine Learning's Automated ML provides charts and\nmetrics to help you identify these risks, and implements best practices to help mitigate\nthem.\nOverfitting in machine learning occurs when a model fits the training data too well, and\nas a result can't accurately predict on unseen test data. In other words, the model has\nmemorized specific patterns and noise in the training data, but is not flexible enough to\nmake predictions on real data.\nConsider the following trained models and their corresponding train and test accuracies.\nModel Train accuracy Test accuracy\nA 99.9% 95%\nB 87% 87%\nC 99.9% 45%\nConsider model A, there is a common misconception that if test accuracy on unseen\ndata is lower than training accuracy, the model is overfitted. However, test accuracy\nshould always be less than training accuracy, and the distinction for overfit vs.\nappropriately fit comes down to how much  less accurate.\nCompare models A and B, model A is a better model because it has higher test\naccuracy, and although the test accuracy is slightly lower at 95%, it is not a significant\ndifference that suggests overfitting is present. Y ou wouldn't choose model B because\nthe train and test accuracies are closer together.\nModel C represents a clear case of overfitting; the training accuracy is high but the test\naccuracy isn't anywhere near as high. This distinction is subjective, but comes from\nknowledge of your problem and data, and what magnitudes of error are acceptable.Identify overfitting\nPrevent overfitting"", ""If it ascertains a better performance through this experiment, then this remedy is\napplied.\nUse a performance metric that deals better with imbalanced data. For example, the\nAUC_weighted is a primary metric that calculates the contribution of every class\nbased on the relative number of samples representing that class, hence is more\nrobust against imbalance.\nThe following techniques are additional options to handle imbalanced data outside of\nAutomated ML.\nResampling to even the class imbalance, either by up-sampling the smaller classes\nor down-sampling the larger classes. These methods require expertise to process\nand analyze.\nReview performance metrics for imbalanced data. For example, the F1 score is the\nharmonic mean of precision and recall. Precision measures a classifier's exactness,\nwhere higher precision indicates fewer false positives, while recall measures a\nclassifier's completeness, where higher recall indicates fewer false negatives.\nSee examples and learn how to build models using Automated ML:\nFollow the Tutorial: T rain an object detection model with automated machine\nlearning and Python .\nConfigure the settings for automatic training experiment:\nIn Azure Machine Learning studio, use these steps .\nWith the Python SDK, use these steps .Next steps"", 'Imbalanced data is commonly found in data for machine learning classification\nscenarios, and refers to data that contains a disproportionate ratio of observations in\neach class. This imbalance can lead to a falsely perceived positive effect of a model\'s\naccuracy, because the input data has bias towards one class, which results in the trained\nmodel to mimic that bias.\nIn addition, Automated ML jobs generate the following charts automatically. These\ncharts help you understand the correctness of the classifications of your model, and\nidentify models potentially impacted by imbalanced data.\nChar t Descr iption\nConfusion\nMatrixEvaluates the correctly classified labels against the actual labels of the data.\nPrecision-recall Evaluates the ratio of correct labels against the ratio of found label instances of\nthe data\nROC Curves Evaluates the ratio of correct labels against the ratio of false-positive labels.\nAs part of its goal of simplifying the machine learning workflow, Automated ML has built\nin capabilities to help deal with imbalanced data such as,\nA weight column: Automated ML creates a column of weights as input to cause\nrows in the data to be weighted up or down, which can be used to make a class\nmore or less ""important.""\nThe algorithms used by Automated ML detect imbalance when the number of\nsamples in the minority class is equal to or fewer than 20% of the number of\nsamples in the majority class, where minority class refers to the one with fewest\nsamples and majority class refers to the one with most samples. Subsequently,\nautomated machine learning will run an experiment with subsampled data to\ncheck if using class weights would remedy this problem and improve performance.Cross-validation isn\'t enabled by default; it must be configured in Automated\nmachine learning settings. However, after cross-validation is configured and a\nvalidation data set has been provided, the process is automated for you.\nIdentify models with imbalanced data\nHandle imbalanced data', 'What is automated machine learning\n(AutoML)?\nArticle •04/13/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAutomated machine learning, also referred to as automated ML or AutoML, is the\nprocess of automating the time-consuming, iterative tasks of machine learning model\ndevelopment. It allows data scientists, analysts, and developers to build ML models with\nhigh scale, efficiency, and productivity all while sustaining model quality. Automated ML\nin Azure Machine Learning is based on a breakthrough from our Microsoft R esearch\ndivision .\nFor code-experienced customers, Azure Machine Learning Python SDK . Get\nstarted with Tutorial: T rain an object detection model (preview) with AutoML and\nPython .\nDuring training, Azure Machine Learning creates a number of pipelines in parallel that\ntry different algorithms and parameters for you. The service iterates through ML\nalgorithms paired with feature selections, where each iteration produces a model with a\ntraining score. The better the score for the metric you want to optimize for, the better\nthe model is considered to ""fit"" your data. It will stop once it hits the exit criteria defined\nin the experiment.\nUsing Azur e Machine Learning , you can design and run your automated ML training\nexperiments with these steps:\n1. Identif y the ML pr oblem  to be solved: classification, forecasting, regression,\ncomputer vision or NLP.\n2. Choose whether y ou want a code-fir st experience or a no-code studio w eb\nexperience : Users who prefer a code-first experience can use the Azure Machine\nLearning SDK v2 or the Azure Machine Learning CLIv2 . Get started with Tutorial:\nTrain an object detection model with AutoML and Python . Users who prefer a\nlimited/no-code experience can use the web interface  in Azure Machine Learning\nstudio at https://ml.azure.com . Get started with Tutorial: Create a classification\nmodel with automated ML in Azure Machine Learning .\nHow does AutoML work?', 'Automated ML in Azure Machine Learning supports model training for computer vision\ntasks like image classification and object detection. T o debug AutoML vision models and\nexplain model predictions, AutoML models for computer vision are integrated with\nResponsible AI dashboard. T o generate R esponsible AI insights for AutoML computer\nvision models, register your best AutoML model in the Azure Machine Learning\nworkspace and run it through the R esponsible AI vision insights pipeline. T o learn, see\nhow to set up AutoML to train computer vision models .\nNotebooks related to the AutoML supported computer vision tasks can be found in\nazureml-examples  repository.\nThe R esponsible AI vision Insights pipeline could be submitted through one of the\nfollowing methods\nPython SDK: T o learn how to submit the pipeline through Python, see the AutoML\nImage Classification scenario with RAI Dashboard sample notebook . For\nconstructing the pipeline, refer to section 5.1 in the notebook.\nAzure CLI: T o submit the pipeline via Azure-CLI, see the component Y AML in\nsection 5.2 of the example notebook linked above.\nUI (via Azure Machine Learning studio): From the Designer in Azure Machine\nLearning studio, the RAI-vision insights component can be used to create and\nsubmit a pipeline.\nIn addition to the list of R esponsible AI vision insights parameters provided in the\nprevious section, the following are parameters to set specifically for AutoML models.Integration with AutoML Image\nMode of submitting the Responsible AI vision insights\npipeline\nResponsible AI vision insights component parameter\n(AutoML specific)\n７ Note\nA few parameters are specific to the XAI algorithm chosen and are optional for\nother algorithms.']",2170
gpt-35-turbo,9,,5.0,,Explain how to use AutoML for forecasting time series data. What specific settings should be adjusted for time series problems?,v1,"To use Azure AutoML for forecasting time series data, you should follow these general steps:

1. **Set Up Your Environment**:
   - Install the Azure Machine Learning Python SDK v2.
   - Create an Azure Machine Learning workspace if you don't already have one.
   - Set up your development environment, which could be a local machine or a cloud-based compute instance.

2. **Prepare Your Data**:
   - Ensure your data is in a format suitable for time series forecasting. This typically means having a time column, one or more feature columns, and a target column that you want to forecast.
   - Split your data into training and testing datasets.

3. **Configure AutoML**:
   - Define the settings and configuration for your AutoML run. For time series forecasting, you need to adjust specific settings such as:
     - `time_column_name`: The name of the column that holds the time information.
     - `grain_column_names`: The columns that uniquely identify a time series when you have multiple series in your dataset.
     - `max_horizon`: The number of periods to forecast.
     - `target_lags`: The number of past periods to use to predict future values.
     - `target_rolling_window_size`: The window size for creating rolling features.
   - Choose the appropriate primary metric for evaluating the forecasting model's performance.

4. **Run AutoML**:
   - Submit the AutoML run with your data and configuration settings. AutoML will automatically try different models and preprocessing steps to find the best solution for your forecasting problem.

5. **Evaluate Results**:
   - Once the run is complete, evaluate the performance of the best model using the primary metric you selected.
   - Review the model explanations to understand the feature importance.

6. **Deploy the Model** (Optional):
   - If you are satisfied with the model's performance, you can deploy it as a web service for real-time predictions or for batch scoring.

For time series problems, it's important to adjust settings that are specific to the nature of time series data, such as the time column, grain columns (if applicable), and forecasting horizon. Additionally, consider the frequency of the data and any seasonal patterns when configuring the AutoML run.

For more detailed guidance, you can refer to the Azure documentation on setting up AutoML for time-series forecasting and explore the hands-on examples provided in the Jupyter notebooks mentioned in the context.","[""Frequently asked questions about\nforecasting in AutoML\nArticle •08/01/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nThis article answers common questions about forecasting in automatic machine learning\n(AutoML). For general information about forecasting methodology in AutoML, see the\nOverview of forecasting methods in AutoML  article.\nYou can start by reading the Set up AutoML to train a time-series forecasting model\narticle. Y ou can also find hands-on examples in several Jupyter notebooks:\nBike share example\nForecasting using deep learning\nMany Models solution\nForecasting recipes\nAdvanced forecasting scenarios\nWe're always working to make AutoML faster and more scalable. T o work as a general\nforecasting platform, AutoML does extensive data validations and complex feature\nengineering, and it searches over a large model space. This complexity can require a lot\nof time, depending on the data and the configuration.\nOne common source of slow runtime is training AutoML with default settings on data\nthat contains numerous time series. The cost of many forecasting methods scales with\nthe number of series. For example, methods like Exponential Smoothing and Prophet\ntrain a model for each time series  in the training data.\nThe Many Models feature of AutoML scales to these scenarios by distributing training\njobs across a compute cluster. It has been successfully applied to data with millions of\ntime series. For more information, see the many models  article section. Y ou can also\nread about the success of Many Models  on a high-profile competition dataset.\nHow do I start building forecasting models in\nAutoML?\nWhy is AutoML slow on my data?"", 'Learn more about how and where to deploy a model .\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps', ""If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about calendar features for time series forecasting in AutoML .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about AutoML forecasting for lagged features .Next steps"", ""Set up AutoML to train a time-series\nforecasting model with SDK and CLI\nArticle •08/02/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn this article, you'll learn how to set up AutoML for time-series forecasting with Azure\nMachine Learning automated ML in the Azure Machine Learning Python SDK .\nTo do so, you:\nFor a low code experience, see the Tutorial: Forecast demand with automated machine\nlearning  for a time-series forecasting example using automated ML in the Azure\nMachine Learning studio .\nAutoML uses standard machine learning models along with well-known time series\nmodels to create forecasts. Our approach incorporates historical information about the\ntarget variable, user-provided features in the input data, and automatically engineered\nfeatures. Model search algorithms then work to find a model with the best predictive\naccuracy. For more details, see our articles on forecasting methodology  and model\nsearch .\nFor this article you need,\nAn Azure Machine Learning workspace. T o create the workspace, see Create\nworkspace resources .\nThe ability to launch AutoML training jobs. Follow the how-to guide for setting up\nAutoML  for details.\nPrepare data for training.＂\nConfigure specific time-series parameters in a Forecasting Job . ＂\nOrchestrate training, inference, and model evaluation using components and\npipelines.＂\nPrerequisites\nTraining  and validation data"", ""See the Why is AutoML slow on my data?  answer to understand why AutoML might be\nslow in your case.\nConsider the following configuration changes that might speed up your job:\nBlock time series models  like ARIMA and Prophet.\nTurn off look-back features like lags and rolling windows.\nReduce:\nThe number of trials/iterations.\nTrial/iteration timeout.\nExperiment timeout.\nThe number of cross-validation folds.\nEnsure that early termination is enabled.\nAutoML forecasting supports four basic configurations:\nConfiguration Scenar io Pros Cons\nDefault\nAutoMLRecommended if the dataset\nhas a small number of time\nseries that have roughly\nsimilar historical behavior.- Simple to configure\nfrom code/SDK or\nAzure Machine\nLearning studio.\n- AutoML can learn\nacross different time\nseries because the\nregression models\npool all series together\nin training. For more\ninformation, see\nModel grouping .- Regression models\nmight be less\naccurate if the time\nseries in the training\ndata have divergent\nbehavior.\n- Time series models\nmight take a long\ntime to train if the\ntraining data has a\nlarge number of\nseries. For more\ninformation, see the\nWhy is AutoML slow\non my data?  answer.\nAutoML with\ndeep learningRecommended for datasets\nwith more than 1,000\nobservations and, potentially,\nnumerous time series that\nexhibit complex patterns.\nWhen it's enabled, AutoML\nwill sweep over temporal- Simple to configure\nfrom code/SDK or\nAzure Machine\nLearning studio.\n- Cross-learning\nopportunities, because- Training can take\nmuch longer\nbecause of the\ncomplexity of DNN\nmodels.\n- Series with smallHow can I make AutoML faster?\nWhat modeling configuration should I use?""]",1662
gpt-35-turbo,10,,5.0,,How do you create a custom deep learning environment with specific Python packages using the AML SDK v2?,v1,"To create a custom deep learning environment with specific Python packages using the Azure Machine Learning SDK v2, you can follow these steps:

1. **Define the Environment**: Use the `Environment` class from the `azure.ai.ml.entities` namespace to define your custom environment. You will need to specify the name, description, the path to your Conda environment file (which lists the Python packages and versions you want to include), and optionally, a base Docker image.

2. **Create a Conda Environment File**: Write a `conda.yaml` file that lists all the Python packages and versions you need for your deep learning environment. This file will be used to create the environment.

3. **Register the Environment**: Use the `create_or_update` method of the `ml_client.environments` to register the environment with your Azure Machine Learning workspace.

Here's an example of how you might do this in Python using the Azure ML SDK v2:

```python
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential
from azure.ai.ml.entities import Environment
import os

# Set up the MLClient with your subscription and resource details
subscription_id = 'your-subscription-id'
resource_group = 'your-resource-group'
workspace_name = 'your-workspace-name'

# Use DefaultAzureCredential or any other suitable credential class
credential = DefaultAzureCredential()

# Create the MLClient with the specified workspace
ml_client = MLClient(credential, subscription_id, resource_group, workspace_name)

# Define the path to your dependencies directory
dependencies_dir = 'path_to_your_dependencies_directory'

# Define the custom environment
custom_env_name = ""deep-learning-env""
job_env = Environment(
    name=custom_env_name,
    description=""Custom environment for deep learning"",
    conda_file=os.path.join(dependencies_dir, ""conda.yaml""),
    image=""mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest"",
)

# Register the environment
job_env = ml_client.environments.create_or_update(job_env)

# Print the environment details
print(
    f""Environment with name {job_env.name} is registered to workspace, ""
    f""the environment version is {job_env.version}""
)
```

In the `conda.yaml` file, you would specify your Python packages like this:

```yaml
name: deep-learning-env
dependencies:
  - python=3.8
  - numpy
  - pandas
  - scikit-learn
  - tensorflow
  - keras
  - pip:
    - some-pip-package
```

Make sure to replace placeholders like `your-subscription-id`, `your-resource-group`, `your-workspace-name`, and `path_to_your_dependencies_directory` with your actual Azure subscription, resource group, workspace name, and local path to the directory containing your `conda.yaml` file. Also, customize the `conda.yaml` file with the specific Python packages and versions that you need for your deep learning tasks.","['The specification contains some usual packages (such as numpy and pip) that you\'ll use\nin your job.\nNext, use the Y AML file to create and register this custom environment in your\nworkspace. The environment will be packaged into a Docker container at runtime.\nPython\nFor more information on creating and using environments, see Create and use software\nenvironments in Azure Machine Learning .\nIn this section, we\'ll begin by introducing the data for training. W e\'ll then cover how to\nrun a training job, using a training script that we\'ve provided. Y ou\'ll learn to build the\ntraining job by configuring the command for running the training script. Then, you\'ll\nsubmit the training job to run in Azure Machine Learning.\nYou\'ll use data from the Modified National Institute of S tandards and T echnology\n(MNIST) database of handwritten digits. This data is sourced from Y an LeCun\'s website\nand stored in an Azure storage account.\nPythonfrom azure.ai.ml.entities import Environment  \ncustom_env_name = ""keras-env""  \njob_env = Environment(  \n    name=custom_env_name,  \n    description= ""Custom environment for keras image classification"" , \n    conda_file=os.path.join(dependencies_dir, ""conda.yaml"" ), \n    image= ""mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest"" , \n) \njob_env = ml_client.environments.create_or_update(job_env)  \nprint( \n    f""Environment with name {job_env.name}  is registered to workspace, the  \nenvironment version is {job_env.version} "" \n)\nConfigure and submit your training  job\nObtain the training data\nweb_path = ""wasbs://datasets@azuremlexamples.blob.core.windows.net/mnist/""', ""In the samples deep learning folder on the notebook server, find a\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\ndeploy-with-k eras.\nYour Jupyter notebook server\nInstall the Azure Machine Learning SDK (v2) .\nDownload the training scripts keras_mnist.py  and utils.py .\nYou can also find a completed Jupyter Notebook version  of this guide on the GitHub\nsamples page.\nBefore you can run the code in this article to create a GPU cluster, you'll need to request\na quota increase  for your workspace.\nThis section sets up the job for training by loading the required Python packages,\nconnecting to a workspace, creating a compute resource to run a command job, and\ncreating an environment to run the job.\nFirst, you'll need to connect to your Azure Machine Learning workspace. The Azure\nMachine Learning workspace  is the top-level resource for the service. It provides you\nwith a centralized place to work with all the artifacts you create when you use Azure\nMachine Learning.\nWe're using DefaultAzureCredential to get access to the workspace. This credential\nshould be capable of handling most Azure SDK authentication scenarios.\nIf DefaultAzureCredential doesn't work for you, see azure-identity reference\ndocumentation  or Set up authentication  for more available credentials.\nPython\nSet up the job\nConnect to the workspace\n# Handle to the workspace  \nfrom azure.ai.ml import MLClient  \n# Authentication package  \nfrom azure.identity import DefaultAzureCredential  \ncredential = DefaultAzureCredential()"", 'Azure Machine Learning allows you to either use a curated (or ready-made)\nenvironment or create a custom environment using a Docker image or a Conda\nconfiguration. In this article, you reuse the curated Azure Machine Learning environment\nAzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu. Use the latest version of this\nenvironment using the @latest directive.\nPython\nIn this section, we begin by introducing the data for training. W e then cover how to run\na training job, using a training script that we\'ve provided. Y ou\'ll learn to build the\ntraining job by configuring the command for running the training script. Then, you\'ll\nsubmit the training job to run in Azure Machine Learning.\nYou can use the dataset in this zipped file . This dataset consists of about 120 training\nimages each for two classes (turkeys and chickens), with 100 validation images for each\nclass. The images are a subset of the Open Images v5 Dataset . The training script\npytorch_tr ain.p y downloads and extracts the dataset.\nIn the prerequisites section, we provided the training script pytorch_tr ain.p y. In practice,\nyou should be able to take any custom training script as is and run it with Azure\nMachine Learning without having to modify your code.\nThe provided training script downloads the data, trains a model, and registers the\nmodel.\nNow that you have all the assets required to run your job, it\'s time to build it using the\nAzure Machine Learning Python SDK v2. For this example, we create a command.\nAn Azure Machine Learning command is a resource that specifies all the details needed to\nexecute your training code in the cloud. These details include the inputs and outputs,curated_env_name = ""AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu@latest""\nConfigure and submit your training  job\nObtain the training data\nPrepare the training script\nBuild the training job', ""2. Create an Azure Machine Learning workspace .\n3. Clone the AzureML-Examples repository .\nBash\n4. Start the notebook server from the directory containing your clone.\nBash\nThese instructions install the base SDK packages necessary for the quickstart and tutorial\nnotebooks. Other sample notebooks might require you to install extra components. For\nmore information, see Install the Azure Machine Learning SDK for Python .\nThe Data Science Virtual Machine (DSVM) is a customized VM image built specifically for\ndoing data science. If you create a DSVM , the SDK and notebook server are installed and\nconfigured for you. However, you'll still need to create a workspace and clone the\nsample repository.\n1. Create an Azure Machine Learning workspace .\n2. Clone the the AzureML-Examples repository .\nBash\n3. Start the notebook server from the directory that contains the clone.\nBash\ngit clone https://github.com/Azure/azureml-examples.git --depth 1\njupyter notebook\nOption 3: Access on a DSVM\ngit clone https://github.com/Azure/azureml-examples.git --depth 1\njupyter notebook\nConnect to a workspace"", ""Manage Azu re Machine Learning\nenvironments with the CLI & SDK (v2)\nArticle •01/03/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAzure Machine Learning environments define the execution environments for your jobs\nor deployments and encapsulate the dependencies for your code. Azure Machine\nLearning uses the environment specification to create the Docker container that your\ntraining or scoring code runs in on the specified compute target. Y ou can define an\nenvironment from a conda specification, Docker image, or Docker build context.\nIn this article, learn how to create and manage Azure Machine Learning environments\nusing the SDK & CLI (v2).\nBefore following the steps in this article, make sure you have the following prerequisites:\nAn Azure Machine Learning workspace. If you don't have one, use the steps in the\nQuickstart: Create workspace resources  article to create one.\nThe Azure CLI and the ml extension or the Azure Machine Learning Python SDK v2:\nTo install the Azure CLI and extension, see Install, set up, and use the CLI (v2) .\nTo install the Python SDK v2, use the following command:\nBash\nPrerequisites\n） Impor tant\nThe CLI examples in this article assume that you are using the Bash (or\ncompatible) shell. For example, from a Linux system or Windows\nSubsyst em for Linux .\npip install azure-ai-ml azure-identity""]",2308
gpt-35-turbo,11,,5.0,,Describe the process of using Azure ML to train a deep learning model on a GPU compute cluster.,v1,"To train a deep learning model on a GPU compute cluster using Azure Machine Learning, you would follow these general steps:

1. **Set Up Your Azure ML Workspace**:
   - Connect to your Azure ML Workspace using the `MLClient` from the Azure ML Python SDK.
   ```python
   from azure.ai.ml import MLClient
   from azure.identity import DefaultAzureCredential

   workspace = '<AZUREML_WORKSPACE_NAME>'
   subscription_id = '<YOUR_SUBSCRIPTION_ID>'
   resource_group = '<YOUR_RESOURCE_GROUP>'

   ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)
   ```

2. **Create or Access a GPU Compute Cluster**:
   - Check if the GPU compute cluster already exists.
   - If it does not exist, create a new GPU compute cluster using the `AmlCompute` class, specifying the GPU VM size (e.g., `STANDARD_NC6` for a VM with one NVIDIA Tesla K80 GPU).
   ```python
   from azure.ai.ml.entities import AmlCompute

   gpu_compute_target = ""gpu-cluster""
   try:
       ml_client.compute.get(gpu_compute_target)
   except Exception:
       print(""Creating a new GPU compute target..."")
       compute = AmlCompute(
           name=gpu_compute_target,
           size=""STANDARD_NC6"",  # Example GPU VM size
           min_instances=0,
           max_instances=4
       )
       ml_client.compute.begin_create_or_update(compute).result()
   ```

3. **Prepare Your Training Script**:
   - Write a Python script (e.g., `train.py`) that defines your deep learning model, training process, and any other necessary code.

4. **Configure the Training Job**:
   - Create a job configuration that specifies the training script, compute target, environment (including necessary packages and frameworks like TensorFlow or PyTorch), and any hyperparameters or data inputs.
   ```python
   from azure.ai.ml import command, Input

   job = command(
       code=""./path_to_your_code"",
       command=""python train.py"",
       compute=gpu_compute_target,
       environment=""<YOUR_ENVIRONMENT_NAME>"",
       inputs={""training_data"": Input(type=""uri_folder"", path=""<PATH_TO_YOUR_DATA>"")},  # Example input
       # Add other configurations like environment variables, resources, etc.
   )
   ```

5. **Submit the Training Job**:
   - Submit the job to the Azure ML workspace to run on the GPU compute cluster.
   ```python
   returned_job = ml_client.create_or_update(job)
   ```

6. **Monitor the Training Job**:
   - Use Azure ML Studio or the SDK to monitor the progress of the training job, view logs, and track metrics.

7. **Retrieve the Trained Model**:
   - Once the training job is complete, you can download the trained model from the output location specified in your training script or job configuration.

Please note that the code snippets provided are for illustrative purposes and will need to be adapted to your specific use case, including the correct VM size for your GPU compute needs, the path to your training script, and the environment setup for your deep learning framework.","['An Azure Machine Learning compute cluster is a fully managed compute resource that\ncan be used to run the training job. In the following examples, a compute cluster named\ncpu-compute is created.\nPython\nTo run this script, you\'ll use a command that executes main.py Python script located\nunder ./sdk/python/jobs/single-step/lightgbm/iris/src/. The command will be runworkspace = \'<AZUREML_WORKSPACE_NAME>\'\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \nresource_group, workspace)\n2. Create a compute resource for training\n７ Note\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\ntraining job .\nPython SDK\nfrom azure.ai.ml.entities import AmlCompute\n# specify aml compute name.\ncpu_compute_target = ""cpu-cluster""\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(""Creating a new cpu compute target..."" )\n    compute = AmlCompute(\n        name=cpu_compute_target, size= ""STANDARD_D2_V2"" , min_instances= 0, \nmax_instances= 4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n4. Submit the training job\nPython SDK', 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""Manage and optimize Azure Machine\nLearning costs\nArticle •08/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.） Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)"", 'For detailed instructions on how to complete these steps, see Secure a training\nenvironment .\nIn this section, you learn how Azure Machine Learning securely communicates between\nservices to submit a training job. This example shows you how all your configurations\nwork together to secure communication.\n1. The client uploads training scripts and training data to storage accounts that are\nsecured with a service or private endpoint.\n2. The client submits a training job to the Azure Machine Learning workspace\nthrough the private endpoint.\n3. Azure Batch service receives the job from the workspace. It then submits the\ntraining job to the compute environment through the public load balancer for the\ncompute resource.\n4. The compute resource receives the job and begins training. The compute resource\nuses information stored in key vault to access storage accounts to download\ntraining files and upload output.Example training job submission', 'like Spark.\nIn the following example script, we provision a Linux compute cluster . You can see the\nAzure Machine Learning pricing  page for the full list of VM sizes and prices. Since we\nneed a GPU cluster for this example, let\'s pick a STAND ARD_NC6  model and create an\nAzure Machine Learning compute.\nPython\nfrom azure.ai.ml.entities import AmlCompute  \ngpu_compute_target = ""gpu-cluster""  \ntry: \n    # let\'s see if the compute target already exists  \n    gpu_cluster = ml_client.compute.get(gpu_compute_target)  \n    print(  \n        f""You already have a cluster named {gpu_compute_target} , we\'ll reuse  \nit as is.""  \n    ) \nexcept Exception:  \n    print( ""Creating a new gpu compute target..."" ) \n    # Let\'s create the Azure ML compute object with the intended parameters  \n    gpu_cluster = AmlCompute(  \n        # Name assigned to the compute cluster  \n        name= ""gpu-cluster"" , \n        # Azure ML Compute is the on-demand VM service  \n        type= ""amlcompute"" , \n        # VM Family  \n        size= ""STANDARD_NC6"" , \n        # Minimum running nodes when there is no job running  \n        min_instances= 0, \n        # Nodes in cluster  \n        max_instances= 4, \n        # How many seconds will the node running after the job termination  \n        idle_time_before_scale_down= 180, \n        # Dedicated or LowPriority. The latter is cheaper but there is a  \nchance of job termination  \n        tier= ""Dedicated"" , \n    ) \n    # Now, we pass the object to MLClient\'s create_or_update method  \n    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()  \nprint( \n    f""AMLCompute with name {gpu_cluster.name}  is created, the compute size  \nis {gpu_cluster.size} "" \n)']",1899
gpt-35-turbo,12,,5.0,,Explain how to deploy a TensorFlow model as a web service in Azure ML. What are the key considerations for deployment?,v1,"To deploy a TensorFlow model as a web service in Azure Machine Learning, you need to follow several key steps and consider certain aspects to ensure a successful deployment. Here's a high-level overview of the process and key considerations:

1. **Register the Model**: First, you need to register your trained TensorFlow model in your Azure Machine Learning workspace. This involves uploading the model files and creating a model registry that keeps track of all your models.

2. **Prepare an Inference Configuration**: Create an inference configuration that specifies the environment needed to host the model. This includes the Python environment with all necessary packages, as well as the scoring script (`score.py`) that will be used to run the model. The scoring script must contain two functions: `init()` to load the model and `run(data)` to run the model on input data.

3. **Prepare an Entry Script**: The entry script, or scoring script, is crucial as it defines how the model will process incoming requests. It should handle pre-processing of input data, model prediction, and post-processing of the model output.

4. **Create a Deployment Configuration**: Depending on whether you want to deploy your model as an online endpoint (real-time inference) or a batch endpoint (batch inference), you will need to create a deployment configuration. This configuration specifies the compute target and other settings like CPU/GPU resources, memory, and instance count.

5. **Deploy the Model**: With the model registered, the inference configuration set, and the deployment configuration ready, you can now deploy the model as a web service on Azure. This can be done through the Azure Machine Learning studio or using the Azure Machine Learning SDK.

6. **Test the Web Service**: After deployment, it's important to test the web service to ensure it's working as expected. You can send sample requests to the endpoint and check if the responses are correct.

Key considerations for deployment:

- **Scalability**: Choose the right compute target and instance size that can scale according to the load. Azure Machine Learning supports both CPU and GPU resources, so select the one that matches your model's requirements.

- **Cost**: Be mindful of the cost implications of the resources you allocate for your service. More powerful instances and higher scalability settings will lead to higher costs.

- **Security**: Ensure that your endpoint is secure. Azure provides features like authentication and network isolation to protect your web service.

- **Monitoring and Logging**: Set up monitoring and logging to keep track of the health and usage of your web service. Azure Machine Learning provides capabilities to monitor metrics and logs.

- **Versioning**: Use model versioning to manage and deploy different versions of your model. This allows for easy rollback and management of multiple model versions.

- **Data Drift Monitoring**: For models in production, it's important to monitor for data drift, which can affect model performance over time. Azure Machine Learning has features to detect and alert you to data drift.

Remember that the specific commands and code snippets to accomplish each step can be found in the Azure Machine Learning documentation or SDK references.","['What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps', 'Tutorial Descr iption\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'To learn more, review these articles:\nDeploy models with REST\nCreate and use managed online endpoints in the studio\nSafe rollout for online endpoints\nHow to autoscale managed online endpoints\nView costs for an Azure Machine Learning managed online endpoint\nAccess Azure resources with a managed online endpoint and managed identity\nTroubleshoot managed online endpoints deployment', ""Option 2: T o deploy a specific model iteration from this experiment.\na. Select the desired model from the Models  tab\nb. Select Deploy  on the top left of the window.\n2. Populate the Deploy model  pane.\nField Value\nName Enter a unique name for your deployment.\nDescription Enter a description to better identify what this deployment is for.\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\nService (AKS)  or Azure Container Inst ance (ACI).\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\ndeploy to.\nEnable\nauthenticationSelect to allow for token-based or key-based authentication.\nUse custom\ndeployment assetsEnable this feature if you want to upload your own scoring script and\nenvironment file. Otherwise, automated ML provides these assets for\nyou by default. Learn more about scoring scripts .\nThe Advanced menu offers default deployment features such as data collection  and\nresource utilization settings. If you wish to override these defaults do so in this\nmenu.\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\ndeployment begins, the Model summar y tab appears. See the deployment\nprogress under the Deploy status  section.\nNow you have an operational web service to generate predictions! Y ou can test the\npredictions by querying the service from Power BI's built in Azure Machine Learning\nsupport .ﾉExpand table\n） Impor tant\nFile names must be under 32 characters and must begin and end with\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\nbetween. Spaces are not allowed."", 'In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\nmodel to an online endpoint. See these other articles to learn more about Azure\nMachine Learning.\nTrack run metrics during training\nTune hyperparameters\nReference architecture for distributed deep learning training in AzureNext steps']",1171
gpt-35-turbo,13,,5.0,,Provide an example of how to use the AML SDK v2 for distributed deep learning training. What configurations are necessary for multi-node training?,v1,"To use Azure Machine Learning SDK v2 for distributed deep learning training, you would typically follow these steps and include the necessary configurations for multi-node training:

1. **Set up your environment**: Install the Azure Machine Learning SDK v2 and configure your environment with the necessary dependencies for your deep learning framework (PyTorch, TensorFlow, etc.).

2. **Create or attach a compute target**: Define a multi-node compute cluster in Azure Machine Learning where the training will occur. This involves specifying the size and number of nodes.

3. **Configure the distributed training settings**: Use the `DistributedConfiguration` class to specify the type of distributed training and the number of worker nodes.

4. **Prepare your training script**: Ensure your training script is compatible with distributed training. For example, with PyTorch, you would use `torch.distributed.launch` or `torch.nn.parallel.DistributedDataParallel`.

5. **Submit the training run**: Use the `CommandJob` class to submit your training script along with the distributed configuration to the compute target.

Here is a simplified example of how you might set up a distributed training job using the Azure Machine Learning SDK v2:

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import CommandJob, Environment, DistributedConfiguration
from azure.identity import DefaultAzureCredential

# Authenticate to Azure
credential = DefaultAzureCredential()
ml_client = MLClient(credential, ""<your-subscription-id>"", ""<your-resource-group>"")

# Define the environment for your training job
environment = Environment(
    image=""mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04"",
    conda_file=""path_to_your_conda_file.yml"",
    name=""my-distributed-training-env"",
    version=""1""
)

# Define the distributed configuration
# For example, using PyTorch with Horovod
distributed_config = DistributedConfiguration(
    process_count_per_instance=4,  # Number of processes per node
    node_count=2,  # Number of nodes in the cluster
    distribution_type=""PyTorch"",  # Specify the framework
)

# Define the command job for training
command_job = CommandJob(
    code=""path_to_your_training_script_folder"",
    command=""python train.py"",  # Your training script
    environment=environment,
    compute=""name_of_your_compute_cluster"",
    distributed=distributed_config,
    resources=ResourceConfiguration(gpu_count=1),  # Specify GPU count if needed
)

# Submit the job
returned_job = ml_client.jobs.create_or_update(command_job)
```

In this example, replace placeholders like `<your-subscription-id>`, `<your-resource-group>`, `path_to_your_conda_file.yml`, `path_to_your_training_script_folder`, and `name_of_your_compute_cluster` with your specific details. The `train.py` is your training script that should be set up for distributed training.

Please note that the Azure Machine Learning SDK v2 is continuously evolving, and the exact classes and methods may change. Always refer to the latest Azure Machine Learning documentation for the most up-to-date information.","[""Distributed training with Azure Machine\nLearning\nArticle •03/27/2023\nIn this article, you learn about distributed training and how Azure Machine Learning\nsupports it for deep learning models.\nIn distributed training the workload to train a model is split up and shared among\nmultiple mini processors, called worker nodes. These worker nodes work in parallel to\nspeed up model training. Distributed training can be used for traditional ML models, but\nis better suited for compute and time intensive tasks, like deep learning  for training\ndeep neural networks.\nThere are two main types of distributed training: data parallelism  and model parallelism .\nFor distributed training on deep learning models, the Azure Machine Learning SDK in\nPython  supports integrations with popular frameworks, PyT orch and T ensorFlow. Both\nframeworks employ data parallelism for distributed training, and can leverage\nhorovod  for optimizing compute speeds.\nDistributed training with PyT orch\nDistributed training with T ensorFlow\nFor ML models that don't require distributed training, see train models with Azure\nMachine Learning  for the different ways to train models using the Python SDK.\nData parallelism is the easiest to implement of the two distributed training approaches,\nand is sufficient for most use cases.\nIn this approach, the data is divided into partitions, where the number of partitions is\nequal to the total number of available nodes, in the compute cluster or serverless\ncompute . The model is copied in each of these worker nodes, and each worker operates\non its own subset of the data. K eep in mind that each node has to have the capacity to\nsupport the model that's being trained, that is the model has to entirely fit on each\nnode. The following diagram provides a visual demonstration of this approach.Deep learning and distributed training\nData parallelism"", ""Distributed GPU training guide (SDK v2)\nArticle •02/02/2024\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nLearn more about using distributed GPU training code in Azure Machine Learning. This\narticle helps you run your existing distributed training code, and offers tips and\nexamples for you to follow for each framework:\nMessage P assing Interface (MPI)\nHorovod\nEnvironment variables from Open MPI\nPyTorch\nTensorFlow\nAccelerate GPU training with InfiniBand\nReview the basic concepts of distributed GPU training , such as data parallelism ,\ndistributed dat a parallelism , and model p arallelism .\nAzure Machine Learning offers an MPI job  to launch a given number of processes in\neach node. Azure Machine Learning constructs the full MPI launch command ( mpirun)\nbehind the scenes. Y ou can't provide your own full head-node-launcher commands like\nmpirun or DeepSpeed launcher.\nPrerequisites\n\uea80 Tip\nIf you don't know which type of parallelism to use, more than 90% of the time you\nshould use distribut ed data p arallelism .\nMPI\n\uea80 Tip\nThe base Docker image used by an Azure Machine Learning MPI job needs to have\nan MPI library installed. Open MPI  is included in all the Azur e Machine Learning\nGPU b ase images . When you use a custom Docker image, you are responsible\nfor making sure the image includes an MPI library. Open MPI is recommended, but"", 'Distributed DNN training achieves scalability using a data partitioning algorithm that\nrespects time series boundaries. The following diagram illustrates a simple example with\ntwo partitions:\nDuring training, the DNN data loaders on each compute load just what they need to\ncomplete an iteration of back-propagation; the whole dataset is nev er read int o\nmemor y. The partitions are further distributed across multiple compute cores (usually\nGPUs) on possibly multiple nodes to accelerate training. Coordination across computes\nis provided by the Horovod  framework.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about deep learning models  for forecasting in AutoML\nNext steps', 'For more information, see What is automated machine learning? .\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\nLearning can automate this task for arbitrary parameterized commands with little\nmodification to your job definition. R esults are visualized in the studio.\nFor more information, see Tune hyperparameters .\nEfficiency of training for deep learning and sometimes classical machine learning\ntraining jobs can be drastically improved via multinode distributed training. Azure\nMachine Learning compute clusters and serverless compute  offer the latest GPU\noptions.\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\nclusters, and serverless compute :\nPyTorch\nTensorFlow\nMPI\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\nAzure S ynapse Analytics Spark clusters.\nFor more information, see Distributed training with Azure Machine Learning .\nScaling an ML project might require scaling embarrassingly parallel model training. This\npattern is common for scenarios like forecasting demand, where a model might be\ntrained for many stores.\nTo bring a model into production, you deploy the model. The Azure Machine Learning\nmanaged endpoints abstract the required infrastructure for both batch or real-time\n(online) model scoring (inferencing).Hyperparameter optimization\nMultinode distributed training\nEmbarrassingly parallel training\nDeploy models', ""Train TensorFlow models at scale with\nAzure Machine Lea rning\nArticle •04/04/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this article, learn how to run your TensorFlow  training scripts at scale using Azure\nMachine Learning Python SDK v2.\nThe example code in this article train a T ensorFlow model to classify handwritten digits,\nusing a deep neural network (DNN); register the model; and deploy it to an online\nendpoint.\nWhether you're developing a T ensorFlow model from the ground-up or you're bringing\nan existing model into the cloud, you can use Azure Machine Learning to scale out\nopen-source training jobs using elastic cloud compute resources. Y ou can build, deploy,\nversion, and monitor production-grade models with Azure Machine Learning.\nTo benefit from this article, you'll need to:\nAccess an Azure subscription. If you don't have one already, create a free\naccount .\nRun the code in this article using either an Azure Machine Learning compute\ninstance or your own Jupyter notebook.\nAzure Machine Learning compute instance—no downloads or installation\nnecessary\nComplete the Create resources to get started  to create a dedicated notebook\nserver pre-loaded with the SDK and the sample repository.\nIn the samples deep learning folder on the notebook server, find a\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\ndeploy-with-t ensor flow.\nYour Jupyter notebook server\nInstall the Azure Machine Learning SDK (v2) .\nDownload the following files:\ntraining script tf_mnist.py\nscoring script score.py\nsample request file sample-request.json\nPrerequisites""]",2187
gpt-35-turbo,14,,5.0,,How can Azure ML's HyperDrive be used to tune hyperparameters of a deep learning model? Give an example of specifying the search space.,v1,"Azure ML's HyperDrive can be used to tune hyperparameters of a deep learning model by automating the process of selecting the best hyperparameters for your model. HyperDrive runs multiple training experiments in parallel, each with different hyperparameter configurations. It then compares the performance of each configuration and provides the best performing hyperparameters based on the defined objective metric.

Here's an example of how you can specify the search space for a deep learning model using HyperDrive in Azure Machine Learning:

```python
from azureml.train.hyperdrive import RandomParameterSampling, HyperDriveConfig, PrimaryMetricGoal
from azureml.train.hyperdrive import choice, loguniform

# Define the parameter search space
param_sampling = RandomParameterSampling({
    'learning_rate': loguniform(-6, -1),  # e.g., 0.0001 to 0.1 on a log scale
    'batch_size': choice(16, 32, 64, 128),  # discrete choices for batch size
    'momentum': uniform(0.85, 0.95),  # a range for momentum values
    # Add other hyperparameters you want to tune
})

# Specify a primary metric to optimize
primary_metric_name = 'accuracy'
primary_metric_goal = PrimaryMetricGoal.MAXIMIZE

# Create a HyperDriveConfig using the estimator (or ScriptRunConfig), hyperparameter sampler, and policy
hyperdrive_config = HyperDriveConfig(
    estimator=estimator,  # or ScriptRunConfig
    hyperparameter_sampling=param_sampling,
    policy=None,  # could be an early termination policy
    primary_metric_name=primary_metric_name,
    primary_metric_goal=primary_metric_goal,
    max_total_runs=100,
    max_concurrent_runs=4
)

# Submit the HyperDrive run to the experiment
hyperdrive_run = experiment.submit(hyperdrive_config)
```

In this example, `RandomParameterSampling` is used to define a random search over the hyperparameter space. The `learning_rate` is searched over a log-uniform distribution between 10^-6 and 10^-1, `batch_size` is chosen from a set of discrete values, and `momentum` is sampled from a uniform distribution between 0.85 and 0.95. The `HyperDriveConfig` object is then created with the defined search space, primary metric, and other configurations like the maximum number of runs and the maximum number of concurrent runs. Finally, the HyperDrive run is submitted to the experiment for execution.

Please note that you need to replace `estimator` with an actual estimator object or a `ScriptRunConfig` object that defines the training script, environment, and compute target for your deep learning model.","['For more information, see What is automated machine learning? .\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\nLearning can automate this task for arbitrary parameterized commands with little\nmodification to your job definition. R esults are visualized in the studio.\nFor more information, see Tune hyperparameters .\nEfficiency of training for deep learning and sometimes classical machine learning\ntraining jobs can be drastically improved via multinode distributed training. Azure\nMachine Learning compute clusters and serverless compute  offer the latest GPU\noptions.\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\nclusters, and serverless compute :\nPyTorch\nTensorFlow\nMPI\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\nAzure S ynapse Analytics Spark clusters.\nFor more information, see Distributed training with Azure Machine Learning .\nScaling an ML project might require scaling embarrassingly parallel model training. This\npattern is common for scenarios like forecasting demand, where a model might be\ntrained for many stores.\nTo bring a model into production, you deploy the model. The Azure Machine Learning\nmanaged endpoints abstract the required infrastructure for both batch or real-time\n(online) model scoring (inferencing).Hyperparameter optimization\nMultinode distributed training\nEmbarrassingly parallel training\nDeploy models', 'Hyperparameter tuning a model (v2)\nArticle •04/04/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAutomate efficient hyperparameter tuning using Azure Machine Learning SDK v2 and\nCLI v2 by way of the S weepJob type.\n1. Define the parameter search space for your trial\n2. Specify the sampling algorithm for your sweep job\n3. Specify the objective to optimize\n4. Specify early termination policy for low-performing jobs\n5. Define limits for the sweep job\n6. Launch an experiment with the defined configuration\n7. Visualize the training jobs\n8. Select the best configuration for your model\nHyperp aramet ers are adjustable parameters that let you control the model training\nprocess. For example, with neural networks, you decide the number of hidden layers and\nthe number of nodes in each layer. Model performance depends heavily on\nhyperparameters.\nHyperp aramet er tuning , also called hyperp aramet er optimization , is the process of\nfinding the configuration of hyperparameters that results in the best performance. The\nprocess is typically computationally expensive and manual.\nAzure Machine Learning lets you automate hyperparameter tuning and run experiments\nin parallel to efficiently optimize hyperparameters.\nTune hyperparameters by exploring the range of values defined for each\nhyperparameter.\nHyperparameters can be discrete or continuous, and has a distribution of values\ndescribed by a parameter expression .\nWhat is hyperparameter tuning ?\nDefine the search space', 'When training computer vision models, model performance depends heavily on the\nhyperparameter values selected. Often, you might want to tune the hyperparameters to\nget optimal performance. For computer vision tasks, you can sweep hyperparameters to\nfind the optimal settings for your model. This feature applies the hyperparameter tuning\ncapabilities in Azure Machine Learning. Learn how to tune hyperparameters .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nYAML\nYou can define the model architectures and hyperparameters to sweep in the parameter\nspace. Y ou can either specify a single model architecture or multiple ones.\nSee Individual trials  for the list of supported model architectures for each task type.\nSee Hyperparameters for computer vision tasks  hyperparameters for each\ncomputer vision task type.Azure CLI\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value:  0.0001\n      max_value:  0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value:  0.0001\n      max_value:  0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\nDefine the parameter search space', 'To tune the model\'s hyperparameters, define the parameter space in which to search\nduring training. Y ou\'ll do this by replacing some of the parameters ( batch_size,\nfirst_layer_neurons, second_layer_neurons, and learning_rate) passed to the training\njob with special inputs from the azure.ml.sweep package.\nPython\nThen, you\'ll configure sweep on the command job, using some sweep-specific\nparameters, such as the primary metric to watch and the sampling algorithm to use.\nIn the following code, we use random sampling to try different configuration sets of\nhyperparameters in an attempt to maximize our primary metric, validation_acc.\nWe also define an early termination policy—the BanditPolicy. This policy operates by\nchecking the job every two iterations. If the primary metric, validation_acc, falls outside\nthe top ten percent range, Azure Machine Learning will terminate the job. This saves the\nmodel from continuing to explore hyperparameters that show no promise of helping to\nreach the target metric.\nPythonfrom azure.ai.ml.sweep import Choice, LogUniform  \n# we will reuse the command_job created before. we call it as a function so  \nthat we can apply inputs  \n# we do not apply the \'iris_csv\' input again -- we will just use what was  \nalready defined earlier  \njob_for_sweep = job(  \n    batch_size=Choice(values=[ 25, 50, 100]), \n    first_layer_neurons=Choice(values=[ 10, 50, 200, 300, 500]), \n    second_layer_neurons=Choice(values=[ 10, 50, 200, 500]), \n    learning_rate=LogUniform(min_value= -6, max_value= -1), \n)\nfrom azure.ai.ml.sweep import BanditPolicy  \nsweep_job = job_for_sweep.sweep(  \n    compute=gpu_compute_target,  \n    sampling_algorithm= ""random"" , \n    primary_metric= ""Accuracy"" , \n    goal= ""Maximize"" , \n    max_total_trials= 20, \n    max_concurrent_trials= 4, \n    early_termination_policy=BanditPolicy(slack_factor= 0.1, \nevaluation_interval= 2), \n)', 'You trained the model with one set of parameters, let\'s now see if you can further\nimprove the accuracy of your model. Y ou can tune and optimize your model\'s\nhyperparameters using Azure Machine Learning\'s sweep  capabilities.\nTo tune the model\'s hyperparameters, define the parameter space in which to search\nduring training. Y ou do this by replacing some of the parameters passed to the training\njob with special inputs from the azure.ml.sweep package.\nSince the training script uses a learning rate schedule to decay the learning rate every\nseveral epochs, you can tune the initial learning rate and the momentum parameters.\nPython\nThen, you can configure sweep on the command job, using some sweep-specific\nparameters, such as the primary metric to watch and the sampling algorithm to use.\nIn the following code, we use random sampling to try different configuration sets of\nhyperparameters in an attempt to maximize our primary metric, best_val_acc.\nWe also define an early termination policy, the BanditPolicy, to terminate poorly\nperforming runs early. The BanditPolicy terminates any run that doesn\'t fall within the\nslack factor of our primary evaluation metric. Y ou apply this policy every epoch (since we\nreport our best_val_acc metric every epoch and evaluation_interval=1). Notice we\ndelay the first policy evaluation until after the first 10 epochs ( delay_evaluation=10).\nPythonfrom azure.ai.ml.sweep import Uniform\n# we will reuse the command_job created before. we call it as a function so  \nthat we can apply inputs\njob_for_sweep = job(\n    learning_rate=Uniform(min_value= 0.0005, max_value= 0.005),\n    momentum=Uniform(min_value= 0.9, max_value= 0.99),\n)\nfrom azure.ai.ml.sweep import BanditPolicy\nsweep_job = job_for_sweep.sweep(\n    compute= ""gpu-cluster"" ,\n    sampling_algorithm= ""random"" ,\n    primary_metric= ""best_val_acc"" ,\n    goal=""Maximize"" ,\n    max_total_trials= 8,\n    max_concurrent_trials= 4,\n    early_termination_policy=BanditPolicy(\n        slack_factor= 0.15, evaluation_interval= 1, delay_evaluation= 10']",2457
gpt-35-turbo,15,,5.0,,"What is the command to update an existing model's properties, such as description or tags, using the Azure CLI?",v1,"The command to update an existing model's properties, such as description or tags, using the Azure CLI is:

```azurecli
az ml model update --name run-model-example --version 1 --set description=""This is an updated description."" --set tags.stage=""Prod""
```

Replace `run-model-example` with the name of your model and `1` with the version number you want to update. Adjust the description and tags as needed for your specific use case.","['Get the details of a specific model:\ncli\nUpdate mutable properties of a specific model:\ncli\nArchiving a model will hide it by default from list queries ( az ml model list). You can\nstill continue to reference and use an archived model in your workflows. Y ou can archive\neither all versions of a model or only a specific version.\nIf you don\'t specify a version, all versions of the model under that given name will be\narchived. If you create a new model version under an archived model container, that\nnew version will automatically be set as archived as well.\nArchive all versions of a model:Show\nAzure CLI\naz ml model show --name run-model-example --version 1  \nUpdate\nAzure CLI\naz ml model update --name  run-model-example --version 1 --set  \ndescription=""This is an updated description."" --set tags.stage=""Prod""  \n） Impor tant\nFor model, only description and tags can be updated. All other properties are\nimmutable; if you need to change any of those properties you should create a new\nversion of the model.\nArchive', 'Execute the following command in the Azure CLI, and update the <> placeholders\nwith your data asset name, version and key-value pair for the tag.\nAzure CLI\nTypically, your ETL processes organize your folder structure on Azure storage by time,\nfor example:\ntext\nThe combination of time/version structured folders and Azure Machine Learning T ables\n(MLTable) allow you to construct versioned datasets. T o show how to achieve versioned\ndata with Azure Machine Learning T ables, we use a hypothetical ex ample . Suppose you\nhave a process that uploads camera images to Azure Blob storage every week, in the\nfollowing structure:\ntextaz ml data update --name <DATA ASSET NAME>  --version  <VERSION>  --set \ntags.<KEY>=<VALUE> \nVersioning best practices\n/ \n└── 📁 mydata \n    ├── 📁  year=2022  \n    │   ├── 📁  month=11  \n    │   │   └── 📄  file1 \n    │   │   └── 📄  file2 \n    │   └── 📁  month=12  \n    │       └── 📄  file1 \n    │   │   └── 📄  file2 \n    └── 📁  year=2023  \n        └── 📁  month=1  \n            └── 📄  file1 \n    │   │   └── 📄  file2 \n/myimages  \n└── 📁 year=2022  \n    ├── 📁  week52 \n    │   ├── 📁  camera1  \n    │   │   └── 🖼  file1.jpeg  \n    │   │   └── 🖼  file2.jpeg  \n    │   └── 📁  camera2  \n    │       └── 🖼  file1.jpeg  \n    │   │   └── 🖼  file2.jpeg', 'The az ml model command can be used for managing Azure Machine Learning models.\nExamples are available in the examples GitHub repository . Several are shown below.\nYAML\nYAML\nInstall and use the CLI (v2)Remarks\nExamples\nYAML: local file\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-file-example  \npath: mlflow-model/model.pkl  \ndescription:  Model created from local file. \nYAML: local folder in MLflow format\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-mlflow-example  \npath: mlflow-model  \ntype: mlflow_model  \ndescription:  Model created from local MLflow model directory.', 'If you want to update the code, model, or environment, update the Y AML file, and\nthen run the az ml online-endpoint update command.\nTo understand how update works:\n1. Open the file online/model-1/onlines coring/s core.py.\n2. Change the last line of the init() function: After logging.info(""Init\ncomplete""), add logging.info(""Updated successfully"").\n3. Save the file.\n4. Run this command:\nAzure CLI(Optional) Update the deployment\nAzure CLI\n７ Note\nIf you update instance count (to scale your deployment) along with other\nmodel settings (such as code, model, or environment) in a single update\ncommand, the scaling operation will be performed first, then the other updates\nwill be applied. It\'s a good practice to perform these operations separately in a\nproduction environment.\naz ml online-deployment update -n blue --endpoint  $ENDPOINT_NAME  -f \nendpoints/online/managed/sample/blue -deployment .yml\n７ Note\nUpdating by using Y AML is declarative. That is, changes in the Y AML are\nreflected in the underlying Azure R esource Manager resources (endpoints\nand deployments). A declarative approach facilitates GitOps : All\nchanges to endpoints and deployments (even instance_count) go\nthrough the Y AML.\n\uea80 Tip', 'then select Regist er.\nThe SDK and CLI (v2) also allow you to manage the lifecycle of your Azure Machine\nLearning model assets.\nList all the models in your workspace:\ncli\nList all the model versions under a given name:\ncli\n\uf80a\nManage models\nList\nAzure CLI\naz ml model list  \nAzure CLI\naz ml model list --name run-model-example']",1297
gpt-35-turbo,16,,5.0,,How does Azure ML integrate with Azure DevOps for implementing CI/CD pipelines for ML models? Describe the steps involved.,v1,"Azure Machine Learning integrates with Azure DevOps to implement CI/CD pipelines for ML models through a process that typically involves the following steps:

1. **Source Control Integration**: Start by setting up your machine learning code and resources in a source control repository, such as Azure Repos or GitHub. This allows you to track changes and collaborate with other developers.

2. **Azure Pipelines Setup**: Create an Azure Pipeline within Azure DevOps. This pipeline defines the continuous integration and deployment process. You can use YAML or the visual designer to configure the pipeline.

3. **Define Build Pipeline**: The build pipeline is responsible for continuous integration. It triggers automatically when changes are pushed to the repository. The pipeline can include steps to:
   - Install dependencies
   - Train the machine learning model
   - Run tests to validate the model
   - Register the model in Azure Machine Learning workspace

4. **Define Release Pipeline**: The release pipeline manages the deployment of the trained model to various environments (e.g., testing, staging, production). It can be set up to trigger after a successful build or manually. The pipeline can include steps to:
   - Retrieve the registered model from Azure Machine Learning workspace
   - Deploy the model to an Azure Machine Learning endpoint, such as an Azure Kubernetes Service (AKS) or Azure Container Instance (ACI)
   - Run additional validation tests post-deployment

5. **Artifact Management**: Use Azure Artifacts to store build artifacts that will be used in the release pipeline. This includes the trained model files and any additional scripts or files needed for deployment.

6. **Monitoring and Feedback**: Once the model is deployed, use Azure Monitor and Application Insights to collect telemetry and monitor the performance of your model in production. This feedback can be used to make informed decisions about future model improvements and iterations.

7. **Iterate and Improve**: As new data becomes available or changes are made to the model, the CI/CD pipeline allows for easy iteration. The entire process from training to deployment can be automated, ensuring that your machine learning models are always up-to-date and leveraging the latest code and data.

By integrating Azure Machine Learning with Azure DevOps, teams can automate the end-to-end lifecycle of machine learning models, ensuring consistent, repeatable, and reliable processes for deploying machine learning solutions at scale.","['Continuous integration and deployment of machine learning models with Azure\nPipelines\nMachine Learning ML Ops  repository\nLearn more by reading and exploring the following resources:\nSet up ML Ops with Azure DevOps\nLearning path: End-to-end ML Ops with Azure Machine Learning\nHow to deploy a model to an online endpoint  with Machine Learning\nTutorial: T rain and deploy a model\nCI/CD of machine learning models with Azure Pipelines\nMachine learning at scale\nAzure AI reference architectures and best practices repo\nNext steps', 'Some key features enabling ML Ops include:\ngit integration.\nMLflow integration.\nMachine learning pipeline scheduling.\nAzure Event Grid integration for custom triggers.\nEase of use with CI/CD tools like GitHub Actions or Azure DevOps.\nMachine Learning also includes features for monitoring and auditing:\nJob artifacts, such as code snapshots, logs, and other outputs.\nLineage between jobs and assets, such as containers, data, and compute resources.\nIf you use Apache Airflow, the airflow-provider-azure-machinelearning  package is a\nprovider that enables you to submit workflows to Azure Machine Learning from Apache\nAirFlow.\nStart using Azure Machine Learning:\nSet up an Azure Machine Learning workspace\nTutorial: Build a first machine learning project\nRun training jobs\nNext steps', 'What are Azu re Machine Learning\npipelines?\nArticle •01/31/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAn Azure Machine Learning pipeline is an independently executable workflow of a\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\nstandardize the best practices of producing a machine learning model, enables the team\nto execute at scale, and improves the model building efficiency.\nThe core of a machine learning pipeline is to split a complete machine learning task into\na multistep workflow. Each step is a manageable component that can be developed,\noptimized, configured, and automated individually. S teps are connected through well-\ndefined interfaces. The Azure Machine Learning pipeline service automatically\norchestrates all the dependencies between pipeline steps. This modular approach brings\ntwo key benefits:\nStandardize the Machine learning operation (ML Ops) practice and support scalable\nteam collaboration\nTraining efficiency and cost reduction\nMachine learning operation (ML Ops) automates the process of building machine\nlearning models and taking the model to production. This is a complex process. It\nusually requires collaboration from different teams with different skills. A well-defined\nmachine learning pipeline can abstract this complex process into a multiple steps\nworkflow, mapping each step to a specific task such that each team can work\nindependently.\nFor example, a typical machine learning project includes the steps of data collection,\ndata preparation, model training, model evaluation, and model deployment. Usually, the\ndata engineers concentrate on data steps, data scientists spend most time on model\nWhy are Azure Machine Learning pipelines\nneeded?\nStandardize the MLOps practice and support scalable\nteam collaboration', 'What are Azu re Machine Learning\npipelines?\nArticle •01/31/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAn Azure Machine Learning pipeline is an independently executable workflow of a\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\nstandardize the best practices of producing a machine learning model, enables the team\nto execute at scale, and improves the model building efficiency.\nThe core of a machine learning pipeline is to split a complete machine learning task into\na multistep workflow. Each step is a manageable component that can be developed,\noptimized, configured, and automated individually. S teps are connected through well-\ndefined interfaces. The Azure Machine Learning pipeline service automatically\norchestrates all the dependencies between pipeline steps. This modular approach brings\ntwo key benefits:\nStandardize the Machine learning operation (ML Ops) practice and support scalable\nteam collaboration\nTraining efficiency and cost reduction\nMachine learning operation (ML Ops) automates the process of building machine\nlearning models and taking the model to production. This is a complex process. It\nusually requires collaboration from different teams with different skills. A well-defined\nmachine learning pipeline can abstract this complex process into a multiple steps\nworkflow, mapping each step to a specific task such that each team can work\nindependently.\nFor example, a typical machine learning project includes the steps of data collection,\ndata preparation, model training, model evaluation, and model deployment. Usually, the\ndata engineers concentrate on data steps, data scientists spend most time on model\nWhy are Azure Machine Learning pipelines\nneeded?\nStandardize the MLOps practice and support scalable\nteam collaboration', 'Use Azu re Pipelines with Azure Machine\nLearning\nArticle •09/29/2023\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\nof the operations you can automate are:\nData preparation (extract, transform, load operations)\nTraining machine learning models with on-demand scale-out and scale-up\nDeployment of machine learning models as public or private web services\nMonitoring deployed machine learning models (such as for performance or data-\ndrift analysis)\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\nmachine learning model to Azure Machine Learning .\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\nv2.\nComplete the Create resources to get started  to:\nCreate a workspace\nCreate a cloud-based compute cluster  to use for training your model\nAzure Machine Learning extension for Azure Pipelines. This extension can be\ninstalled from the Visual S tudio marketplace at\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-\nv2.\nFork the following repo at GitHub:Prerequisites\nStep 1: Get the code\nhttps://github.com/azure/azureml-examples']",1739
gpt-35-turbo,17,,5.0,,Explain the process of setting up a real-time inferencing endpoint using a deep learning model in Azure ML. What are the performance tuning options available?,v1,"To set up a real-time inferencing endpoint using a deep learning model in Azure Machine Learning, you would typically follow these steps:

1. **Register the Model**:
   - First, you need to register your deep learning model in Azure ML. This can be done using the Azure ML Python SDK or the Azure CLI. You'll need to provide the model name, path to the model file, and any other relevant metadata.

2. **Create Scoring Script**:
   - Create a scoring script (entry script) that loads your model and defines how the model will process incoming requests. This script should include two main functions: `init()` to load the model and `run(raw_data)` to process the data and return predictions.

3. **Define an Inference Configuration**:
   - Define an inference configuration that specifies the environment needed for your model. This includes the dependencies, such as the necessary Python packages, and the scoring script.

4. **Define an Endpoint**:
   - Specify the endpoint name and choose the authentication mode (key-based or token-based).

5. **Deploy the Model to an Online Endpoint**:
   - Deploy your model to an online endpoint. You can choose between a managed online endpoint or a Kubernetes online endpoint, depending on your needs. Managed online endpoints are fully managed by Azure and are recommended for most use cases.

6. **Test the Endpoint**:
   - Once the endpoint is deployed, you can test it by sending HTTP requests with the input data and receiving the predictions in response.

For performance tuning of real-time inferencing endpoints in Azure ML, you have several options:

1. **Instance Type Selection**:
   - Choose the right compute instance type with the appropriate CPU/GPU resources based on the model's requirements and expected workload.

2. **Scaling**:
   - Configure autoscaling policies to automatically scale the number of instances based on the load. This helps in handling varying numbers of inference requests while optimizing costs.

3. **Concurrency**:
   - Adjust the concurrency level, which is the number of parallel requests that an instance can process. This should be tuned based on the model's complexity and resource requirements.

4. **Model Optimization**:
   - Optimize your deep learning model for inference, for example, by using model quantization or pruning techniques to reduce the model size and improve latency.

5. **Caching**:
   - Implement caching for frequent inference requests to reduce latency and compute overhead.

6. **Batching**:
   - If applicable, use batch processing to process multiple inference requests at once, which can improve throughput.

7. **Monitoring and Diagnostics**:
   - Utilize Azure ML's monitoring and diagnostics features to analyze the endpoint's performance and identify bottlenecks or issues.

Remember to review the Azure ML documentation for the latest features and best practices, as Azure ML services are frequently updated with new capabilities and enhancements.","['Online endpoints and deployments for\nreal-time inference\nArticle •10/24/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAzure Machine Learning allows you to perform real-time inferencing on data by using\nmodels that are deployed to online endpoints . Inferencing is the process of applying new\ninput data to a machine learning model to generate outputs. While these outputs are\ntypically referred to as ""predictions,"" inferencing can be used to generate outputs for\nother machine learning tasks, such as classification and clustering.\nOnline endpoints  deploy models to a web server that can return predictions under the\nHTTP protocol. Use online endpoints to operationalize models for real-time inference in\nsynchronous low-latency requests. W e recommend using them when:\nTo define an endpoint, you need to specify:\nEndpoint name : This name must be unique in the Azure region. For more\ninformation on the naming rules, see endpoint limits .\nAuthentication mode : You can choose between key-based authentication mode\nand Azure Machine Learning token-based authentication mode for the endpoint. A\nkey doesn\'t expire, but a token does expire. For more information on\nauthenticating, see Authenticate to an online endpoint .\nAzure Machine Learning provides the convenience of using managed online endpoints\nfor deploying your ML models in a turnkey manner. This is the recommended  way to use\nonline endpoints in Azure Machine Learning. Managed online endpoints work with\npowerful CPU and GPU machines in Azure in a scalable, fully managed way. These\nendpoints also take care of serving, scaling, securing, and monitoring your models, to\nfree you from the overhead of setting up and managing the underlying infrastructure. T o\nOnline  endpoints\nYou have low-latency requirements＂\nYour model can answer the request in a relatively short amount of time＂\nYour model\'s inputs fit on the HT TP payload of the request ＂\nYou need to scale up in terms of number of requests＂', 'In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\nmodel to an online endpoint. See these other articles to learn more about Azure\nMachine Learning.\nTrack run metrics during training\nTune hyperparameters\nReference architecture for distributed deep learning training in AzureNext steps', ""Batch scoring, or batch infer encing , involves invoking an endpoint with a reference to\ndata. The batch endpoint runs jobs asynchronously to process data in parallel on\ncompute clusters and store the data for further analysis.\nReal-time s coring, or online infer encing , involves invoking an endpoint with one or more\nmodel deployments and receiving a response in near real time via HT TPS. T raffic can be\nsplit across multiple deployments, allowing for testing new model versions by diverting\nsome amount of traffic initially and increasing after confidence in the new model is\nestablished.\nFor more information, see:\nDeploy a model with a real-time managed endpoint\nUse batch endpoints for scoring\nDevOps for ML models, often called ML Ops, is a process for developing models for\nproduction. A model's lifecycle from training to deployment must be auditable if not\nreproducible.\nLearn more about MLOps in Azure Machine Learning .\nMachine Learning is built with the model lifecycle in mind. Y ou can audit the model\nlifecycle down to a specific commit and environment.Real-time and batch scoring (inferencing)\nMLOps: DevOps for machine learning\nML model lifecycle\nIntegrations enabling MLOPs"", 'What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps', ""Currently, Azure Machine Learning supports fine-tuning models for the following\nlanguage tasks:\nText classification\nToken classification\nQuestion answering\nSummarization\nTranslation\nTo enable users to quickly get started with fine-tuning, we have published samples (both\nPython notebooks and CLI examples) for each task in the azureml-examples git repo\nFinetune samples . Each model card also links to fine-tuning samples for supported\nfine-tuning tasks.\nYou can deploy foundation models (both pre-trained models from the model catalog,\nand fine-tuned models, once they're registered to your workspace) to an endpoint that\ncan then be used for inferencing. Deployment to both real time endpoints and batch\nendpoints is supported. Y ou can deploy these models by using either the Deploy UI\nwizard or by using the code based samples linked from the model card.\nYou can invoke the Deploy UI form by selecting the Deploy  button on the model card\nfor any foundation model, and selecting either R eal-time endpoint or Batch endpoint\nDeploying foundation models to endpoints for\ninferencing\nDeploying using the studio\n\uf80a\nDeployment settings""]",1597
gpt-35-turbo,18,,5.0,,How can you automate the process of model retraining and deployment using Azure ML Pipelines?,v1,"You can automate the process of model retraining and deployment using Azure ML Pipelines by following these steps:

1. **Create an Azure ML Workspace**: This is the foundational block for any machine learning work in Azure ML. It provides a centralized place to work with all the artifacts you create when you use Azure Machine Learning.

2. **Create a Compute Cluster**: You need a cloud-based compute cluster that can be used to train your model. This allows you to scale out and scale up your training process as needed.

3. **Install Azure Machine Learning Extension for Azure Pipelines**: This extension allows you to integrate Azure Machine Learning with Azure DevOps. You can install it from the Visual Studio marketplace.

4. **Fork and Clone the Sample Repository**: Fork the sample repository provided in the tutorial to get the code that you'll use to define your pipeline.

5. **Define Your Pipeline**: Using the Azure Machine Learning Python SDK v2, you can define a pipeline that includes steps for data preparation, model training, and model deployment. Each step in the pipeline can be a Python script, a specialized step like a data transfer step, or a command that runs in a specified compute context.

6. **Set Up Automated Retraining**: You can set triggers in your Azure DevOps pipeline to automatically start the retraining process. This can be time-based (e.g., every week), or you can use a more complex logic based on model performance metrics or data drift detection.

7. **Deploy the Model**: After the model is retrained, you can use the pipeline to deploy the model as a web service on Azure Container Instances (ACI), Azure Kubernetes Service (AKS), or other supported compute targets.

8. **Monitor the Model**: Once deployed, you can set up monitoring for your model to track its performance and detect any data drift. This can also be automated using Azure ML's monitoring capabilities.

By setting up these steps in an Azure DevOps pipeline, you can automate the entire lifecycle of your machine learning model from retraining to deployment, ensuring that your model is always up-to-date and performing optimally.","['Use Azu re Pipelines with Azure Machine\nLearning\nArticle •09/29/2023\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\nof the operations you can automate are:\nData preparation (extract, transform, load operations)\nTraining machine learning models with on-demand scale-out and scale-up\nDeployment of machine learning models as public or private web services\nMonitoring deployed machine learning models (such as for performance or data-\ndrift analysis)\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\nmachine learning model to Azure Machine Learning .\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\nv2.\nComplete the Create resources to get started  to:\nCreate a workspace\nCreate a cloud-based compute cluster  to use for training your model\nAzure Machine Learning extension for Azure Pipelines. This extension can be\ninstalled from the Visual S tudio marketplace at\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-\nv2.\nFork the following repo at GitHub:Prerequisites\nStep 1: Get the code\nhttps://github.com/azure/azureml-examples', 'Continuous integration and deployment of machine learning models with Azure\nPipelines\nMachine Learning ML Ops  repository\nLearn more by reading and exploring the following resources:\nSet up ML Ops with Azure DevOps\nLearning path: End-to-end ML Ops with Azure Machine Learning\nHow to deploy a model to an online endpoint  with Machine Learning\nTutorial: T rain and deploy a model\nCI/CD of machine learning models with Azure Pipelines\nMachine learning at scale\nAzure AI reference architectures and best practices repo\nNext steps', ""6. Select main as a branch and choose /mlops/devops-pipelines/deploy-model-\ntraining-pipeline.yml, then select Continue .\n7. Save and Run  the pipeline\nThis scenario includes prebuilt workflows for two approaches to deploying a trained\nmodel, batch scoring or a deploying a model to an endpoint for real-time scoring. Y ou\nmay run either or both of these workflows to test the performance of the model in your\nAzure ML workspace. IN this example we will be using real-time scoring.\n1. Go to ADO pipelines\n2. Select New Pipeline .\n3. Select Azur e Repos Git .７ Note\nAt this point, the infrastructure is configured and the Prototyping Loop of the\nMLOps Architecture is deployed. you're ready to move to our trained model to\nproduction.\nDeploying the Trained model\nDeploy ML model endpoint"", '3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\nMachine Learning in many different ways .\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\niterations over different models, hyperparameter settings, advanced\npreprocessing/featurization, and what metrics to look at when determining the\nbest model.\n5. Submit the training job.\n6. Review the r esults\nThe following diagram illustrates this process. \nYou can also inspect the logged job information, which contains metrics  gathered\nduring the job. The training job produces a Python serialized object ( .pkl file) that\ncontains the model and data preprocessing.\nWhile model building is automated, you can also learn how important or relevant\nfeatures are  to the generated models.\nApply automated ML when you want Azure Machine Learning to train and tune a model\nfor you using the target metric you specify. Automated ML democratizes the machine\nlearning model development process, and empowers its users, no matter their data\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\nML professionals and developers across industries can use automated ML to:\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\nforecasting, computer vision & NLP', '4. When the pipeline has complete successfully, you can find your Azure Machine\nLearning W orkspace and associated resources by logging in to the Azure P ortal.\nNext, a model training and scoring pipelines will be deployed into the new\nMachine Learning environment.\nThe solution accelerator includes code and data for a sample end-to-end machine\nlearning pipeline which runs a linear regression to predict taxi fares in NY C. The pipeline\nis made up of components, each serving different functions, which can be registered\nwith the workspace, versioned, and reused with various inputs and outputs. Sample\npipelines and workflows for the Computer Vision and NLP scenarios will have different\nsteps and deployment steps.\nThis training pipeline contains the following steps:\nPrepare Data\nThis component takes multiple taxi datasets (yellow and green) and merges/filters\nthe data, and prepare the train/val and evaluation datasets.\nInput: Local data under ./data/ (multiple .csv files)\nOutput: Single prepared dataset (.csv) and train/val/test datasets.\nTrain Model\nThis component trains a Linear R egressor with the training set.\nInput: T raining dataset\nOutput: T rained model (pickle format)\nEvaluat e Model\nThis component uses the trained model to predict taxi fares on the test set.\nInput: ML model and T est dataset\nOutput: P erformance of model and a deploy flag whether to deploy or not.Sample Training  and Deployment Scenario']",1575
gpt-35-turbo,19,,5.0,,Describe the approach to use Azure ML for implementing a recommendation system. What kind of data and algorithms are typically involved?,v1,"To implement a recommendation system using Azure Machine Learning (Azure ML), you would typically follow these steps:

1. **Data Upload and Exploration**:
   - You would start by uploading your data to Azure ML, which could include user data, item data, and interaction data (such as ratings, clicks, or purchases).
   - You would then explore and preprocess your data using Azure ML notebooks or scripts to ensure it is clean and formatted correctly for model training.

2. **Model Development**:
   - On a cloud workstation provided by Azure ML, you would begin prototyping your recommendation system. This could involve feature engineering to create meaningful attributes that the recommendation algorithm can use.

3. **Model Training**:
   - You would select an appropriate machine learning algorithm for your recommendation system. Common algorithms for recommendation systems include collaborative filtering, content-based filtering, and hybrid methods.
   - Azure ML allows you to train your model at scale, leveraging the cloud's computational resources. You can use Azure ML's automated machine learning capabilities to find the best model or tune hyperparameters manually.

4. **Model Deployment**:
   - Once the model is trained and evaluated, you would deploy it as an online endpoint using Azure ML. This endpoint can then serve real-time recommendations to users.

5. **Production Pipelines**:
   - To operationalize the recommendation system, you would create production pipelines in Azure ML. These pipelines can automate the entire workflow, from data preprocessing to model retraining and deployment, ensuring that the system remains up-to-date and performs well.

**Data Involved**:
- User data might include demographic information, preferences, or past behavior.
- Item data could consist of descriptions, categories, or other features of the items being recommended.
- Interaction data is crucial as it captures the interactions between users and items, such as ratings, views, or purchases.

**Algorithms Typically Involved**:
- **Collaborative Filtering**: This method makes automatic predictions about the interests of a user by collecting preferences from many users. It can be memory-based or model-based.
- **Content-Based Filtering**: This approach uses item features to recommend additional items similar to what the user likes, based on their previous actions or explicit feedback.
- **Hybrid Methods**: These combine collaborative and content-based filtering to improve recommendation quality and overcome the limitations inherent in any single approach.

Azure ML provides the tools and infrastructure to build, train, and deploy recommendation systems at scale, leveraging various data sources and machine learning algorithms.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Tell us about y our PDF experience.\nAzur e Machine Learning documentation\nLearn how to train and deploy models and manage the ML lifecycle (ML Ops) with Azure\nMachine Learning. Tutorials, code examples, API references, and more.\nOverview\nｅOVERVIE W\nWhat is Azure Machine Learning?\nSetup & quickstar t\nｆQUICKST ART\nCreate resources\nGet started with Azure Machine Learning\nStart with the b asics\nｇTUTORIAL\nPrepare and explore data\nDevelop on a cloud workstation\nTrain a model\nDeploy a model\nSet up a reusable pipeline\nWork with data\nｃHOW-T O GUIDE\nUse Apache Spark in Azure Machine Learning\nCreate data assets', ""What is Azure Machine Learning?\nArticle •01/29/2024\nAzure Machine Learning is a cloud service for accelerating and managing the machine\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\nin their day-to-day workflows to train and deploy models and manage machine learning\noperations (ML Ops).\nYou can create a model in Machine Learning or use a model built from an open-source\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\nretrain, and redeploy models.\nMachine Learning is for individuals and teams implementing ML Ops within their\norganization to bring ML models into production in a secure and auditable production\nenvironment.\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\nday workflows. Application developers can use tools for integrating models into\napplications or services. Platform developers can use a robust set of tools, backed by\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\nbased access control for infrastructure. Y ou can set up a project to deny access to\nprotected data and select operations.\nML projects often require a team with a varied skill set to build and maintain. Machine\nLearning has tools that help enable you to:\uea80 Tip\nFree trial!  If you don't have an Azure subscription, create a free account before you\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\nto spend on Azure services. After they're used up, you can keep the account and\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\nchange your settings and ask to be charged.\nWho is Azure Machine Learning for?\nProductivity for everyone on the team"", ""If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about calendar features for time series forecasting in AutoML .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about AutoML forecasting for lagged features .Next steps"", 'For training in developing AI and Machine Learning solutions with Microsoft, see\nMicrosoft Learn training .\nChoose a Microsoft cognitive services technology\nArtificial intelligence (AI) architecture design\nHow Azure Machine Learning works: resources and assetsRelated resources']",1297
gpt-35-turbo,20,,5.0,,How can you leverage Azure ML's built-in support for ONNX models for interoperability across different frameworks?,v1,"Azure Machine Learning (Azure ML) provides built-in support for ONNX (Open Neural Network Exchange) models, which facilitates interoperability across different machine learning frameworks. Here's how you can leverage this support:

1. **Model Conversion to ONNX**: You can convert models from various frameworks such as TensorFlow, PyTorch, Scikit-Learn, Keras, and others into the ONNX format. Azure ML supports the conversion process through its integration with the ONNX converters available for these frameworks.

2. **Model Deployment**: Once you have an ONNX model, Azure ML allows you to deploy it as a web service on Azure Container Instances (ACI), Azure Kubernetes Service (AKS), or on IoT devices. The ONNX Runtime is used to serve the model, ensuring efficient inference across different platforms.

3. **Inference Optimization**: ONNX Runtime, which is integrated with Azure ML, provides optimizations for different hardware accelerators. This means that you can deploy your ONNX models and expect them to run efficiently on various hardware types, including CPUs, GPUs, and FPGAs.

4. **MLOps**: Azure ML supports MLOps practices, which include versioning, monitoring, and CI/CD for machine learning models. You can manage ONNX models within Azure ML, keeping track of different versions and their performance over time.

5. **Scalability and High Availability**: By deploying ONNX models on Azure ML, you can take advantage of Azure's infrastructure to scale your model to handle high loads and ensure high availability.

6. **Integration with Azure Services**: ONNX models deployed on Azure ML can easily integrate with other Azure services, such as Azure Functions, Azure Logic Apps, and Azure IoT Edge, allowing for a seamless flow of data and predictions across your applications.

To leverage these capabilities, you would typically start by training your model in your preferred framework, convert it to ONNX format, and then use Azure ML's tools and services to deploy and manage the model in production.","[""ONNX and Azure Machine Learning:\nCreate and accelerate ML models\nArticle •06/13/2023\nLearn how using the Open Neural Network Exchange  (ONNX) can help optimize the\ninference of your machine learning model. Inference, or model scoring, is the phase\nwhere the deployed model is used for prediction, most commonly on production data.\nOptimizing machine learning models for inference (or model scoring) is difficult since\nyou need to tune the model and the inference library to make the most of the hardware\ncapabilities. The problem becomes extremely hard if you want to get optimal\nperformance on different kinds of platforms (cloud/edge, CPU/GPU, etc.), since each one\nhas different capabilities and characteristics. The complexity increases if you have\nmodels from a variety of frameworks that need to run on a variety of platforms. It's very\ntime consuming to optimize all the different combinations of frameworks and hardware.\nA solution to train once in your preferred framework and run anywhere on the cloud or\nedge is needed. This is where ONNX comes in.\nMicrosoft and a community of partners created ONNX as an open standard for\nrepresenting machine learning models. Models from many frameworks  including\nTensorFlow, PyT orch, SciKit-Learn, K eras, Chainer, MXNet, MA TLAB, and SparkML can be\nexported or converted to the standard ONNX format. Once the models are in the ONNX\nformat, they can be run on a variety of platforms and devices.\nONNX Runtime  is a high-performance inference engine for deploying ONNX models\nto production. It's optimized for both cloud and edge and works on Linux, Windows,\nand Mac. Written in C++, it also has C, Python, C#, Java, and JavaScript (Node.js) APIs\nfor usage in a variety of environments. ONNX Runtime supports both DNN and\ntraditional ML models and integrates with accelerators on different hardware such as\nTensorR T on NVidia GPUs, OpenVINO on Intel processors, DirectML on Windows, and\nmore. By using ONNX Runtime, you can benefit from the extensive production-grade\noptimizations, testing, and ongoing improvements.\nONNX Runtime is used in high-scale Microsoft services such as Bing, Office, and Azure\nAI. Performance gains are dependent on a number of factors, but these Microsoft\nservices have seen an average 2x per formance gain on CPU . In addition to Azure\nMachine Learning services, ONNX Runtime also runs in other products that support\nMachine Learning workloads, including:\nWindows: The runtime is built into Windows as part of Windows Machine Learning\nand runs on hundreds of millions of devices."", 'Azure SQL product family: Run native scoring on data in Azure SQL Edge  and\nAzure SQL Managed Instance .\nML.NET: Run ONNX models in ML.NET .\nYou can obtain ONNX models in several ways:\nTrain a new ONNX model in Azure Machine Learning (see examples at the bottom\nof this article) or by using automated Machine Learning capabilities\nConvert existing model from another format to ONNX (see the tutorials )\nGet a pre-trained ONNX model from the ONNX Model Zoo\nGenerate a customized ONNX model from Azure Custom Vision service\nMany models including image classification, object detection, and text processing can\nbe represented as ONNX models. If you run into an issue with a model that cannot be\nconverted successfully, please file an issue in the GitHub of the respective converter that\nyou used. Y ou can continue using your existing format model until the issue is\naddressed.\nWith Azure Machine Learning, you can deploy, manage, and monitor your ONNX\nmodels. Using the standard deployment workflow  and ONNX Runtime, you can create a\nREST endpoint hosted in the cloud. See example Jupyter notebooks at the end of this\narticle to try it out for yourself.\n\uf80a\nGet ONNX models\nDeploy ONNX models in Azure\nInstall and use ONNX Runtime with Python', 'See the AutoML package  for changing default ensemble settings in automated machine\nlearning.\nWith Azure Machine Learning, you can use automated ML to build a Python model and\nhave it converted to the ONNX format. Once the models are in the ONNX format, they\ncan be run on a variety of platforms and devices. Learn more about accelerating ML\nmodels with ONNX .\nSee how to convert to ONNX format in this Jupyter notebook example . Learn which\nalgorithms are supported in ONNX .\nThe ONNX runtime also supports C#, so you can use the model built automatically in\nyour C# apps without any need for recoding or any of the network latencies that REST\nendpoints introduce. Learn more about using an AutoML ONNX model in a .NET\napplication with ML.NET  and inferencing ONNX models with the ONNX runtime C#\nAPI .\nThere are multiple resources to get you up and running with AutoML.\nTutorials are end-to-end introductory examples of AutoML scenarios.\nFor a code fir st experience , follow the Tutorial: T rain an object detection model\nwith AutoML and Python\nFor a low or no-code experience , see the Tutorial: T rain a classification model with\nno-code AutoML in Azure Machine Learning studio .\nHow-to articles provide additional detail into what functionality automated ML offers.\nFor example,\nConfigure the settings for automatic training experiments\nWithout code in the Azure Machine Learning studio .\nWith the Python SDK .\nLearn how to train computer vision models with Python .\nLearn how to view the generated code from your automated ML models (SDK v1) .AutoML & ONNX\nNext steps\nTutorials/ how-tos', ""Make predictions with ONNX on\ncomputer vision models from AutoML\nArticle •04/04/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this article, you will learn how to use Open Neural Network Exchange (ONNX) to\nmake predictions on computer vision models generated from automated machine\nlearning (AutoML) in Azure Machine Learning.\nTo use ONNX for predictions, you need to:\n1. Download ONNX model files from an AutoML training run.\n2. Understand the inputs and outputs of an ONNX model.\n3. Preprocess your data so that it's in the required format for input images.\n4. Perform inference with ONNX Runtime for Python.\n5. Visualize predictions for object detection and instance segmentation tasks.\nONNX  is an open standard for machine learning and deep learning models. It enables\nmodel import and export (interoperability) across the popular AI frameworks. For more\ndetails, explore the ONNX GitHub project .\nONNX Runtime  is an open-source project that supports cross-platform inference.\nONNX Runtime provides APIs across programming languages (including Python, C++,\nC#, C, Java, and JavaScript). Y ou can use these APIs to perform inference on input\nimages. After you have the model that has been exported to ONNX format, you can use\nthese APIs on any programming language that your project needs.\nIn this guide, you'll learn how to use Python APIs for ONNX Runtime  to make\npredictions on images for popular vision tasks. Y ou can use these ONNX exported\nmodels across languages.\nGet an AutoML-trained computer vision model for any of the supported image\ntasks: classification, object detection, or instance segmentation. Learn more about\nAutoML support for computer vision tasks .\nInstall the onnxruntime  package. The methods in this article have been tested\nwith versions 1.3.0 to 1.8.0.\nPrerequisites"", ""runs from experimentation in the studio user interface. Or they can use versioned assets\nfor jobs like environments and storage references.\nFor more information, see Manage Azure Machine Learning workspaces .\nWhen a project is ready for operationalization, users' work can be automated in an ML\npipeline and triggered on a schedule or HT TPS request.\nYou can deploy models to the managed inferencing solution, for both real-time and\nbatch deployments, abstracting away the infrastructure management typically required\nfor deploying models.\nIn Azure Machine Learning, you can run your training script in the cloud or build a\nmodel from scratch. Customers often bring models they've built and trained in open-\nsource frameworks so that they can operationalize them in the cloud.\nData scientists can use models in Azure Machine Learning that they've created in\ncommon Python frameworks, such as:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nLightGBM\nOther languages and frameworks are also supported:\nR\n.NET\nFor more information, see Open-source integration with Azure Machine Learning .\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\nexperience and intuition to select the right data featurization and algorithm for training.\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\nLearning studio UI or the Python SDK.Train models\nOpen and interoperable\nAutomated featurization and algorithm selection""]",2284
gpt-35-turbo,21,,5.0,,What Azure storage services can be integrated with Azure Machine Learning for storing datasets?,v1,"Azure Machine Learning can integrate with several Azure storage services for storing datasets, including:

1. Azure Blob Storage: Used for storing large amounts of unstructured data, such as text or binary data.
2. Azure Data Lake Storage: An enterprise-wide hyper-scale repository for big data analytic workloads, enabling you to store massive amounts of data.
3. Azure File Storage: Offers shared storage for applications using the standard SMB protocol.
4. Azure SQL Database: A relational database service that can be used to store structured data.

These services can be used to store and manage the data that you use for training and deploying machine learning models with Azure Machine Learning.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore"", ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", ""Service Usage Example datanumber of nodes; datastore names and tags, and\ndescriptions on assets like models; and data label names.\nAzure AI\nSearchStores indexes that\nhelp with querying\nyour machine learning\ncontent.These indexes are built on top of the data stored in Azure\nCosmos DB.\nAzure\nStorageStores metadata\nrelated to Azure\nMachine Learning\npipeline data.Data can include designer pipeline names, pipeline layout,\nand execution properties.\nFrom the perspective of data lifecycle management, data in the preceding resources is\ncreated and deleted as you create and delete corresponding objects in Azure Machine\nLearning.\nYour Azure Machine Learning workspace reads and writes data by using its managed\nidentity. This identity is granted access to the resources through a role assignment\n(Azure role-based access control) on the data resources. The encryption key that you\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\nused to create indexes for Azure AI Search at runtime.\nExtra networking controls are configured when you create a private link endpoint on\nyour workspace to allow for inbound connectivity. This configuration includes the\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\nNetwork access is restricted to only trusted Microsoft services.\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\nthe data.\nWhen you use a customer-managed key, the resources are in your Azure subscription\nand encrypted with your key. While these resources exist in your subscription, Microsoft\nmanages them. They're automatically created and configured when you create your\nAzure Machine Learning workspace.\nThese Microsoft-managed resources are located in a new Azure resource group that's\ncreated in your subscription. This resource group is separate from the resource group\nfor your workspace. It contains the Microsoft-managed resources that your key is used\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\nresource group name><GUID>.Customer-managed keys""]",1689
gpt-35-turbo,22,,5.0,,How do you authenticate Azure Machine Learning with Azure Blob Storage to access datasets?,v1,"To authenticate Azure Machine Learning with Azure Blob Storage for accessing datasets, you can use identity-based data access by connecting to an Azure Machine Learning datastore. Here are the steps to set up identity-based authentication:

1. **Create a Managed Identity**:
   - For an Azure Machine Learning compute instance or a training job, you can use a system-assigned managed identity or a user-assigned managed identity.

2. **Assign the Appropriate Role**:
   - Assign at least the ""Storage Blob Data Reader"" role to the managed identity for the Azure Blob Storage account you want to access. This can be done through the Azure portal, Azure CLI, or Azure PowerShell.

3. **Configure Azure Machine Learning Datastore**:
   - In Azure Machine Learning, create a new datastore or update an existing one to use the managed identity for authentication. You can do this through the Azure Machine Learning studio or by using the Azure Machine Learning SDK.

4. **Access the Datastore in Your Code**:
   - When working with data interactively, such as in a Jupyter Notebook, or when submitting a training job, you can access the datastore using the Azure Machine Learning SDK. The SDK will handle the authentication using the managed identity, and you won't need to provide any credentials in your code.

Here's an example of how you might access a blob storage using the Azure Machine Learning SDK in Python:

```python
from azureml.core import Workspace, Datastore

# Connect to your Azure ML Workspace
workspace = Workspace.from_config()

# Get the datastore
datastore = Datastore.get(workspace, datastore_name='your_datastore_name')

# Now you can use the datastore to access datasets
```

Remember, when using identity-based data access, Azure Machine Learning will prompt you for your Microsoft Entra token for data access authentication, and you won't need to store any credentials in the datastore. This approach enhances security by managing data access at the storage level and keeping credentials confidential.","['You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity', 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.', 'means that in order to successfully read data from storage, the managed identity of the\ncompute cluster where the deployment is running must have at least Storage Blob Data\nReader  access to the storage account.\nTo configure the compute cluster for data access, follow these steps:\n1. Go to Azure Machine Learning studio .\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\ndeployment is using.\n3. Assign a managed identity to the compute cluster:\na. In the Managed identity  section, verify if the compute has a managed identity\nassigned. If not, select the option Edit.\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\nusing a S ystem-Assigned Managed Identity, it is named as ""[workspace\nname]/computes/[compute cluster name]"".\nc. Save the changes.\n4. Go to the Azure portal  and navigate to the associated storage account where the\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\nstorage account where those assets are placed.', ""When you use an Azure S torage Account from Azure Machine Learning studio, you must\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\naccount:\nBlob Data R eader\nIf the storage account uses a private endpoint to connect to the VNet, you must\ngrant the Reader  role for the storage account private endpoint to the managed\nidentity.\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\nNetwork .\nThe following sections explain the limitations of using an Azure S torage Account, with\nyour workspace, in a VNet.\nTo secure communication between Azure Machine Learning and Azure S torage\nAccounts, configure the storage to Grant access to trusted Azure services .\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\ncan normally be used to allow your client to directly connect over the internet. However,\nwhen using studio, your client doesn't connect to the storage account. The Azure\nMachine Learning service that makes the request connects to the storage account. The\nIP address of the service isn't documented, and it changes frequently. Enabling the\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\nconfiguration .\uea80 Tip\nTo access data from outside Azure Machine Learning, for example with Azure\nStorage Explorer, that access probably relies on the user identity. For specific\ninformation, review the documentation for the tool or service you're using. For\nmore information about how Azure Machine Learning works with data, see Setup\nauthentication betw een Azur e Machine Learning and other ser vices .\nAzure Storage Account\nSecure communication with Azure Storage Account\nAzure Storage firewall"", ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.""]",1921
gpt-35-turbo,23,,5.0,,What steps are involved in uploading a dataset to Azure Blob Storage and then accessing it from an Azure Machine Learning experiment?,v1,"To upload a dataset to Azure Blob Storage and then access it from an Azure Machine Learning experiment, you can follow these steps:

1. **Upload Data to Azure Blob Storage:**
   - Use Azure Storage Explorer to transfer your data to Azure Blob Storage. This is recommended for a large amount of data due to its robustness and speed.
   - Organize your data in Blob Storage, potentially within a specific container or subfolder.

2. **Create a Dataset in Azure Machine Learning:**
   - In Azure Machine Learning Studio, select **Create** to start creating a new dataset.
   - Enter a **Name** for your dataset and, optionally, a description.
   - Choose the **Dataset type**. Select **Tabular** if you're using a .csv or .tsv file with rows containing responses, or select **File** if you're using separate .txt files for each response.
   - Select **Next**.

3. **Specify Datastore and Path:**
   - Select **From Azure storage**, then select **Next**.
   - Choose the datastore that points to your Blob Storage.
   - If your data is in a subfolder, use the **Browse** option to select the path. To include all files in subfolders, append `/**` to the path. To include all data in the current container and its subfolders, append `**/*.*` to the path.
   - Select **Next**.

4. **Finalize Dataset Creation:**
   - Review your settings and then select **Create** to finalize the dataset creation.

5. **Access the Dataset in an Experiment:**
   - In your Azure Machine Learning experiment, access the dataset by selecting the data asset you created.
   - Use the dataset in your experiment's code by referencing it through the Azure Machine Learning SDK.

By following these steps, you can upload your data to Azure Blob Storage, create a dataset in Azure Machine Learning, and then use that dataset in your machine learning experiments.","[""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data"", '2. Select your subscription and the workspace you created.\n3. Select Get star ted.\n4. In the left pane, select Automat ed ML  under the Author  section.\n5. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning dataset. Doing so, allows you to ensure that your\ndata is formatted appropriately for your experiment.\n1. On the Select dataset  form, select From local files  from the +Create dataset  drop-\ndown.\na. On the Basic info  form, give your dataset a name and provide an optional\ndescription. The dataset type should default to Tabular , since automated ML in\nAzure Machine Learning studio currently only supports tabular datasets.\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is the storage location where you upload your data\nfile.\nd. Select Uplo ad files  from the Uplo ad drop-down.\ne. Choose the bike-no.csv  file on your local computer. This is the file you\ndownloaded as a prerequisite .\nf. Select Next\nWhen the upload is complete, the Settings and preview form is pre-populated\nbased on the file type.\ng. Verify that the Settings and pr eview  form is populated as follows and select\nNext .Create and load dataset\nﾉExpand table', ""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data"", ""Since this is your first automated ML experiment, you'll see an empty list and links\nto documentation.\n3. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning data asset. In the case of this tutorial, you can think\nof a data asset as your dataset for the AutoML job. Doing so, allows you to ensure that\nyour data is formatted appropriately for your experiment.\n1. Create a new data asset by selecting From local files  from the +Create data asset\ndrop-down.\na. On the Basic info  form, give your data asset a name and provide an optional\ndescription. The automated ML interface currently only supports\nTabularDatasets, so the dataset type should default to Tabular .\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is where you'll upload your data file to make it\navailable to your workspace.\nd. Select Uplo ad files  from the Uplo ad drop-down.Create and load a dataset as a data asset"", 'b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably.']",2132
gpt-35-turbo,24,,5.0,,"Can Azure Machine Learning directly access data stored in Azure Data Lake Storage Gen2? If so, how?",v1,"Yes, Azure Machine Learning can directly access data stored in Azure Data Lake Storage Gen2. To access the data, you can use the Azure Machine Learning SDK to create a datastore that points to your Azure Data Lake Storage Gen2 account. Here's a high-level overview of the steps you would typically follow:

1. **Create a Data Lake Storage Gen2 Account**: If you don't already have one, you need to create an Azure Data Lake Storage Gen2 account through the Azure portal.

2. **Set Up Authentication**: Ensure that you have the necessary permissions set up for Azure Machine Learning to access the Data Lake Storage Gen2. This could involve setting up service principal authentication or using managed identities for Azure resources.

3. **Create a Datastore in Azure Machine Learning**: Use the Azure Machine Learning SDK to create a datastore that references your Data Lake Storage Gen2. You can do this by using the `Datastore.register_azure_blob_container` method (since Data Lake Storage Gen2 is built on top of Azure Blob Storage) and providing the necessary account information and credentials.

4. **Access Data in Experiments**: Once the datastore is registered, you can access the data from your Azure Machine Learning experiments, training runs, or pipelines by referencing the datastore and the specific paths to the data you need.

Here's a sample code snippet to register a Data Lake Storage Gen2 as a datastore in Azure Machine Learning:

```python
from azureml.core import Workspace, Datastore

# Get the Azure ML workspace
workspace = Workspace.from_config()

# Register a new datastore
datastore_name = 'my_data_lake_gen2_datastore'
datastore = Datastore.register_azure_blob_container(
    workspace=workspace,
    datastore_name=datastore_name,
    container_name='my-container',
    account_name='my-datalake-account',
    account_key='my-account-key',  # or use sas_token
    protocol='https',
    endpoint='core.windows.net'
)

# Now you can use the datastore in your training scripts or pipelines
```

Remember to replace `'my-container'`, `'my-datalake-account'`, and `'my-account-key'` with your actual container name, account name, and account key or SAS token.

Please note that the code and steps above are for illustrative purposes, and you should refer to the latest Azure Machine Learning SDK documentation for the most up-to-date methods and best practices.","[""For information on regenerating the access keys, see Regenerate storage account access\nkeys.\nAzure Data Lake S torage Gen2 is built on top of Azure Blob S torage and is designed for\nbig data analytics in enterprises. Data Lake S torage Gen2 is used as a datastore for\nAzure Machine Learning. Like Azure Blob S torage, the data at rest is encrypted with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Data Lake\nStorage, see Azure S torage encryption with customer-managed keys in Azure K ey Vault.\nThe Azure Machine Learning service supports data from the following data sources.\nTransparent data encryption helps protect Azure SQL Database against the threat of\nmalicious offline activity by encrypting data at rest. By default, transparent data\nencryption is enabled for all newly deployed SQL databases that use Microsoft-\nmanaged keys.\nFor information on how to use customer-managed keys for transparent data encryption,\nsee Azure SQL Database transparent data encryption .Azure Data Lake Storage\n７ Note\nOn Feb 29, 2024 , Azure Data Lake S torage Gen1 will be retired. For more\ninformation, see the official announcement . If you use Azure Data Lake S torage\nGen1, make sure to migrate to Azure Data Lake S torage Gen2 before that date. T o\nlearn how, see Migrat e Azur e Data Lak e Storage fr om Gen1 t o Gen2 by using the\nAzur e por tal.\nUnless you already have an Azure Data Lake S torage Gen1 account, you can't create\nnew ones.\nAzure relational databases\nAzure SQL Database\nAzure Database for PostgreSQL"", 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity', 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore""]",1898
gpt-35-turbo,25,,5.0,,How does Azure Machine Learning support versioning of datasets stored in Azure Blob Storage?,v1,"Azure Machine Learning supports dataset versioning by allowing users to create and manage versions of datasets that are registered in the Azure Machine Learning workspace. When you register a dataset in Azure Machine Learning, you can specify a version number. If you do not specify a version, Azure Machine Learning will automatically assign a version number starting with 1 and incrementing with each new registration.

Here's how you can work with dataset versioning in Azure Blob Storage with Azure Machine Learning:

1. **Register a Dataset**: When you register a dataset from Azure Blob Storage, you can specify a version tag. If you don't specify a version, Azure Machine Learning will automatically version it for you.

2. **Retrieve a Dataset**: You can retrieve a specific version of a dataset by specifying the version number when you get the dataset from your workspace.

3. **Update a Dataset**: When you have new data or changes to your dataset, you can register it as a new version. This allows you to track changes over time and maintain a history of your datasets.

4. **List Dataset Versions**: You can list all the versions of a dataset registered in your workspace to see the history and changes over time.

5. **Promote a Dataset Version**: You can promote a specific version of a dataset to be the default version that is retrieved when the version number is not specified.

Dataset versioning in Azure Machine Learning is particularly useful for maintaining reproducibility in experiments, as you can always go back and retrieve the exact version of the dataset that was used for a particular run or experiment.","[""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'You now have two versions of the data, where the name of the version corresponds to\nthe date the images were uploaded to storage:\n1. 20230108:  The images up to 2023-Jan-08.\n2. 20230115:  The images up to 2023-Jan-15.\nIn both cases, ML Table constructs a table of paths that only include the images up t o\nthose dat es.\nIn an Azure Machine Learning job you can mount or download those paths in the\nversioned ML Table to your compute target using either the eval_download or\neval_mount modes:\nPython\nfrom azure.ai.ml import MLClient, command, Input  \nfrom azure.ai.ml.entities import Environment  \nfrom azure.identity import DefaultAzureCredential  \nfrom azure.ai.ml.constants import InputOutputModes  \n# connect to the AzureML workspace  \nml_client = MLClient.from_config(\n    DefaultAzureCredential()  \n) \n# Get the 20230115 version of the data  \ndata_asset = ml_client.data.get(name= ""myimages"" , version= ""20230115"" ) \ninput = {  \n    ""images"" : Input(type= ""mltable"" , \n                   path=data_asset.id,  \n                   mode=InputOutputModes.EVAL_MOUNT  \n            )  \n} \ncmd = """""" \nls ${{inputs.images}}/**  \n"""""" \njob = command(  \n    command=cmd,  \n    inputs=input,  \n    compute= ""cpu-cluster"" , \n    environment= ""azureml://registries/azureml/environments/sklearn-\n1.1/versions/4""  \n) \nml_client.jobs.create_or_update(job)', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore"", ""Only Azure Blob S torage Accounts are supported for outputs. For instance, Azure Data Lake\nStorage Gen2 isn't supported as output in batch deployment jobs. If you need to output the\ndata to a different location/sink, use the Azure Data F actory Copy activity after the execution of\nthe batch job.\nUse low priority VMs in batch deployments\nAuthorization on batch endpoints\nNetwork isolation in batch endpointsNext steps""]",1574
gpt-35-turbo,26,,5.0,,What is the recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage?,v1,The recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage is to use Azure Storage Explorer. It provides a faster and more robust method for transferring a large amount of data compared to uploading local files directly. Storage Explorer should be used as the default method for moving files into Azure Machine Learning.,"[""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", ""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data"", 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity']",1699
gpt-35-turbo,27,,5.0,,How do you use the Azure Machine Learning SDK to programmatically download data from Azure Blob Storage?,v1,"To programmatically download data from Azure Blob Storage using the Azure Machine Learning SDK, you would typically follow these steps:

1. **Set up Azure Blob Storage**: Ensure that your data is stored in an Azure Blob Storage container.

2. **Install the Azure Machine Learning SDK**: If you haven't already, install the Azure Machine Learning SDK for Python using pip:

   ```bash
   pip install azureml-sdk
   ```

3. **Authenticate to Azure**: Use the SDK to authenticate to Azure. This can be done using a service principal or interactive authentication methods provided by the SDK.

4. **Create a Workspace object**: Instantiate a workspace object from the SDK to interact with your Azure Machine Learning workspace.

   ```python
   from azureml.core import Workspace
   ws = Workspace.from_config()
   ```

5. **Access the Datastore**: Azure Machine Learning workspaces have a default datastore, which is typically backed by an Azure Blob Storage account. You can access this datastore or any other registered datastore in your workspace.

   ```python
   from azureml.core import Datastore
   datastore = Datastore.get(ws, datastore_name='your_datastore_name')
   ```

6. **Download Data**: Use the `download` or `download_to_path` method of the `Datastore` class to download files from the datastore to your local environment or compute target in Azure Machine Learning.

   ```python
   datastore.download(target_path='local_path', 
                      prefix='folder_or_file_prefix', 
                      show_progress=True)
   ```

   - `target_path` is the local directory where files will be downloaded.
   - `prefix` is used to filter the files to download by their path prefix.
   - `show_progress` will display the progress of the download if set to `True`.

Please note that the exact code and methods may vary depending on the version of the SDK and the specific requirements of your task. Always refer to the latest Azure Machine Learning SDK documentation for the most up-to-date information.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""Tutorial: Upload, access and explore\nyour data in Azure Machine Learning\nArticle •12/27/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this tutorial you learn how to:\nThe start of a machine learning project typically involves exploratory data analysis (ED A),\ndata-preprocessing (cleaning, feature engineering), and the building of Machine\nLearning model prototypes to validate hypotheses. This prototyping  project phase is\nhighly interactive. It lends itself to development in an IDE or a Jupyter notebook, with a\nPython int eractive console. This tutorial describes these ideas.\nThis video shows how to get started in Azure Machine Learning studio so that you can\nfollow the steps in the tutorial. The video shows how to create a notebook, clone the\nnotebook, create a compute instance, and download the data needed for the tutorial.\nThe steps are also described in the following sections.\n1. To use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\n2. Sign in to studio  and select your workspace if it's not already open.\n3. Open or create a notebook in your workspace:\nCreate a new notebook , if you want to copy/paste code into cells.\nOr, open tutorials/get -star ted-not ebooks/explor e-data.ipynb  from the\nSamples  section of studio. Then select Clone  to add the notebook to your\nFiles. (See where to find Samples .)\nUpload your data to cloud storage＂\nCreate an Azure Machine Learning data asset＂\nAccess your data in a notebook for interactive development＂\nCreate new versions of data assets＂\nhttps://learn-video.azurefd.net/vod/player?id=514a29e2-0ae7-4a5d-a537-\n8f10681f5545&locale=en-us&embedUrl=%2F azure%2Fmachine-learning%2Ftutorial-\nexplore-data\nPrerequisites"", ""Data from Azure Machine Learning registered data stores can be directly referenced by\nbatch deployments jobs. In this example, you first upload some data to the default data\nstore in the Azure Machine Learning workspace and then run a batch deployment on it.\nFollow these steps to run a batch endpoint job using data stored in a data store.\n1. Access the default data store in the Azure Machine Learning workspace. If your\ndata is in a different store, you can use that store instead. Y ou're not required to\nuse the default data store.\nAzure CLI\n2. You need to upload some sample data to the data store. This example assumes\nyou already uploaded the sample data included in the repo in the folder\nsdk/python/endpoints/batch/deploy-models/heart-classifier-mlflow/data in the\nfolder heart-disease-uci-unlabeled in the blob storage account. Ensure you've\ndone that before moving forward.\n3. Create the input or request:\nPlace the file path in the following variable:Azure CLI\nDATASTORE_ID=$(az ml datastore show -n workspaceblobstore | jq -r \n'.id')\n７ Note\nData stores ID would look like\n/subscriptions/<subscription>/resourceGroups/<resource-\ngroup>/providers/Microsoft.MachineLearningServices/workspaces/<worksp\nace>/datastores/<data-store>.\n\uea80 Tip\nThe default blob data store in a workspace is called worksp aceblobst ore. You\ncan skip this step if you already know the resource ID of the default data store\nin your workspace.\nAzure CLI"", ""Acces s data in a job\nArticle •02/06/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2 (current)\nIn this article you learn:\nAn Azure subscription. If you don't have an Azure subscription, create a free account before you begin.\nTry the free or paid version of Azure Machine Learning .\nThe Azure Machine Learning SDK for Python v2 .\nAn Azure Machine Learning workspace\nBefore you explore the detailed options available to you when you access data, we first describe the relevant\ncode snippets for data access.\nIn this example, you submit an Azure Machine Learning job that accesses data from a public  blob storage\naccount. However, you can adapt the snippet to access your own data in a private Azure S torage account.\nUpdate the path as described here. Azure Machine Learning seamlessly handles authentication to cloud\nstorage, with Microsoft Entra passthrough. When you submit a job, you can choose:\nUser identity:  Passthrough your Microsoft Entra identity to access the data\nManaged identity:  Use the managed identity of the compute target to access data\nNone:  Don't specify an identity to access the data. Use None when using credential-based (key/SAS\ntoken) datastores or when accessing public data\nHow to read data from Azure storage in an Azure Machine Learning job.＂\nHow to write data from your Azure Machine Learning job to Azure S torage. ＂\nThe difference between mount  and downlo ad modes. ＂\nHow to use user identity and managed identity to access data.＂\nMount settings available in a job.＂\nOptimum mount settings for common scenarios.＂\nHow to access V1 data assets.＂\nPrerequisites\nQuickstart\nRead data from Azure storage in an Azure Machine Learning job\n\uea80 Tip\nIf you use keys or SAS tokens to authenticate, we suggest that you create an Azur e Machine Learning\ndatast ore, because the runtime will automatically connect to storage without exposure of the key/token.\nPython SDK"", ""Acces s data from Azure cloud storage during\ninteractive development\nArticle •09/13/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nA machine learning project typically starts with exploratory data analysis (ED A), data-preprocessing\n(cleaning, feature engineering), and includes building prototypes of ML models to validate hypotheses.\nThis prototyping  project phase is highly interactive in nature, and it lends itself to development in a\nJupyter notebook, or an IDE with a Python int eractive console. In this article you'll learn how to:\nAn Azure Machine Learning workspace. For more information, see Manage Azure Machine Learning\nworkspaces in the portal or with the Python SDK (v2) .\nAn Azure Machine Learning Datastore. For more information, see Create datastores .\nAn Azure Machine Learning datastore is a reference to an existing  Azure storage account. The benefits of\ndatastore creation and use include:\nAccess data from a Azure Machine Learning Datastores URI as if it were a file system.＂\nMaterialize data into P andas using mltable Python library. ＂\nMaterialize Azure Machine Learning data assets into P andas using mltable Python library. ＂\nMaterialize data through an explicit download with the azcopy utility. ＂\nPrerequisites\n\uea80 Tip\nThe guidance in this article describes data access during interactive development. It applies to any\nhost that can run a Python session. This can include your local machine, a cloud VM, a GitHub\nCodespace, etc. W e recommend use of an Azure Machine Learning compute instance - a fully\nmanaged and pre-configured cloud workstation. For more information, see Create an Azur e\nMachine Learning comput e instance .\n） Impor tant\nEnsure you have the latest azure-fsspec and mltable python libraries installed in your python\nenvironment:\nBash\npip install -U azureml-fsspec mltable\nAccess data from a datastore URI, like a filesystem\nA common, easy-to-use API to interact with different storage types (Blob/Files/ADLS).＂""]",2164
gpt-35-turbo,28,,5.0,,What are the benefits of integrating Azure Data Lake Storage with Azure Machine Learning for big data scenarios?,v1,"Integrating Azure Data Lake Storage with Azure Machine Learning offers several benefits for big data scenarios:

1. **Scalability**: Azure Data Lake Storage is designed to handle large volumes of data, scaling to petabytes of information. This makes it an ideal storage solution for big data scenarios where data volumes can grow rapidly.

2. **Performance**: Azure Data Lake Storage provides high-performance storage, which is crucial when running complex machine learning algorithms that require efficient data retrieval.

3. **Data Exploration and Accessibility**: Azure Machine Learning allows you to easily access and explore your data stored in Azure Data Lake Storage. You can retrieve the data from notebooks and scripts, which simplifies the process of data analysis and model development.

4. **Integration with Azure Services**: Azure Data Lake Storage is well-integrated with other Azure services, including Azure Machine Learning. This seamless integration allows for easier orchestration of machine learning workflows and pipelines.

5. **Security and Compliance**: Azure Data Lake Storage offers robust security features, including encryption and access control, which are essential for protecting sensitive big data. When integrated with Azure Machine Learning, you can maintain compliance with industry standards and regulations.

6. **Cost-Effectiveness**: By using Azure Data Lake Storage, you can store large amounts of data cost-effectively. Azure Machine Learning can then process this data without the need to move it, which can reduce data egress costs.

7. **Advanced Analytics**: With Azure Machine Learning, you can build and train advanced machine learning models on the data stored in Azure Data Lake Storage. This allows for deep insights and analytics on big data sets.

8. **Streamlined Workflows**: Azure Machine Learning provides tools to create production-grade machine learning pipelines. When combined with Azure Data Lake Storage, you can create a multistep workflow that includes data storage, preprocessing, model training, and deployment, all within the Azure ecosystem.

By leveraging the strengths of both Azure Data Lake Storage and Azure Machine Learning, organizations can build powerful, scalable, and efficient big data solutions in the cloud.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Azur e Machine Learning designer : Use the designer to train and deploy ML\nmodels without writing any code. Drag and drop datasets and components to\ncreate ML pipelines.\nAutomat ed machine learning UI : Learn how to create automated ML experiments\nwith an easy-to-use interface.\nData labeling : Use Machine Learning data labeling to efficiently coordinate image\nlabeling  or text labeling  projects.\nMachine Learning integrates with the Azure cloud platform to add security to ML\nprojects.\nSecurity integrations include:\nAzure Virtual Networks with network security groups.\nAzure K ey Vault, where you can save security secrets, such as access information\nfor storage accounts.\nAzure Container R egistry set up behind a virtual network.\nFor more information, see Tutorial: Set up a secure workspace .\nOther integrations with Azure services support an ML project from end to end. They\ninclude:Enterprise-readiness and security\nAzure integrations for complete solutio ns', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore"", ""What is Azure Machine Learning?\nArticle •01/29/2024\nAzure Machine Learning is a cloud service for accelerating and managing the machine\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\nin their day-to-day workflows to train and deploy models and manage machine learning\noperations (ML Ops).\nYou can create a model in Machine Learning or use a model built from an open-source\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\nretrain, and redeploy models.\nMachine Learning is for individuals and teams implementing ML Ops within their\norganization to bring ML models into production in a secure and auditable production\nenvironment.\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\nday workflows. Application developers can use tools for integrating models into\napplications or services. Platform developers can use a robust set of tools, backed by\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\nbased access control for infrastructure. Y ou can set up a project to deny access to\nprotected data and select operations.\nML projects often require a team with a varied skill set to build and maintain. Machine\nLearning has tools that help enable you to:\uea80 Tip\nFree trial!  If you don't have an Azure subscription, create a free account before you\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\nto spend on Azure services. After they're used up, you can keep the account and\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\nchange your settings and ask to be charged.\nWho is Azure Machine Learning for?\nProductivity for everyone on the team"", 'Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .']",1444
gpt-35-turbo,29,,5.0,,How can you secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces?,v1,"To secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces, you can use the following methods:

1. **Encryption at Rest**: By default, all data stored in Azure Blob Storage is encrypted at rest with Microsoft-managed keys. This ensures that your data is secure when it is not being accessed or moved.

2. **Customer-Managed Keys**: For additional control, you can use your own encryption keys for data stored in Azure Blob Storage. This is done through Azure Storage encryption with customer-managed keys in Azure Key Vault. By integrating with Azure Key Vault, you can manage the encryption keys and control the key lifecycle, including rotation and revocation.

3. **Secure Data Access**: When setting up your Azure Machine Learning workspace, ensure that you configure the storage account and other data services with proper access controls. Use Azure role-based access control (RBAC) to grant the minimum necessary permissions to the workspace and its associated compute resources.

4. **Datastore Configuration**: In Azure Machine Learning, you can register datastores that reference the data in your Azure Storage account. When you use datastores, you can manage data connections securely without having to hard-code your credentials.

5. **Network Security**: Use Azure Virtual Network to isolate your Azure Machine Learning environment and control the flow of data. You can also use private endpoints for your storage account to ensure that data access is restricted to your virtual network.

6. **Monitoring and Auditing**: Enable Azure Monitor and Azure Storage logging to track access and usage of your data. This can help you identify and respond to potential security threats.

By combining these methods, you can secure your data both at rest and in transit when using Azure Machine Learning workspaces.","[""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", 'This article is part of a series on securing an Azure Machine Learning workflow. See the\nother articles in this series:\nSecure the workspace resources\nSecure machine learning registries\nSecure the training environment\nSecure the inference environment\nEnable studio functionality\nUse custom DNS\nUse a firewall\nAPI platform network isolationNext steps', ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."", ""When you use an Azure S torage Account from Azure Machine Learning studio, you must\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\naccount:\nBlob Data R eader\nIf the storage account uses a private endpoint to connect to the VNet, you must\ngrant the Reader  role for the storage account private endpoint to the managed\nidentity.\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\nNetwork .\nThe following sections explain the limitations of using an Azure S torage Account, with\nyour workspace, in a VNet.\nTo secure communication between Azure Machine Learning and Azure S torage\nAccounts, configure the storage to Grant access to trusted Azure services .\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\ncan normally be used to allow your client to directly connect over the internet. However,\nwhen using studio, your client doesn't connect to the storage account. The Azure\nMachine Learning service that makes the request connects to the storage account. The\nIP address of the service isn't documented, and it changes frequently. Enabling the\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\nconfiguration .\uea80 Tip\nTo access data from outside Azure Machine Learning, for example with Azure\nStorage Explorer, that access probably relies on the user identity. For specific\ninformation, review the documentation for the tool or service you're using. For\nmore information about how Azure Machine Learning works with data, see Setup\nauthentication betw een Azur e Machine Learning and other ser vices .\nAzure Storage Account\nSecure communication with Azure Storage Account\nAzure Storage firewall"", 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity']",1779
gpt-35-turbo,30,,5.0,,What role do Azure Storage accounts play in deploying Azure Machine Learning models as web services?,v1,"Azure Storage accounts play a crucial role in deploying Azure Machine Learning models as web services in several ways:

1. **Model Storage**: The storage accounts in your Azure subscription are used to store your machine learning models. When you train a model using Azure Machine Learning, the model is saved in your Azure Storage account.

2. **Data Storage**: Training data, training logs, and other artifacts related to the machine learning process are stored in Azure Storage. This data can be accessed by the compute resources used during the training and deployment process.

3. **Operational Storage**: When deploying a model as a web service, especially in an Azure Kubernetes Service (AKS) environment, the web service may need to access various files such as scripts or additional dependencies. These files are typically stored in Azure Storage and are accessed by the web service during operation.

4. **Endpoint Accessibility**: The deployed web service may use the storage account as a backend to retrieve or store data. For instance, it might fetch input data for predictions or store output results.

5. **Secure Access**: Azure Machine Learning services can securely access the storage account using private endpoints, ensuring that data is transferred within the Azure network without exposure to the public internet.

6. **Managed Identity**: When deploying a model as a web service, you can use a managed identity for the service to securely access the Azure Storage without storing credentials in your code.

In summary, Azure Storage accounts provide a secure and scalable way to store and manage the data and models that are essential for deploying and operating Azure Machine Learning models as web services.","[""Scenar io Requir ed\ninboundRequir ed outbound Additional\nconfiguration\nUse Azure\nKubernetes\nServiceNot applicable For information on the outbound\nconfiguration for AKS, see Secure\nAzure Kubernetes Service\ninferencing environment .\nUse Docker\nimages that\nAzure\nMachine\nLearning\nmanagesNot applicable Microsoft Artifact R egistry\nviennaglobal.azurecr.io\nglobal container registryIf the container registry\nfor your workspace is\nbehind the virtual\nnetwork, configure the\nworkspace to use a\ncompute cluster to build\nimages. For more\ninformation, see Secure\nan Azure Machine\nLearning workspace with\nvirtual networks .\nAzure Machine Learning uses multiple storage accounts. Each stores different data and\nhas a different purpose:\nYour st orage : The storage accounts in your Azure subscription store your data and\nartifacts, such as models, training data, training logs, and Python scripts. For\nexample, the default  storage account for your workspace is in your subscription.\nThe Azure Machine Learning compute instance and compute cluster access file and\nblob data in this storage over ports 445 (SMB) and 443 (HT TPS).\nWhen you're using a compute instance or compute cluster, your storage account is\nmounted as a file share via the SMB protocol. The compute instance and cluster\nuse this file share to store items like the data, models, Jupyter notebooks, and\ndatasets. The compute instance and cluster use the private endpoint when they\naccess the storage account.\nMicrosoft storage : The Azure Machine Learning compute instance and compute\ncluster rely on Azure Batch. They access storage located in a Microsoft\nsubscription. This storage is used only for the management of the compute\ninstance or cluster. None of your data is stored here. The compute instance and\ncompute cluster access the blob, table, and queue data in this storage, by using\nport 443 (HT TPS).Purposes of storage accounts"", 'Azure Machine Learning supports storage accounts configured to use either a private\nendpoint or service endpoint.\n1. In the Azure portal, select the Azure S torage Account.\n2. Use the information in Use private endpoints for Azure S torage  to add private\nendpoints for the following storage resources:\nBlob\nFile\nQueue  - Only needed if you plan to use Batch endpoints  or the\nParallelRunS tep in an Azure Machine Learning pipeline.\nTable - Only needed if you plan to use Batch endpoints  or the\nParallelRunS tep in an Azure Machine Learning pipeline.Securing a workspace with private endpoints does not ensure end-to-end security\nby itself. Y ou must follow the steps in the rest of this article, and the VNet series, to\nsecure individual components of your solution. For example, if you use a private\nendpoint for the workspace, but your Azure S torage Account is not behind the\nVNet, traffic between the workspace and storage does not use the VNet for\nsecurity.\nSecure Azure storage accounts\nPrivate endpoint', 'What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps', ""Azure Machine Learning offers many ways to deploy MLflow models to online and batch\nendpoints. Y ou can deploy models, using the following tools:\nEach workflow has different capabilities, particularly around which type of compute they\ncan target. The following table shows the different capabilities.\nScenar io MLflow SDK Azur e Machine\nLearning CLI/SDKAzur e Machine\nLearning studio\nDeploy to managed online\nendpointsSee example See example See example\nDeploy to managed online\nendpoints (with a scoring script)Not\nsupportedSee example See example\nDeploy to batch endpoints Not\nsupportedSee example See example\nDeploy to batch endpoints (with\na scoring script)Not\nsupportedSee example See example\nDeploy to web services\n(ACI/AKS)Legacy\nsupportNot supported Not supported\nDeploy to web services (A CI/AKS\n- with a scoring script)Not\nsupportedLegacy support Legacy support\n Deployment to online endpoints that are in workspaces with private link enabled\nrequires you to package models before deployment (preview) .\n We recommend switching to managed online endpoints  instead.\n MLflow (OSS) doesn't have the concept of a scoring script and doesn't support batch\nexecution currently.\nUse the MLflow SDK if both of these conditions apply:Deployment tools\nMLflow SDK＂\nAzure Machine Learning CLI＂\nAzure Machine Learning SDK for Python＂\nAzure Machine Learning studio＂\nﾉExpand table\n1 1 1\n3\n3\n3\n22 2\n32 2\n1\n2\n3\nWhich deployment tool to use?"", 'region basis. When you sign up for Azure Machine Learning studio, you receive a default\nVM quota for several VM families available in the region. Y ou can continue to create\ndeployments until you reach your quota limit. Once you reach this limit, you can request\na quota increase.\nModels deployed as a service with pay-as-you-go are protected by Azure AI content\nsafety. When deployed to real-time endpoints, you can opt out of this capability. With\nAzure AI content safety enabled, both the prompt and completion pass through an\nensemble of classification models aimed at detecting and preventing the output of\nharmful content. The content filtering system detects and takes action on specific\ncategories of potentially harmful content in both input prompts and output\ncompletions. Learn more about Azure AI Content Safety .\nModel Catalog and Collections\nDeploy and score a machine learning model by using an online endpoint\nPlan and manage costs for Azure AI S tudioContent filtering\nRelated content']",1486
gpt-35-turbo,31,,5.0,,How do you monitor data access and usage when integrating Azure Machine Learning with Azure Storage?,v1,"To monitor data access and usage when integrating Azure Machine Learning with Azure Storage, you can use Azure Monitor, which provides comprehensive monitoring of your Azure resources and the applications you run on Azure. Here are the steps you can follow:

1. **Enable Diagnostic Settings**: Configure the diagnostic settings for your Azure Storage account to send logs and metrics to Azure Monitor. This can include read, write, and delete operations on your storage blobs or files.

2. **Log Analytics Workspace**: Set up a Log Analytics workspace in Azure Monitor to query and visualize the logs collected from your Azure Storage account.

3. **Metrics**: Use Azure Monitor metrics to get performance metrics in real-time for your Azure Storage account.

4. **Alerts**: Create alert rules in Azure Monitor to get notified about specific conditions in the log data or metrics that may indicate unusual access patterns or potential security concerns.

5. **Azure Storage Analytics**: Use Azure Storage Analytics to track, analyze, and debug your applications that use Azure Storage.

6. **Azure Machine Learning Monitoring**: Use the monitoring features in Azure Machine Learning to track data drift, model performance, and other metrics related to your machine learning workflows.

7. **Access Control (IAM)**: Ensure that you have the correct Identity and Access Management (IAM) policies in place to control who has access to your Azure Storage resources.

8. **Network Security**: Use network security tools such as Azure Firewall and Network Security Groups to restrict access to your Azure Storage account.

By combining these Azure services and features, you can effectively monitor data access and usage in your Azure Storage account when it's integrated with Azure Machine Learning. For detailed instructions and best practices, you should refer to the Azure documentation and specifically the guidance on monitoring Azure Machine Learning.","['To learn how to monitor resources and logs from Azure Machine Learning, see\nMonitoring Azure Machine Learning .', 'Monitor Azure Machine Learning\nArticle •11/06/2023\nWhen you have critical applications and business processes relying on Azure resources, you\nwant to monitor those resources for their availability, performance, and operation. This\narticle describes the monitoring data generated by Azure Machine Learning and how to\nanalyze and alert on this data with Azure Monitor.\nAzure Machine Learning creates monitoring data using Azure Monitor , which is a full stack\nmonitoring service in Azure. Azure Monitor provides a complete set of features to monitor\nyour Azure resources. It can also monitor resources in other clouds and on-premises.\nStart with the article Monitoring Azure resources with Azure Monitor , which describes the\nfollowing concepts:\nWhat is Azure Monitor?\nCosts associated with monitoring\nMonitoring data collected in Azure\nConfiguring data collection\nStandard tools in Azure for analyzing and alerting on monitoring data\nThe following sections build on this article by describing the specific data gathered for\nAzure Machine Learning. These sections also provide examples for configuring data\ncollection and analyzing this data with Azure tools.\uea80 Tip\nThe information in this document is primarily for administrat ors, as it describes\nmonitoring for the Azure Machine Learning service and associated Azure services. If\nyou are a data scientist  or developer , and want to monitor information specific to your\nmodel tr aining r uns, see the following documents:\nStart, monit or, and cancel training runs\nLog metrics for training runs\nTrack experiments with MLflow\nIf you want to monitor information generated by models deployed to online\nendpoints, see Monit or online endpoints .\nWhat is Azure Monitor?', ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."", 'Monitor online endpoints\nArticle •10/24/2023\nAzure Machine Learning uses integration with Azure Monitor to track and monitor\nmetrics and logs for online endpoints . You can view metrics in charts, compare between\nendpoints and deployments, pin to Azure portal dashboards, configure alerts, query\nfrom log tables, and push logs to supported targets. Y ou can also use Application\nInsights to analyze events from user containers.\nMetrics : For endpoint-level metrics such as request latency, requests per minute,\nnew connections per second, and network bytes, you can drill down to see details\nat the deployment level or status level. Deployment-level metrics such as CPU/GPU\nutilization and memory or disk utilization can also be drilled down to instance\nlevel. Azure Monitor allows tracking these metrics in charts and setting up\ndashboards and alerts for further analysis.\nLogs : You can send metrics to the Log Analytics workspace where you can query\nthe logs using Kusto query syntax. Y ou can also send metrics to Azure S torage\naccounts and/or Event Hubs for further processing. In addition, you can use\ndedicated log tables for online endpoint related events, traffic, and console\n(container) logs. Kusto query allows complex analysis and joining of multiple\ntables.\nApplication insights : Curated environments include integration with Application\nInsights, and you can enable or disable this integration when you create an online\ndeployment. Built-in metrics and logs are sent to Application Insights, and you can\nuse the built-in features of Application Insights (such as Live metrics, T ransaction\nsearch, F ailures, and P erformance) for further analysis.\nIn this article you learn how to:\nDeploy an Azure Machine Learning online endpoint.Choose the right method to view and track metrics and logs＂\nView metrics for your online endpoint＂\nCreate a dashboard for your metrics＂\nCreate a metric alert＂\nView logs for your online endpoint＂\nUse Application Insights to track metrics and logs＂\nPrerequisites', ""Use monitoring view in the Azure Machine Learning studio  to check the CPU and memory utilization of\nyour job. If you're low on CPU and memory, consider increasing the compute target VM size.\nConsider increasing RSLEX_DOWNLOADER_THREADS if you're downloading and you don't utilize the CPU and\nmemory. If you use mount, you should increase DATASET_MOUNT_READ_BUFFER_BLOCK_COUNT to do more\nprefetching, and increase DATASET_MOUNT_READ_THREADS for more read threads.\nIf the metrics show low SuccessE2ELat ency and low SuccessSer verLat ency, but the client experiences high\nlatency , you have a delay in the storage request that reaches the service. Y ou should check:\nWhether the number of threads used for mount/download\n(DATASET_MOUNT_READ_THREADS/RSLEX_DOWNLOADER_THREADS) is set too low, relative to the number of cores\navailable on the compute target. If the setting is too low, increase the number of threads.\nWhether the number of retries for downloading ( AZUREML_DATASET_HTTP_RETRY_COUNT) is set too high. If\nso, decrease the number of retries.\nFrom the Azure Machine Learning studio, you can also monitor the compute target disk IO and usage during\nyour job execution. Navigate to your job, and select the Monit oring  tab. This tab provides insights on the\nresources of your job, on a 30 day rolling basis. For example:\nAzure Machine Learning data runtime doesn't use the last RESERVED_FREE_DISK_SPACE bytes of disk space, to\nkeep the compute healthy (the default value is 150MB). If your disk is full, your code is writing files to disk\nwithout declaring the files as an output. Therefore, check your code to make sure that data isn't being written\nerroneously to temporary disk. If you must write files to temporary disk, and that resource is becoming full,\nconsider:\nIncreasing the VM Size to one that has a larger temporary disk\nSetting a T TL on the cached data ( DATASET_MOUNT_ATTRIBUTE_CACHE_TTL), to purge your data from diskMonitor disk usage during a job\n\uf80a\n７ Note\nJob monitoring supports only compute resources that Azure Machine Learning manages. Jobs with a\nruntime of less than 5 minutes will not have enough data to populate this view.""]",1892
gpt-35-turbo,32,,5.0,,"Can Azure Machine Learning use data stored in Azure File Storage? If so, what are the use cases?",v1,"Yes, Azure Machine Learning can use data stored in Azure File Storage. Azure File Storage provides shared storage that can be accessed via the SMB protocol, and it can be mounted onto Azure Machine Learning compute instances or pipelines. Here are some use cases for using Azure File Storage with Azure Machine Learning:

1. **Data Access**: You can store your datasets in Azure File Storage and access them from your Azure Machine Learning workspace. This is useful for scenarios where you have large datasets that need to be shared across multiple experiments or users.

2. **Persistent Storage**: When using Azure Machine Learning compute instances, you can mount Azure File Shares to ensure that your data persists even when the compute instance is shut down. This is particularly useful for saving intermediate results or datasets that are used frequently.

3. **Collaboration**: Since Azure File Storage can be accessed from anywhere, it facilitates collaboration among data scientists who are working on the same project but may be located in different geographies.

4. **Training and Model Development**: You can use the data stored in Azure File Storage for training machine learning models directly within Azure Machine Learning. This allows you to work with large datasets without the need to move data around.

5. **Pipeline Input/Output**: In machine learning pipelines, you can use Azure File Storage as a source or destination for data processing steps. This enables you to create complex workflows where data is passed between different steps and stored reliably.

Remember to configure the appropriate access permissions and network settings to ensure secure and efficient access to the data stored in Azure File Storage when using it with Azure Machine Learning.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore"", ""Attached comput e: You can attach your own compute resources to your\nworkspace and use them for training and inference.\nMachine Learning allows you to work with different types of data:\nURIs (a location in local or cloud storage):\nuri_folder\nuri_file\nTables (a tabular data abstraction):\nmltable\nPrimitives:\nstring\nboolean\nnumber\nFor most scenarios, you use URIs ( uri_folder and uri_file) to identify a location in\nstorage that can be easily mapped to the file system of a compute node in a job by\neither mounting or downloading the storage to the node.\nThe mltable parameter is an abstraction for tabular data that's used for automated\nmachine learning (AutoML) jobs, parallel jobs, and some advanced scenarios. If you're\nstarting to use Machine Learning and aren't using AutoML, we strongly encourage you\nto begin with URIs.\nMachine Learning datastores securely keep the connection information to your data\nstorage on Azure so that you don't have to code it in your scripts. Y ou can register and\ncreate a datastore to easily connect to your storage account and access the data in your\nunderlying storage service. The Azure Machine Learning CLI v2 and SDK v2 support the\nfollowing types of cloud-based storage services:\nAzure Blob S torage container\nAzure Files share\nAzure Data Lake S torage\nAzure Data Lake S torage Gen2Data\nDatastore\nEnvironment"", ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.""]",1745
gpt-35-turbo,33,,5.0,,What are the best practices for managing input and output data for Azure Machine Learning pipelines using Azure Storage?,v1,"When managing input and output data for Azure Machine Learning pipelines using Azure Storage, consider the following best practices:

1. **Use Azure Machine Learning Datastores**: Register your Azure Storage accounts (Azure Blob Storage, Azure Data Lake Storage Gen1, and Azure Data Lake Storage Gen2) as Azure Machine Learning datastores. This allows you to manage and access data within your Azure Machine Learning workspace and ensures compatibility with Azure Machine Learning pipelines.

2. **Flatten Nested Folder Structures**: Azure Machine Learning batch endpoint jobs do not support nested folder structures. If your data is organized in nested folders, you will need to flatten the structure before the data can be used in a batch job.

3. **Data Movement with Azure Data Factory**: If your input data resides in a storage service not directly supported by Azure Machine Learning, use Azure Data Factory to copy the data to a supported Azure Storage account before executing the batch job.

4. **Scoring Script Compatibility**: Ensure that the scoring script used in your pipeline deployment is capable of handling the data format and structure as it will be provided to the model. This includes being aware of any limitations related to the use of MLflow models in batch deployments.

5. **Write Back to the Same Storage Account**: To streamline the process, it is recommended to register the storage account used by Azure Data Factory as a datastore in Azure Machine Learning. This allows you to easily write results back to the same storage account from which the input data was read.

6. **Monitor Data Access and Permissions**: Regularly review and manage access permissions to your Azure Storage accounts to ensure that only authorized users and services can access or modify the data.

7. **Data Versioning**: Use versioning or snapshot features in Azure Storage to maintain versions of your datasets. This can be useful for tracking changes, reproducing experiments, and rolling back to previous data states if necessary.

8. **Data Cleanup**: Implement a strategy for cleaning up unused data to manage storage costs and maintain an organized data environment.

By following these best practices, you can effectively manage data inputs and outputs for your Azure Machine Learning pipelines, ensuring smooth operation and compatibility with Azure Storage services.","[""7. Your pipeline is ready to be used.\nWhen calling Azure Machine Learning batch deployments consider the following limitations:\nOnly Azure Machine Learning data stores or Azure S torage Accounts (Azure Blob S torage, Azure\nData Lake S torage Gen1, Azure Data Lake S torage Gen2) are supported as inputs. If your input\ndata is in another source, use the Azure Data F actory Copy activity before the execution of the\nbatch job to sink the data to a compatible store.\nBatch endpoint jobs don't explore nested folders and hence can't work with nested folder\nstructures. If your data is distributed in multiple folders, notice that you will have to flatten the\nstructure.\nMake sure that your scoring script provided in the deployment can handle the data as it is\nexpected to be fed into the job. If the model is MLflow, read the limitation in terms of the file\ntype supported by the moment at Using MLflow models in batch deployments .\nOnly registered Azure Machine Learning data stores are supported by the moment. W e\nrecommend you to register the storage account your Azure Data F actory is using as a Data S tore\nin Azure Machine Learning. In that way, you will be able to write back to the same storage\naccount from where you are reading.\nLimitations\nData inputs\nData outputs"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.', ""original notebook or Python code, changes the training input from local data to\nparameterized values, split the training code into multiple steps as needed, perform unit\ntest of each step, and finally wraps all steps into a pipeline.\nOnce the teams get familiar with pipelines and want to do more machine learning\nprojects using pipelines, they'll find the first approach is hard to scale. The second\napproach is set up a few pipeline templates, each try to solve one specific machine\nlearning problem. The template predefines the pipeline structure including how many\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\nlearning project, the team first forks one template repo. The team leader then assigns\nmembers which step they need to work on. The data scientists and data engineers do\ntheir regular work. When they're happy with their result, they structure their code to fit\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\nexecuted or automated. If there's any change, each member only needs to work on their\npiece of code without touching the rest of the pipeline code.\nOnce a team has built a collection of machine learnings pipelines and reusable\ncomponents, they could start to build the machine learning pipeline from cloning\nprevious pipeline or tie existing reusable component together. At this stage, the team's\noverall productivity will be improved significantly.\nAzure Machine Learning offers different methods to build a pipeline. For users who are\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\npipelines by using registered components .\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\nfollowing table lists the different pipelines and what they're used for:\nScenar io Primar y\npersonaAzur e\nofferingOSS\nofferingCanonical\npipeStrengths\nModel\norchestration\n(Machine\nlearning)Data\nscientistAzure\nMachine\nLearning\nPipelinesKubeflow\nPipelinesData ->\nModelDistribution,\ncaching, code-first,\nreuse\nData\norchestrationData\nengineerAzure Data\nFactoryApache\nAirflowData -> Data Strongly typed\nmovement, data-Which Azure pipeline technology should I use?\nﾉExpand table"", ""original notebook or Python code, changes the training input from local data to\nparameterized values, split the training code into multiple steps as needed, perform unit\ntest of each step, and finally wraps all steps into a pipeline.\nOnce the teams get familiar with pipelines and want to do more machine learning\nprojects using pipelines, they'll find the first approach is hard to scale. The second\napproach is set up a few pipeline templates, each try to solve one specific machine\nlearning problem. The template predefines the pipeline structure including how many\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\nlearning project, the team first forks one template repo. The team leader then assigns\nmembers which step they need to work on. The data scientists and data engineers do\ntheir regular work. When they're happy with their result, they structure their code to fit\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\nexecuted or automated. If there's any change, each member only needs to work on their\npiece of code without touching the rest of the pipeline code.\nOnce a team has built a collection of machine learnings pipelines and reusable\ncomponents, they could start to build the machine learning pipeline from cloning\nprevious pipeline or tie existing reusable component together. At this stage, the team's\noverall productivity will be improved significantly.\nAzure Machine Learning offers different methods to build a pipeline. For users who are\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\npipelines by using registered components .\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\nfollowing table lists the different pipelines and what they're used for:\nScenar io Primar y\npersonaAzur e\nofferingOSS\nofferingCanonical\npipeStrengths\nModel\norchestration\n(Machine\nlearning)Data\nscientistAzure\nMachine\nLearning\nPipelinesKubeflow\nPipelinesData ->\nModelDistribution,\ncaching, code-first,\nreuse\nData\norchestrationData\nengineerAzure Data\nFactoryApache\nAirflowData -> Data Strongly typed\nmovement, data-Which Azure pipeline technology should I use?\nﾉExpand table""]",2152
gpt-35-turbo,34,,5.0,,How can you optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training?,v1,"To optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training, you can follow these best practices:

1. **Use Azure Blob Storage or Azure File Share**: Store your datasets in Azure Blob Storage or Azure File Share, which are natively supported by Azure Machine Learning and provide high-throughput connectivity.

2. **Leverage Datastores**: Register your Azure Storage account as a datastore in Azure Machine Learning. Datastores abstract the Azure Storage details and provide a more efficient way to access data during training.

3. **Utilize Azure Machine Learning Datasets**: Create datasets in Azure Machine Learning to manage and version your data. Datasets optimize data access by providing mechanisms like data caching and incremental loading.

4. **Cache Data**: For compute targets that support it, such as Azure Machine Learning Compute Clusters, you can cache the data on the compute nodes. This reduces the data transfer time for subsequent training runs.

5. **Optimize Data Format**: Store your data in a format that is optimized for both storage and compute operations, such as Apache Parquet, which is a columnar storage format that supports efficient compression and encoding schemes.

6. **Minimize Data Movement**: Perform data preprocessing and feature engineering steps within the same region as your storage and compute resources to minimize data movement and latency.

7. **Use Azure's High Bandwidth Networking**: If you are using Azure Machine Learning Compute Instances or Compute Clusters, ensure they are in the same region as your Azure Storage to take advantage of Azure's high bandwidth networking.

8. **Parallelize Data Transfer**: When moving large volumes of data, use parallelism to transfer multiple files at once. Azure Machine Learning's built-in data transfer capabilities can help with this.

9. **Monitor and Optimize**: Continuously monitor the data transfer times and throughput. Use Azure Monitor and Azure Machine Learning's monitoring capabilities to identify bottlenecks and optimize accordingly.

By implementing these strategies, you can reduce latency, increase throughput, and ensure that your model training processes are as efficient as possible when accessing and transferring data between Azure Machine Learning and Azure Storage services.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps', ""12,000 Mbps) would load the same data in ~33 secs (assuming you saturate the NIC at 1500-MB/s\nthroughput).\nStorage tier : For most scenarios - including Large Language Models (LLM) - standard storage provides\nthe best cost/performance profile. However, if you have many small files , premium  storage offers a\nbetter cost/performance profile. For more information, read Azure S torage options .\nStorage lo ad: If the storage account is under high load - for example, many GPU nodes in a cluster\nrequesting data - then you risk hitting the egress capacity of storage. For more information, read\nStorage load . If you have many small files that need access in parallel, you might hit the request limits\nof storage. R ead up-to-date information on the limits for both egress capacity and storage requests in\nScale targets for standard storage accounts .\nData access p attern in user code : When you use mount mode, data is fetched based on the open/read\nactions in your code. For example, when reading random sections of a large file, the default data\nprefetching settings of mounts can lead to downloads of blocks that won't be read. Y ou might need to\ntune some settings to reach maximum throughput. For more information, read Optimum mount\nsettings for common scenarios .\nTo access the logs of the data runtime from your job:\n1. Select Outputs+Logs  tab from the job page.\n2. Select the system_logs  folder, followed by data_cap ability  folder.\n3. You should see two log files:\nThe log file data-cap ability .log shows the high-level information about the time spent on key data loading\ntasks. For example, when you download data, the runtime logs the download activity start and finish times:\nlog\nIf the download throughput is a fraction of the expected network bandwidth for the VM size, you can inspect\nthe log file rslex.log.<TIMEST AMP> . This file contains all the fine-grain logging from the Rust-based\nruntime; for example, parallelization:\nlogUsing logs to diagnose issues\n\uf80a\nINFO 2023-05-18 17:14:47,790 sdk_logger.py:44 [28] - ActivityStarted, download\nINFO 2023-05-18 17:14:50,295 sdk_logger.py:44 [28] - ActivityCompleted: Activity=download,  \nHowEnded=Success, Duration=2504.39 [ms]"", ""Manage and optimize Azure Machine\nLearning costs\nArticle •08/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.） Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)"", ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage""]",1922
gpt-35-turbo,35,,5.0,,What considerations should be taken into account for data privacy and compliance when integrating Azure Machine Learning with Azure Storage?,v1,"When integrating Azure Machine Learning with Azure Storage for data privacy and compliance, the following considerations should be taken into account:

1. **Encryption at Rest**: Ensure that all data stored in Azure Blob Storage is encrypted at rest. Azure Storage encrypts data with Microsoft-managed keys by default. For additional control, you can use customer-managed keys in Azure Key Vault.

2. **Encryption in Transit**: Data should be encrypted in transit to protect it from being intercepted during transmission. Azure services typically use Transport Layer Security (TLS) to secure data in transit.

3. **Access Control**: Implement proper access control to the storage account. Use Azure role-based access control (RBAC) to grant the minimum necessary permissions to users and services that need to access the data.

4. **Monitoring and Auditing**: Enable Azure Storage logging and Azure Monitor to keep track of access and usage of the storage account. Regularly audit these logs to detect and respond to any unauthorized access.

5. **Data Residency and Compliance**: Be aware of the data residency requirements that may be imposed by different jurisdictions. Ensure that the data is stored in a region that complies with the necessary regulations.

6. **Data Retention Policies**: Define and implement data retention policies in line with compliance requirements. Ensure that data is retained for the required period and securely deleted when it is no longer needed.

7. **Key Management**: Regularly rotate and manage encryption keys. If you are using customer-managed keys, you should have a process in place for key rotation and revocation.

8. **Secure Data Handling**: When processing data, ensure that secure data handling practices are followed within Azure Machine Learning environments, such as sanitizing input data and securing data processing endpoints.

9. **Compliance Certifications**: Check Azure's compliance offerings to ensure that they meet the necessary standards and certifications required for your industry and type of data.

10. **Data Backup**: Implement a strategy for backing up data. Azure Storage provides options for data redundancy to protect against data loss.

By considering these points, you can help ensure that your integration of Azure Machine Learning with Azure Storage adheres to data privacy and compliance standards.","[""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.', 'Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps', 'Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore""]",1852
gpt-35-turbo,36,,5.0,,How can you use managed identities to securely access data in Azure Storage from Azure Machine Learning?,v1,"To securely access data in Azure Storage from Azure Machine Learning using managed identities, you can follow these steps:

1. **Enable Managed Identity on Azure Machine Learning Workspace:**
   - In the Azure portal, navigate to your Azure Machine Learning workspace.
   - Under the ""Settings"" section, find and click on ""Managed identities.""
   - Enable the system-assigned managed identity for your workspace.

2. **Assign Permissions to Managed Identity:**
   - Go to the Azure Storage account you want to access.
   - Navigate to the ""Access control (IAM)"" section.
   - Click on ""Add role assignment.""
   - Select the appropriate role for the managed identity, such as ""Storage Blob Data Reader"" for read-only access.
   - Assign this role to the managed identity associated with your Azure Machine Learning workspace.

3. **Configure Azure Machine Learning to Use Managed Identity:**
   - When setting up your training job or any other operation that requires data access, configure the job to use the system-assigned managed identity.
   - This can be done by specifying the managed identity in the job's compute target configuration.

4. **Access Data in Code:**
   - In your training script or Jupyter Notebook, use the Azure SDK to access the storage service.
   - The SDK will automatically use the managed identity of the compute resource to authenticate against the storage service without the need for explicit credentials.

By following these steps, Azure Machine Learning will use the managed identity to authenticate and access the data in Azure Storage securely. This approach avoids the need to store sensitive credentials and allows for centralized access management through Azure role-based access control (RBAC).","['You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity', ""Certain machine learning scenarios involve working with private data. In such cases, data\nscientists may not have direct access to data as Microsoft Entra users. In this scenario,\nthe managed identity of a compute can be used for data access authentication. In this\nscenario, the data can only be accessed from a compute instance or a machine learning\ncompute cluster executing a training job. With this approach, the admin grants the\ncompute instance or compute cluster managed identity S torage Blob Data R eader\npermissions on the storage. The individual data scientists don't need to be granted\naccess.\nTo enable authentication with compute managed identity:\nCreate compute with managed identity enabled. See the compute cluster  section,\nor for compute instance, the Assign managed identity  section.\nGrant compute managed identity at least S torage Blob Data R eader role on the\nstorage account.\nCreate any datastores with identity-based authentication enabled. See Create\ndatastores .\nOnce the identity-based authentication is enabled, the compute managed identity is\nused by default when accessing data within your training jobs. Optionally, you can\nauthenticate with user identity using the steps described in next section.\nFor information on using configuring Azure RBA C for the storage, see role-based access\ncontrols .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nWhen training on Azure Machine Learning compute clusters , you can authenticate to\nstorage with your user Microsoft Entra token.\nThis authentication mode allows you to:７ Note\nThe name of the created system managed identity for compute instance or cluster\nwill be in the format /workspace-name/computes/compute-name in your Microsoft\nEntra ID.\nAccess data for training jobs on compute clusters using\nuser identity"", 'means that in order to successfully read data from storage, the managed identity of the\ncompute cluster where the deployment is running must have at least Storage Blob Data\nReader  access to the storage account.\nTo configure the compute cluster for data access, follow these steps:\n1. Go to Azure Machine Learning studio .\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\ndeployment is using.\n3. Assign a managed identity to the compute cluster:\na. In the Managed identity  section, verify if the compute has a managed identity\nassigned. If not, select the option Edit.\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\nusing a S ystem-Assigned Managed Identity, it is named as ""[workspace\nname]/computes/[compute cluster name]"".\nc. Save the changes.\n4. Go to the Azure portal  and navigate to the associated storage account where the\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\nstorage account where those assets are placed.', 'Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .', 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.']",1593
gpt-35-turbo,37,,5.0,,What are the implications of storage redundancy options in Azure Storage on data availability for Azure Machine Learning experiments?,v1,"Storage redundancy options in Azure Storage can have significant implications on data availability for Azure Machine Learning experiments. Here are some key points to consider:

1. **Locally Redundant Storage (LRS)**: This redundancy option stores three copies of your data in a storage scale unit in a single data center. While LRS is generally less expensive, it may not be suitable for high availability scenarios because if the data center goes down, you may lose access to your data, which would impact your Azure Machine Learning experiments.

2. **Zone-Redundant Storage (ZRS)**: ZRS replicates your data across three Azure availability zones in the same region. This increases data availability since even if one zone is compromised, the other zones can provide access to your data. For Azure Machine Learning, this means experiments can continue running even if one zone has issues.

3. **Geo-Redundant Storage (GRS)** and **Geo-Zone-Redundant Storage (GZRS)**: These options provide even higher levels of data availability by replicating data to a secondary region that is hundreds of miles away from the primary location. GRS and GZRS are suitable for disaster recovery scenarios. For Azure Machine Learning, this means that in the event of a regional outage, your data is still accessible from the secondary region, allowing experiments to resume or continue.

4. **Read-Access Geo-Redundant Storage (RA-GRS)** and **Read-Access Geo-Zone-Redundant Storage (RA-GZRS)**: These options not only replicate your data to a secondary region but also provide read access to the data in the secondary location. This can be beneficial for Azure Machine Learning experiments if there is a need to read data from the secondary region without any disruption in case the primary region is not available.

Choosing the right redundancy option depends on the specific requirements for data availability, durability, and the tolerance for downtime in your Azure Machine Learning experiments. It's important to balance these requirements with cost considerations, as higher redundancy levels come with increased costs.","['setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps', 'Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps', ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", ""Service Usage Example datanumber of nodes; datastore names and tags, and\ndescriptions on assets like models; and data label names.\nAzure AI\nSearchStores indexes that\nhelp with querying\nyour machine learning\ncontent.These indexes are built on top of the data stored in Azure\nCosmos DB.\nAzure\nStorageStores metadata\nrelated to Azure\nMachine Learning\npipeline data.Data can include designer pipeline names, pipeline layout,\nand execution properties.\nFrom the perspective of data lifecycle management, data in the preceding resources is\ncreated and deleted as you create and delete corresponding objects in Azure Machine\nLearning.\nYour Azure Machine Learning workspace reads and writes data by using its managed\nidentity. This identity is granted access to the resources through a role assignment\n(Azure role-based access control) on the data resources. The encryption key that you\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\nused to create indexes for Azure AI Search at runtime.\nExtra networking controls are configured when you create a private link endpoint on\nyour workspace to allow for inbound connectivity. This configuration includes the\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\nNetwork access is restricted to only trusted Microsoft services.\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\nthe data.\nWhen you use a customer-managed key, the resources are in your Azure subscription\nand encrypted with your key. While these resources exist in your subscription, Microsoft\nmanages them. They're automatically created and configured when you create your\nAzure Machine Learning workspace.\nThese Microsoft-managed resources are located in a new Azure resource group that's\ncreated in your subscription. This resource group is separate from the resource group\nfor your workspace. It contains the Microsoft-managed resources that your key is used\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\nresource group name><GUID>.Customer-managed keys"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.']",1870
gpt-35-turbo,38,,5.0,,How do you automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage?,v1,"To automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage, you can use Azure services such as Azure Backup or Azure Data Factory, along with Azure Blob Storage lifecycle management policies. Here's a general approach to automate this process:

1. **Azure Backup:**
   - You can use Azure Backup to create backup policies for Azure Blob Storage. This service allows you to define the frequency and retention of the backups.
   - Set up a Recovery Services vault in Azure and configure the backup policy for the storage account where your datasets are stored.
   - Azure Backup will manage the backups according to the policy, and you can recover the data when needed through the Azure portal.

2. **Azure Data Factory:**
   - Create an Azure Data Factory instance.
   - Use the Data Factory pipelines to copy data from your primary Azure Blob Storage account to a secondary storage account for backup purposes.
   - Schedule the pipeline to run at your desired frequency to ensure regular backups.

3. **Azure Blob Storage Lifecycle Management:**
   - Configure lifecycle management policies on your Azure Blob Storage account to automatically move data to a cooler storage tier (e.g., from ""Hot"" to ""Cool"" or ""Archive"") or delete old data that is no longer needed.
   - This can help manage costs and ensure that you're only keeping the necessary backups.

4. **Azure Automation:**
   - Use Azure Automation to run runbooks that can automate the backup process using PowerShell or Python scripts.
   - These scripts can be scheduled to run at regular intervals and can leverage Azure Storage SDKs to manage the backup process.

5. **Azure Event Grid and Azure Functions:**
   - Use Azure Event Grid to trigger Azure Functions or Logic Apps when new data is uploaded to Blob Storage.
   - The triggered function can then handle the backup process by copying the data to another storage account or initiating a Data Factory pipeline.

Remember to ensure that your backup strategy complies with your organization's data retention and recovery requirements. Additionally, consider encrypting the backups and managing access with Azure Role-Based Access Control (RBAC) to maintain security and compliance.","[""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps', 'setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps', ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.""]",1756
gpt-35-turbo,39,,5.0,,What tools and techniques are available for analyzing storage costs associated with Azure Machine Learning projects?,v1,"To analyze storage costs associated with Azure Machine Learning projects, you can use the following tools and techniques:

1. **Azure Pricing Calculator**: Before adding any resources, you can use the Azure Pricing Calculator to estimate the costs of the storage resources you plan to use with Azure Machine Learning.

2. **Cost Management Features**: After you've started using Azure Machine Learning resources, you can utilize Azure Cost Management features to monitor and manage your costs. This includes setting budgets, reviewing forecasted costs, and identifying spending trends.

3. **Cost Analysis in Azure Cost Management**: This tool supports most Azure account types and allows you to analyze your spending on Azure services, including storage costs related to Azure Machine Learning.

4. **Azure Advisor**: Azure Advisor can provide personalized recommendations to help optimize your Azure resources for cost, performance, availability, and security, including recommendations on how to reduce storage costs.

5. **Tags and Resource Groups**: By organizing resources into resource groups and using tags, you can track costs more effectively. This helps in attributing costs to specific projects or departments.

6. **Export Cost Management Data**: You can export cost management data for further analysis using tools like Power BI or Excel, which can help you perform a more detailed analysis of your storage costs.

Remember to ensure that you have at least read access to the Azure account to view cost data, and you may need to assign access to others who need to manage or analyze costs.","[""Plan to manage costs for Azure Machine\nLearning\nArticle •03/31/2023\nThis article describes how to plan and manage costs for Azure Machine Learning. First,\nyou use the Azure pricing calculator to help plan for costs before you add any resources.\nNext, as you add the Azure resources, review the estimated costs.\nAfter you've started using Azure Machine Learning resources, use the cost management\nfeatures to set budgets and monitor costs. Also review the forecasted costs and identify\nspending trends to identify areas where you might want to act.\nUnderstand that the costs for Azure Machine Learning are only a portion of the monthly\ncosts in your Azure bill. If you are using other Azure services, you're billed for all the\nAzure services and resources used in your Azure subscription, including the third-party\nservices. This article explains how to plan for and manage costs for Azure Machine\nLearning. After you're familiar with managing costs for Azure Machine Learning, apply\nsimilar methods to manage costs for all the Azure services used in your subscription.\nFor more information on optimizing costs, see how to manage and optimize cost in\nAzure Machine Learning .\nCost analysis in Cost Management supports most Azure account types, but not all of\nthem. T o view the full list of supported account types, see Understand Cost\nManagement data .\nTo view cost data, you need at least read access for an Azure account. For information\nabout assigning access to Azure Cost Management data, see Assign access to data .） Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nPrerequisites"", ""Use the Azure pricing calculator  to estimate costs before you create the\nresources in an Azure Machine Learning workspace. On the left, select AI +\nMachine Learning , then select Azur e Machine Learning  to begin.\nThe following screenshot shows the cost estimation by using the calculator:\nAs you add new resources to your workspace, return to this calculator and add the same\nresource here to update your cost estimates.\nFor more information, see Azure Machine Learning pricing .\nAzure Machine Learning runs on Azure infrastructure that accrues costs along with\nAzure Machine Learning when you deploy the new resource. It's important toEstimate costs before using Azure Machine\nLearning\nUnderstand the full billing  model for Azure\nMachine Learning"", ""Manage and optimize Azure Machine\nLearning costs\nArticle •08/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.） Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)"", 'monthly schedule and set a custom date range. Exporting cost data is the\nrecommended way to retrieve cost datasets.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nDelete instances and clusters if you do not plan on using them in the near future.\nFor more information, see manage and optimize costs in Azure Machine Learning .\nManage and optimize costs in Azure Machine Learning .\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscale\nLearn how to optimize your cloud investment with Azure Cost Management .\nLearn more about managing costs with cost analysis .\nLearn about how to prevent unexpected costs .\nTake the Cost Management  guided learning course.Other ways to manage and reduce costs for\nAzure Machine Learning\nNext steps', ""To view Azure Machine Learning costs in cost analysis:\n1. Sign in to the Azure portal.\n2. Open the scope in the Azure portal and select Cost analysis  in the menu. For\nexample, go to Subscriptions , select a subscription from the list, and then select\nCost analysis  in the menu. Select Scope  to switch to a different scope in cost\nanalysis.\n3. By default, cost for services are shown in the first donut chart. Select the area in the\nchart labeled Azure Machine Learning.\nActual monthly costs are shown when you initially open cost analysis. Here's an example\nshowing all monthly usage costs.\nTo narrow costs for a single service, like Azure Machine Learning, select Add filt er and\nthen select Service name . Then, select virtual machines .\nHere's an example showing costs for just Azure Machine Learning.\n\uf80a""]",1502
gpt-35-turbo,40,,5.0,,How can data engineers streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics?,v1,"Data engineers can streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics by leveraging several Azure services and features that work together to facilitate a smooth and efficient data pipeline. Here are some steps and best practices to consider:

1. **Azure Data Lake Storage (ADLS) Gen 2**: Utilize ADLS Gen 2 as the primary data storage solution. It is highly scalable and integrates with Azure services seamlessly. Ensure that the storage account is properly set up and that the necessary role assignments are in place for access control.

2. **Azure Synapse Analytics**: Use Azure Synapse Analytics to create an Apache Spark pool. This allows for interactive data wrangling and preprocessing using Azure Machine Learning Notebooks. The integration with Azure Machine Learning enables you to directly access and process data stored in ADLS Gen 2.

3. **Azure Machine Learning Pipelines**: Implement Azure Machine Learning Pipelines to automate the data ingestion and preprocessing steps. Pipelines can be triggered on a schedule or in response to events, ensuring that the data is ready for real-time analytics when needed.

4. **Real-time Ingestion with Azure Event Hubs or Azure IoT Hub**: For real-time data streams, consider using Azure Event Hubs or Azure IoT Hub to ingest data into Azure. These services can handle large volumes of data in real-time and can be connected to Azure Machine Learning for processing.

5. **Azure Functions**: Use Azure Functions to create serverless event-driven processes that can respond to data ingestion events and trigger Azure Machine Learning pipelines or other workflows.

6. **Azure Key Vault**: Store sensitive information such as credentials, access keys, and Shared Access Signature (SAS) tokens securely in Azure Key Vault. This ensures that the data ingestion process is secure and that secrets are managed properly.

7. **Monitoring and Logging**: Set up monitoring and logging using Azure Monitor and Azure Log Analytics to track the data ingestion process and ensure that it is performing as expected.

By following these steps and utilizing the integrated Azure services, data engineers can create a robust and efficient data ingestion pipeline that feeds into Azure Machine Learning for real-time analytics.","[""Interactive Data Wrangling with Apache\nSpark in Azure Machine Learning\nArticle •10/05/2023\nData wrangling becomes one of the most important steps in machine learning projects.\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides access\nto an Apache Spark pool - backed by Azure S ynapse - for interactive data wrangling\nusing Azure Machine Learning Notebooks.\nIn this article, you'll learn how to perform data wrangling using\nServerless Spark compute\nAttached S ynapse Spark pool\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an Azure\nData Lake S torage (ADLS) Gen 2 storage account .\n(Optional): An Azure K ey Vault. See Create an Azure K ey Vault.\n(Optional): A Service Principal. See Create a Service Principal .\n(Optional): An attached S ynapse Spark pool in the Azure Machine Learning\nworkspace .\nBefore you start your data wrangling tasks, learn about the process of storing secrets\nAzure Blob storage account access key\nShared Access Signature (SAS) token\nAzure Data Lake S torage (ADLS) Gen 2 service principal information\nin the Azure K ey Vault. Y ou also need to know how to handle role assignments in the\nAzure storage accounts. The following sections review these concepts. Then, we'll\nexplore the details of interactive data wrangling using the Spark pools in Azure Machine\nLearning Notebooks.Prerequisites\n\uea80 Tip"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""runs from experimentation in the studio user interface. Or they can use versioned assets\nfor jobs like environments and storage references.\nFor more information, see Manage Azure Machine Learning workspaces .\nWhen a project is ready for operationalization, users' work can be automated in an ML\npipeline and triggered on a schedule or HT TPS request.\nYou can deploy models to the managed inferencing solution, for both real-time and\nbatch deployments, abstracting away the infrastructure management typically required\nfor deploying models.\nIn Azure Machine Learning, you can run your training script in the cloud or build a\nmodel from scratch. Customers often bring models they've built and trained in open-\nsource frameworks so that they can operationalize them in the cloud.\nData scientists can use models in Azure Machine Learning that they've created in\ncommon Python frameworks, such as:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nLightGBM\nOther languages and frameworks are also supported:\nR\n.NET\nFor more information, see Open-source integration with Azure Machine Learning .\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\nexperience and intuition to select the right data featurization and algorithm for training.\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\nLearning studio UI or the Python SDK.Train models\nOpen and interoperable\nAutomated featurization and algorithm selection"", ""training and evaluation, the machine learning engineers focus on model deployment\nand automation of the entire workflow. By leveraging machine learning pipeline, each\nteam only needs to work on building their own steps. The best way of building steps is\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\none step in a machine learning pipeline. All these steps built by different users are finally\nintegrated into one workflow through the pipeline definition. The pipeline is a\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\nits steps can be standardized by each company's preferred DevOps practice. The\npipeline can be further versioned and automated. If the ML projects are described as a\npipeline, then the best ML Ops practice is already applied.\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\nimproves large model training's efficiency and reduces cost. T aking modern natural\nlanguage model training as an example. It requires pre-processing large amounts of\ndata and GPU intensive transformer model training. It takes hours to days to train a\nmodel each time. When the model is being built, the data scientist wants to test\ndifferent training code or hyperparameters and run the training many times to get the\nbest model performance. For most of these trainings, there's usually small changes from\none training to another one. It will be a significant waste if every time the full training\nfrom data processing to model training takes place. By using machine learning pipeline,\nit can automatically calculate which steps result is unchanged and reuse outputs from\nprevious training. Additionally, the machine learning pipeline supports running each\nstep on different computation resources. Such that, the memory heavy data processing\nwork and run-on high memory CPU machines, and the computation intensive training\ncan run on expensive GPU machines. By properly choosing which step to run on which\ntype of machines, the training cost can be significantly reduced.\nDepending on what a machine learning project already has, the starting point of\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\nbuilding a pipeline.\nThe first approach usually applies to the team that hasn't used pipeline before and\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\ntypically have developed some machine learning models on their local environment\nusing their favorite tools. Machine learning engineers need to take data scientists'\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\nGetting  started best practices"", ""training and evaluation, the machine learning engineers focus on model deployment\nand automation of the entire workflow. By leveraging machine learning pipeline, each\nteam only needs to work on building their own steps. The best way of building steps is\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\none step in a machine learning pipeline. All these steps built by different users are finally\nintegrated into one workflow through the pipeline definition. The pipeline is a\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\nits steps can be standardized by each company's preferred DevOps practice. The\npipeline can be further versioned and automated. If the ML projects are described as a\npipeline, then the best ML Ops practice is already applied.\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\nimproves large model training's efficiency and reduces cost. T aking modern natural\nlanguage model training as an example. It requires pre-processing large amounts of\ndata and GPU intensive transformer model training. It takes hours to days to train a\nmodel each time. When the model is being built, the data scientist wants to test\ndifferent training code or hyperparameters and run the training many times to get the\nbest model performance. For most of these trainings, there's usually small changes from\none training to another one. It will be a significant waste if every time the full training\nfrom data processing to model training takes place. By using machine learning pipeline,\nit can automatically calculate which steps result is unchanged and reuse outputs from\nprevious training. Additionally, the machine learning pipeline supports running each\nstep on different computation resources. Such that, the memory heavy data processing\nwork and run-on high memory CPU machines, and the computation intensive training\ncan run on expensive GPU machines. By properly choosing which step to run on which\ntype of machines, the training cost can be significantly reduced.\nDepending on what a machine learning project already has, the starting point of\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\nbuilding a pipeline.\nThe first approach usually applies to the team that hasn't used pipeline before and\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\ntypically have developed some machine learning models on their local environment\nusing their favorite tools. Machine learning engineers need to take data scientists'\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\nGetting  started best practices""]",2231
gpt-35-turbo,41,,3.0,,How can I using MSI with Storage?,v1,"To use Managed Service Identity (MSI) with Azure Storage, you would typically set up an Azure service with a Managed Identity and then grant that identity access to an Azure Storage resource. Here's a general outline of the steps you would follow:

1. **Enable Managed Identity for your Azure service**: For Azure Machine Learning, you can enable Managed Identity for your Azure Machine Learning workspace.

2. **Assign the Managed Identity access to Azure Storage**: You would assign the appropriate role to the Managed Identity for your Azure Storage account. This is typically done through Azure Role-Based Access Control (RBAC). Common roles include ""Storage Blob Data Reader"" for read-only access and ""Storage Blob Data Contributor"" for read-write access.

3. **Access Azure Storage from Azure Machine Learning using the Managed Identity**: When your Azure Machine Learning service needs to access Azure Storage, it can use its Managed Identity to authenticate. You don't need to manage any credentials in your code, as the identity is managed by Azure.

If you need specific instructions on how to configure Managed Identity with Azure Machine Learning and Azure Storage, you would typically refer to the Azure documentation or use the Azure portal, Azure CLI, or Azure PowerShell to configure the settings.

Please note that the context provided does not include specific instructions on setting up MSI with Azure Storage within Azure Machine Learning, so the above steps are general guidelines. For detailed steps, you should consult the official Azure documentation.","[""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data"", 'Service tag Protocol Port\nStorage.<region> TCP 443\nFor more information, see How to secure training environments  and Configure inbound\nand outbound network traffic .\nUse the following steps to enable a storage endpoint for the subnet that contains your\nAzure Machine Learning compute clusters and compute instances:\n1. From the Azure portal , select the Azur e Vir tual Netw ork for your Azure Machine\nLearning workspace.\n2. From the left of the page, select Subnets  and then select the subnet that contains\nyour compute cluster and compute instance.\n3. In the form that appears, expand the Services  dropdown and then enable\nMicrosoft.Storage . Select Save to save these changes.\n4. Apply the service endpoint policy to your workspace subnet.７ Note\nFor the storage outbound, a Service Endpoint P olicy will be applied in a later\nstep to limit outbound traffic.\n3. Enable storage endpoint for the subnet\n\uf80a', ""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data"", 'b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably.', 'b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably.']",1906
gpt-35-turbo,42,,5.0,,How can I browse data in the AML workspace?,v1,"To browse data in an Azure Machine Learning (AML) workspace, you can follow these steps:

1. **Access Azure Machine Learning Studio**: Open Azure Machine Learning Studio by navigating to https://ml.azure.com. You will need to sign in with the credentials associated with your Azure subscription.

2. **Select Your Workspace**: Once you are in the Azure Machine Learning Studio, select the appropriate workspace from the top right corner if it's not already selected.

3. **Navigate to the Datasets**: On the left side navigation panel, click on ""Datasets"" to view the datasets that are available in your workspace.

4. **Explore Datasets**: In the Datasets section, you can see a list of all datasets that you have created or registered in your workspace. You can click on any dataset to explore its details, such as schema, profile, and explore the data itself.

5. **Preview Data**: When you select a specific dataset, you can preview the data by clicking on the ""Preview"" tab. This will show you a sample of the data contained in the dataset.

6. **Data Details**: You can also view the details of the dataset, including its version, description, and metadata, by clicking on the ""Details"" tab.

Remember that the Azure Machine Learning Studio interface may change over time, so the exact steps might differ slightly depending on the current design and features available at the time you are accessing it.","[""You'll need the following items:\nTo use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\nRun one or more jobs in your workspace to have results available in the\ndashboard. Complete Tutorial: T rain a model in Azure Machine Learning  if you\ndon't have any jobs yet.\nEnable this preview feature via the preview panel.\nNext, let's view your jobs in the studio:\nSelect Jobs on the left side navigation panel.\nSelect either All experiments  to view all the jobs in an experiment or select All jobs\nto view all the jobs submitted in the workspace.\nYou are now on the default dashboard view where you will find your job list\nconsolidated into the left side bar and dashboard content on the right.Items marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nPrerequisites\nExplore the dashboard view"", ""The following features of Azure Machine Learning studio use data profiling:\nDataset: Explore the dataset from the studio.\nDesigner: Visualize module output data.\nAutoML: View a data preview or profile and choose a target column.\nLabeling: Use labels to prepare data for a machine learning project.\nData profiling depends on the ability of the Azure Machine Learning managed service to\naccess the default Azure storage account for your workspace. The managed service\ndoesn 't exist in y our vir tual netw ork, so it can't directly access the storage account in the\nvirtual network. Instead, the workspace uses a service principal to access storage.\nTo allow access to the storage account, configure the storage account to allow a\nresource instance for your workspace or select Allow Azur e ser vices on the trust edScenario: Use AutoML, the designer, the\ndataset, and the datastore from the studio\n\uea80 Tip\nYou can provide a service principal when you're creating the workspace. If you\ndon't, one is created for you and has the same name as your workspace."", ""You'll need the following items:\nTo use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\nRun one or more jobs in your workspace to have results available in the\ndashboard. Complete Tutorial: T rain a model in Azure Machine Learning  if you\ndon't have any jobs yet.\nEnable this preview feature via the preview panel.\nSelect Jobs on the left side navigation panel.\nSelect either All experiments  to view all the jobs in an experiment or select All jobs\nto view all the jobs submitted in the workspace.\nSelect List view  at the top to switch into List view .\nThe job display name is an optional and customizable name that you can provide for\nyour job. Y ou can edit this directly in your jobs list view by selecting the pencil icon\nwhen you move your mouse over a job name.\nCustomizing the name may help you organize and label your training jobs easily.Prerequisites\nView jobs list\nJob display name\nSelect and pin columns"", 'Azure Machine Learning extension to view, update and create workspace assets like\ncomputes, data, environments, jobs and more.\nWhen you use VS Code for the W eb, the latest versions of these extensions are\nautomatically available to you. If you use the desktop application , you may need to\ninstall them.\nWhen you launch VS Code connected to a compute instance  for the first time, make\nsure you follow these steps and take a few moments to orient yourself to the tools in\nyour integrated development environment.\n1. Locate the Azure extension and sign in\n2. Once your subscriptions are listed, you can filter to the ones you use frequently.\nYou can also pin workspaces you use most often within the subscriptions.\n3. The workspace you launched the VS Code remote connection from (the workspace\nthe compute instance is in) should be automatically set as the default. Y ou can\nupdate the default workspace from the VS Code status bar.\n4. If you plan to use the Azure Machine Learning CLI, open a terminal from the menu,\nand sign in to the Azure Machine Learning CLI using az login --identity.', ""2. Expand your workspace node.\n3. Expand the Envir onments  node.\n4. Right-click the environment you want to view and select View Envir onment .\nAlternatively, use the > Azure ML: View Environment command in the command palette.\nThe quickest way to create a job is by clicking the Create Job  icon in the extension's\nactivity bar.\nUsing the resource nodes in the Azure Machine Learning view:\n1. Expand the subscription node that contains your workspace.\n2. Expand your workspace node.\n3. Right-click the Experiments  node in your workspace and select Create Job .\n4. Choose your job type.\n5. A specification file appears. Configure the specification file.\n6. Right-click the specification file and select Azur eML: Ex ecute YAML .\nAlternatively, use the > Azure ML: Create Job command in the command palette.\nTo view your job in Azure Machine Learning studio:\n1. Expand the subscription node that contains your workspace.\n2. Expand the Experiments  node inside your workspace.\n3. Right-click the experiment you want to view and select View Experiment in S tudio .\n4. A prompt appears asking you to open the experiment URL in Azure Machine\nLearning studio. Select Open .\nAlternatively, use the > Azure ML: View Experiment in Studio command respectively in\nthe command palette.\nAs you're running your job, you may want to see its progress. T o track the progress of a\njob in Azure Machine Learning studio from the extension:\n1. Expand the subscription node that contains your workspace.\n2. Expand the Experiments  node inside your workspace.\n3. Expand the job node you want to track progress for.Create job\nView job\nTrack job progress""]",1570
gpt-35-turbo,43,,5.0,,How to use run spark jobs and integrate with ADLS Gen2?,v1,"To run Apache Spark jobs and integrate with Azure Data Lake Storage (ADLS) Gen 2 using Azure Machine Learning, you can follow these steps:

1. **Prerequisites**:
   - Ensure you have an Azure subscription. If you don't, you can create a free account.
   - Create an Azure Machine Learning workspace. You can find instructions on how to do this in the Azure documentation.
   - Set up an ADLS Gen 2 storage account. Guidance for creating one is available in the Azure documentation.
   - Create an Azure Machine Learning compute instance if you don't already have one.
   - Install the Azure Machine Learning CLI (Command Line Interface), specifically the Azure CLI ml extension v2.

2. **Configure Storage Access**:
   - Before submitting a Spark job, make sure that the input and output data paths in your ADLS Gen 2 storage account are accessible.
   - Assign the 'Contributor' and 'Storage Blob Data Contributor' roles to the user identity of the logged-in user. This will enable read and write access to the storage account.

3. **Submit a Spark Job**:
   - Use Azure Machine Learning's serverless Spark compute to submit your job. This can be done through the Azure Machine Learning Studio or using the Azure CLI with the Azure Machine Learning extension.
   - When submitting the job, you will need to specify the details of your Spark job, including the input and output paths to your ADLS Gen 2 storage account.

4. **User Identity Passthrough**:
   - If your Spark job requires access to resources using the same identity as the user who is submitting the job, you can use user identity passthrough. This ensures that the job has the necessary permissions to access the data in ADLS Gen 2.

5. **Monitor and Manage the Spark Job**:
   - After submitting the job, you can monitor its progress and manage it through the Azure Machine Learning workspace.

For detailed instructions and examples, you should refer to the Azure Machine Learning documentation, specifically the section on running Apache Spark jobs. The documentation will provide you with step-by-step guidance and examples of how to configure and submit your Spark jobs in the Azure environment.","[""Quickstart: Apache Spark jobs in Azure\nMachine Lea rning\nArticle •05/23/2023\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides easy\naccess to distributed computing capability - backed by Azure S ynapse - for scaling\nApache Spark jobs on Azure Machine Learning.\nIn this quickstart guide, you learn how to submit a Spark job using Azure Machine\nLearning serverless Spark compute, Azure Data Lake S torage (ADLS) Gen 2 storage\naccount, and user identity passthrough in a few simple steps.\nFor more information about Apache Sp ark in Azur e Machine Learning  concepts, see\nthis resource .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an\nAzure Data Lake S torage (ADLS) Gen 2 storage account .\nCreate an Azure Machine Learning compute instance .\nInstall Azure Machine Learning CLI .\nBefore we submit an Apache Spark job, we must ensure that input, and output, data\npaths are accessible. Assign Contribut or and Storage Blob Data Contribut or roles to\nthe user identity of the logged-in user to enable read and write access.\nTo assign appropriate roles to the user identity:Prerequisites\nCLI\nAdd role assignments in Azure storage\naccounts"", '16. Repeat steps 2-13 for Storage Blob Contribut or role assignment.\nData in the Azure Data Lake S torage (ADLS) Gen 2 storage account should become\naccessible once the user identity has appropriate roles assigned.\nA Spark job requires a Python script that takes arguments, which can be developed by\nmodifying the Python code developed from interactive data wrangling . A sample Python\nscript is shown here.\nPython\n\uf80a\nCreate parametrized Python code\n# titanic.py  \nimport argparse  \nfrom operator import add \nimport pyspark.pandas as pd \nfrom pyspark.ml.feature import Imputer  \nparser = argparse.ArgumentParser()  \nparser.add_argument( ""--titanic_data"" ) \nparser.add_argument( ""--wrangled_data"" ) \nargs = parser.parse_args()  \nprint(args.wrangled_data)  \nprint(args.titanic_data)  \ndf = pd.read_csv(args.titanic_data, index_col= ""PassengerId"" ) \nimputer = Imputer(inputCols=[ ""Age""], outputCol= ""Age"").setStrategy(\n    ""mean"" \n)  # Replace missing values in Age column with the mean value  \ndf.fillna(  \n    value={ ""Cabin"": ""None""}, inplace= True \n)  # Fill Cabin column with value ""None"" if missing', ""Interactive Data Wrangling with Apache\nSpark in Azure Machine Learning\nArticle •10/05/2023\nData wrangling becomes one of the most important steps in machine learning projects.\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides access\nto an Apache Spark pool - backed by Azure S ynapse - for interactive data wrangling\nusing Azure Machine Learning Notebooks.\nIn this article, you'll learn how to perform data wrangling using\nServerless Spark compute\nAttached S ynapse Spark pool\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an Azure\nData Lake S torage (ADLS) Gen 2 storage account .\n(Optional): An Azure K ey Vault. See Create an Azure K ey Vault.\n(Optional): A Service Principal. See Create a Service Principal .\n(Optional): An attached S ynapse Spark pool in the Azure Machine Learning\nworkspace .\nBefore you start your data wrangling tasks, learn about the process of storing secrets\nAzure Blob storage account access key\nShared Access Signature (SAS) token\nAzure Data Lake S torage (ADLS) Gen 2 service principal information\nin the Azure K ey Vault. Y ou also need to know how to handle role assignments in the\nAzure storage accounts. The following sections review these concepts. Then, we'll\nexplore the details of interactive data wrangling using the Spark pools in Azure Machine\nLearning Notebooks.Prerequisites\n\uea80 Tip"", 'Standard_E4s_v3\nStandard_E8s_v3\nStandard_E16s_v3\nStandard_E32s_v3\nStandard_E64s_v3\n4. Input a Spark Session timeout  value, in minutes.\n5. Select whether to Dynamically allocat e executors\n6. Select the number of Executors for the Spark session.\n7. Select Executor size  from the dropdown menu.\n8. Select Driver size  from the dropdown menu.\n9. To use a Conda file to configure a Spark session, check the Uplo ad conda file\ncheckbox. Then, select Browse , and choose the Conda file with the Spark session\nconfiguration you want.\n10. Add Configuration settings  properties, input values in the Proper ty and Value\ntextboxes, and select Add.\n11. Select Apply .\n12. Select Stop session  in the Configur e new session?  pop-up.\nThe session configuration changes persist and become available to another notebook\nsession that is started using the serverless Spark compute.\nYou can access and wrangle data stored in Azure Data Lake S torage (ADLS) Gen 2\nstorage accounts with abfss:// data URIs following one of the two data access\nmechanisms:\nUser identity passthrough\nService principal-based data access\uea80 Tip\nIf you use session-level Conda packages, you can impr ove the Spark session cold\nstart time if you set the configuration variable spark.hadoop.aml.enable_cache to\ntrue.\nImport and wrangle data from Azure Data Lake Storage\n(ADLS) Gen 2\n\uea80 Tip', 'Install and use the CLI (v2)\nSubmit Spark jobs in Azure Machine Learning    outputs:  \n      wrangled_data:  \n        type:  uri_folder  \n        path:  azureml://datastores/workspaceblobstore/paths/data/wrangled/  \n        mode:  direct \n    identity:  \n      type:  user_identity  \n    compute:  mysparkcompute  \nNext steps']",1798
gpt-35-turbo,44,,5.0,,How does batch inferencing work in parallel mode?,v1,"In Azure Machine Learning, batch inferencing in parallel mode involves distributing the inference workload across multiple nodes and workers to process large amounts of data efficiently. Here's how it generally works:

1. **Deployment Configuration**: You configure the deployment to specify the number of workers on each node and the size of each mini-batch. The configuration determines how the data is partitioned and processed in parallel.

2. **Mini-Batch Processing**: The batch endpoints call the `run()` function in your scoring script once per mini-batch. You have the option to process the entire mini-batch at once, or to process the data file by file, or row by row for tabular data.

3. **Memory Management**: When running multiple workers on the same instance, it's important to remember that memory is shared across all workers. If you increase the number of workers, you may need to decrease the mini-batch size or adjust the scoring strategy to prevent out-of-memory exceptions, especially if the data size and compute resources remain constant.

4. **Inference Strategies**:
   - **Batch Level**: You might run inference over the entire batch at once to achieve high throughput, which is beneficial when using GPUs or when the data loader can handle batching, such as TensorFlow or PyTorch data loaders.
   - **File Level**: Alternatively, you can run inference file by file if that suits your data and memory constraints better.
   - **Row Level**: For tabular data, you might choose to run inference row by row.

5. **High Throughput Deployments**: For scenarios where high throughput is desired, you would typically process the entire batch of data at once to fully utilize the inference device's capabilities.

6. **Scaling**: Azure Machine Learning allows you to scale out your batch inferencing by adding more nodes or instances to the cluster, enabling parallel processing of multiple mini-batches simultaneously.

Remember that when running inference at the batch level, you need to have good control over the input data size to manage memory requirements effectively and avoid out-of-memory issues. The ability to load the entire mini-batch into memory depends on various factors, including the mini-batch size, instance size, number of workers, and the data itself.","[""Your deployment configuration controls the size of each mini-batch and the number of\nworkers on each node. T ake into account them when deciding if you want to read the\nentire mini-batch to perform inference, or if you want to run inference file by file, or row\nby row (for tabular). See Running inference at the mini-batch, file or the row level  to see\nthe different approaches.\nWhen running multiple workers on the same instance, take into account that memory is\nshared across all the workers. Usually, increasing the number of workers per node\nshould be accompanied by a decrease in the mini-batch size or by a change in the\nscoring strategy (if data size and compute SKU remains the same).\nBatch endpoints will call the run() function in your scoring script once per mini-batch.\nHowever, you will have the power to decide if you want to run the inference over the\nentire batch, over one file at a time, or over one row at a time (if your data happens to\nbe tabular).\nYou will typically want to run inference over the batch all at once when you want to\nachieve high throughput in your batch scoring process. This is the case for instance if\nyou run inference over a GPU where you want to achieve saturation of the inference\ndevice. Y ou may also be relying on a data loader that can handle the batching itself if\ndata doesn't fit on memory, like TensorFlow or PyTorch data loaders. On those cases,\nyou may want to consider running inference on the entire batch.\nFor an example about how to achieve it, see High throughput deployments . This\nexample processes an entire batch of files at a time.Running inference at the mini-batch, file or the row level\nMini-batch level\n２ Warning\nRunning inference at the batch level may require having high control over the input\ndata size to be able to correctly account for the memory requirements and avoid\nout of memory exceptions. Whether you are able or not of loading the entire mini-\nbatch in memory will depend on the size of the mini-batch, the size of the instances\nin the cluster, the number of workers on each node, and the size of the mini-batch.\nFile level"", 'One of the easiest ways to perform inference is by iterating over all the files in the mini-\nbatch and run your model over it. In some cases, like image processing, this may be a\ngood idea. If your data is tabular, you may need to make a good estimation about the\nnumber of rows on each file to estimate if your model is able to handle the memory\nrequirements to not just load the entire data into memory but also to perform inference\nover it. R emember that some models (specially those based on recurrent neural\nnetworks) will unfold and present a memory footprint that may not be linear with the\nnumber of rows. If your model is expensive in terms of memory, please consider running\ninference at the row level.\nFor an example about how to achieve it see Image processing with batch deployments .\nThis example processes a file at a time.\nFor models that present challenges in the size of their inputs, you may want to consider\nrunning inference at the row level. Y our batch deployment will still provide your scoring\nscript with a mini-batch of files, however, you will read one file, one row at a time. This\nmay look inefficient but for some deep learning models may be the only way to perform\ninference without scaling up your hardware requirements.\nFor an example about how to achieve it see Text processing with batch deployments .\nThis example processes a row at a time.\nThe environment variable AZUREML_MODEL_DIR contains the path to where the selected\nmodel is located and it is typically used in the init() function to load the model into\nmemory. However, some models may contain their files inside of a folder and you may\nneed to account for that when loading them. Y ou can identify the folder structure of\nyour model as follows:\n1. Go to Azure Machine Learning portal .\n2. Go to the section Models .\uea80 Tip\nIf file sizes are too big to be readed even at once, please consider breaking down\nfiles into multiple smaller files to account for better parallelization.\nRow level (tabular)\nUsing models that are folders', ""Batch scoring, or batch infer encing , involves invoking an endpoint with a reference to\ndata. The batch endpoint runs jobs asynchronously to process data in parallel on\ncompute clusters and store the data for further analysis.\nReal-time s coring, or online infer encing , involves invoking an endpoint with one or more\nmodel deployments and receiving a response in near real time via HT TPS. T raffic can be\nsplit across multiple deployments, allowing for testing new model versions by diverting\nsome amount of traffic initially and increasing after confidence in the new model is\nestablished.\nFor more information, see:\nDeploy a model with a real-time managed endpoint\nUse batch endpoints for scoring\nDevOps for ML models, often called ML Ops, is a process for developing models for\nproduction. A model's lifecycle from training to deployment must be auditable if not\nreproducible.\nLearn more about MLOps in Azure Machine Learning .\nMachine Learning is built with the model lifecycle in mind. Y ou can audit the model\nlifecycle down to a specific commit and environment.Real-time and batch scoring (inferencing)\nMLOps: DevOps for machine learning\nML model lifecycle\nIntegrations enabling MLOPs"", ""There are two types of deployments in batch endpoints:\nModel deployments\nPipeline component deployment\nModel deployment allows operationalizing model inference at scale, processing big\namounts of data in a low latency and asynchronous way. Scalability is automatically\ninstrumented by Azure Machine Learning by providing parallelization of the inferencing\nprocesses across multiple nodes in a compute cluster.\nUse Model deployments  when:\nThe main benefit of this kind of deployments is that you can use the very same assets\ndeployed in the online world (Online Endpoints) but now to run at scale in batch. If your\nmodel requires simple pre or pos processing, you can author an scoring script  that\nperforms the data transformations required.\nTo create a model deployment in a batch endpoint, you need to specify the following\nelements:Model deployments\nYou have expensive models that requires a longer time to run inference.＂\nYou need to perform inference over large amounts of data, distributed in multiple\nfiles.＂\nYou don't have low latency requirements.＂\nYou can take advantage of parallelization.＂"", 'You use the updated settings to save the Y AML file.\nPython\nThe batch inference has these steps:\n1. You use the same built-in feature retrieval component for feature retrieval that you\nused in the training pipeline (covered in the third tutorial). For pipeline training,\nyou provided a feature retrieval specification as a component input. For batch\ninference, you pass the registered model as the input. The component looks for\nthe feature retrieval specification in the model artifact.\nAdditionally, for training, the observation data had the target variable. However,\nthe batch inference observation data doesn\'t have the target variable. The feature\nretrieval step joins the observation data with the features and outputs the data for\nbatch inference.\n2. The pipeline uses the batch inference input data from previous step, runs inference\non the model, and appends the predicted value as output.fs_client.feature_sets.begin_create_or_update(transactions_fset_config)\nprint(fs_poller.result())\n(Optional) Save the YAML file for the feature\nset asset\nPython SDK\n## uncomment and run\n# transactions_fset_config.dump(root_dir +  \n""/featurestore/featuresets/transactions/featureset_asset_offline_enabled\n_with_schedule.yaml"")\nRun the batch inference pipeline\n７ Note\nYou use a job for batch inference in this example. Y ou can also use batch\nendpoints in Azure Machine Learning.']",2098
gpt-35-turbo,45,,5.0,,How to deploy custom vision model to real time endpoint?,v1,"To deploy a custom vision model to a real-time endpoint in Azure Machine Learning, you can follow these general steps:

1. **Register the Model**: First, you need to register your trained custom vision model in Azure Machine Learning. You can do this by using the Azure Machine Learning SDK or the Azure portal. When registering the model, you provide the model name, path to the model file, and any tags or properties.

    ```python
    from azureml.core import Workspace, Model

    workspace = Workspace.from_config()
    model = Model.register(workspace=workspace,
                           model_name='custom_vision_model',  # Name of the registered model in your workspace.
                           model_path='./model',  # Local path to the model
                           description='Custom vision model')
    ```

2. **Create Scoring Script**: Create a scoring script (`score.py`) that will be used by the web service to make predictions. This script should include two main functions: `init()` to load the model and `run(raw_data)` to make predictions.

    ```python
    def init():
        global model
        # Load the model from a file referenced by the registered model.
        model = Model.get_model_path('custom_vision_model')

    def run(raw_data):
        try:
            data = json.loads(raw_data)['data']
            # Make prediction using the loaded model
            predictions = model.predict(data)
            return json.dumps(predictions)
        except Exception as e:
            error = str(e)
            return error
    ```

3. **Create Inference Configuration**: Create an inference configuration that specifies the environment and the scoring script.

    ```python
    from azureml.core.model import InferenceConfig
    from azureml.core.environment import Environment

    environment = Environment(name='myenv')
    environment.docker.enabled = True
    environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['numpy', 'tensorflow'])

    inference_config = InferenceConfig(entry_script='score.py', environment=environment)
    ```

4. **Deploy the Model**: Deploy the model to a real-time endpoint, such as Azure Kubernetes Service (AKS) or Azure Container Instances (ACI).

    ```python
    from azureml.core.webservice import AciWebservice, Webservice

    deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)
    service = Model.deploy(workspace=workspace,
                           name='custom-vision-service',
                           models=[model],
                           inference_config=inference_config,
                           deployment_config=deployment_config)
    service.wait_for_deployment(show_output=True)
    ```

5. **Test the Endpoint**: Once the service is deployed, you can test the real-time endpoint by sending HTTP requests with the input data.

    ```python
    import requests

    url = service.scoring_uri
    headers = {'Content-Type': 'application/json'}
    test_data = json.dumps({'data': ...})  # Replace with your input data
    response = requests.post(url, data=test_data, headers=headers)
    print(response.json())
    ```

Please note that the actual code and parameters might vary depending on the specifics of your custom vision model and the environment setup. Always refer to the latest Azure Machine Learning documentation for the most up-to-date instructions.","['Deploy a trained model\nDeploy a pipeline with batch endpoints', 'Deploy a trained model', 'With the trained model registered in the Machine learning workspace, you are ready to\ndeploy the model for scoring.\nThis scenario includes prebuilt workflows for two approaches to deploying a trained\nmodel, batch scoring or a deploying a model to an endpoint for real-time scoring. Y ou\nmay run either or both of these workflows to test the performance of the model in your\nAzure Machine Learning workspace.\n1. In your GitHub project repository (ex: taxi-fare-regression), select Actions\n2. Select the deploy-online-endpoint -pipeline  from the workflows listed on the left\nand click Run w orkflow  to execute the online endpoint deployment pipeline\nworkflow. The steps in this pipeline will create an online endpoint in your Machine\nLearning workspace, create a deployment of your model to this endpoint, then\nallocate traffic to the endpoint.７ Note\nIf you want to check the output of each individual step, for example to view output\nof a failed run, click a job output, and then click each step in the job to view any\noutput of that step.\nDeploying the Trained Model\nOnline Endpoint', ""Alternatively Y ou can deploy the model from the Azure Machine Learning studio UI .\nNavigate to the model you wish to deploy in the Models  tab of the automated ML job\nand select on Deploy  and select Deploy t o real-time endpoint  .\n.\nthis is how your review page looks like. we can select instance type, instance count and\nset traffic percentage for the current deployment.\n.az ml online-endpoint update --name 'od-fridge-items-endpoint'  --traffic  \n'od-fridge-items-mlflow-deploy=100'  --workspace-name  \n[YOUR_AZURE_ WORKSPACE]  --resource-group  [YOUR_AZURE_RESOURCE_ GROUP] --\nsubscription  [YOUR_AZURE_ SUBSCRIPTION]"", 'If you directly deploy real-time endpoint from a previous completed real-time inference\npipeline, it may run into errors.\nRecommendation : clone the inference pipeline and submit it again, then deploy to real-\ntime endpoint.\nTutorial: Build a model in designer to predict auto pricesDeploy real-time endpoint from real-time inference\npipeline issue\nNext steps']",1144
gpt-35-turbo-ft,1,,3.0,,How do you initiate a new Azure ML experiment using the Azure CLI?,v1,"To initiate a new Azure ML experiment using the Azure CLI, you can use the `az ml experiment submit` command. This command allows you to submit a new experiment run to your Azure Machine Learning workspace.","[""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\nAzure CLI\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\nso one cluster can host one or many batch deployments (along with other workloads if\ndesired).\nThis article uses a compute created here named batch-cluster. Adjust as needed and\nreference your compute using azureml:<your-compute-name> or create one as shown.\nAzure CLIaz extension add -n ml \nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>  \naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>  \nCreate compute\nAzure CLI\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \n0 --max-instances  5 \n７ Note"", 'Azure CLI\nIf you\'re not on the compute instance, omit --identity and follow the prompt to\nopen a browser window to authenticate.\n3. Make sure you have the most recent versions of the CLI and the ml extension:\nAzure CLI\n4. If you have multiple Azure subscriptions, set the active subscription to the one\nyou\'re using for your workspace. (Y ou can skip this step if you only have access to\na single subscription.) R eplace <SUBSCRIPTION-NAME> with your subscription name.\nAlso remove the brackets <>.\nAzure CLI\n5. Now use CLI to submit the job. If you\'re doing this on a compute instance in your\nworkspace, you can use environment variables for the workspace name and\nresource group as show in the following code. If you aren\'t on a compute instance,\nreplace these values with your workspace name and resource group.\nAzure CLI\nOnce you\'ve submitted the job, you can check the status and results in studio:\n1. Sign in to Azure Machine Learning studio .\n2. Select your workspace if it isn\'t already loaded.\n3. On the left navigation, select Jobs.\n4. Select the Experiment name  that you used to train your model.\n5. Select the Display name  of the job to view details and artifacts of the job,\nincluding metrics, images, child jobs, outputs, logs, and code used in the job.az login --identity\naz upgrade\naz account set --subscription  ""<SUBSCRIPTION-NAME>""\naz ml job create -f job.yml  --workspace-name  $CI_WORKSPACE  --resource-\ngroup $CI_RESOURCE_GROUP\nRegister model', ""Azure CLI\nIf you don't have it, create a cluster called cpu-cluster by running:\nAzure CLI\nNow, create a pipeline job defined in the pipeline.yml file with the following command.\nThe compute target is referenced in the pipeline.yml file as azureml:cpu-cluster. If your\ncompute target uses a different name, remember to update it in the pipeline.yml file.\nAzure CLI\nYou should receive a JSON dictionary with information about the pipeline job including:\nKey Descr iption\nname The GUID-based name of the job.\nexperiment_name The name under which jobs will be organized in studio.\nservices.Studio.endpoint A URL for monitoring and reviewing the pipeline job.\nstatus The status of the job. This will likely be Preparing at this point.\nOpen the services.Studio.endpoint URL to see a graph visualization of the pipeline.az ml compute list\n７ Note\nSkip this step to use serverless comput e.\naz ml compute create -n cpu-cluster  --type amlcompute --min-instances  0 --\nmax-instances  10\naz ml job create --file pipeline.yml\nﾉExpand table"", ""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nIn this example, we are going to create a deployment that can write directly to the\noutput folder of the batch deployment job. The deployment will use this feature to write\ncustom parquet files.Azure CLI\naz extension add -n ml\n７ Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nCreating a batch deployment with a custom\noutput"", ""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\ninference over new data:\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\n７ Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nSteps""]",1300
gpt-35-turbo-ft,2,,5.0,,Describe the process of creating a new compute instance via the Azure ML CLI. What parameters are necessary?,v1,"To create a new compute instance via the Azure ML CLI, you need to specify parameters such as the instance name, the virtual network and subnet, and the size of the instance. Additionally, you may need to provide authentication details and specify the Docker image to use.","['Create an Azure Machine Learning\ncompute instance\nArticle •12/08/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nLearn how to create a compute instance  in your Azure Machine Learning workspace.\nUse a compute instance as your fully configured and managed development\nenvironment in the cloud. For development and testing, you can also use the instance as\na training compute target . A compute instance can run multiple jobs in parallel and has\na job queue. As a development environment, a compute instance can\'t be shared with\nother users in your workspace.\nIn this article, you learn how to create a compute instance. See Manage an Azure\nMachine Learning compute instance  for steps to manage start, stop, restart, delete a\ncompute instance.\nYou can also use a setup script  to create the compute instance with your own custom\nenvironment.\nCompute instances can run jobs securely in a virtual network environment , without\nrequiring enterprises to open up SSH ports. The job executes in a containerized\nenvironment and packages your model dependencies in a Docker container.\nAn Azure Machine Learning workspace. For more information, see Create an Azure\nMachine Learning workspace . In the storage account, the ""Allow storage account\nkey access"" option must be enabled for compute instance creation to be\nsuccessful.\nChoose the tab for the environment you\'re using for other prerequisites.\n７ Note\nThis article uses CLI v2 in some examples. If you are still using CLI v1, see Create an\nAzur e Machine Learning comput e clust er CLI v1) .\nPrerequisites', ""For more information on creating a compute cluster and compute cluster, including how\nto do so with Python and the CLI, see the following articles:\nCreate a compute cluster\nCreate a compute instance\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nWhen Azure Container R egistry is behind the virtual network, Azure Machine Learning\ncan't use it to directly build Docker images (used for training and deployment). Instead,\nconfigure the workspace to use the compute cluster you created earlier. Use the\nfollowing steps to create a compute cluster and configure the workspace to use it to\nbuild images:\n1. Navigate to https://shell.azure.com/  to open the Azure Cloud Shell.\n2. From the Cloud Shell, use the following command to install the 2.0 CLI for Azure\nMachine Learning:\nAzure CLIAllow inbound T CP traffic on ports 29876-29877 from the\nBatchNodeManagement service tag.\nAllow inbound T CP traffic on port 44224 from the AzureMachineLearning\nservice tag.\nThe following screenshot shows an example of these rules:\nConfigure image builds\naz extension add -n ml"", ""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\nAzure CLI\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\nso one cluster can host one or many batch deployments (along with other workloads if\ndesired).\nThis article uses a compute created here named batch-cluster. Adjust as needed and\nreference your compute using azureml:<your-compute-name> or create one as shown.\nAzure CLIaz extension add -n ml \nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>  \naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>  \nCreate compute\nAzure CLI\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \n0 --max-instances  5 \n７ Note"", ""To create a workspace using CLI v2, use the following command:\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nBash\nFor more information, see workspace Y AML schema .\nA compute is a designated compute resource where you run your job or host your\nendpoint. Azure Machine Learning supports the following types of compute:\nComput e clust er - a managed-compute infrastructure that allows you to easily\ncreate a cluster of CPU or GPU compute nodes in the cloud.\nComput e instance  - a fully configured and managed development environment in\nthe cloud. Y ou can use the instance as a training or inference compute for\ndevelopment and testing. It's similar to a virtual machine on the cloud.\nInfer ence clust er - used to deploy trained machine learning models to Azure\nKubernetes Service. Y ou can create an Azure Kubernetes Service (AKS) cluster from\nyour Azure Machine Learning workspace, or attach an existing AKS cluster.\nAttached comput e - You can attach your own compute resources to your\nworkspace and use them for training and inference.\nTo create a compute using CLI v2, use the following command:\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)Azure CLI\naz ml workspace create --file my_workspace.yml  \nCompute\n７ Note\nInstead of creating a compute cluster, use serverless comput e (preview) to\noffload compute lifecycle management to Azure Machine Learning.\nAzure CLI"", 'Manage an Azure Machine Learning\ncompute instance\nArticle •07/06/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nLearn how to manage a compute instance  in your Azure Machine Learning workspace.\nUse a compute instance as your fully configured and managed development\nenvironment in the cloud. For development and testing, you can also use the instance as\na training compute target . A compute instance can run multiple jobs in parallel and has\na job queue. As a development environment, a compute instance can\'t be shared with\nother users in your workspace.\nIn this article, you learn how to start, stop, restart, delete) a compute instance. See\nCreate an Azure Machine Learning compute instance  to learn how to create a compute\ninstance.\nAn Azure Machine Learning workspace. For more information, see Create an Azure\nMachine Learning workspace . In the storage account, the ""Allow storage account\nkey access"" option must be enabled for compute instance creation to be\nsuccessful.\nThe Azure CLI extension for Machine Learning service (v2) , Azure Machine\nLearning Python SDK (v2) , or the Azure Machine Learning Visual S tudio Code\nextension .\nIf using the Python SDK, set up your development environment with a workspace .\nOnce your environment is set up, attach to the workspace in your Python script:\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\n７ Note\nThis article shows CLI v2 in the sections below. If you are still using CLI v1, see\nCreate an Azur e Machine Learning comput e clust er CLI v1) .\nPrerequisites']",1508
gpt-35-turbo-ft,3,,3.0,,Explain how to submit a training job using Python and the AML SDK v2. Include an example of specifying a compute target.,v1,"To submit a training job using Python and the Azure Machine Learning SDK v2, you need to define a script run configuration, which includes the training script, environment, and compute target. Here's an example of how to do this:

```python
from azureml.core import Workspace, Experiment, ScriptRunConfig
from azureml.core.compute import ComputeTarget, AmlCompute
from azureml.core.environment import Environment
from azureml.core.conda_dependencies import CondaDependencies

# Load workspace
ws = Workspace.from_config()

# Define the experiment
experiment_name = 'training-job-example'
experiment = Experiment(workspace=ws, name=experiment_name)

# Define the compute target
compute_target = ComputeTarget(workspace=ws, name='my-compute')

# Create a Python environment
env = Environment('training-env')
env.python.user_managed_dependencies = True
env.docker.enabled = False
deps = CondaDependencies.create(pip_packages=['azureml-defaults', 'scikit-learn'])
env.python.conda_dependencies = deps

# Define the script run configuration
script_config = ScriptRunConfig(source_directory='.',
                                script='train.py',
                                compute_target=compute_target,
                                environment=env)

# Submit the job
run = experiment.submit(script_config)
```

In this example, `train.py` is the training script, `my-compute` is the name of the compute target, and `training-env` is the name of the environment.","[""1. On the top bar above your opened notebook, create a compute instance if you\ndon't already have one.\n2. If the compute instance is stopped, select Start comput e and wait until it is\nrunning.\n3. Make sure that the kernel, found on the top right, is Python 3.10 - SDK v2. If not,\nuse the dropdown to select this kernel.\n4. If you see a banner that says you need to be authenticated, select Authenticat e.\nTo train a model, you need to submit a job. The type of job you'll submit in this tutorial\nis a command job . Azure Machine Learning offers several different types of jobs to train\nmodels. Users can select their method of training based on complexity of the model,\ndata size, and training speed requirements. In this tutorial, you'll learn how to submit a\ncommand job  to run a training s cript.\nA command job is a function that allows you to submit a custom training script to train\nyour model. This can also be defined as a custom training job. A command job in Azure\nMachine Learning is a type of job that runs a script or command in a specified\nenvironment. Y ou can use command jobs to train models, process data, or any other\ncustom code you want to execute in the cloud.Set your kernel\n\uf80a\n\uf80a\n\uf80a\n） Impor tant\nThe rest of this tutorial contains cells of the tutorial notebook. Copy/paste them\ninto your new notebook, or switch to the notebook now if you cloned it.\nUse a command job to train a model in Azure\nMachine Learning"", 'For detailed instructions on how to complete these steps, see Secure a training\nenvironment .\nIn this section, you learn how Azure Machine Learning securely communicates between\nservices to submit a training job. This example shows you how all your configurations\nwork together to secure communication.\n1. The client uploads training scripts and training data to storage accounts that are\nsecured with a service or private endpoint.\n2. The client submits a training job to the Azure Machine Learning workspace\nthrough the private endpoint.\n3. Azure Batch service receives the job from the workspace. It then submits the\ntraining job to the compute environment through the public load balancer for the\ncompute resource.\n4. The compute resource receives the job and begins training. The compute resource\nuses information stored in key vault to access storage accounts to download\ntraining files and upload output.Example training job submission', 'Python\nView more examples of training with serverless compute at:-\nQuick S tart\nTrain Model\nThere\'s no need to specify compute for AutoML jobs. R esources can be optionally\nspecified. If instance count isn\'t specified, then it\'s defaulted based on\nmax_concurrent_trials and max_nodes parameters. If you submit an AutoML image\nclassification or NLP task with no instance type, the GPU VM size is automatically\nselected. It\'s possible to submit AutoML job through CLIs, SDK, or S tudio. T o submit\nAutoML jobs with serverless compute in studio first enable the submit a training job in\nstudio (preview)  feature in the preview panel.Python SDK\nfrom azure.ai.ml import command\nfrom azure.ai.ml import MLClient      # Handle to the workspace\nfrom azure.identity import DefaultAzureCredential     # Authentication  \npackage\nfrom azure.ai.ml.entities import ResourceConfiguration\nfrom azure.ai.ml.entities import UserIdentityConfiguration \ncredential = DefaultAzureCredential()\n# Get a handle to the workspace. You can find the info on the workspace  \ntab on ml.azure.com\nml_client = MLClient(\n    credential=credential,\n    subscription_id= ""<Azure subscription id>"" , \n    resource_group_name= ""<Azure resource group>"" ,\n    workspace_name= ""<Azure Machine Learning Workspace>"" ,\n)\njob = command(\n    command= ""echo \'hello world\'"" ,\n    environment= ""AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest"" ,\n         identity=UserIdentityConfiguration(),\n    queue_settings={\n      ""job_tier"" : ""Standard""   \n    }\n)\njob.resources = ResourceConfiguration(instance_type= ""Standard_E4s_v3"" , \ninstance_count= 1)\n# submit the command job\nml_client.create_or_update(job)\nAutoML job', ""Designer : Azure Machine Learning designer provides an easy entry-point into\nmachine learning for building proof of concepts, or for users with little coding\nexperience. It allows you to train models using a drag and drop web-based UI. Y ou\ncan use Python code as part of the design, or train models without writing any\ncode.\nAzur e CLI : The machine learning CLI provides commands for common tasks with\nAzure Machine Learning, and is often used for scripting and aut omating tasks . For\nexample, once you've created a training script or pipeline, you might use the Azure\nCLI to start a training job on a schedule or when the data files used for training are\nupdated. For training models, it provides commands that submit training jobs. It\ncan submit jobs using run configurations or pipelines.\nEach of these training methods can use different types of compute resources for\ntraining. Collectively, these resources are referred to as comput e targets. A compute\ntarget can be a local machine or a cloud resource, such as an Azure Machine Learning\nCompute, Azure HDInsight, or a remote virtual machine.\nThe Azure Machine Learning SDK for Python allows you to build and run machine\nlearning workflows with Azure Machine Learning. Y ou can interact with the service from\nan interactive Python session, Jupyter Notebooks, Visual S tudio Code, or other IDE.\nInstall/update the SDK\nConfigure a development environment for Azure Machine Learning\nA generic training job with Azure Machine Learning can be defined using the\ncommand() . The command is then used, along with your training script(s) to train a\nmodel on the specified compute target.\nYou may start with a command for your local computer, and then switch to one for a\ncloud-based compute target as needed. When changing the compute target, you only\nchange the compute parameter in the command that you use. A run also logs\ninformation about the training job, such as the inputs, outputs, and logs.\nTutorial: T rain your first ML model\nExamples: Jupyter Notebook and Python examples of training modelsPython SDK\nSubmit a command\nAutomated Machine Learning"", 'An Azure Machine Learning compute cluster is a fully managed compute resource that\ncan be used to run the training job. In the following examples, a compute cluster named\ncpu-compute is created.\nPython\nTo run this script, you\'ll use a command that executes main.py Python script located\nunder ./sdk/python/jobs/single-step/lightgbm/iris/src/. The command will be runworkspace = \'<AZUREML_WORKSPACE_NAME>\'\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \nresource_group, workspace)\n2. Create a compute resource for training\n７ Note\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\ntraining job .\nPython SDK\nfrom azure.ai.ml.entities import AmlCompute\n# specify aml compute name.\ncpu_compute_target = ""cpu-cluster""\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(""Creating a new cpu compute target..."" )\n    compute = AmlCompute(\n        name=cpu_compute_target, size= ""STANDARD_D2_V2"" , min_instances= 0, \nmax_instances= 4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n4. Submit the training job\nPython SDK']",1906
gpt-35-turbo-ft,4,,5.0,,What is the command to list all the models registered in an Azure ML Workspace using the Azure CLI?,v1,The command to list all the models registered in an Azure ML Workspace using the Azure CLI is `az ml model list`.,"['then select Regist er.\nThe SDK and CLI (v2) also allow you to manage the lifecycle of your Azure Machine\nLearning model assets.\nList all the models in your workspace:\ncli\nList all the model versions under a given name:\ncli\n\uf80a\nManage models\nList\nAzure CLI\naz ml model list  \nAzure CLI\naz ml model list --name run-model-example', 'Install the Azure CLI and the ml extension to the Azure CLI. For more\ninformation on installing the CLI, see Install and set up the CLI (v2) .\nFirst, connect to the Azure Machine Learning workspace where you\'ll work.\nAzure CLI\nYou can deploy only registered models to online endpoints. In this case, you already\nhave a local copy of the model in the repository, so you only need to publish the model\nto the registry in the workspace. Y ou can skip this step if the model you\'re trying to\ndeploy is already registered.\nAzure CLI\nIf your model was logged inside of a run, you can register it directly.\nTo register the model, you need to know the location where it is stored. If you\'re using\nMLflow\'s autolog feature, the path to the model depends on the model type and\nframework. Y ou should check the jobs output to identify the name of the model\'s folder.\nThis folder contains a file named MLModel.Connect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nRegister the model\nAzure CLI\nMODEL_NAME= \'sklearn-diabetes\'\naz ml model create --name $MODEL_NAME  --type ""mlflow_model""  --path \n""endpoints/online/ncd/sklearn-diabetes/model""\nWhat if your model was logged inside of a run?', 'Install the Azure CLI and the ml extension to the Azure CLI. For more\ninformation on installing the CLI, see Install and set up the CLI (v2) .\nFirst, connect to the Azure Machine Learning workspace where you\'ll work.\nAzure CLI\nYou can deploy only registered models to online endpoints. In this case, you already\nhave a local copy of the model in the repository, so you only need to publish the model\nto the registry in the workspace. Y ou can skip this step if the model you\'re trying to\ndeploy is already registered.\nAzure CLI\nIf your model was logged inside of a run, you can register it directly.\nTo register the model, you need to know the location where it is stored. If you\'re using\nMLflow\'s autolog feature, the path to the model depends on the model type and\nframework. Y ou should check the jobs output to identify the name of the model\'s folder.\nThis folder contains a file named MLModel.Connect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nRegister the model\nAzure CLI\nMODEL_NAME= \'sklearn-diabetes\'\naz ml model create --name $MODEL_NAME  --type ""mlflow_model""  --path \n""endpoints/online/ncd/sklearn-diabetes/model""\nWhat if your model was logged inside of a run?', 'The az ml model command can be used for managing Azure Machine Learning models.\nExamples are available in the examples GitHub repository . Several are shown below.\nYAML\nYAML\nInstall and use the CLI (v2)Remarks\nExamples\nYAML: local file\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-file-example  \npath: mlflow-model/model.pkl  \ndescription:  Model created from local file. \nYAML: local folder in MLflow format\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-mlflow-example  \npath: mlflow-model  \ntype: mlflow_model  \ndescription:  Model created from local MLflow model directory.', ""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\ninference over new data:\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\n７ Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nSteps""]",1126
gpt-35-turbo-ft,5,,3.0,,Provide an example of how to use the AML SDK v2 to stream logs from a running experiment in Python.,v1,"To stream logs from a running experiment using Azure Machine Learning SDK v2 in Python, you can use the `Stream` class from the `azure.ai.ml` package. Here's an example of how to achieve this:

```python
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the workspace
ml_client = MLClient.from_config(DefaultAzureCredential())

# Get the run ID of the experiment
run_id = ""<your_run_id>""

# Stream logs from the running experiment
stream = ml_client.streams.get(run_id)
for line in stream:
    print(line)
```

Replace `<your_run_id>` with the actual run ID of the experiment you want to stream logs from.","['SDK v2\nPython\nFunctionality in SDK v1 Rough mapping in SDK v2\nexperiment.submit MLCLient.jobs.create_or_update\nScriptRunConfig() command()\nFor more information, see:\nV1 - Experiment\nV2 - Command Jobaml_url = run.get_portal_url()  \nprint(aml_url)  \n#import required libraries  \nfrom azure.ai.ml import MLClient, command  \nfrom azure.ai.ml.entities import Environment  \nfrom azure.identity import DefaultAzureCredential  \n#connect to the workspace  \nml_client = MLClient.from_config(DefaultAzureCredential())  \n# set up pytorch environment  \nenv = Environment(  \n    image= ""mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04"" , \n    conda_file= ""pytorch-env.yml"" , \n    name= ""pytorch-env""  \n) \n# define the command  \ncommand_job = command(  \n    code= ""./src"", \n    command= ""train.py"" , \n    environment=env,  \n    compute= ""cpu-cluster"" , \n) \nreturned_job = ml_client.jobs.create_or_update(command_job)  \nreturned_job  \nMapping of key functionality in v1 and v2\nNext steps', ""Explore Azu re Machine Learning with\nJupyter Notebooks\nArticle •02/06/2024\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nThe AzureML-Examples  repository includes the latest (v2) Azure Machine Learning\nPython CLI and SDK samples. For information on the various example types, see the\nreadme .\nThis article shows you how to access the repository from the following environments:\nAzure Machine Learning compute instance\nYour own compute resource\nData Science Virtual Machine\nThe easiest way to get started with the samples is to complete the Create resources to\nget started . Once completed, you'll have a dedicated notebook server preloaded with\nthe SDK and the Azure Machine Learning Notebooks repository. No downloads or\ninstallation necessary.\nTo view example notebooks:\n1. Sign in to studio  and select your workspace if necessary.\n2. Select Notebooks .\n3. Select the Samples  tab. Use the SDK v2  folder for examples using Python SDK v2.\n4. Open the notebook you want to run. Select Clone this not ebook  to create a copy\nin your workspace file share. This action will copy the notebook along with any\ndependent resources.\nIf you'd like to bring your own notebook server for local development, follow these\nsteps on your computer.\n1. Use the instructions at Azure Machine Learning SDK  to install the Azure Machine\nLearning SDK (v2) for Python\nOption 1: Access on Azure Machine Learning\ncompute instance (recommended)\nOption 2: Access on your own notebook server"", ""Query & co mpare experiments and runs\nwith MLflow\nArticle •06/26/2023\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\nYou don't need to install any specific SDK to manage what happens inside of a training\njob, creating a more seamless transition between local runs and the cloud by removing\ncloud-specific dependencies. In this article, you'll learn how to query and compare\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\nin Python.\nMLflow allows you to:\nCreate, query, delete and search for experiments in a workspace.\nQuery, delete, and search for runs in a workspace.\nTrack and retrieve metrics, parameters, artifacts and models from runs.\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\ndetailed comparison between MLflow Open-Source and MLflow when connected to\nAzure Machine Learning.\nQuery and searching experiments and runs is also available using the MLflow REST API.\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\nconsume it.\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\nMLflow azureml-mlflow.７ Note\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\ntracking capabilities. This applies not just for logging but also for querying the\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\nexplains how to use MLflow to manage experiments and runs in Azure Machine\nLearning.\nREST API\nPrerequisites"", ""Query & co mpare experiments and runs\nwith MLflow\nArticle •06/26/2023\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\nYou don't need to install any specific SDK to manage what happens inside of a training\njob, creating a more seamless transition between local runs and the cloud by removing\ncloud-specific dependencies. In this article, you'll learn how to query and compare\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\nin Python.\nMLflow allows you to:\nCreate, query, delete and search for experiments in a workspace.\nQuery, delete, and search for runs in a workspace.\nTrack and retrieve metrics, parameters, artifacts and models from runs.\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\ndetailed comparison between MLflow Open-Source and MLflow when connected to\nAzure Machine Learning.\nQuery and searching experiments and runs is also available using the MLflow REST API.\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\nconsume it.\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\nMLflow azureml-mlflow.７ Note\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\ntracking capabilities. This applies not just for logging but also for querying the\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\nexplains how to use MLflow to manage experiments and runs in Azure Machine\nLearning.\nREST API\nPrerequisites"", 'SDK v2. Full sample link\nPython\'--learning_rate\' , 0.1,\'--model_output\' , model_output],\n    source_directory=script_folder,\n    compute_target=aml_compute,\n    allow_reuse= True,\n)\nscore_output = OutputFileDatasetConfig( \'score_output\' )\nscore_step = PythonScriptStep(\n    name=""score step"" ,\n    script_name= ""score.py"" ,\n    arguments=[ \'--model_input\' ,model_output.as_input( \'model_input\' ), \'-\n-test_data\' , input_ds.as_named_input( \'test_data\' ).as_mount(), \'--\nscore_output\' , score_output],\n    source_directory=script_folder,\n    compute_target=aml_compute,\n    allow_reuse= True,\n)\neval_output = OutputFileDatasetConfig( \'eval_output\' )\neval_step = PythonScriptStep(\n    name=""eval step"" ,\n    script_name= ""eval.py"" ,\n    arguments=[ \'--\nscoring_result\' ,score_output.as_input( \'scoring_result\' ), \'--\neval_output\' , eval_output],\n    source_directory=script_folder,\n    compute_target=aml_compute,\n    allow_reuse= True,\n)\n# built pipeline\nfrom azureml.pipeline.core import Pipeline\npipeline_steps = [train_step, score_step, eval_step]\npipeline = Pipeline(workspace = workspace, steps=pipeline_steps)\nprint(""Pipeline is built."" )\npipeline_run = experiment.submit(pipeline, regenerate_outputs= False)\nprint(""Pipeline submitted for execution."" )\n# import required libraries\nfrom azure.identity import DefaultAzureCredential,  \nInteractiveBrowserCredential\nfrom azure.ai.ml import MLClient, Input\nfrom azure.ai.ml.dsl import pipeline']",1733
gpt-35-turbo-ft,6,,5.0,,"How can you configure an AutoML run for a classification task using the AML SDK v2, including specifying primary metric and training data?",v1,"To configure an AutoML run for a classification task using the AML SDK v2, you can use the provided sample code as a reference. Here's a breakdown of the key steps:

1. Define the AutoML settings, including the primary metric, training data, and other configurations.
2. Submit the AutoML run to the experiment.
3. Retrieve the URL for monitoring the run's progress and results.

The provided code snippet demonstrates these steps, including the use of the `classification` factory function to create the AutoML classification job.","['SDK v2: Below is a sample AutoML classification task. For the entire code, check\nout our examples repo .\nPython    debug_log= ""automl_errors.log"" , \n    compute_target=compute_target,  \n    training_data=training_data,  \n    label_column_name=label_column_name,  \n    **automl_settings,  \n) \n# Submit run  \nremote_run = experiment.submit(automl_config, show_output= False) \nazureml_url = remote_run.get_portal_url()  \nprint(azureml_url)  \n# Imports  \nfrom azure.ai.ml import automl, Input, MLClient  \nfrom azure.ai.ml.constants import AssetTypes  \nfrom azure.ai.ml.automl import ( \n    classification,  \n    ClassificationPrimaryMetrics,\n    ClassificationModels,  \n) \n# Create MLTables for training dataset  \n# Note that AutoML Job can also take in tabular data  \nmy_training_data_input = Input(  \n    type=AssetTypes.MLTABLE, path= ""./data/training-mltable-folder""  \n) \n# Create the AutoML classification job with the related factory-\nfunction.  \nclassification_job = automl.classification(  \n    compute= ""<compute_name>"" , \n    experiment_name= ""<exp_name?"" , \n    training_data=my_training_data_input,  \n    target_column_name= ""<name_of_target_column>"" , \n    primary_metric= ""accuracy"" , \n    n_cross_validations= 5, \n    enable_model_explainability= True, \n    tags={ ""my_custom_tag"" : ""My custom value"" }, \n) \n# Limits are all optional  \nclassification_job.set_limits(  \n    timeout_minutes= 600, \n    trial_timeout_minutes= 20, \n    max_trials= 5, \n    max_concurrent_trials = 4,', 'Upgrade AutoML to SDK v2\nArticle •04/04/2023\nIn SDK v2, ""experiments"" and ""runs"" are consolidated into jobs.\nIn SDK v1, AutoML was primarily configured and run using the AutoMLConfig class. In\nSDK v2, this class has been converted to an AutoML job. Although there are some\ndifferences in the configuration options, by and large, naming & functionality has been\npreserved in V2.\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\nSDK v1: Below is a sample AutoML classification task. For the entire code, check\nout our examples repo .\nPythonSubmit AutoML run\n# Imports  \nimport azureml.core  \nfrom azureml.core.experiment import Experiment  \nfrom azureml.core.workspace import Workspace  \nfrom azureml.core.dataset import Dataset  \nfrom azureml.train.automl import AutoMLConfig  \nfrom azureml.train.automl.run import AutoMLRun    \n# Load tabular dataset  \ndata = ""<url_to_data>""  \ndataset = Dataset.Tabular.from_delimited_files(data)  \ntraining_data, validation_data = dataset.random_split(percentage= 0.8, \nseed=223) \nlabel_column_name = ""Class"" \n# Configure Auto ML settings  \nautoml_settings = {  \n    ""n_cross_validations"" : 3, \n    ""primary_metric"" : ""average_precision_score_weighted"" , \n    ""enable_early_stopping"" : True, \n    ""max_concurrent_iterations"" : 2,   \n    ""experiment_timeout_hours"" : 0.25,   \n    ""verbosity"" : logging.INFO,  \n} \n# Put together an AutoML job constructor  \nautoml_config = AutoMLConfig(  \n    task= ""classification"" ,', '3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\nMachine Learning in many different ways .\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\niterations over different models, hyperparameter settings, advanced\npreprocessing/featurization, and what metrics to look at when determining the\nbest model.\n5. Submit the training job.\n6. Review the r esults\nThe following diagram illustrates this process. \nYou can also inspect the logged job information, which contains metrics  gathered\nduring the job. The training job produces a Python serialized object ( .pkl file) that\ncontains the model and data preprocessing.\nWhile model building is automated, you can also learn how important or relevant\nfeatures are  to the generated models.\nApply automated ML when you want Azure Machine Learning to train and tune a model\nfor you using the target metric you specify. Automated ML democratizes the machine\nlearning model development process, and empowers its users, no matter their data\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\nML professionals and developers across industries can use automated ML to:\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\nforecasting, computer vision & NLP', 'iii. Select Create to create your compute target.\nThis tak es a couple minut es to complet e.\niv. After creation, select your new compute target from the drop-down list.\ne. Select Next .\n3. On the Select task and settings  form, complete the setup for your automated ML\nexperiment by specifying the machine learning task type and configuration\nsettings.\na. Select Classification  as the machine learning task type.\nb. Select View additional configuration settings  and populate the fields as\nfollows. These settings are to better control the training job. Otherwise, defaults\nare applied based on experiment selection and data.\nAdditional\xa0configurations Descr iption Value\xa0for\xa0tut orial\nPrimary metric Evaluation metric that the\nmachine learning algorithm\nwill be measured by.AUC_weighted\nExplain best model Automatically shows\nexplainability on the bestEnable', ""An Azure subscription. (If you don't have an Azure subscription, create a free\naccount  before you begin)\nAn Azure Machine Learning experiment created with either:\nThe Azure Machine Learning studio  (no code required)\nThe Azure Machine Learning Python SDK\nAfter your automated ML experiment completes, a history of the jobs can be found via:\nA browser with Azure Machine Learning studio\nA Jupyter notebook using the JobDetails Jupyter widget\nThe following steps and video, show you how to view the run history and model\nevaluation metrics and charts in the studio:\n1. Sign into the studio  and navigate to your workspace.\n2. In the left menu, select Jobs.\n3. Select your experiment from the list of experiments.\n4. In the table at the bottom of the page, select an automated ML job.\n5. In the Models  tab, select the Algorithm name  for the model you want to evaluate.\n6. In the Metrics  tab, use the checkboxes on the left to view metrics and charts.\nAutomated ML calculates performance metrics for each classification model generated\nfor your experiment. These metrics are based on the scikit learn implementation.\nMany classification metrics are defined for binary classification on two classes, and\nrequire averaging over classes to produce one score for multi-class classification. Scikit-\nlearn provides several averaging methods, three of which automated ML exposes:\nmacr o, micro, and weight ed.\nMacr o - Calculate the metric for each class and take the unweighted average\nMicro - Calculate the metric globally by counting the total true positives, false\nnegatives, and false positives (independent of classes).\nWeight ed - Calculate the metric for each class and take the weighted average\nbased on the number of samples per class.\nWhile each averaging method has its benefits, one common consideration when\nselecting the appropriate method is class imbalance. If classes have different numbers of\nsamples, it might be more informative to use a macro average where minority classes\nView job results\nClassification metrics""]",1737
gpt-35-turbo-ft,7,,5.0,,Describe the steps to retrieve the best model from an AutoML run using the AML SDK v2.,v1,"To retrieve the best model from an AutoML run using the Azure Machine Learning SDK v2, you can follow these steps:

1. **Get the Best Model**: Use the `get_output` method on the AutoML run to retrieve the best model and its associated metrics.

2. **Register the Model**: Optionally, you can register the best model with the Azure Machine Learning workspace using the `register_model` method.

3. **Download the Model**: If needed, you can download the best model to your local environment using the `download` method.

Would you like more detailed information on any of these steps?","['Learn more about how and where to deploy a model .\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps', ""Navigate to the Models  tab to see the algorithms (models) tested. By default, the\nmodels are ordered by metric score as they complete. For this tutorial, the model that\nscores the highest based on the chosen Normalized r oot mean squar ed err or metric is\nat the top of the list.\nWhile you wait for all of the experiment models to finish, select the Algorithm name  of\na completed model to explore its performance details.\nThe following example navigates to select a model from the list of models that the job\ncreated. Then, you select the Overview  and the Metrics  tabs to view the selected\nmodel's properties, metrics and performance charts.\nAutomated machine learning in Azure Machine Learning studio allows you to deploy the\nbest model as a web service in a few steps. Deployment is the integration of the model\nso it can predict on new data and identify potential areas of opportunity.\nFor this experiment, deployment to a web service means that the bike share company\nnow has an iterative and scalable web solution for forecasting bike share rental demand.\nOnce the job is complete, navigate back to parent job page by selecting Job 1  at the top\nof your screen.\nIn the Best model summar y section, the best model in the context of this experiment, is\nselected based on the Normalized r oot mean squar ed err or metric.\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\nThe deployment process entails several steps including registering the model,Deploy the model"", ""To run a fine-tuned model job from Machine Learning, in order to deploy an Azure\nOpenAI model:\n1. After you've finished fine-tuning an Azure OpenAI model, find the registered\nmodel in the Models  list with the name provided during fine-tuning and select the\nmodel you want to deploy.\n2. Select Deploy  and name the deployment. The model is deployed to the default\nAzure OpenAI resource linked to your workspace.\nTo enable users to quickly get started with code-based fine-tuning, we've published\nsamples (both Python notebooks and Azure CLI examples) to the azureml-ex amples\nGitHub repo:\nSDK example\nCLI exampleDeploy fine-tuned models\nFine-tuning by using code-based samples"", ""Option 2: T o deploy a specific model iteration from this experiment.\na. Select the desired model from the Models  tab\nb. Select Deploy  on the top left of the window.\n2. Populate the Deploy model  pane.\nField Value\nName Enter a unique name for your deployment.\nDescription Enter a description to better identify what this deployment is for.\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\nService (AKS)  or Azure Container Inst ance (ACI).\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\ndeploy to.\nEnable\nauthenticationSelect to allow for token-based or key-based authentication.\nUse custom\ndeployment assetsEnable this feature if you want to upload your own scoring script and\nenvironment file. Otherwise, automated ML provides these assets for\nyou by default. Learn more about scoring scripts .\nThe Advanced menu offers default deployment features such as data collection  and\nresource utilization settings. If you wish to override these defaults do so in this\nmenu.\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\ndeployment begins, the Model summar y tab appears. See the deployment\nprogress under the Deploy status  section.\nNow you have an operational web service to generate predictions! Y ou can test the\npredictions by querying the service from Power BI's built in Azure Machine Learning\nsupport .ﾉExpand table\n） Impor tant\nFile names must be under 32 characters and must begin and end with\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\nbetween. Spaces are not allowed."", 'The automated machine learning interface allows you to deploy the best model as a\nweb service in a few steps. Deployment is the integration of the model so it can predict\non new data and identify potential areas of opportunity.\nFor this experiment, deployment to a web service means that the financial institution\nnow has an iterative and scalable web solution for identifying potential fixed term\ndeposit customers.\nCheck to see if your experiment run is complete. T o do so, navigate back to the parent\njob page by selecting Job 1  at the top of your screen. A Complet ed status is shown on\nthe top left of the screen.\nOnce the experiment run is complete, the Details  page is populated with a Best model\nsummar y section. In this experiment context, VotingEnsemble  is considered the best\nmodel, based on the AUC_w eight ed metric.\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\nThe deployment process entails several steps including registering the model,\ngenerating resources, and configuring them for the web service.\n1. Select VotingEnsemble  to open the model-specific page.\n2. Select the Deploy  menu in the top-left and select Deploy t o web ser vice.\n3. Populate the Deploy a model  pane as follows:\nField Value\nDeployment name my-automl-deploy\nDeployment\ndescriptionMy first automated machine learning experiment deployment\nCompute type Select Azure Container Instance (A CI)\nEnable\nauthenticationDisable.\nUse custom\ndeploymentsDisable. Allows for the default driver file (scoring script) and\nenvironment file to be auto-generated.\nFor this example, we use the defaults provided in the Advanced menu.\n4. Select Deploy .Deploy the best model']",1333
gpt-35-turbo-ft,8,,5.0,,What are the AutoML capabilities for handling imbalanced data in Azure ML?,v1,"Azure Machine Learning's Automated ML provides capabilities to handle imbalanced data by default. It offers charts and metrics to help identify imbalanced data risks and implements best practices to mitigate them. Additionally, it can automatically apply techniques such as class balancing, resampling, and weighted loss functions to address imbalanced data during model training.","[""Prevent overfitting and imbalanced data\nwith Automated ML\nArticle •06/16/2023\nOverfitting and imbalanced data are common pitfalls when you build machine learning\nmodels. By default, Azure Machine Learning's Automated ML provides charts and\nmetrics to help you identify these risks, and implements best practices to help mitigate\nthem.\nOverfitting in machine learning occurs when a model fits the training data too well, and\nas a result can't accurately predict on unseen test data. In other words, the model has\nmemorized specific patterns and noise in the training data, but is not flexible enough to\nmake predictions on real data.\nConsider the following trained models and their corresponding train and test accuracies.\nModel Train accuracy Test accuracy\nA 99.9% 95%\nB 87% 87%\nC 99.9% 45%\nConsider model A, there is a common misconception that if test accuracy on unseen\ndata is lower than training accuracy, the model is overfitted. However, test accuracy\nshould always be less than training accuracy, and the distinction for overfit vs.\nappropriately fit comes down to how much  less accurate.\nCompare models A and B, model A is a better model because it has higher test\naccuracy, and although the test accuracy is slightly lower at 95%, it is not a significant\ndifference that suggests overfitting is present. Y ou wouldn't choose model B because\nthe train and test accuracies are closer together.\nModel C represents a clear case of overfitting; the training accuracy is high but the test\naccuracy isn't anywhere near as high. This distinction is subjective, but comes from\nknowledge of your problem and data, and what magnitudes of error are acceptable.Identify overfitting\nPrevent overfitting"", ""If it ascertains a better performance through this experiment, then this remedy is\napplied.\nUse a performance metric that deals better with imbalanced data. For example, the\nAUC_weighted is a primary metric that calculates the contribution of every class\nbased on the relative number of samples representing that class, hence is more\nrobust against imbalance.\nThe following techniques are additional options to handle imbalanced data outside of\nAutomated ML.\nResampling to even the class imbalance, either by up-sampling the smaller classes\nor down-sampling the larger classes. These methods require expertise to process\nand analyze.\nReview performance metrics for imbalanced data. For example, the F1 score is the\nharmonic mean of precision and recall. Precision measures a classifier's exactness,\nwhere higher precision indicates fewer false positives, while recall measures a\nclassifier's completeness, where higher recall indicates fewer false negatives.\nSee examples and learn how to build models using Automated ML:\nFollow the Tutorial: T rain an object detection model with automated machine\nlearning and Python .\nConfigure the settings for automatic training experiment:\nIn Azure Machine Learning studio, use these steps .\nWith the Python SDK, use these steps .Next steps"", 'Imbalanced data is commonly found in data for machine learning classification\nscenarios, and refers to data that contains a disproportionate ratio of observations in\neach class. This imbalance can lead to a falsely perceived positive effect of a model\'s\naccuracy, because the input data has bias towards one class, which results in the trained\nmodel to mimic that bias.\nIn addition, Automated ML jobs generate the following charts automatically. These\ncharts help you understand the correctness of the classifications of your model, and\nidentify models potentially impacted by imbalanced data.\nChar t Descr iption\nConfusion\nMatrixEvaluates the correctly classified labels against the actual labels of the data.\nPrecision-recall Evaluates the ratio of correct labels against the ratio of found label instances of\nthe data\nROC Curves Evaluates the ratio of correct labels against the ratio of false-positive labels.\nAs part of its goal of simplifying the machine learning workflow, Automated ML has built\nin capabilities to help deal with imbalanced data such as,\nA weight column: Automated ML creates a column of weights as input to cause\nrows in the data to be weighted up or down, which can be used to make a class\nmore or less ""important.""\nThe algorithms used by Automated ML detect imbalance when the number of\nsamples in the minority class is equal to or fewer than 20% of the number of\nsamples in the majority class, where minority class refers to the one with fewest\nsamples and majority class refers to the one with most samples. Subsequently,\nautomated machine learning will run an experiment with subsampled data to\ncheck if using class weights would remedy this problem and improve performance.Cross-validation isn\'t enabled by default; it must be configured in Automated\nmachine learning settings. However, after cross-validation is configured and a\nvalidation data set has been provided, the process is automated for you.\nIdentify models with imbalanced data\nHandle imbalanced data', 'What is automated machine learning\n(AutoML)?\nArticle •04/13/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAutomated machine learning, also referred to as automated ML or AutoML, is the\nprocess of automating the time-consuming, iterative tasks of machine learning model\ndevelopment. It allows data scientists, analysts, and developers to build ML models with\nhigh scale, efficiency, and productivity all while sustaining model quality. Automated ML\nin Azure Machine Learning is based on a breakthrough from our Microsoft R esearch\ndivision .\nFor code-experienced customers, Azure Machine Learning Python SDK . Get\nstarted with Tutorial: T rain an object detection model (preview) with AutoML and\nPython .\nDuring training, Azure Machine Learning creates a number of pipelines in parallel that\ntry different algorithms and parameters for you. The service iterates through ML\nalgorithms paired with feature selections, where each iteration produces a model with a\ntraining score. The better the score for the metric you want to optimize for, the better\nthe model is considered to ""fit"" your data. It will stop once it hits the exit criteria defined\nin the experiment.\nUsing Azur e Machine Learning , you can design and run your automated ML training\nexperiments with these steps:\n1. Identif y the ML pr oblem  to be solved: classification, forecasting, regression,\ncomputer vision or NLP.\n2. Choose whether y ou want a code-fir st experience or a no-code studio w eb\nexperience : Users who prefer a code-first experience can use the Azure Machine\nLearning SDK v2 or the Azure Machine Learning CLIv2 . Get started with Tutorial:\nTrain an object detection model with AutoML and Python . Users who prefer a\nlimited/no-code experience can use the web interface  in Azure Machine Learning\nstudio at https://ml.azure.com . Get started with Tutorial: Create a classification\nmodel with automated ML in Azure Machine Learning .\nHow does AutoML work?', 'Automated ML in Azure Machine Learning supports model training for computer vision\ntasks like image classification and object detection. T o debug AutoML vision models and\nexplain model predictions, AutoML models for computer vision are integrated with\nResponsible AI dashboard. T o generate R esponsible AI insights for AutoML computer\nvision models, register your best AutoML model in the Azure Machine Learning\nworkspace and run it through the R esponsible AI vision insights pipeline. T o learn, see\nhow to set up AutoML to train computer vision models .\nNotebooks related to the AutoML supported computer vision tasks can be found in\nazureml-examples  repository.\nThe R esponsible AI vision Insights pipeline could be submitted through one of the\nfollowing methods\nPython SDK: T o learn how to submit the pipeline through Python, see the AutoML\nImage Classification scenario with RAI Dashboard sample notebook . For\nconstructing the pipeline, refer to section 5.1 in the notebook.\nAzure CLI: T o submit the pipeline via Azure-CLI, see the component Y AML in\nsection 5.2 of the example notebook linked above.\nUI (via Azure Machine Learning studio): From the Designer in Azure Machine\nLearning studio, the RAI-vision insights component can be used to create and\nsubmit a pipeline.\nIn addition to the list of R esponsible AI vision insights parameters provided in the\nprevious section, the following are parameters to set specifically for AutoML models.Integration with AutoML Image\nMode of submitting the Responsible AI vision insights\npipeline\nResponsible AI vision insights component parameter\n(AutoML specific)\n７ Note\nA few parameters are specific to the XAI algorithm chosen and are optional for\nother algorithms.']",1855
gpt-35-turbo-ft,9,,5.0,,Explain how to use AutoML for forecasting time series data. What specific settings should be adjusted for time series problems?,v1,"To use AutoML for forecasting time series data, you should specify the time column and the target column in your dataset. Additionally, you can adjust the forecast horizon, the maximum forecast horizon, and the time series identifier columns. These settings can be adjusted in the AutoMLConfig object when creating an experiment.","[""Frequently asked questions about\nforecasting in AutoML\nArticle •08/01/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nThis article answers common questions about forecasting in automatic machine learning\n(AutoML). For general information about forecasting methodology in AutoML, see the\nOverview of forecasting methods in AutoML  article.\nYou can start by reading the Set up AutoML to train a time-series forecasting model\narticle. Y ou can also find hands-on examples in several Jupyter notebooks:\nBike share example\nForecasting using deep learning\nMany Models solution\nForecasting recipes\nAdvanced forecasting scenarios\nWe're always working to make AutoML faster and more scalable. T o work as a general\nforecasting platform, AutoML does extensive data validations and complex feature\nengineering, and it searches over a large model space. This complexity can require a lot\nof time, depending on the data and the configuration.\nOne common source of slow runtime is training AutoML with default settings on data\nthat contains numerous time series. The cost of many forecasting methods scales with\nthe number of series. For example, methods like Exponential Smoothing and Prophet\ntrain a model for each time series  in the training data.\nThe Many Models feature of AutoML scales to these scenarios by distributing training\njobs across a compute cluster. It has been successfully applied to data with millions of\ntime series. For more information, see the many models  article section. Y ou can also\nread about the success of Many Models  on a high-profile competition dataset.\nHow do I start building forecasting models in\nAutoML?\nWhy is AutoML slow on my data?"", 'Learn more about how and where to deploy a model .\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps', ""If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about calendar features for time series forecasting in AutoML .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about AutoML forecasting for lagged features .Next steps"", ""Set up AutoML to train a time-series\nforecasting model with SDK and CLI\nArticle •08/02/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn this article, you'll learn how to set up AutoML for time-series forecasting with Azure\nMachine Learning automated ML in the Azure Machine Learning Python SDK .\nTo do so, you:\nFor a low code experience, see the Tutorial: Forecast demand with automated machine\nlearning  for a time-series forecasting example using automated ML in the Azure\nMachine Learning studio .\nAutoML uses standard machine learning models along with well-known time series\nmodels to create forecasts. Our approach incorporates historical information about the\ntarget variable, user-provided features in the input data, and automatically engineered\nfeatures. Model search algorithms then work to find a model with the best predictive\naccuracy. For more details, see our articles on forecasting methodology  and model\nsearch .\nFor this article you need,\nAn Azure Machine Learning workspace. T o create the workspace, see Create\nworkspace resources .\nThe ability to launch AutoML training jobs. Follow the how-to guide for setting up\nAutoML  for details.\nPrepare data for training.＂\nConfigure specific time-series parameters in a Forecasting Job . ＂\nOrchestrate training, inference, and model evaluation using components and\npipelines.＂\nPrerequisites\nTraining  and validation data"", ""See the Why is AutoML slow on my data?  answer to understand why AutoML might be\nslow in your case.\nConsider the following configuration changes that might speed up your job:\nBlock time series models  like ARIMA and Prophet.\nTurn off look-back features like lags and rolling windows.\nReduce:\nThe number of trials/iterations.\nTrial/iteration timeout.\nExperiment timeout.\nThe number of cross-validation folds.\nEnsure that early termination is enabled.\nAutoML forecasting supports four basic configurations:\nConfiguration Scenar io Pros Cons\nDefault\nAutoMLRecommended if the dataset\nhas a small number of time\nseries that have roughly\nsimilar historical behavior.- Simple to configure\nfrom code/SDK or\nAzure Machine\nLearning studio.\n- AutoML can learn\nacross different time\nseries because the\nregression models\npool all series together\nin training. For more\ninformation, see\nModel grouping .- Regression models\nmight be less\naccurate if the time\nseries in the training\ndata have divergent\nbehavior.\n- Time series models\nmight take a long\ntime to train if the\ntraining data has a\nlarge number of\nseries. For more\ninformation, see the\nWhy is AutoML slow\non my data?  answer.\nAutoML with\ndeep learningRecommended for datasets\nwith more than 1,000\nobservations and, potentially,\nnumerous time series that\nexhibit complex patterns.\nWhen it's enabled, AutoML\nwill sweep over temporal- Simple to configure\nfrom code/SDK or\nAzure Machine\nLearning studio.\n- Cross-learning\nopportunities, because- Training can take\nmuch longer\nbecause of the\ncomplexity of DNN\nmodels.\n- Series with smallHow can I make AutoML faster?\nWhat modeling configuration should I use?""]",1229
gpt-35-turbo-ft,10,,5.0,,How do you create a custom deep learning environment with specific Python packages using the AML SDK v2?,v1,"To create a custom deep learning environment with specific Python packages using the AML SDK v2, you can define a custom environment configuration and register it with your workspace. Here's an example of how to do this:

```python
from azure.ai.ml.entities import Environment

# Define the custom environment
custom_env_name = ""keras-env""
job_env = Environment(
    name=custom_env_name,
    description=""Custom environment for keras image classification"",
    conda_file=os.path.join(dependencies_dir, ""conda.yaml""),
    image=""mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest"",
)

# Register the custom environment
job_env = ml_client.environments.create_or_update(job_env)
print(
    f""Environment with name {job_env.name} is registered to workspace, the environment version is {job_env.version}""
)
```

In this example, you define a custom environment named ""keras-env"" with a description and specify the required Python packages in a Conda environment file. Then, you register the environment with your workspace.","['The specification contains some usual packages (such as numpy and pip) that you\'ll use\nin your job.\nNext, use the Y AML file to create and register this custom environment in your\nworkspace. The environment will be packaged into a Docker container at runtime.\nPython\nFor more information on creating and using environments, see Create and use software\nenvironments in Azure Machine Learning .\nIn this section, we\'ll begin by introducing the data for training. W e\'ll then cover how to\nrun a training job, using a training script that we\'ve provided. Y ou\'ll learn to build the\ntraining job by configuring the command for running the training script. Then, you\'ll\nsubmit the training job to run in Azure Machine Learning.\nYou\'ll use data from the Modified National Institute of S tandards and T echnology\n(MNIST) database of handwritten digits. This data is sourced from Y an LeCun\'s website\nand stored in an Azure storage account.\nPythonfrom azure.ai.ml.entities import Environment  \ncustom_env_name = ""keras-env""  \njob_env = Environment(  \n    name=custom_env_name,  \n    description= ""Custom environment for keras image classification"" , \n    conda_file=os.path.join(dependencies_dir, ""conda.yaml"" ), \n    image= ""mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest"" , \n) \njob_env = ml_client.environments.create_or_update(job_env)  \nprint( \n    f""Environment with name {job_env.name}  is registered to workspace, the  \nenvironment version is {job_env.version} "" \n)\nConfigure and submit your training  job\nObtain the training data\nweb_path = ""wasbs://datasets@azuremlexamples.blob.core.windows.net/mnist/""', ""In the samples deep learning folder on the notebook server, find a\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\ndeploy-with-k eras.\nYour Jupyter notebook server\nInstall the Azure Machine Learning SDK (v2) .\nDownload the training scripts keras_mnist.py  and utils.py .\nYou can also find a completed Jupyter Notebook version  of this guide on the GitHub\nsamples page.\nBefore you can run the code in this article to create a GPU cluster, you'll need to request\na quota increase  for your workspace.\nThis section sets up the job for training by loading the required Python packages,\nconnecting to a workspace, creating a compute resource to run a command job, and\ncreating an environment to run the job.\nFirst, you'll need to connect to your Azure Machine Learning workspace. The Azure\nMachine Learning workspace  is the top-level resource for the service. It provides you\nwith a centralized place to work with all the artifacts you create when you use Azure\nMachine Learning.\nWe're using DefaultAzureCredential to get access to the workspace. This credential\nshould be capable of handling most Azure SDK authentication scenarios.\nIf DefaultAzureCredential doesn't work for you, see azure-identity reference\ndocumentation  or Set up authentication  for more available credentials.\nPython\nSet up the job\nConnect to the workspace\n# Handle to the workspace  \nfrom azure.ai.ml import MLClient  \n# Authentication package  \nfrom azure.identity import DefaultAzureCredential  \ncredential = DefaultAzureCredential()"", 'Azure Machine Learning allows you to either use a curated (or ready-made)\nenvironment or create a custom environment using a Docker image or a Conda\nconfiguration. In this article, you reuse the curated Azure Machine Learning environment\nAzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu. Use the latest version of this\nenvironment using the @latest directive.\nPython\nIn this section, we begin by introducing the data for training. W e then cover how to run\na training job, using a training script that we\'ve provided. Y ou\'ll learn to build the\ntraining job by configuring the command for running the training script. Then, you\'ll\nsubmit the training job to run in Azure Machine Learning.\nYou can use the dataset in this zipped file . This dataset consists of about 120 training\nimages each for two classes (turkeys and chickens), with 100 validation images for each\nclass. The images are a subset of the Open Images v5 Dataset . The training script\npytorch_tr ain.p y downloads and extracts the dataset.\nIn the prerequisites section, we provided the training script pytorch_tr ain.p y. In practice,\nyou should be able to take any custom training script as is and run it with Azure\nMachine Learning without having to modify your code.\nThe provided training script downloads the data, trains a model, and registers the\nmodel.\nNow that you have all the assets required to run your job, it\'s time to build it using the\nAzure Machine Learning Python SDK v2. For this example, we create a command.\nAn Azure Machine Learning command is a resource that specifies all the details needed to\nexecute your training code in the cloud. These details include the inputs and outputs,curated_env_name = ""AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu@latest""\nConfigure and submit your training  job\nObtain the training data\nPrepare the training script\nBuild the training job', ""2. Create an Azure Machine Learning workspace .\n3. Clone the AzureML-Examples repository .\nBash\n4. Start the notebook server from the directory containing your clone.\nBash\nThese instructions install the base SDK packages necessary for the quickstart and tutorial\nnotebooks. Other sample notebooks might require you to install extra components. For\nmore information, see Install the Azure Machine Learning SDK for Python .\nThe Data Science Virtual Machine (DSVM) is a customized VM image built specifically for\ndoing data science. If you create a DSVM , the SDK and notebook server are installed and\nconfigured for you. However, you'll still need to create a workspace and clone the\nsample repository.\n1. Create an Azure Machine Learning workspace .\n2. Clone the the AzureML-Examples repository .\nBash\n3. Start the notebook server from the directory that contains the clone.\nBash\ngit clone https://github.com/Azure/azureml-examples.git --depth 1\njupyter notebook\nOption 3: Access on a DSVM\ngit clone https://github.com/Azure/azureml-examples.git --depth 1\njupyter notebook\nConnect to a workspace"", ""Manage Azu re Machine Learning\nenvironments with the CLI & SDK (v2)\nArticle •01/03/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAzure Machine Learning environments define the execution environments for your jobs\nor deployments and encapsulate the dependencies for your code. Azure Machine\nLearning uses the environment specification to create the Docker container that your\ntraining or scoring code runs in on the specified compute target. Y ou can define an\nenvironment from a conda specification, Docker image, or Docker build context.\nIn this article, learn how to create and manage Azure Machine Learning environments\nusing the SDK & CLI (v2).\nBefore following the steps in this article, make sure you have the following prerequisites:\nAn Azure Machine Learning workspace. If you don't have one, use the steps in the\nQuickstart: Create workspace resources  article to create one.\nThe Azure CLI and the ml extension or the Azure Machine Learning Python SDK v2:\nTo install the Azure CLI and extension, see Install, set up, and use the CLI (v2) .\nTo install the Python SDK v2, use the following command:\nBash\nPrerequisites\n） Impor tant\nThe CLI examples in this article assume that you are using the Bash (or\ncompatible) shell. For example, from a Linux system or Windows\nSubsyst em for Linux .\npip install azure-ai-ml azure-identity""]",1916
gpt-35-turbo-ft,11,,5.0,,Describe the process of using Azure ML to train a deep learning model on a GPU compute cluster.,v1,"To train a deep learning model on a GPU compute cluster using Azure ML, you would follow these general steps:

1. **Create or Connect to an Azure ML Workspace**: This is the central place where you manage your Azure ML resources.

2. **Define a GPU Compute Target**: You would define a compute target that specifies the GPU resources required for training.

3. **Prepare Training Script**: Create a training script that specifies the deep learning model, data, and any dependencies.

4. **Submit the Training Job**: Submit the training job to the Azure ML service, specifying the compute target and other job settings.

5. **Monitor and Manage the Job**: Monitor the job's progress and manage the resources used during training.

6. **Retrieve and Register Model**: Once training is complete, retrieve the trained model and register it in the Azure ML workspace for later use.

Let me know if you need more detailed information on any of these steps.","['An Azure Machine Learning compute cluster is a fully managed compute resource that\ncan be used to run the training job. In the following examples, a compute cluster named\ncpu-compute is created.\nPython\nTo run this script, you\'ll use a command that executes main.py Python script located\nunder ./sdk/python/jobs/single-step/lightgbm/iris/src/. The command will be runworkspace = \'<AZUREML_WORKSPACE_NAME>\'\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \nresource_group, workspace)\n2. Create a compute resource for training\n７ Note\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\ntraining job .\nPython SDK\nfrom azure.ai.ml.entities import AmlCompute\n# specify aml compute name.\ncpu_compute_target = ""cpu-cluster""\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(""Creating a new cpu compute target..."" )\n    compute = AmlCompute(\n        name=cpu_compute_target, size= ""STANDARD_D2_V2"" , min_instances= 0, \nmax_instances= 4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n4. Submit the training job\nPython SDK', 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""Manage and optimize Azure Machine\nLearning costs\nArticle •08/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.） Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)"", 'For detailed instructions on how to complete these steps, see Secure a training\nenvironment .\nIn this section, you learn how Azure Machine Learning securely communicates between\nservices to submit a training job. This example shows you how all your configurations\nwork together to secure communication.\n1. The client uploads training scripts and training data to storage accounts that are\nsecured with a service or private endpoint.\n2. The client submits a training job to the Azure Machine Learning workspace\nthrough the private endpoint.\n3. Azure Batch service receives the job from the workspace. It then submits the\ntraining job to the compute environment through the public load balancer for the\ncompute resource.\n4. The compute resource receives the job and begins training. The compute resource\nuses information stored in key vault to access storage accounts to download\ntraining files and upload output.Example training job submission', 'like Spark.\nIn the following example script, we provision a Linux compute cluster . You can see the\nAzure Machine Learning pricing  page for the full list of VM sizes and prices. Since we\nneed a GPU cluster for this example, let\'s pick a STAND ARD_NC6  model and create an\nAzure Machine Learning compute.\nPython\nfrom azure.ai.ml.entities import AmlCompute  \ngpu_compute_target = ""gpu-cluster""  \ntry: \n    # let\'s see if the compute target already exists  \n    gpu_cluster = ml_client.compute.get(gpu_compute_target)  \n    print(  \n        f""You already have a cluster named {gpu_compute_target} , we\'ll reuse  \nit as is.""  \n    ) \nexcept Exception:  \n    print( ""Creating a new gpu compute target..."" ) \n    # Let\'s create the Azure ML compute object with the intended parameters  \n    gpu_cluster = AmlCompute(  \n        # Name assigned to the compute cluster  \n        name= ""gpu-cluster"" , \n        # Azure ML Compute is the on-demand VM service  \n        type= ""amlcompute"" , \n        # VM Family  \n        size= ""STANDARD_NC6"" , \n        # Minimum running nodes when there is no job running  \n        min_instances= 0, \n        # Nodes in cluster  \n        max_instances= 4, \n        # How many seconds will the node running after the job termination  \n        idle_time_before_scale_down= 180, \n        # Dedicated or LowPriority. The latter is cheaper but there is a  \nchance of job termination  \n        tier= ""Dedicated"" , \n    ) \n    # Now, we pass the object to MLClient\'s create_or_update method  \n    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()  \nprint( \n    f""AMLCompute with name {gpu_cluster.name}  is created, the compute size  \nis {gpu_cluster.size} "" \n)']",1428
gpt-35-turbo-ft,12,,5.0,,Explain how to deploy a TensorFlow model as a web service in Azure ML. What are the key considerations for deployment?,v1,"To deploy a TensorFlow model as a web service in Azure ML, you would typically follow these steps:

1. **Prepare the Model**: Ensure that your TensorFlow model is trained and saved in a format that Azure ML can use for deployment, such as a SavedModel or a TensorFlow Lite model.

2. **Create an Inference Configuration**: Define the environment and entry script for your model. This includes specifying the dependencies required to run your model, such as the TensorFlow runtime.

3. **Create a Deployment Configuration**: Specify the compute target and other settings for the deployment, such as the number of instances and the type of instance to use.

4. **Deploy the Model**: Use Azure ML to deploy the model as a web service, which will create an endpoint that can be used to make predictions.

Key considerations for deployment include:
- **Scalability**: Consider the expected load on your service and choose an appropriate compute target and instance type.
- **Security**: Ensure that your endpoint is secured and that any sensitive data is handled appropriately.
- **Monitoring**: Set up monitoring to track the performance and usage of your endpoint.
- **Versioning**: Consider how you will manage and update different versions of your model.

Would you like more detailed information on any of these steps or considerations?","['What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps', 'Tutorial Descr iption\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'To learn more, review these articles:\nDeploy models with REST\nCreate and use managed online endpoints in the studio\nSafe rollout for online endpoints\nHow to autoscale managed online endpoints\nView costs for an Azure Machine Learning managed online endpoint\nAccess Azure resources with a managed online endpoint and managed identity\nTroubleshoot managed online endpoints deployment', ""Option 2: T o deploy a specific model iteration from this experiment.\na. Select the desired model from the Models  tab\nb. Select Deploy  on the top left of the window.\n2. Populate the Deploy model  pane.\nField Value\nName Enter a unique name for your deployment.\nDescription Enter a description to better identify what this deployment is for.\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\nService (AKS)  or Azure Container Inst ance (ACI).\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\ndeploy to.\nEnable\nauthenticationSelect to allow for token-based or key-based authentication.\nUse custom\ndeployment assetsEnable this feature if you want to upload your own scoring script and\nenvironment file. Otherwise, automated ML provides these assets for\nyou by default. Learn more about scoring scripts .\nThe Advanced menu offers default deployment features such as data collection  and\nresource utilization settings. If you wish to override these defaults do so in this\nmenu.\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\ndeployment begins, the Model summar y tab appears. See the deployment\nprogress under the Deploy status  section.\nNow you have an operational web service to generate predictions! Y ou can test the\npredictions by querying the service from Power BI's built in Azure Machine Learning\nsupport .ﾉExpand table\n） Impor tant\nFile names must be under 32 characters and must begin and end with\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\nbetween. Spaces are not allowed."", 'In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\nmodel to an online endpoint. See these other articles to learn more about Azure\nMachine Learning.\nTrack run metrics during training\nTune hyperparameters\nReference architecture for distributed deep learning training in AzureNext steps']",812
gpt-35-turbo-ft,13,,5.0,,Provide an example of how to use the AML SDK v2 for distributed deep learning training. What configurations are necessary for multi-node training?,v1,"To perform distributed deep learning training using the Azure Machine Learning SDK v2, you would typically need to define a multi-node configuration, specify the required resources, and set up the necessary environment. Here's a high-level example of how you might achieve this:

1. Define a multi-node configuration:
   - Specify the number of worker nodes and their configurations.
   - Define the distribution strategy (e.g., data parallelism) and any additional settings for the distributed training job.

2. Specify the required resources:
   - Define the compute target for the distributed training job, such as an Azure Machine Learning compute cluster.

3. Set up the environment:
   - Define the training script and any dependencies required for the distributed training job, such as the deep learning framework (e.g., PyTorch or TensorFlow) and any additional libraries.

4. Submit the job:
   - Submit the distributed training job to the Azure Machine Learning service, specifying the defined configuration, resources, and environment settings.

This is a simplified overview, and the specific implementation details would depend on the deep learning framework, the specific requirements of the model, and the Azure Machine Learning SDK v2 features.","[""Distributed training with Azure Machine\nLearning\nArticle •03/27/2023\nIn this article, you learn about distributed training and how Azure Machine Learning\nsupports it for deep learning models.\nIn distributed training the workload to train a model is split up and shared among\nmultiple mini processors, called worker nodes. These worker nodes work in parallel to\nspeed up model training. Distributed training can be used for traditional ML models, but\nis better suited for compute and time intensive tasks, like deep learning  for training\ndeep neural networks.\nThere are two main types of distributed training: data parallelism  and model parallelism .\nFor distributed training on deep learning models, the Azure Machine Learning SDK in\nPython  supports integrations with popular frameworks, PyT orch and T ensorFlow. Both\nframeworks employ data parallelism for distributed training, and can leverage\nhorovod  for optimizing compute speeds.\nDistributed training with PyT orch\nDistributed training with T ensorFlow\nFor ML models that don't require distributed training, see train models with Azure\nMachine Learning  for the different ways to train models using the Python SDK.\nData parallelism is the easiest to implement of the two distributed training approaches,\nand is sufficient for most use cases.\nIn this approach, the data is divided into partitions, where the number of partitions is\nequal to the total number of available nodes, in the compute cluster or serverless\ncompute . The model is copied in each of these worker nodes, and each worker operates\non its own subset of the data. K eep in mind that each node has to have the capacity to\nsupport the model that's being trained, that is the model has to entirely fit on each\nnode. The following diagram provides a visual demonstration of this approach.Deep learning and distributed training\nData parallelism"", ""Distributed GPU training guide (SDK v2)\nArticle •02/02/2024\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nLearn more about using distributed GPU training code in Azure Machine Learning. This\narticle helps you run your existing distributed training code, and offers tips and\nexamples for you to follow for each framework:\nMessage P assing Interface (MPI)\nHorovod\nEnvironment variables from Open MPI\nPyTorch\nTensorFlow\nAccelerate GPU training with InfiniBand\nReview the basic concepts of distributed GPU training , such as data parallelism ,\ndistributed dat a parallelism , and model p arallelism .\nAzure Machine Learning offers an MPI job  to launch a given number of processes in\neach node. Azure Machine Learning constructs the full MPI launch command ( mpirun)\nbehind the scenes. Y ou can't provide your own full head-node-launcher commands like\nmpirun or DeepSpeed launcher.\nPrerequisites\n\uea80 Tip\nIf you don't know which type of parallelism to use, more than 90% of the time you\nshould use distribut ed data p arallelism .\nMPI\n\uea80 Tip\nThe base Docker image used by an Azure Machine Learning MPI job needs to have\nan MPI library installed. Open MPI  is included in all the Azur e Machine Learning\nGPU b ase images . When you use a custom Docker image, you are responsible\nfor making sure the image includes an MPI library. Open MPI is recommended, but"", 'Distributed DNN training achieves scalability using a data partitioning algorithm that\nrespects time series boundaries. The following diagram illustrates a simple example with\ntwo partitions:\nDuring training, the DNN data loaders on each compute load just what they need to\ncomplete an iteration of back-propagation; the whole dataset is nev er read int o\nmemor y. The partitions are further distributed across multiple compute cores (usually\nGPUs) on possibly multiple nodes to accelerate training. Coordination across computes\nis provided by the Horovod  framework.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about deep learning models  for forecasting in AutoML\nNext steps', 'For more information, see What is automated machine learning? .\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\nLearning can automate this task for arbitrary parameterized commands with little\nmodification to your job definition. R esults are visualized in the studio.\nFor more information, see Tune hyperparameters .\nEfficiency of training for deep learning and sometimes classical machine learning\ntraining jobs can be drastically improved via multinode distributed training. Azure\nMachine Learning compute clusters and serverless compute  offer the latest GPU\noptions.\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\nclusters, and serverless compute :\nPyTorch\nTensorFlow\nMPI\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\nAzure S ynapse Analytics Spark clusters.\nFor more information, see Distributed training with Azure Machine Learning .\nScaling an ML project might require scaling embarrassingly parallel model training. This\npattern is common for scenarios like forecasting demand, where a model might be\ntrained for many stores.\nTo bring a model into production, you deploy the model. The Azure Machine Learning\nmanaged endpoints abstract the required infrastructure for both batch or real-time\n(online) model scoring (inferencing).Hyperparameter optimization\nMultinode distributed training\nEmbarrassingly parallel training\nDeploy models', ""Train TensorFlow models at scale with\nAzure Machine Lea rning\nArticle •04/04/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this article, learn how to run your TensorFlow  training scripts at scale using Azure\nMachine Learning Python SDK v2.\nThe example code in this article train a T ensorFlow model to classify handwritten digits,\nusing a deep neural network (DNN); register the model; and deploy it to an online\nendpoint.\nWhether you're developing a T ensorFlow model from the ground-up or you're bringing\nan existing model into the cloud, you can use Azure Machine Learning to scale out\nopen-source training jobs using elastic cloud compute resources. Y ou can build, deploy,\nversion, and monitor production-grade models with Azure Machine Learning.\nTo benefit from this article, you'll need to:\nAccess an Azure subscription. If you don't have one already, create a free\naccount .\nRun the code in this article using either an Azure Machine Learning compute\ninstance or your own Jupyter notebook.\nAzure Machine Learning compute instance—no downloads or installation\nnecessary\nComplete the Create resources to get started  to create a dedicated notebook\nserver pre-loaded with the SDK and the sample repository.\nIn the samples deep learning folder on the notebook server, find a\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\ndeploy-with-t ensor flow.\nYour Jupyter notebook server\nInstall the Azure Machine Learning SDK (v2) .\nDownload the following files:\ntraining script tf_mnist.py\nscoring script score.py\nsample request file sample-request.json\nPrerequisites""]",1769
gpt-35-turbo-ft,14,,5.0,,How can Azure ML's HyperDrive be used to tune hyperparameters of a deep learning model? Give an example of specifying the search space.,v1,"Azure ML's HyperDrive can be used to tune hyperparameters of a deep learning model by defining a hyperparameter search space and specifying the primary metric to optimize. Here's an example of specifying the search space for HyperDrive:

```python
from azureml.train.hyperdrive import RandomParameterSampling, choice, uniform

param_sampling = RandomParameterSampling( {
        '--batch-size': choice(16, 32, 64),
        '--learning-rate': uniform(0.01, 0.1)
    }
)
```

In this example, we define a search space for the hyperparameters `batch-size` and `learning-rate` with specific choices and ranges.","['For more information, see What is automated machine learning? .\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\nLearning can automate this task for arbitrary parameterized commands with little\nmodification to your job definition. R esults are visualized in the studio.\nFor more information, see Tune hyperparameters .\nEfficiency of training for deep learning and sometimes classical machine learning\ntraining jobs can be drastically improved via multinode distributed training. Azure\nMachine Learning compute clusters and serverless compute  offer the latest GPU\noptions.\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\nclusters, and serverless compute :\nPyTorch\nTensorFlow\nMPI\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\nAzure S ynapse Analytics Spark clusters.\nFor more information, see Distributed training with Azure Machine Learning .\nScaling an ML project might require scaling embarrassingly parallel model training. This\npattern is common for scenarios like forecasting demand, where a model might be\ntrained for many stores.\nTo bring a model into production, you deploy the model. The Azure Machine Learning\nmanaged endpoints abstract the required infrastructure for both batch or real-time\n(online) model scoring (inferencing).Hyperparameter optimization\nMultinode distributed training\nEmbarrassingly parallel training\nDeploy models', 'Hyperparameter tuning a model (v2)\nArticle •04/04/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAutomate efficient hyperparameter tuning using Azure Machine Learning SDK v2 and\nCLI v2 by way of the S weepJob type.\n1. Define the parameter search space for your trial\n2. Specify the sampling algorithm for your sweep job\n3. Specify the objective to optimize\n4. Specify early termination policy for low-performing jobs\n5. Define limits for the sweep job\n6. Launch an experiment with the defined configuration\n7. Visualize the training jobs\n8. Select the best configuration for your model\nHyperp aramet ers are adjustable parameters that let you control the model training\nprocess. For example, with neural networks, you decide the number of hidden layers and\nthe number of nodes in each layer. Model performance depends heavily on\nhyperparameters.\nHyperp aramet er tuning , also called hyperp aramet er optimization , is the process of\nfinding the configuration of hyperparameters that results in the best performance. The\nprocess is typically computationally expensive and manual.\nAzure Machine Learning lets you automate hyperparameter tuning and run experiments\nin parallel to efficiently optimize hyperparameters.\nTune hyperparameters by exploring the range of values defined for each\nhyperparameter.\nHyperparameters can be discrete or continuous, and has a distribution of values\ndescribed by a parameter expression .\nWhat is hyperparameter tuning ?\nDefine the search space', 'When training computer vision models, model performance depends heavily on the\nhyperparameter values selected. Often, you might want to tune the hyperparameters to\nget optimal performance. For computer vision tasks, you can sweep hyperparameters to\nfind the optimal settings for your model. This feature applies the hyperparameter tuning\ncapabilities in Azure Machine Learning. Learn how to tune hyperparameters .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nYAML\nYou can define the model architectures and hyperparameters to sweep in the parameter\nspace. Y ou can either specify a single model architecture or multiple ones.\nSee Individual trials  for the list of supported model architectures for each task type.\nSee Hyperparameters for computer vision tasks  hyperparameters for each\ncomputer vision task type.Azure CLI\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value:  0.0001\n      max_value:  0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value:  0.0001\n      max_value:  0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\nDefine the parameter search space', 'To tune the model\'s hyperparameters, define the parameter space in which to search\nduring training. Y ou\'ll do this by replacing some of the parameters ( batch_size,\nfirst_layer_neurons, second_layer_neurons, and learning_rate) passed to the training\njob with special inputs from the azure.ml.sweep package.\nPython\nThen, you\'ll configure sweep on the command job, using some sweep-specific\nparameters, such as the primary metric to watch and the sampling algorithm to use.\nIn the following code, we use random sampling to try different configuration sets of\nhyperparameters in an attempt to maximize our primary metric, validation_acc.\nWe also define an early termination policy—the BanditPolicy. This policy operates by\nchecking the job every two iterations. If the primary metric, validation_acc, falls outside\nthe top ten percent range, Azure Machine Learning will terminate the job. This saves the\nmodel from continuing to explore hyperparameters that show no promise of helping to\nreach the target metric.\nPythonfrom azure.ai.ml.sweep import Choice, LogUniform  \n# we will reuse the command_job created before. we call it as a function so  \nthat we can apply inputs  \n# we do not apply the \'iris_csv\' input again -- we will just use what was  \nalready defined earlier  \njob_for_sweep = job(  \n    batch_size=Choice(values=[ 25, 50, 100]), \n    first_layer_neurons=Choice(values=[ 10, 50, 200, 300, 500]), \n    second_layer_neurons=Choice(values=[ 10, 50, 200, 500]), \n    learning_rate=LogUniform(min_value= -6, max_value= -1), \n)\nfrom azure.ai.ml.sweep import BanditPolicy  \nsweep_job = job_for_sweep.sweep(  \n    compute=gpu_compute_target,  \n    sampling_algorithm= ""random"" , \n    primary_metric= ""Accuracy"" , \n    goal= ""Maximize"" , \n    max_total_trials= 20, \n    max_concurrent_trials= 4, \n    early_termination_policy=BanditPolicy(slack_factor= 0.1, \nevaluation_interval= 2), \n)', 'You trained the model with one set of parameters, let\'s now see if you can further\nimprove the accuracy of your model. Y ou can tune and optimize your model\'s\nhyperparameters using Azure Machine Learning\'s sweep  capabilities.\nTo tune the model\'s hyperparameters, define the parameter space in which to search\nduring training. Y ou do this by replacing some of the parameters passed to the training\njob with special inputs from the azure.ml.sweep package.\nSince the training script uses a learning rate schedule to decay the learning rate every\nseveral epochs, you can tune the initial learning rate and the momentum parameters.\nPython\nThen, you can configure sweep on the command job, using some sweep-specific\nparameters, such as the primary metric to watch and the sampling algorithm to use.\nIn the following code, we use random sampling to try different configuration sets of\nhyperparameters in an attempt to maximize our primary metric, best_val_acc.\nWe also define an early termination policy, the BanditPolicy, to terminate poorly\nperforming runs early. The BanditPolicy terminates any run that doesn\'t fall within the\nslack factor of our primary evaluation metric. Y ou apply this policy every epoch (since we\nreport our best_val_acc metric every epoch and evaluation_interval=1). Notice we\ndelay the first policy evaluation until after the first 10 epochs ( delay_evaluation=10).\nPythonfrom azure.ai.ml.sweep import Uniform\n# we will reuse the command_job created before. we call it as a function so  \nthat we can apply inputs\njob_for_sweep = job(\n    learning_rate=Uniform(min_value= 0.0005, max_value= 0.005),\n    momentum=Uniform(min_value= 0.9, max_value= 0.99),\n)\nfrom azure.ai.ml.sweep import BanditPolicy\nsweep_job = job_for_sweep.sweep(\n    compute= ""gpu-cluster"" ,\n    sampling_algorithm= ""random"" ,\n    primary_metric= ""best_val_acc"" ,\n    goal=""Maximize"" ,\n    max_total_trials= 8,\n    max_concurrent_trials= 4,\n    early_termination_policy=BanditPolicy(\n        slack_factor= 0.15, evaluation_interval= 1, delay_evaluation= 10']",2035
gpt-35-turbo-ft,15,,5.0,,"What is the command to update an existing model's properties, such as description or tags, using the Azure CLI?",v1,"To update an existing model's properties, such as description or tags, using the Azure CLI, you can use the following command:

```bash
az ml model update --name run-model-example --version 1 --set description=""This is an updated description."" --set tags.stage=""Prod""
```","['Get the details of a specific model:\ncli\nUpdate mutable properties of a specific model:\ncli\nArchiving a model will hide it by default from list queries ( az ml model list). You can\nstill continue to reference and use an archived model in your workflows. Y ou can archive\neither all versions of a model or only a specific version.\nIf you don\'t specify a version, all versions of the model under that given name will be\narchived. If you create a new model version under an archived model container, that\nnew version will automatically be set as archived as well.\nArchive all versions of a model:Show\nAzure CLI\naz ml model show --name run-model-example --version 1  \nUpdate\nAzure CLI\naz ml model update --name  run-model-example --version 1 --set  \ndescription=""This is an updated description."" --set tags.stage=""Prod""  \n） Impor tant\nFor model, only description and tags can be updated. All other properties are\nimmutable; if you need to change any of those properties you should create a new\nversion of the model.\nArchive', 'Execute the following command in the Azure CLI, and update the <> placeholders\nwith your data asset name, version and key-value pair for the tag.\nAzure CLI\nTypically, your ETL processes organize your folder structure on Azure storage by time,\nfor example:\ntext\nThe combination of time/version structured folders and Azure Machine Learning T ables\n(MLTable) allow you to construct versioned datasets. T o show how to achieve versioned\ndata with Azure Machine Learning T ables, we use a hypothetical ex ample . Suppose you\nhave a process that uploads camera images to Azure Blob storage every week, in the\nfollowing structure:\ntextaz ml data update --name <DATA ASSET NAME>  --version  <VERSION>  --set \ntags.<KEY>=<VALUE> \nVersioning best practices\n/ \n└── 📁 mydata \n    ├── 📁  year=2022  \n    │   ├── 📁  month=11  \n    │   │   └── 📄  file1 \n    │   │   └── 📄  file2 \n    │   └── 📁  month=12  \n    │       └── 📄  file1 \n    │   │   └── 📄  file2 \n    └── 📁  year=2023  \n        └── 📁  month=1  \n            └── 📄  file1 \n    │   │   └── 📄  file2 \n/myimages  \n└── 📁 year=2022  \n    ├── 📁  week52 \n    │   ├── 📁  camera1  \n    │   │   └── 🖼  file1.jpeg  \n    │   │   └── 🖼  file2.jpeg  \n    │   └── 📁  camera2  \n    │       └── 🖼  file1.jpeg  \n    │   │   └── 🖼  file2.jpeg', 'The az ml model command can be used for managing Azure Machine Learning models.\nExamples are available in the examples GitHub repository . Several are shown below.\nYAML\nYAML\nInstall and use the CLI (v2)Remarks\nExamples\nYAML: local file\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-file-example  \npath: mlflow-model/model.pkl  \ndescription:  Model created from local file. \nYAML: local folder in MLflow format\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-mlflow-example  \npath: mlflow-model  \ntype: mlflow_model  \ndescription:  Model created from local MLflow model directory.', 'If you want to update the code, model, or environment, update the Y AML file, and\nthen run the az ml online-endpoint update command.\nTo understand how update works:\n1. Open the file online/model-1/onlines coring/s core.py.\n2. Change the last line of the init() function: After logging.info(""Init\ncomplete""), add logging.info(""Updated successfully"").\n3. Save the file.\n4. Run this command:\nAzure CLI(Optional) Update the deployment\nAzure CLI\n７ Note\nIf you update instance count (to scale your deployment) along with other\nmodel settings (such as code, model, or environment) in a single update\ncommand, the scaling operation will be performed first, then the other updates\nwill be applied. It\'s a good practice to perform these operations separately in a\nproduction environment.\naz ml online-deployment update -n blue --endpoint  $ENDPOINT_NAME  -f \nendpoints/online/managed/sample/blue -deployment .yml\n７ Note\nUpdating by using Y AML is declarative. That is, changes in the Y AML are\nreflected in the underlying Azure R esource Manager resources (endpoints\nand deployments). A declarative approach facilitates GitOps : All\nchanges to endpoints and deployments (even instance_count) go\nthrough the Y AML.\n\uea80 Tip', 'then select Regist er.\nThe SDK and CLI (v2) also allow you to manage the lifecycle of your Azure Machine\nLearning model assets.\nList all the models in your workspace:\ncli\nList all the model versions under a given name:\ncli\n\uf80a\nManage models\nList\nAzure CLI\naz ml model list  \nAzure CLI\naz ml model list --name run-model-example']",1262
gpt-35-turbo-ft,16,,5.0,,How does Azure ML integrate with Azure DevOps for implementing CI/CD pipelines for ML models? Describe the steps involved.,v1,"Azure ML integrates with Azure DevOps for CI/CD by leveraging Azure Pipelines. The steps involved include defining build and release pipelines, setting up triggers, and configuring environment variables for model deployment. Would you like a detailed walkthrough of these steps?","['Continuous integration and deployment of machine learning models with Azure\nPipelines\nMachine Learning ML Ops  repository\nLearn more by reading and exploring the following resources:\nSet up ML Ops with Azure DevOps\nLearning path: End-to-end ML Ops with Azure Machine Learning\nHow to deploy a model to an online endpoint  with Machine Learning\nTutorial: T rain and deploy a model\nCI/CD of machine learning models with Azure Pipelines\nMachine learning at scale\nAzure AI reference architectures and best practices repo\nNext steps', 'Some key features enabling ML Ops include:\ngit integration.\nMLflow integration.\nMachine learning pipeline scheduling.\nAzure Event Grid integration for custom triggers.\nEase of use with CI/CD tools like GitHub Actions or Azure DevOps.\nMachine Learning also includes features for monitoring and auditing:\nJob artifacts, such as code snapshots, logs, and other outputs.\nLineage between jobs and assets, such as containers, data, and compute resources.\nIf you use Apache Airflow, the airflow-provider-azure-machinelearning  package is a\nprovider that enables you to submit workflows to Azure Machine Learning from Apache\nAirFlow.\nStart using Azure Machine Learning:\nSet up an Azure Machine Learning workspace\nTutorial: Build a first machine learning project\nRun training jobs\nNext steps', 'What are Azu re Machine Learning\npipelines?\nArticle •01/31/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAn Azure Machine Learning pipeline is an independently executable workflow of a\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\nstandardize the best practices of producing a machine learning model, enables the team\nto execute at scale, and improves the model building efficiency.\nThe core of a machine learning pipeline is to split a complete machine learning task into\na multistep workflow. Each step is a manageable component that can be developed,\noptimized, configured, and automated individually. S teps are connected through well-\ndefined interfaces. The Azure Machine Learning pipeline service automatically\norchestrates all the dependencies between pipeline steps. This modular approach brings\ntwo key benefits:\nStandardize the Machine learning operation (ML Ops) practice and support scalable\nteam collaboration\nTraining efficiency and cost reduction\nMachine learning operation (ML Ops) automates the process of building machine\nlearning models and taking the model to production. This is a complex process. It\nusually requires collaboration from different teams with different skills. A well-defined\nmachine learning pipeline can abstract this complex process into a multiple steps\nworkflow, mapping each step to a specific task such that each team can work\nindependently.\nFor example, a typical machine learning project includes the steps of data collection,\ndata preparation, model training, model evaluation, and model deployment. Usually, the\ndata engineers concentrate on data steps, data scientists spend most time on model\nWhy are Azure Machine Learning pipelines\nneeded?\nStandardize the MLOps practice and support scalable\nteam collaboration', 'What are Azu re Machine Learning\npipelines?\nArticle •01/31/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAn Azure Machine Learning pipeline is an independently executable workflow of a\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\nstandardize the best practices of producing a machine learning model, enables the team\nto execute at scale, and improves the model building efficiency.\nThe core of a machine learning pipeline is to split a complete machine learning task into\na multistep workflow. Each step is a manageable component that can be developed,\noptimized, configured, and automated individually. S teps are connected through well-\ndefined interfaces. The Azure Machine Learning pipeline service automatically\norchestrates all the dependencies between pipeline steps. This modular approach brings\ntwo key benefits:\nStandardize the Machine learning operation (ML Ops) practice and support scalable\nteam collaboration\nTraining efficiency and cost reduction\nMachine learning operation (ML Ops) automates the process of building machine\nlearning models and taking the model to production. This is a complex process. It\nusually requires collaboration from different teams with different skills. A well-defined\nmachine learning pipeline can abstract this complex process into a multiple steps\nworkflow, mapping each step to a specific task such that each team can work\nindependently.\nFor example, a typical machine learning project includes the steps of data collection,\ndata preparation, model training, model evaluation, and model deployment. Usually, the\ndata engineers concentrate on data steps, data scientists spend most time on model\nWhy are Azure Machine Learning pipelines\nneeded?\nStandardize the MLOps practice and support scalable\nteam collaboration', 'Use Azu re Pipelines with Azure Machine\nLearning\nArticle •09/29/2023\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\nof the operations you can automate are:\nData preparation (extract, transform, load operations)\nTraining machine learning models with on-demand scale-out and scale-up\nDeployment of machine learning models as public or private web services\nMonitoring deployed machine learning models (such as for performance or data-\ndrift analysis)\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\nmachine learning model to Azure Machine Learning .\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\nv2.\nComplete the Create resources to get started  to:\nCreate a workspace\nCreate a cloud-based compute cluster  to use for training your model\nAzure Machine Learning extension for Azure Pipelines. This extension can be\ninstalled from the Visual S tudio marketplace at\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-\nv2.\nFork the following repo at GitHub:Prerequisites\nStep 1: Get the code\nhttps://github.com/azure/azureml-examples']",1320
gpt-35-turbo-ft,17,,5.0,,Explain the process of setting up a real-time inferencing endpoint using a deep learning model in Azure ML. What are the performance tuning options available?,v1,"To set up a real-time inferencing endpoint using a deep learning model in Azure ML, you would follow these steps:

1. **Model Training**: Train your deep learning model using Azure ML's training capabilities, ensuring it's compatible with real-time inferencing requirements.

2. **Model Registration**: Register the trained model in your Azure ML workspace, which makes it available for deployment.

3. **Deployment Configuration**: Define the deployment configuration, specifying the compute target, environment, and other settings for the real-time endpoint.

4. **Endpoint Deployment**: Deploy the model as a real-time endpoint, which will make it accessible for inferencing via HTTP requests.

Regarding performance tuning options, Azure ML provides several options to optimize real-time inferencing performance:

- **Scaling**: Azure ML allows you to scale the underlying infrastructure to handle increased inferencing loads.

- **Hardware Selection**: You can choose the appropriate hardware for your inferencing needs, including CPU and GPU options.

- **Monitoring and Logging**: Azure ML provides monitoring and logging capabilities to track and optimize endpoint performance over time.

- **Auto-scaling**: Azure ML can automatically scale the endpoint based on usage patterns, ensuring optimal performance and cost efficiency.

These options enable you to fine-tune the performance of your real-time inferencing endpoint to meet your specific requirements.","['Online endpoints and deployments for\nreal-time inference\nArticle •10/24/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAzure Machine Learning allows you to perform real-time inferencing on data by using\nmodels that are deployed to online endpoints . Inferencing is the process of applying new\ninput data to a machine learning model to generate outputs. While these outputs are\ntypically referred to as ""predictions,"" inferencing can be used to generate outputs for\nother machine learning tasks, such as classification and clustering.\nOnline endpoints  deploy models to a web server that can return predictions under the\nHTTP protocol. Use online endpoints to operationalize models for real-time inference in\nsynchronous low-latency requests. W e recommend using them when:\nTo define an endpoint, you need to specify:\nEndpoint name : This name must be unique in the Azure region. For more\ninformation on the naming rules, see endpoint limits .\nAuthentication mode : You can choose between key-based authentication mode\nand Azure Machine Learning token-based authentication mode for the endpoint. A\nkey doesn\'t expire, but a token does expire. For more information on\nauthenticating, see Authenticate to an online endpoint .\nAzure Machine Learning provides the convenience of using managed online endpoints\nfor deploying your ML models in a turnkey manner. This is the recommended  way to use\nonline endpoints in Azure Machine Learning. Managed online endpoints work with\npowerful CPU and GPU machines in Azure in a scalable, fully managed way. These\nendpoints also take care of serving, scaling, securing, and monitoring your models, to\nfree you from the overhead of setting up and managing the underlying infrastructure. T o\nOnline  endpoints\nYou have low-latency requirements＂\nYour model can answer the request in a relatively short amount of time＂\nYour model\'s inputs fit on the HT TP payload of the request ＂\nYou need to scale up in terms of number of requests＂', 'In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\nmodel to an online endpoint. See these other articles to learn more about Azure\nMachine Learning.\nTrack run metrics during training\nTune hyperparameters\nReference architecture for distributed deep learning training in AzureNext steps', ""Batch scoring, or batch infer encing , involves invoking an endpoint with a reference to\ndata. The batch endpoint runs jobs asynchronously to process data in parallel on\ncompute clusters and store the data for further analysis.\nReal-time s coring, or online infer encing , involves invoking an endpoint with one or more\nmodel deployments and receiving a response in near real time via HT TPS. T raffic can be\nsplit across multiple deployments, allowing for testing new model versions by diverting\nsome amount of traffic initially and increasing after confidence in the new model is\nestablished.\nFor more information, see:\nDeploy a model with a real-time managed endpoint\nUse batch endpoints for scoring\nDevOps for ML models, often called ML Ops, is a process for developing models for\nproduction. A model's lifecycle from training to deployment must be auditable if not\nreproducible.\nLearn more about MLOps in Azure Machine Learning .\nMachine Learning is built with the model lifecycle in mind. Y ou can audit the model\nlifecycle down to a specific commit and environment.Real-time and batch scoring (inferencing)\nMLOps: DevOps for machine learning\nML model lifecycle\nIntegrations enabling MLOPs"", 'What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps', ""Currently, Azure Machine Learning supports fine-tuning models for the following\nlanguage tasks:\nText classification\nToken classification\nQuestion answering\nSummarization\nTranslation\nTo enable users to quickly get started with fine-tuning, we have published samples (both\nPython notebooks and CLI examples) for each task in the azureml-examples git repo\nFinetune samples . Each model card also links to fine-tuning samples for supported\nfine-tuning tasks.\nYou can deploy foundation models (both pre-trained models from the model catalog,\nand fine-tuned models, once they're registered to your workspace) to an endpoint that\ncan then be used for inferencing. Deployment to both real time endpoints and batch\nendpoints is supported. Y ou can deploy these models by using either the Deploy UI\nwizard or by using the code based samples linked from the model card.\nYou can invoke the Deploy UI form by selecting the Deploy  button on the model card\nfor any foundation model, and selecting either R eal-time endpoint or Batch endpoint\nDeploying foundation models to endpoints for\ninferencing\nDeploying using the studio\n\uf80a\nDeployment settings""]",1272
gpt-35-turbo-ft,18,,5.0,,How can you automate the process of model retraining and deployment using Azure ML Pipelines?,v1,"To automate the process of model retraining and deployment using Azure ML Pipelines, you can define a pipeline that includes the necessary steps for data preparation, model training, and model deployment. This pipeline can be executed on a schedule or triggered by specific events. The Azure ML Pipelines provide a way to orchestrate and automate the end-to-end machine learning lifecycle, including data preparation, model training, and deployment.","['Use Azu re Pipelines with Azure Machine\nLearning\nArticle •09/29/2023\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\nof the operations you can automate are:\nData preparation (extract, transform, load operations)\nTraining machine learning models with on-demand scale-out and scale-up\nDeployment of machine learning models as public or private web services\nMonitoring deployed machine learning models (such as for performance or data-\ndrift analysis)\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\nmachine learning model to Azure Machine Learning .\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\nv2.\nComplete the Create resources to get started  to:\nCreate a workspace\nCreate a cloud-based compute cluster  to use for training your model\nAzure Machine Learning extension for Azure Pipelines. This extension can be\ninstalled from the Visual S tudio marketplace at\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-\nv2.\nFork the following repo at GitHub:Prerequisites\nStep 1: Get the code\nhttps://github.com/azure/azureml-examples', 'Continuous integration and deployment of machine learning models with Azure\nPipelines\nMachine Learning ML Ops  repository\nLearn more by reading and exploring the following resources:\nSet up ML Ops with Azure DevOps\nLearning path: End-to-end ML Ops with Azure Machine Learning\nHow to deploy a model to an online endpoint  with Machine Learning\nTutorial: T rain and deploy a model\nCI/CD of machine learning models with Azure Pipelines\nMachine learning at scale\nAzure AI reference architectures and best practices repo\nNext steps', ""6. Select main as a branch and choose /mlops/devops-pipelines/deploy-model-\ntraining-pipeline.yml, then select Continue .\n7. Save and Run  the pipeline\nThis scenario includes prebuilt workflows for two approaches to deploying a trained\nmodel, batch scoring or a deploying a model to an endpoint for real-time scoring. Y ou\nmay run either or both of these workflows to test the performance of the model in your\nAzure ML workspace. IN this example we will be using real-time scoring.\n1. Go to ADO pipelines\n2. Select New Pipeline .\n3. Select Azur e Repos Git .７ Note\nAt this point, the infrastructure is configured and the Prototyping Loop of the\nMLOps Architecture is deployed. you're ready to move to our trained model to\nproduction.\nDeploying the Trained model\nDeploy ML model endpoint"", '3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\nMachine Learning in many different ways .\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\niterations over different models, hyperparameter settings, advanced\npreprocessing/featurization, and what metrics to look at when determining the\nbest model.\n5. Submit the training job.\n6. Review the r esults\nThe following diagram illustrates this process. \nYou can also inspect the logged job information, which contains metrics  gathered\nduring the job. The training job produces a Python serialized object ( .pkl file) that\ncontains the model and data preprocessing.\nWhile model building is automated, you can also learn how important or relevant\nfeatures are  to the generated models.\nApply automated ML when you want Azure Machine Learning to train and tune a model\nfor you using the target metric you specify. Automated ML democratizes the machine\nlearning model development process, and empowers its users, no matter their data\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\nML professionals and developers across industries can use automated ML to:\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\nforecasting, computer vision & NLP', '4. When the pipeline has complete successfully, you can find your Azure Machine\nLearning W orkspace and associated resources by logging in to the Azure P ortal.\nNext, a model training and scoring pipelines will be deployed into the new\nMachine Learning environment.\nThe solution accelerator includes code and data for a sample end-to-end machine\nlearning pipeline which runs a linear regression to predict taxi fares in NY C. The pipeline\nis made up of components, each serving different functions, which can be registered\nwith the workspace, versioned, and reused with various inputs and outputs. Sample\npipelines and workflows for the Computer Vision and NLP scenarios will have different\nsteps and deployment steps.\nThis training pipeline contains the following steps:\nPrepare Data\nThis component takes multiple taxi datasets (yellow and green) and merges/filters\nthe data, and prepare the train/val and evaluation datasets.\nInput: Local data under ./data/ (multiple .csv files)\nOutput: Single prepared dataset (.csv) and train/val/test datasets.\nTrain Model\nThis component trains a Linear R egressor with the training set.\nInput: T raining dataset\nOutput: T rained model (pickle format)\nEvaluat e Model\nThis component uses the trained model to predict taxi fares on the test set.\nInput: ML model and T est dataset\nOutput: P erformance of model and a deploy flag whether to deploy or not.Sample Training  and Deployment Scenario']",1229
gpt-35-turbo-ft,19,,3.0,,Describe the approach to use Azure ML for implementing a recommendation system. What kind of data and algorithms are typically involved?,v1,"To implement a recommendation system with Azure ML, you would typically use collaborative filtering algorithms and work with user-item interaction data. Azure ML provides the infrastructure to train, deploy, and manage recommendation models at scale. You can use Azure ML to preprocess and feature engineer your data, train recommendation models using collaborative filtering or deep learning algorithms, and deploy these models as endpoints for real-time or batch inference.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Tell us about y our PDF experience.\nAzur e Machine Learning documentation\nLearn how to train and deploy models and manage the ML lifecycle (ML Ops) with Azure\nMachine Learning. Tutorials, code examples, API references, and more.\nOverview\nｅOVERVIE W\nWhat is Azure Machine Learning?\nSetup & quickstar t\nｆQUICKST ART\nCreate resources\nGet started with Azure Machine Learning\nStart with the b asics\nｇTUTORIAL\nPrepare and explore data\nDevelop on a cloud workstation\nTrain a model\nDeploy a model\nSet up a reusable pipeline\nWork with data\nｃHOW-T O GUIDE\nUse Apache Spark in Azure Machine Learning\nCreate data assets', ""What is Azure Machine Learning?\nArticle •01/29/2024\nAzure Machine Learning is a cloud service for accelerating and managing the machine\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\nin their day-to-day workflows to train and deploy models and manage machine learning\noperations (ML Ops).\nYou can create a model in Machine Learning or use a model built from an open-source\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\nretrain, and redeploy models.\nMachine Learning is for individuals and teams implementing ML Ops within their\norganization to bring ML models into production in a secure and auditable production\nenvironment.\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\nday workflows. Application developers can use tools for integrating models into\napplications or services. Platform developers can use a robust set of tools, backed by\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\nbased access control for infrastructure. Y ou can set up a project to deny access to\nprotected data and select operations.\nML projects often require a team with a varied skill set to build and maintain. Machine\nLearning has tools that help enable you to:\uea80 Tip\nFree trial!  If you don't have an Azure subscription, create a free account before you\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\nto spend on Azure services. After they're used up, you can keep the account and\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\nchange your settings and ask to be charged.\nWho is Azure Machine Learning for?\nProductivity for everyone on the team"", ""If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about calendar features for time series forecasting in AutoML .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about AutoML forecasting for lagged features .Next steps"", 'For training in developing AI and Machine Learning solutions with Microsoft, see\nMicrosoft Learn training .\nChoose a Microsoft cognitive services technology\nArtificial intelligence (AI) architecture design\nHow Azure Machine Learning works: resources and assetsRelated resources']",873
gpt-35-turbo-ft,20,,5.0,,How can you leverage Azure ML's built-in support for ONNX models for interoperability across different frameworks?,v1,"Azure ML's built-in support for ONNX models allows you to leverage interoperability across different frameworks by enabling you to train your preferred model in any framework and then convert it to ONNX format. Once in ONNX format, the model can be deployed and run on a variety of platforms and devices, including Azure ML, without the need for additional conversion or optimization steps.","[""ONNX and Azure Machine Learning:\nCreate and accelerate ML models\nArticle •06/13/2023\nLearn how using the Open Neural Network Exchange  (ONNX) can help optimize the\ninference of your machine learning model. Inference, or model scoring, is the phase\nwhere the deployed model is used for prediction, most commonly on production data.\nOptimizing machine learning models for inference (or model scoring) is difficult since\nyou need to tune the model and the inference library to make the most of the hardware\ncapabilities. The problem becomes extremely hard if you want to get optimal\nperformance on different kinds of platforms (cloud/edge, CPU/GPU, etc.), since each one\nhas different capabilities and characteristics. The complexity increases if you have\nmodels from a variety of frameworks that need to run on a variety of platforms. It's very\ntime consuming to optimize all the different combinations of frameworks and hardware.\nA solution to train once in your preferred framework and run anywhere on the cloud or\nedge is needed. This is where ONNX comes in.\nMicrosoft and a community of partners created ONNX as an open standard for\nrepresenting machine learning models. Models from many frameworks  including\nTensorFlow, PyT orch, SciKit-Learn, K eras, Chainer, MXNet, MA TLAB, and SparkML can be\nexported or converted to the standard ONNX format. Once the models are in the ONNX\nformat, they can be run on a variety of platforms and devices.\nONNX Runtime  is a high-performance inference engine for deploying ONNX models\nto production. It's optimized for both cloud and edge and works on Linux, Windows,\nand Mac. Written in C++, it also has C, Python, C#, Java, and JavaScript (Node.js) APIs\nfor usage in a variety of environments. ONNX Runtime supports both DNN and\ntraditional ML models and integrates with accelerators on different hardware such as\nTensorR T on NVidia GPUs, OpenVINO on Intel processors, DirectML on Windows, and\nmore. By using ONNX Runtime, you can benefit from the extensive production-grade\noptimizations, testing, and ongoing improvements.\nONNX Runtime is used in high-scale Microsoft services such as Bing, Office, and Azure\nAI. Performance gains are dependent on a number of factors, but these Microsoft\nservices have seen an average 2x per formance gain on CPU . In addition to Azure\nMachine Learning services, ONNX Runtime also runs in other products that support\nMachine Learning workloads, including:\nWindows: The runtime is built into Windows as part of Windows Machine Learning\nand runs on hundreds of millions of devices."", 'Azure SQL product family: Run native scoring on data in Azure SQL Edge  and\nAzure SQL Managed Instance .\nML.NET: Run ONNX models in ML.NET .\nYou can obtain ONNX models in several ways:\nTrain a new ONNX model in Azure Machine Learning (see examples at the bottom\nof this article) or by using automated Machine Learning capabilities\nConvert existing model from another format to ONNX (see the tutorials )\nGet a pre-trained ONNX model from the ONNX Model Zoo\nGenerate a customized ONNX model from Azure Custom Vision service\nMany models including image classification, object detection, and text processing can\nbe represented as ONNX models. If you run into an issue with a model that cannot be\nconverted successfully, please file an issue in the GitHub of the respective converter that\nyou used. Y ou can continue using your existing format model until the issue is\naddressed.\nWith Azure Machine Learning, you can deploy, manage, and monitor your ONNX\nmodels. Using the standard deployment workflow  and ONNX Runtime, you can create a\nREST endpoint hosted in the cloud. See example Jupyter notebooks at the end of this\narticle to try it out for yourself.\n\uf80a\nGet ONNX models\nDeploy ONNX models in Azure\nInstall and use ONNX Runtime with Python', 'See the AutoML package  for changing default ensemble settings in automated machine\nlearning.\nWith Azure Machine Learning, you can use automated ML to build a Python model and\nhave it converted to the ONNX format. Once the models are in the ONNX format, they\ncan be run on a variety of platforms and devices. Learn more about accelerating ML\nmodels with ONNX .\nSee how to convert to ONNX format in this Jupyter notebook example . Learn which\nalgorithms are supported in ONNX .\nThe ONNX runtime also supports C#, so you can use the model built automatically in\nyour C# apps without any need for recoding or any of the network latencies that REST\nendpoints introduce. Learn more about using an AutoML ONNX model in a .NET\napplication with ML.NET  and inferencing ONNX models with the ONNX runtime C#\nAPI .\nThere are multiple resources to get you up and running with AutoML.\nTutorials are end-to-end introductory examples of AutoML scenarios.\nFor a code fir st experience , follow the Tutorial: T rain an object detection model\nwith AutoML and Python\nFor a low or no-code experience , see the Tutorial: T rain a classification model with\nno-code AutoML in Azure Machine Learning studio .\nHow-to articles provide additional detail into what functionality automated ML offers.\nFor example,\nConfigure the settings for automatic training experiments\nWithout code in the Azure Machine Learning studio .\nWith the Python SDK .\nLearn how to train computer vision models with Python .\nLearn how to view the generated code from your automated ML models (SDK v1) .AutoML & ONNX\nNext steps\nTutorials/ how-tos', ""Make predictions with ONNX on\ncomputer vision models from AutoML\nArticle •04/04/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this article, you will learn how to use Open Neural Network Exchange (ONNX) to\nmake predictions on computer vision models generated from automated machine\nlearning (AutoML) in Azure Machine Learning.\nTo use ONNX for predictions, you need to:\n1. Download ONNX model files from an AutoML training run.\n2. Understand the inputs and outputs of an ONNX model.\n3. Preprocess your data so that it's in the required format for input images.\n4. Perform inference with ONNX Runtime for Python.\n5. Visualize predictions for object detection and instance segmentation tasks.\nONNX  is an open standard for machine learning and deep learning models. It enables\nmodel import and export (interoperability) across the popular AI frameworks. For more\ndetails, explore the ONNX GitHub project .\nONNX Runtime  is an open-source project that supports cross-platform inference.\nONNX Runtime provides APIs across programming languages (including Python, C++,\nC#, C, Java, and JavaScript). Y ou can use these APIs to perform inference on input\nimages. After you have the model that has been exported to ONNX format, you can use\nthese APIs on any programming language that your project needs.\nIn this guide, you'll learn how to use Python APIs for ONNX Runtime  to make\npredictions on images for popular vision tasks. Y ou can use these ONNX exported\nmodels across languages.\nGet an AutoML-trained computer vision model for any of the supported image\ntasks: classification, object detection, or instance segmentation. Learn more about\nAutoML support for computer vision tasks .\nInstall the onnxruntime  package. The methods in this article have been tested\nwith versions 1.3.0 to 1.8.0.\nPrerequisites"", ""runs from experimentation in the studio user interface. Or they can use versioned assets\nfor jobs like environments and storage references.\nFor more information, see Manage Azure Machine Learning workspaces .\nWhen a project is ready for operationalization, users' work can be automated in an ML\npipeline and triggered on a schedule or HT TPS request.\nYou can deploy models to the managed inferencing solution, for both real-time and\nbatch deployments, abstracting away the infrastructure management typically required\nfor deploying models.\nIn Azure Machine Learning, you can run your training script in the cloud or build a\nmodel from scratch. Customers often bring models they've built and trained in open-\nsource frameworks so that they can operationalize them in the cloud.\nData scientists can use models in Azure Machine Learning that they've created in\ncommon Python frameworks, such as:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nLightGBM\nOther languages and frameworks are also supported:\nR\n.NET\nFor more information, see Open-source integration with Azure Machine Learning .\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\nexperience and intuition to select the right data featurization and algorithm for training.\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\nLearning studio UI or the Python SDK.Train models\nOpen and interoperable\nAutomated featurization and algorithm selection""]",1953
gpt-35-turbo-ft,21,,5.0,,What Azure storage services can be integrated with Azure Machine Learning for storing datasets?,v1,"Azure Machine Learning can integrate with various Azure storage services such as Azure Blob Storage, Azure File Storage, and Azure Data Lake Storage for storing datasets.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore"", ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", ""Service Usage Example datanumber of nodes; datastore names and tags, and\ndescriptions on assets like models; and data label names.\nAzure AI\nSearchStores indexes that\nhelp with querying\nyour machine learning\ncontent.These indexes are built on top of the data stored in Azure\nCosmos DB.\nAzure\nStorageStores metadata\nrelated to Azure\nMachine Learning\npipeline data.Data can include designer pipeline names, pipeline layout,\nand execution properties.\nFrom the perspective of data lifecycle management, data in the preceding resources is\ncreated and deleted as you create and delete corresponding objects in Azure Machine\nLearning.\nYour Azure Machine Learning workspace reads and writes data by using its managed\nidentity. This identity is granted access to the resources through a role assignment\n(Azure role-based access control) on the data resources. The encryption key that you\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\nused to create indexes for Azure AI Search at runtime.\nExtra networking controls are configured when you create a private link endpoint on\nyour workspace to allow for inbound connectivity. This configuration includes the\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\nNetwork access is restricted to only trusted Microsoft services.\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\nthe data.\nWhen you use a customer-managed key, the resources are in your Azure subscription\nand encrypted with your key. While these resources exist in your subscription, Microsoft\nmanages them. They're automatically created and configured when you create your\nAzure Machine Learning workspace.\nThese Microsoft-managed resources are located in a new Azure resource group that's\ncreated in your subscription. This resource group is separate from the resource group\nfor your workspace. It contains the Microsoft-managed resources that your key is used\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\nresource group name><GUID>.Customer-managed keys""]",1588
gpt-35-turbo-ft,22,,5.0,,How do you authenticate Azure Machine Learning with Azure Blob Storage to access datasets?,v1,"To authenticate Azure Machine Learning with Azure Blob Storage for dataset access, you can use identity-based data access. This approach prompts you for your Microsoft Entra token for data access authentication, ensuring that your credentials are kept confidential. Additionally, you must have at least Storage Blob Data Reader access to the storage account.","['You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity', 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.', 'means that in order to successfully read data from storage, the managed identity of the\ncompute cluster where the deployment is running must have at least Storage Blob Data\nReader  access to the storage account.\nTo configure the compute cluster for data access, follow these steps:\n1. Go to Azure Machine Learning studio .\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\ndeployment is using.\n3. Assign a managed identity to the compute cluster:\na. In the Managed identity  section, verify if the compute has a managed identity\nassigned. If not, select the option Edit.\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\nusing a S ystem-Assigned Managed Identity, it is named as ""[workspace\nname]/computes/[compute cluster name]"".\nc. Save the changes.\n4. Go to the Azure portal  and navigate to the associated storage account where the\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\nstorage account where those assets are placed.', ""When you use an Azure S torage Account from Azure Machine Learning studio, you must\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\naccount:\nBlob Data R eader\nIf the storage account uses a private endpoint to connect to the VNet, you must\ngrant the Reader  role for the storage account private endpoint to the managed\nidentity.\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\nNetwork .\nThe following sections explain the limitations of using an Azure S torage Account, with\nyour workspace, in a VNet.\nTo secure communication between Azure Machine Learning and Azure S torage\nAccounts, configure the storage to Grant access to trusted Azure services .\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\ncan normally be used to allow your client to directly connect over the internet. However,\nwhen using studio, your client doesn't connect to the storage account. The Azure\nMachine Learning service that makes the request connects to the storage account. The\nIP address of the service isn't documented, and it changes frequently. Enabling the\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\nconfiguration .\uea80 Tip\nTo access data from outside Azure Machine Learning, for example with Azure\nStorage Explorer, that access probably relies on the user identity. For specific\ninformation, review the documentation for the tool or service you're using. For\nmore information about how Azure Machine Learning works with data, see Setup\nauthentication betw een Azur e Machine Learning and other ser vices .\nAzure Storage Account\nSecure communication with Azure Storage Account\nAzure Storage firewall"", ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.""]",1584
gpt-35-turbo-ft,23,,5.0,,What steps are involved in uploading a dataset to Azure Blob Storage and then accessing it from an Azure Machine Learning experiment?,v1,"To upload a dataset to Azure Blob Storage and access it from an Azure Machine Learning experiment, you would typically follow these steps:

1. **Upload Data to Azure Blob Storage**: Use Azure Portal, Azure CLI, or Azure SDK to upload your data to Azure Blob Storage.

2. **Register Datastore**: Register the Azure Blob Storage as a Datastore in your Azure Machine Learning workspace.

3. **Create Dataset**: Create a dataset in your Azure Machine Learning workspace, referencing the registered Datastore and the path to your data.

4. **Access Dataset in Experiment**: In your experiment script, reference the dataset by its name to access the data during the experiment run.

Would you like more detailed information on any of these steps?","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data"", ""Tutorial: Upload, access and explore\nyour data in Azure Machine Learning\nArticle •12/27/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this tutorial you learn how to:\nThe start of a machine learning project typically involves exploratory data analysis (ED A),\ndata-preprocessing (cleaning, feature engineering), and the building of Machine\nLearning model prototypes to validate hypotheses. This prototyping  project phase is\nhighly interactive. It lends itself to development in an IDE or a Jupyter notebook, with a\nPython int eractive console. This tutorial describes these ideas.\nThis video shows how to get started in Azure Machine Learning studio so that you can\nfollow the steps in the tutorial. The video shows how to create a notebook, clone the\nnotebook, create a compute instance, and download the data needed for the tutorial.\nThe steps are also described in the following sections.\n1. To use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\n2. Sign in to studio  and select your workspace if it's not already open.\n3. Open or create a notebook in your workspace:\nCreate a new notebook , if you want to copy/paste code into cells.\nOr, open tutorials/get -star ted-not ebooks/explor e-data.ipynb  from the\nSamples  section of studio. Then select Clone  to add the notebook to your\nFiles. (See where to find Samples .)\nUpload your data to cloud storage＂\nCreate an Azure Machine Learning data asset＂\nAccess your data in a notebook for interactive development＂\nCreate new versions of data assets＂\nhttps://learn-video.azurefd.net/vod/player?id=514a29e2-0ae7-4a5d-a537-\n8f10681f5545&locale=en-us&embedUrl=%2F azure%2Fmachine-learning%2Ftutorial-\nexplore-data\nPrerequisites"", '2. Select your subscription and the workspace you created.\n3. Select Get star ted.\n4. In the left pane, select Automat ed ML  under the Author  section.\n5. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning dataset. Doing so, allows you to ensure that your\ndata is formatted appropriately for your experiment.\n1. On the Select dataset  form, select From local files  from the +Create dataset  drop-\ndown.\na. On the Basic info  form, give your dataset a name and provide an optional\ndescription. The dataset type should default to Tabular , since automated ML in\nAzure Machine Learning studio currently only supports tabular datasets.\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is the storage location where you upload your data\nfile.\nd. Select Uplo ad files  from the Uplo ad drop-down.\ne. Choose the bike-no.csv  file on your local computer. This is the file you\ndownloaded as a prerequisite .\nf. Select Next\nWhen the upload is complete, the Settings and preview form is pre-populated\nbased on the file type.\ng. Verify that the Settings and pr eview  form is populated as follows and select\nNext .Create and load dataset\nﾉExpand table', ""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data""]",1837
gpt-35-turbo-ft,24,,5.0,,"Can Azure Machine Learning directly access data stored in Azure Data Lake Storage Gen2? If so, how?",v1,"Yes, Azure Machine Learning can directly access data stored in Azure Data Lake Storage Gen2. To do so, you would typically define a Datastore in your Azure Machine Learning workspace that points to your Azure Data Lake Storage Gen2 account. This Datastore can then be used to reference data in your training scripts, pipelines, and other AML artifacts.","[""For information on regenerating the access keys, see Regenerate storage account access\nkeys.\nAzure Data Lake S torage Gen2 is built on top of Azure Blob S torage and is designed for\nbig data analytics in enterprises. Data Lake S torage Gen2 is used as a datastore for\nAzure Machine Learning. Like Azure Blob S torage, the data at rest is encrypted with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Data Lake\nStorage, see Azure S torage encryption with customer-managed keys in Azure K ey Vault.\nThe Azure Machine Learning service supports data from the following data sources.\nTransparent data encryption helps protect Azure SQL Database against the threat of\nmalicious offline activity by encrypting data at rest. By default, transparent data\nencryption is enabled for all newly deployed SQL databases that use Microsoft-\nmanaged keys.\nFor information on how to use customer-managed keys for transparent data encryption,\nsee Azure SQL Database transparent data encryption .Azure Data Lake Storage\n７ Note\nOn Feb 29, 2024 , Azure Data Lake S torage Gen1 will be retired. For more\ninformation, see the official announcement . If you use Azure Data Lake S torage\nGen1, make sure to migrate to Azure Data Lake S torage Gen2 before that date. T o\nlearn how, see Migrat e Azur e Data Lak e Storage fr om Gen1 t o Gen2 by using the\nAzur e por tal.\nUnless you already have an Azure Data Lake S torage Gen1 account, you can't create\nnew ones.\nAzure relational databases\nAzure SQL Database\nAzure Database for PostgreSQL"", 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity', 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore""]",1477
gpt-35-turbo-ft,25,,3.0,,How does Azure Machine Learning support versioning of datasets stored in Azure Blob Storage?,v1,"Azure Machine Learning supports versioning of datasets stored in Azure Blob Storage by leveraging the inherent versioning capabilities of Azure Blob Storage. When you upload a new version of a dataset to Azure Blob Storage, it automatically creates a new version of the blob, allowing you to maintain a history of changes. Additionally, Azure Machine Learning provides tools to manage and track dataset versions within your machine learning workflows.","[""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore"", ""Only Azure Blob S torage Accounts are supported for outputs. For instance, Azure Data Lake\nStorage Gen2 isn't supported as output in batch deployment jobs. If you need to output the\ndata to a different location/sink, use the Azure Data F actory Copy activity after the execution of\nthe batch job.\nUse low priority VMs in batch deployments\nAuthorization on batch endpoints\nNetwork isolation in batch endpointsNext steps"", 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity']",1307
gpt-35-turbo-ft,26,,5.0,,What is the recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage?,v1,The recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage is to use Azure Storage Explorer for faster and more robust data transfer. This is the default way to move files and create datasets from data stored in Blob Storage.,"[""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", ""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data"", 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity']",1685
gpt-35-turbo-ft,27,,5.0,,How do you use the Azure Machine Learning SDK to programmatically download data from Azure Blob Storage?,v1,"To programmatically download data from Azure Blob Storage using the Azure Machine Learning SDK, you can use the `Datastore` and `Dataset` classes. First, you need to register your Azure Blob Storage account as a datastore in your workspace. Then, you can create a dataset from the datastore and download the data programmatically. Would you like a code example to illustrate this process?","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data"", ""Tutorial: Upload, access and explore\nyour data in Azure Machine Learning\nArticle •12/27/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this tutorial you learn how to:\nThe start of a machine learning project typically involves exploratory data analysis (ED A),\ndata-preprocessing (cleaning, feature engineering), and the building of Machine\nLearning model prototypes to validate hypotheses. This prototyping  project phase is\nhighly interactive. It lends itself to development in an IDE or a Jupyter notebook, with a\nPython int eractive console. This tutorial describes these ideas.\nThis video shows how to get started in Azure Machine Learning studio so that you can\nfollow the steps in the tutorial. The video shows how to create a notebook, clone the\nnotebook, create a compute instance, and download the data needed for the tutorial.\nThe steps are also described in the following sections.\n1. To use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\n2. Sign in to studio  and select your workspace if it's not already open.\n3. Open or create a notebook in your workspace:\nCreate a new notebook , if you want to copy/paste code into cells.\nOr, open tutorials/get -star ted-not ebooks/explor e-data.ipynb  from the\nSamples  section of studio. Then select Clone  to add the notebook to your\nFiles. (See where to find Samples .)\nUpload your data to cloud storage＂\nCreate an Azure Machine Learning data asset＂\nAccess your data in a notebook for interactive development＂\nCreate new versions of data assets＂\nhttps://learn-video.azurefd.net/vod/player?id=514a29e2-0ae7-4a5d-a537-\n8f10681f5545&locale=en-us&embedUrl=%2F azure%2Fmachine-learning%2Ftutorial-\nexplore-data\nPrerequisites"", '2. Select your subscription and the workspace you created.\n3. Select Get star ted.\n4. In the left pane, select Automat ed ML  under the Author  section.\n5. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning dataset. Doing so, allows you to ensure that your\ndata is formatted appropriately for your experiment.\n1. On the Select dataset  form, select From local files  from the +Create dataset  drop-\ndown.\na. On the Basic info  form, give your dataset a name and provide an optional\ndescription. The dataset type should default to Tabular , since automated ML in\nAzure Machine Learning studio currently only supports tabular datasets.\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is the storage location where you upload your data\nfile.\nd. Select Uplo ad files  from the Uplo ad drop-down.\ne. Choose the bike-no.csv  file on your local computer. This is the file you\ndownloaded as a prerequisite .\nf. Select Next\nWhen the upload is complete, the Settings and preview form is pre-populated\nbased on the file type.\ng. Verify that the Settings and pr eview  form is populated as follows and select\nNext .Create and load dataset\nﾉExpand table', ""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data""]",1764
gpt-35-turbo-ft,28,,5.0,,What are the benefits of integrating Azure Data Lake Storage with Azure Machine Learning for big data scenarios?,v1,"Azure Data Lake Storage integration with Azure Machine Learning offers benefits such as scalable storage for large datasets, seamless data access from AML workspaces, and the ability to leverage big data processing capabilities for training and inference tasks.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Azur e Machine Learning designer : Use the designer to train and deploy ML\nmodels without writing any code. Drag and drop datasets and components to\ncreate ML pipelines.\nAutomat ed machine learning UI : Learn how to create automated ML experiments\nwith an easy-to-use interface.\nData labeling : Use Machine Learning data labeling to efficiently coordinate image\nlabeling  or text labeling  projects.\nMachine Learning integrates with the Azure cloud platform to add security to ML\nprojects.\nSecurity integrations include:\nAzure Virtual Networks with network security groups.\nAzure K ey Vault, where you can save security secrets, such as access information\nfor storage accounts.\nAzure Container R egistry set up behind a virtual network.\nFor more information, see Tutorial: Set up a secure workspace .\nOther integrations with Azure services support an ML project from end to end. They\ninclude:Enterprise-readiness and security\nAzure integrations for complete solutio ns', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore"", ""What is Azure Machine Learning?\nArticle •01/29/2024\nAzure Machine Learning is a cloud service for accelerating and managing the machine\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\nin their day-to-day workflows to train and deploy models and manage machine learning\noperations (ML Ops).\nYou can create a model in Machine Learning or use a model built from an open-source\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\nretrain, and redeploy models.\nMachine Learning is for individuals and teams implementing ML Ops within their\norganization to bring ML models into production in a secure and auditable production\nenvironment.\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\nday workflows. Application developers can use tools for integrating models into\napplications or services. Platform developers can use a robust set of tools, backed by\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\nbased access control for infrastructure. Y ou can set up a project to deny access to\nprotected data and select operations.\nML projects often require a team with a varied skill set to build and maintain. Machine\nLearning has tools that help enable you to:\uea80 Tip\nFree trial!  If you don't have an Azure subscription, create a free account before you\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\nto spend on Azure services. After they're used up, you can keep the account and\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\nchange your settings and ask to be charged.\nWho is Azure Machine Learning for?\nProductivity for everyone on the team"", 'Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .']",1078
gpt-35-turbo-ft,29,,5.0,,How can you secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces?,v1,"To secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces, you can leverage the encryption at rest feature provided by Azure Blob Storage. All data stored in Azure Blob Storage is encrypted at rest with Microsoft-managed keys. Additionally, you can use your own keys for data stored in Azure Blob Storage by following the guidelines for Azure Storage encryption with customer-managed keys in Azure Key Vault.","[""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", 'This article is part of a series on securing an Azure Machine Learning workflow. See the\nother articles in this series:\nSecure the workspace resources\nSecure machine learning registries\nSecure the training environment\nSecure the inference environment\nEnable studio functionality\nUse custom DNS\nUse a firewall\nAPI platform network isolationNext steps', ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."", ""When you use an Azure S torage Account from Azure Machine Learning studio, you must\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\naccount:\nBlob Data R eader\nIf the storage account uses a private endpoint to connect to the VNet, you must\ngrant the Reader  role for the storage account private endpoint to the managed\nidentity.\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\nNetwork .\nThe following sections explain the limitations of using an Azure S torage Account, with\nyour workspace, in a VNet.\nTo secure communication between Azure Machine Learning and Azure S torage\nAccounts, configure the storage to Grant access to trusted Azure services .\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\ncan normally be used to allow your client to directly connect over the internet. However,\nwhen using studio, your client doesn't connect to the storage account. The Azure\nMachine Learning service that makes the request connects to the storage account. The\nIP address of the service isn't documented, and it changes frequently. Enabling the\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\nconfiguration .\uea80 Tip\nTo access data from outside Azure Machine Learning, for example with Azure\nStorage Explorer, that access probably relies on the user identity. For specific\ninformation, review the documentation for the tool or service you're using. For\nmore information about how Azure Machine Learning works with data, see Setup\nauthentication betw een Azur e Machine Learning and other ser vices .\nAzure Storage Account\nSecure communication with Azure Storage Account\nAzure Storage firewall"", 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity']",1514
gpt-35-turbo-ft,30,,5.0,,What role do Azure Storage accounts play in deploying Azure Machine Learning models as web services?,v1,"Azure Storage accounts play a crucial role in deploying Azure Machine Learning models as web services by storing model artifacts, deployment configurations, and other resources required for the deployment process.","[""Scenar io Requir ed\ninboundRequir ed outbound Additional\nconfiguration\nUse Azure\nKubernetes\nServiceNot applicable For information on the outbound\nconfiguration for AKS, see Secure\nAzure Kubernetes Service\ninferencing environment .\nUse Docker\nimages that\nAzure\nMachine\nLearning\nmanagesNot applicable Microsoft Artifact R egistry\nviennaglobal.azurecr.io\nglobal container registryIf the container registry\nfor your workspace is\nbehind the virtual\nnetwork, configure the\nworkspace to use a\ncompute cluster to build\nimages. For more\ninformation, see Secure\nan Azure Machine\nLearning workspace with\nvirtual networks .\nAzure Machine Learning uses multiple storage accounts. Each stores different data and\nhas a different purpose:\nYour st orage : The storage accounts in your Azure subscription store your data and\nartifacts, such as models, training data, training logs, and Python scripts. For\nexample, the default  storage account for your workspace is in your subscription.\nThe Azure Machine Learning compute instance and compute cluster access file and\nblob data in this storage over ports 445 (SMB) and 443 (HT TPS).\nWhen you're using a compute instance or compute cluster, your storage account is\nmounted as a file share via the SMB protocol. The compute instance and cluster\nuse this file share to store items like the data, models, Jupyter notebooks, and\ndatasets. The compute instance and cluster use the private endpoint when they\naccess the storage account.\nMicrosoft storage : The Azure Machine Learning compute instance and compute\ncluster rely on Azure Batch. They access storage located in a Microsoft\nsubscription. This storage is used only for the management of the compute\ninstance or cluster. None of your data is stored here. The compute instance and\ncompute cluster access the blob, table, and queue data in this storage, by using\nport 443 (HT TPS).Purposes of storage accounts"", 'Azure Machine Learning supports storage accounts configured to use either a private\nendpoint or service endpoint.\n1. In the Azure portal, select the Azure S torage Account.\n2. Use the information in Use private endpoints for Azure S torage  to add private\nendpoints for the following storage resources:\nBlob\nFile\nQueue  - Only needed if you plan to use Batch endpoints  or the\nParallelRunS tep in an Azure Machine Learning pipeline.\nTable - Only needed if you plan to use Batch endpoints  or the\nParallelRunS tep in an Azure Machine Learning pipeline.Securing a workspace with private endpoints does not ensure end-to-end security\nby itself. Y ou must follow the steps in the rest of this article, and the VNet series, to\nsecure individual components of your solution. For example, if you use a private\nendpoint for the workspace, but your Azure S torage Account is not behind the\nVNet, traffic between the workspace and storage does not use the VNet for\nsecurity.\nSecure Azure storage accounts\nPrivate endpoint', 'What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps', ""Azure Machine Learning offers many ways to deploy MLflow models to online and batch\nendpoints. Y ou can deploy models, using the following tools:\nEach workflow has different capabilities, particularly around which type of compute they\ncan target. The following table shows the different capabilities.\nScenar io MLflow SDK Azur e Machine\nLearning CLI/SDKAzur e Machine\nLearning studio\nDeploy to managed online\nendpointsSee example See example See example\nDeploy to managed online\nendpoints (with a scoring script)Not\nsupportedSee example See example\nDeploy to batch endpoints Not\nsupportedSee example See example\nDeploy to batch endpoints (with\na scoring script)Not\nsupportedSee example See example\nDeploy to web services\n(ACI/AKS)Legacy\nsupportNot supported Not supported\nDeploy to web services (A CI/AKS\n- with a scoring script)Not\nsupportedLegacy support Legacy support\n Deployment to online endpoints that are in workspaces with private link enabled\nrequires you to package models before deployment (preview) .\n We recommend switching to managed online endpoints  instead.\n MLflow (OSS) doesn't have the concept of a scoring script and doesn't support batch\nexecution currently.\nUse the MLflow SDK if both of these conditions apply:Deployment tools\nMLflow SDK＂\nAzure Machine Learning CLI＂\nAzure Machine Learning SDK for Python＂\nAzure Machine Learning studio＂\nﾉExpand table\n1 1 1\n3\n3\n3\n22 2\n32 2\n1\n2\n3\nWhich deployment tool to use?"", 'region basis. When you sign up for Azure Machine Learning studio, you receive a default\nVM quota for several VM families available in the region. Y ou can continue to create\ndeployments until you reach your quota limit. Once you reach this limit, you can request\na quota increase.\nModels deployed as a service with pay-as-you-go are protected by Azure AI content\nsafety. When deployed to real-time endpoints, you can opt out of this capability. With\nAzure AI content safety enabled, both the prompt and completion pass through an\nensemble of classification models aimed at detecting and preventing the output of\nharmful content. The content filtering system detects and takes action on specific\ncategories of potentially harmful content in both input prompts and output\ncompletions. Learn more about Azure AI Content Safety .\nModel Catalog and Collections\nDeploy and score a machine learning model by using an online endpoint\nPlan and manage costs for Azure AI S tudioContent filtering\nRelated content']",1204
gpt-35-turbo-ft,31,,5.0,,How do you monitor data access and usage when integrating Azure Machine Learning with Azure Storage?,v1,"To monitor data access and usage when integrating Azure Machine Learning with Azure Storage, you can leverage Azure Monitor to track metrics, logs, and activity. Additionally, Azure Storage provides detailed logging and monitoring capabilities, including Storage Analytics, which can be used to monitor access patterns and usage.","['To learn how to monitor resources and logs from Azure Machine Learning, see\nMonitoring Azure Machine Learning .', 'Monitor Azure Machine Learning\nArticle •11/06/2023\nWhen you have critical applications and business processes relying on Azure resources, you\nwant to monitor those resources for their availability, performance, and operation. This\narticle describes the monitoring data generated by Azure Machine Learning and how to\nanalyze and alert on this data with Azure Monitor.\nAzure Machine Learning creates monitoring data using Azure Monitor , which is a full stack\nmonitoring service in Azure. Azure Monitor provides a complete set of features to monitor\nyour Azure resources. It can also monitor resources in other clouds and on-premises.\nStart with the article Monitoring Azure resources with Azure Monitor , which describes the\nfollowing concepts:\nWhat is Azure Monitor?\nCosts associated with monitoring\nMonitoring data collected in Azure\nConfiguring data collection\nStandard tools in Azure for analyzing and alerting on monitoring data\nThe following sections build on this article by describing the specific data gathered for\nAzure Machine Learning. These sections also provide examples for configuring data\ncollection and analyzing this data with Azure tools.\uea80 Tip\nThe information in this document is primarily for administrat ors, as it describes\nmonitoring for the Azure Machine Learning service and associated Azure services. If\nyou are a data scientist  or developer , and want to monitor information specific to your\nmodel tr aining r uns, see the following documents:\nStart, monit or, and cancel training runs\nLog metrics for training runs\nTrack experiments with MLflow\nIf you want to monitor information generated by models deployed to online\nendpoints, see Monit or online endpoints .\nWhat is Azure Monitor?', ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."", 'Monitor online endpoints\nArticle •10/24/2023\nAzure Machine Learning uses integration with Azure Monitor to track and monitor\nmetrics and logs for online endpoints . You can view metrics in charts, compare between\nendpoints and deployments, pin to Azure portal dashboards, configure alerts, query\nfrom log tables, and push logs to supported targets. Y ou can also use Application\nInsights to analyze events from user containers.\nMetrics : For endpoint-level metrics such as request latency, requests per minute,\nnew connections per second, and network bytes, you can drill down to see details\nat the deployment level or status level. Deployment-level metrics such as CPU/GPU\nutilization and memory or disk utilization can also be drilled down to instance\nlevel. Azure Monitor allows tracking these metrics in charts and setting up\ndashboards and alerts for further analysis.\nLogs : You can send metrics to the Log Analytics workspace where you can query\nthe logs using Kusto query syntax. Y ou can also send metrics to Azure S torage\naccounts and/or Event Hubs for further processing. In addition, you can use\ndedicated log tables for online endpoint related events, traffic, and console\n(container) logs. Kusto query allows complex analysis and joining of multiple\ntables.\nApplication insights : Curated environments include integration with Application\nInsights, and you can enable or disable this integration when you create an online\ndeployment. Built-in metrics and logs are sent to Application Insights, and you can\nuse the built-in features of Application Insights (such as Live metrics, T ransaction\nsearch, F ailures, and P erformance) for further analysis.\nIn this article you learn how to:\nDeploy an Azure Machine Learning online endpoint.Choose the right method to view and track metrics and logs＂\nView metrics for your online endpoint＂\nCreate a dashboard for your metrics＂\nCreate a metric alert＂\nView logs for your online endpoint＂\nUse Application Insights to track metrics and logs＂\nPrerequisites', ""Use monitoring view in the Azure Machine Learning studio  to check the CPU and memory utilization of\nyour job. If you're low on CPU and memory, consider increasing the compute target VM size.\nConsider increasing RSLEX_DOWNLOADER_THREADS if you're downloading and you don't utilize the CPU and\nmemory. If you use mount, you should increase DATASET_MOUNT_READ_BUFFER_BLOCK_COUNT to do more\nprefetching, and increase DATASET_MOUNT_READ_THREADS for more read threads.\nIf the metrics show low SuccessE2ELat ency and low SuccessSer verLat ency, but the client experiences high\nlatency , you have a delay in the storage request that reaches the service. Y ou should check:\nWhether the number of threads used for mount/download\n(DATASET_MOUNT_READ_THREADS/RSLEX_DOWNLOADER_THREADS) is set too low, relative to the number of cores\navailable on the compute target. If the setting is too low, increase the number of threads.\nWhether the number of retries for downloading ( AZUREML_DATASET_HTTP_RETRY_COUNT) is set too high. If\nso, decrease the number of retries.\nFrom the Azure Machine Learning studio, you can also monitor the compute target disk IO and usage during\nyour job execution. Navigate to your job, and select the Monit oring  tab. This tab provides insights on the\nresources of your job, on a 30 day rolling basis. For example:\nAzure Machine Learning data runtime doesn't use the last RESERVED_FREE_DISK_SPACE bytes of disk space, to\nkeep the compute healthy (the default value is 150MB). If your disk is full, your code is writing files to disk\nwithout declaring the files as an output. Therefore, check your code to make sure that data isn't being written\nerroneously to temporary disk. If you must write files to temporary disk, and that resource is becoming full,\nconsider:\nIncreasing the VM Size to one that has a larger temporary disk\nSetting a T TL on the cached data ( DATASET_MOUNT_ATTRIBUTE_CACHE_TTL), to purge your data from diskMonitor disk usage during a job\n\uf80a\n７ Note\nJob monitoring supports only compute resources that Azure Machine Learning manages. Jobs with a\nruntime of less than 5 minutes will not have enough data to populate this view.""]",1598
gpt-35-turbo-ft,32,,5.0,,"Can Azure Machine Learning use data stored in Azure File Storage? If so, what are the use cases?",v1,"Yes, Azure Machine Learning can use data stored in Azure File Storage. This is useful for scenarios where you need to access shared data across multiple compute targets, such as training clusters or inference endpoints. For example, you can use Azure File Storage to store training data, model checkpoints, and other artifacts that need to be accessed by different compute resources.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore"", ""Attached comput e: You can attach your own compute resources to your\nworkspace and use them for training and inference.\nMachine Learning allows you to work with different types of data:\nURIs (a location in local or cloud storage):\nuri_folder\nuri_file\nTables (a tabular data abstraction):\nmltable\nPrimitives:\nstring\nboolean\nnumber\nFor most scenarios, you use URIs ( uri_folder and uri_file) to identify a location in\nstorage that can be easily mapped to the file system of a compute node in a job by\neither mounting or downloading the storage to the node.\nThe mltable parameter is an abstraction for tabular data that's used for automated\nmachine learning (AutoML) jobs, parallel jobs, and some advanced scenarios. If you're\nstarting to use Machine Learning and aren't using AutoML, we strongly encourage you\nto begin with URIs.\nMachine Learning datastores securely keep the connection information to your data\nstorage on Azure so that you don't have to code it in your scripts. Y ou can register and\ncreate a datastore to easily connect to your storage account and access the data in your\nunderlying storage service. The Azure Machine Learning CLI v2 and SDK v2 support the\nfollowing types of cloud-based storage services:\nAzure Blob S torage container\nAzure Files share\nAzure Data Lake S torage\nAzure Data Lake S torage Gen2Data\nDatastore\nEnvironment"", ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.""]",1497
gpt-35-turbo-ft,33,,5.0,,What are the best practices for managing input and output data for Azure Machine Learning pipelines using Azure Storage?,v1,"When managing input and output data for Azure Machine Learning pipelines using Azure Storage, it's important to consider the following best practices:

1. **Data Store Compatibility**: Ensure that your input data is stored in Azure Machine Learning data stores or Azure Storage Accounts (Azure Blob Storage, Azure Data Lake Storage Gen1, Azure Data Lake Storage Gen2).

2. **Flattening Folder Structures**: Batch endpoint jobs don't support nested folder structures, so flatten the structure if your data is distributed across multiple folders.

3. **Scoring Script Compatibility**: Ensure that your scoring script provided in the deployment can handle the data as expected to be fed into the job.

4. **Data Store Registration**: Register the storage account your Azure Data Factory is using as a Data Store in Azure Machine Learning to enable writing back to the same storage account from where you are reading.

These practices will help ensure smooth data management within Azure Machine Learning pipelines using Azure Storage.","[""7. Your pipeline is ready to be used.\nWhen calling Azure Machine Learning batch deployments consider the following limitations:\nOnly Azure Machine Learning data stores or Azure S torage Accounts (Azure Blob S torage, Azure\nData Lake S torage Gen1, Azure Data Lake S torage Gen2) are supported as inputs. If your input\ndata is in another source, use the Azure Data F actory Copy activity before the execution of the\nbatch job to sink the data to a compatible store.\nBatch endpoint jobs don't explore nested folders and hence can't work with nested folder\nstructures. If your data is distributed in multiple folders, notice that you will have to flatten the\nstructure.\nMake sure that your scoring script provided in the deployment can handle the data as it is\nexpected to be fed into the job. If the model is MLflow, read the limitation in terms of the file\ntype supported by the moment at Using MLflow models in batch deployments .\nOnly registered Azure Machine Learning data stores are supported by the moment. W e\nrecommend you to register the storage account your Azure Data F actory is using as a Data S tore\nin Azure Machine Learning. In that way, you will be able to write back to the same storage\naccount from where you are reading.\nLimitations\nData inputs\nData outputs"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.', ""original notebook or Python code, changes the training input from local data to\nparameterized values, split the training code into multiple steps as needed, perform unit\ntest of each step, and finally wraps all steps into a pipeline.\nOnce the teams get familiar with pipelines and want to do more machine learning\nprojects using pipelines, they'll find the first approach is hard to scale. The second\napproach is set up a few pipeline templates, each try to solve one specific machine\nlearning problem. The template predefines the pipeline structure including how many\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\nlearning project, the team first forks one template repo. The team leader then assigns\nmembers which step they need to work on. The data scientists and data engineers do\ntheir regular work. When they're happy with their result, they structure their code to fit\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\nexecuted or automated. If there's any change, each member only needs to work on their\npiece of code without touching the rest of the pipeline code.\nOnce a team has built a collection of machine learnings pipelines and reusable\ncomponents, they could start to build the machine learning pipeline from cloning\nprevious pipeline or tie existing reusable component together. At this stage, the team's\noverall productivity will be improved significantly.\nAzure Machine Learning offers different methods to build a pipeline. For users who are\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\npipelines by using registered components .\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\nfollowing table lists the different pipelines and what they're used for:\nScenar io Primar y\npersonaAzur e\nofferingOSS\nofferingCanonical\npipeStrengths\nModel\norchestration\n(Machine\nlearning)Data\nscientistAzure\nMachine\nLearning\nPipelinesKubeflow\nPipelinesData ->\nModelDistribution,\ncaching, code-first,\nreuse\nData\norchestrationData\nengineerAzure Data\nFactoryApache\nAirflowData -> Data Strongly typed\nmovement, data-Which Azure pipeline technology should I use?\nﾉExpand table"", ""original notebook or Python code, changes the training input from local data to\nparameterized values, split the training code into multiple steps as needed, perform unit\ntest of each step, and finally wraps all steps into a pipeline.\nOnce the teams get familiar with pipelines and want to do more machine learning\nprojects using pipelines, they'll find the first approach is hard to scale. The second\napproach is set up a few pipeline templates, each try to solve one specific machine\nlearning problem. The template predefines the pipeline structure including how many\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\nlearning project, the team first forks one template repo. The team leader then assigns\nmembers which step they need to work on. The data scientists and data engineers do\ntheir regular work. When they're happy with their result, they structure their code to fit\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\nexecuted or automated. If there's any change, each member only needs to work on their\npiece of code without touching the rest of the pipeline code.\nOnce a team has built a collection of machine learnings pipelines and reusable\ncomponents, they could start to build the machine learning pipeline from cloning\nprevious pipeline or tie existing reusable component together. At this stage, the team's\noverall productivity will be improved significantly.\nAzure Machine Learning offers different methods to build a pipeline. For users who are\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\npipelines by using registered components .\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\nfollowing table lists the different pipelines and what they're used for:\nScenar io Primar y\npersonaAzur e\nofferingOSS\nofferingCanonical\npipeStrengths\nModel\norchestration\n(Machine\nlearning)Data\nscientistAzure\nMachine\nLearning\nPipelinesKubeflow\nPipelinesData ->\nModelDistribution,\ncaching, code-first,\nreuse\nData\norchestrationData\nengineerAzure Data\nFactoryApache\nAirflowData -> Data Strongly typed\nmovement, data-Which Azure pipeline technology should I use?\nﾉExpand table""]",1907
gpt-35-turbo-ft,34,,5.0,,How can you optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training?,v1,"To optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training, you can consider the following best practices:

1. **Datastore Configuration**: Use Azure Blob Storage as a datastore for your Azure Machine Learning workspace. This allows you to efficiently access and manage your data.

2. **Data Preparation**: Preprocess and optimize your data before transferring it to Azure Storage. This can include compression, data partitioning, and using efficient file formats.

3. **Data Transfer Optimization**: Leverage Azure's high-speed data transfer options, such as AzCopy or Azure Data Box, to efficiently move large volumes of data to and from Azure Storage.

4. **Compute and Storage Co-location**: Whenever possible, choose compute resources that are geographically close to your Azure Storage account to minimize data transfer latency.

5. **Network Bandwidth**: Consider the network bandwidth available to your compute resources and optimize data transfer accordingly.

By following these best practices, you can ensure efficient data transfer and storage management for model training in Azure Machine Learning.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps', ""12,000 Mbps) would load the same data in ~33 secs (assuming you saturate the NIC at 1500-MB/s\nthroughput).\nStorage tier : For most scenarios - including Large Language Models (LLM) - standard storage provides\nthe best cost/performance profile. However, if you have many small files , premium  storage offers a\nbetter cost/performance profile. For more information, read Azure S torage options .\nStorage lo ad: If the storage account is under high load - for example, many GPU nodes in a cluster\nrequesting data - then you risk hitting the egress capacity of storage. For more information, read\nStorage load . If you have many small files that need access in parallel, you might hit the request limits\nof storage. R ead up-to-date information on the limits for both egress capacity and storage requests in\nScale targets for standard storage accounts .\nData access p attern in user code : When you use mount mode, data is fetched based on the open/read\nactions in your code. For example, when reading random sections of a large file, the default data\nprefetching settings of mounts can lead to downloads of blocks that won't be read. Y ou might need to\ntune some settings to reach maximum throughput. For more information, read Optimum mount\nsettings for common scenarios .\nTo access the logs of the data runtime from your job:\n1. Select Outputs+Logs  tab from the job page.\n2. Select the system_logs  folder, followed by data_cap ability  folder.\n3. You should see two log files:\nThe log file data-cap ability .log shows the high-level information about the time spent on key data loading\ntasks. For example, when you download data, the runtime logs the download activity start and finish times:\nlog\nIf the download throughput is a fraction of the expected network bandwidth for the VM size, you can inspect\nthe log file rslex.log.<TIMEST AMP> . This file contains all the fine-grain logging from the Rust-based\nruntime; for example, parallelization:\nlogUsing logs to diagnose issues\n\uf80a\nINFO 2023-05-18 17:14:47,790 sdk_logger.py:44 [28] - ActivityStarted, download\nINFO 2023-05-18 17:14:50,295 sdk_logger.py:44 [28] - ActivityCompleted: Activity=download,  \nHowEnded=Success, Duration=2504.39 [ms]"", ""Manage and optimize Azure Machine\nLearning costs\nArticle •08/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.） Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)"", ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage""]",1703
gpt-35-turbo-ft,35,,5.0,,What considerations should be taken into account for data privacy and compliance when integrating Azure Machine Learning with Azure Storage?,v1,"When integrating Azure Machine Learning with Azure Storage, several considerations for data privacy and compliance should be taken into account. These include encryption at rest and in transit, access control, and compliance with industry-specific regulations such as GDPR or HIPAA. Azure Machine Learning provides encryption at rest for data stored in Azure Blob Storage, and it's recommended to use Azure Key Vault for managing customer-managed keys. Additionally, access control mechanisms should be implemented to ensure that only authorized users and services can access the data. Finally, compliance with industry-specific regulations should be ensured by following best practices and leveraging Azure's compliance offerings.","[""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.', 'Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps', 'Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore""]",1537
gpt-35-turbo-ft,36,,5.0,,How can you use managed identities to securely access data in Azure Storage from Azure Machine Learning?,v1,"To securely access data in Azure Storage from Azure Machine Learning using managed identities, you can configure your compute resources to use a managed identity. This allows the compute to access the storage without needing to manage credentials.","['You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity', ""Certain machine learning scenarios involve working with private data. In such cases, data\nscientists may not have direct access to data as Microsoft Entra users. In this scenario,\nthe managed identity of a compute can be used for data access authentication. In this\nscenario, the data can only be accessed from a compute instance or a machine learning\ncompute cluster executing a training job. With this approach, the admin grants the\ncompute instance or compute cluster managed identity S torage Blob Data R eader\npermissions on the storage. The individual data scientists don't need to be granted\naccess.\nTo enable authentication with compute managed identity:\nCreate compute with managed identity enabled. See the compute cluster  section,\nor for compute instance, the Assign managed identity  section.\nGrant compute managed identity at least S torage Blob Data R eader role on the\nstorage account.\nCreate any datastores with identity-based authentication enabled. See Create\ndatastores .\nOnce the identity-based authentication is enabled, the compute managed identity is\nused by default when accessing data within your training jobs. Optionally, you can\nauthenticate with user identity using the steps described in next section.\nFor information on using configuring Azure RBA C for the storage, see role-based access\ncontrols .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nWhen training on Azure Machine Learning compute clusters , you can authenticate to\nstorage with your user Microsoft Entra token.\nThis authentication mode allows you to:７ Note\nThe name of the created system managed identity for compute instance or cluster\nwill be in the format /workspace-name/computes/compute-name in your Microsoft\nEntra ID.\nAccess data for training jobs on compute clusters using\nuser identity"", 'means that in order to successfully read data from storage, the managed identity of the\ncompute cluster where the deployment is running must have at least Storage Blob Data\nReader  access to the storage account.\nTo configure the compute cluster for data access, follow these steps:\n1. Go to Azure Machine Learning studio .\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\ndeployment is using.\n3. Assign a managed identity to the compute cluster:\na. In the Managed identity  section, verify if the compute has a managed identity\nassigned. If not, select the option Edit.\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\nusing a S ystem-Assigned Managed Identity, it is named as ""[workspace\nname]/computes/[compute cluster name]"".\nc. Save the changes.\n4. Go to the Azure portal  and navigate to the associated storage account where the\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\nstorage account where those assets are placed.', 'Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .', 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.']",1301
gpt-35-turbo-ft,37,,5.0,,What are the implications of storage redundancy options in Azure Storage on data availability for Azure Machine Learning experiments?,v1,"The storage redundancy options in Azure Storage can impact the availability and durability of data used in Azure Machine Learning experiments. By choosing the appropriate redundancy level, you can ensure that your data is highly available and resilient to failures.","['setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps', 'Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps', ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", ""Service Usage Example datanumber of nodes; datastore names and tags, and\ndescriptions on assets like models; and data label names.\nAzure AI\nSearchStores indexes that\nhelp with querying\nyour machine learning\ncontent.These indexes are built on top of the data stored in Azure\nCosmos DB.\nAzure\nStorageStores metadata\nrelated to Azure\nMachine Learning\npipeline data.Data can include designer pipeline names, pipeline layout,\nand execution properties.\nFrom the perspective of data lifecycle management, data in the preceding resources is\ncreated and deleted as you create and delete corresponding objects in Azure Machine\nLearning.\nYour Azure Machine Learning workspace reads and writes data by using its managed\nidentity. This identity is granted access to the resources through a role assignment\n(Azure role-based access control) on the data resources. The encryption key that you\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\nused to create indexes for Azure AI Search at runtime.\nExtra networking controls are configured when you create a private link endpoint on\nyour workspace to allow for inbound connectivity. This configuration includes the\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\nNetwork access is restricted to only trusted Microsoft services.\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\nthe data.\nWhen you use a customer-managed key, the resources are in your Azure subscription\nand encrypted with your key. While these resources exist in your subscription, Microsoft\nmanages them. They're automatically created and configured when you create your\nAzure Machine Learning workspace.\nThese Microsoft-managed resources are located in a new Azure resource group that's\ncreated in your subscription. This resource group is separate from the resource group\nfor your workspace. It contains the Microsoft-managed resources that your key is used\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\nresource group name><GUID>.Customer-managed keys"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.']",1494
gpt-35-turbo-ft,38,,5.0,,How do you automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage?,v1,"To automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage, you can leverage Azure's built-in capabilities for data protection and recovery. Azure Blob Storage, which is commonly used for storing datasets in Azure Machine Learning, provides features for data backup and recovery, including versioning, soft delete, and the ability to configure lifecycle management policies to move data to cooler storage tiers or delete it as it ages.

Additionally, you can use Azure Data Factory to create data pipelines that automate the movement and transformation of data between different storage services, including Azure Blob Storage. By defining data-driven workflows, you can orchestrate the backup and recovery processes for your datasets, ensuring that they are protected and available when needed.

Would you like to explore any specific Azure services or configurations for automating backup and recovery further?","[""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps', 'setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps', '2. Select your subscription and the workspace you created.\n3. Select Get star ted.\n4. In the left pane, select Automat ed ML  under the Author  section.\n5. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning dataset. Doing so, allows you to ensure that your\ndata is formatted appropriately for your experiment.\n1. On the Select dataset  form, select From local files  from the +Create dataset  drop-\ndown.\na. On the Basic info  form, give your dataset a name and provide an optional\ndescription. The dataset type should default to Tabular , since automated ML in\nAzure Machine Learning studio currently only supports tabular datasets.\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is the storage location where you upload your data\nfile.\nd. Select Uplo ad files  from the Uplo ad drop-down.\ne. Choose the bike-no.csv  file on your local computer. This is the file you\ndownloaded as a prerequisite .\nf. Select Next\nWhen the upload is complete, the Settings and preview form is pre-populated\nbased on the file type.\ng. Verify that the Settings and pr eview  form is populated as follows and select\nNext .Create and load dataset\nﾉExpand table']",1528
gpt-35-turbo-ft,39,,5.0,,What tools and techniques are available for analyzing storage costs associated with Azure Machine Learning projects?,v1,"Azure Machine Learning provides cost management features to help you analyze and manage costs. You can use the Azure pricing calculator to plan for costs before adding resources, review estimated costs as you add resources, set budgets, and monitor costs. Additionally, you can review forecasted costs and identify spending trends to manage costs effectively.","[""Plan to manage costs for Azure Machine\nLearning\nArticle •03/31/2023\nThis article describes how to plan and manage costs for Azure Machine Learning. First,\nyou use the Azure pricing calculator to help plan for costs before you add any resources.\nNext, as you add the Azure resources, review the estimated costs.\nAfter you've started using Azure Machine Learning resources, use the cost management\nfeatures to set budgets and monitor costs. Also review the forecasted costs and identify\nspending trends to identify areas where you might want to act.\nUnderstand that the costs for Azure Machine Learning are only a portion of the monthly\ncosts in your Azure bill. If you are using other Azure services, you're billed for all the\nAzure services and resources used in your Azure subscription, including the third-party\nservices. This article explains how to plan for and manage costs for Azure Machine\nLearning. After you're familiar with managing costs for Azure Machine Learning, apply\nsimilar methods to manage costs for all the Azure services used in your subscription.\nFor more information on optimizing costs, see how to manage and optimize cost in\nAzure Machine Learning .\nCost analysis in Cost Management supports most Azure account types, but not all of\nthem. T o view the full list of supported account types, see Understand Cost\nManagement data .\nTo view cost data, you need at least read access for an Azure account. For information\nabout assigning access to Azure Cost Management data, see Assign access to data .） Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nPrerequisites"", ""Use the Azure pricing calculator  to estimate costs before you create the\nresources in an Azure Machine Learning workspace. On the left, select AI +\nMachine Learning , then select Azur e Machine Learning  to begin.\nThe following screenshot shows the cost estimation by using the calculator:\nAs you add new resources to your workspace, return to this calculator and add the same\nresource here to update your cost estimates.\nFor more information, see Azure Machine Learning pricing .\nAzure Machine Learning runs on Azure infrastructure that accrues costs along with\nAzure Machine Learning when you deploy the new resource. It's important toEstimate costs before using Azure Machine\nLearning\nUnderstand the full billing  model for Azure\nMachine Learning"", ""Manage and optimize Azure Machine\nLearning costs\nArticle •08/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.） Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)"", 'monthly schedule and set a custom date range. Exporting cost data is the\nrecommended way to retrieve cost datasets.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nDelete instances and clusters if you do not plan on using them in the near future.\nFor more information, see manage and optimize costs in Azure Machine Learning .\nManage and optimize costs in Azure Machine Learning .\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscale\nLearn how to optimize your cloud investment with Azure Cost Management .\nLearn more about managing costs with cost analysis .\nLearn about how to prevent unexpected costs .\nTake the Cost Management  guided learning course.Other ways to manage and reduce costs for\nAzure Machine Learning\nNext steps', ""To view Azure Machine Learning costs in cost analysis:\n1. Sign in to the Azure portal.\n2. Open the scope in the Azure portal and select Cost analysis  in the menu. For\nexample, go to Subscriptions , select a subscription from the list, and then select\nCost analysis  in the menu. Select Scope  to switch to a different scope in cost\nanalysis.\n3. By default, cost for services are shown in the first donut chart. Select the area in the\nchart labeled Azure Machine Learning.\nActual monthly costs are shown when you initially open cost analysis. Here's an example\nshowing all monthly usage costs.\nTo narrow costs for a single service, like Azure Machine Learning, select Add filt er and\nthen select Service name . Then, select virtual machines .\nHere's an example showing costs for just Azure Machine Learning.\n\uf80a""]",1276
gpt-35-turbo-ft,40,,3.0,,How can data engineers streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics?,v1,"To streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics, data engineers can use Azure Data Factory to orchestrate the movement of data from Azure Storage to Azure Machine Learning. Additionally, they can leverage Azure Event Grid to trigger real-time analytics processes in Azure Machine Learning when new data is ingested into Azure Storage.","[""Interactive Data Wrangling with Apache\nSpark in Azure Machine Learning\nArticle •10/05/2023\nData wrangling becomes one of the most important steps in machine learning projects.\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides access\nto an Apache Spark pool - backed by Azure S ynapse - for interactive data wrangling\nusing Azure Machine Learning Notebooks.\nIn this article, you'll learn how to perform data wrangling using\nServerless Spark compute\nAttached S ynapse Spark pool\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an Azure\nData Lake S torage (ADLS) Gen 2 storage account .\n(Optional): An Azure K ey Vault. See Create an Azure K ey Vault.\n(Optional): A Service Principal. See Create a Service Principal .\n(Optional): An attached S ynapse Spark pool in the Azure Machine Learning\nworkspace .\nBefore you start your data wrangling tasks, learn about the process of storing secrets\nAzure Blob storage account access key\nShared Access Signature (SAS) token\nAzure Data Lake S torage (ADLS) Gen 2 service principal information\nin the Azure K ey Vault. Y ou also need to know how to handle role assignments in the\nAzure storage accounts. The following sections review these concepts. Then, we'll\nexplore the details of interactive data wrangling using the Spark pools in Azure Machine\nLearning Notebooks.Prerequisites\n\uea80 Tip"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""runs from experimentation in the studio user interface. Or they can use versioned assets\nfor jobs like environments and storage references.\nFor more information, see Manage Azure Machine Learning workspaces .\nWhen a project is ready for operationalization, users' work can be automated in an ML\npipeline and triggered on a schedule or HT TPS request.\nYou can deploy models to the managed inferencing solution, for both real-time and\nbatch deployments, abstracting away the infrastructure management typically required\nfor deploying models.\nIn Azure Machine Learning, you can run your training script in the cloud or build a\nmodel from scratch. Customers often bring models they've built and trained in open-\nsource frameworks so that they can operationalize them in the cloud.\nData scientists can use models in Azure Machine Learning that they've created in\ncommon Python frameworks, such as:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nLightGBM\nOther languages and frameworks are also supported:\nR\n.NET\nFor more information, see Open-source integration with Azure Machine Learning .\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\nexperience and intuition to select the right data featurization and algorithm for training.\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\nLearning studio UI or the Python SDK.Train models\nOpen and interoperable\nAutomated featurization and algorithm selection"", ""training and evaluation, the machine learning engineers focus on model deployment\nand automation of the entire workflow. By leveraging machine learning pipeline, each\nteam only needs to work on building their own steps. The best way of building steps is\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\none step in a machine learning pipeline. All these steps built by different users are finally\nintegrated into one workflow through the pipeline definition. The pipeline is a\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\nits steps can be standardized by each company's preferred DevOps practice. The\npipeline can be further versioned and automated. If the ML projects are described as a\npipeline, then the best ML Ops practice is already applied.\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\nimproves large model training's efficiency and reduces cost. T aking modern natural\nlanguage model training as an example. It requires pre-processing large amounts of\ndata and GPU intensive transformer model training. It takes hours to days to train a\nmodel each time. When the model is being built, the data scientist wants to test\ndifferent training code or hyperparameters and run the training many times to get the\nbest model performance. For most of these trainings, there's usually small changes from\none training to another one. It will be a significant waste if every time the full training\nfrom data processing to model training takes place. By using machine learning pipeline,\nit can automatically calculate which steps result is unchanged and reuse outputs from\nprevious training. Additionally, the machine learning pipeline supports running each\nstep on different computation resources. Such that, the memory heavy data processing\nwork and run-on high memory CPU machines, and the computation intensive training\ncan run on expensive GPU machines. By properly choosing which step to run on which\ntype of machines, the training cost can be significantly reduced.\nDepending on what a machine learning project already has, the starting point of\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\nbuilding a pipeline.\nThe first approach usually applies to the team that hasn't used pipeline before and\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\ntypically have developed some machine learning models on their local environment\nusing their favorite tools. Machine learning engineers need to take data scientists'\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\nGetting  started best practices"", ""training and evaluation, the machine learning engineers focus on model deployment\nand automation of the entire workflow. By leveraging machine learning pipeline, each\nteam only needs to work on building their own steps. The best way of building steps is\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\none step in a machine learning pipeline. All these steps built by different users are finally\nintegrated into one workflow through the pipeline definition. The pipeline is a\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\nits steps can be standardized by each company's preferred DevOps practice. The\npipeline can be further versioned and automated. If the ML projects are described as a\npipeline, then the best ML Ops practice is already applied.\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\nimproves large model training's efficiency and reduces cost. T aking modern natural\nlanguage model training as an example. It requires pre-processing large amounts of\ndata and GPU intensive transformer model training. It takes hours to days to train a\nmodel each time. When the model is being built, the data scientist wants to test\ndifferent training code or hyperparameters and run the training many times to get the\nbest model performance. For most of these trainings, there's usually small changes from\none training to another one. It will be a significant waste if every time the full training\nfrom data processing to model training takes place. By using machine learning pipeline,\nit can automatically calculate which steps result is unchanged and reuse outputs from\nprevious training. Additionally, the machine learning pipeline supports running each\nstep on different computation resources. Such that, the memory heavy data processing\nwork and run-on high memory CPU machines, and the computation intensive training\ncan run on expensive GPU machines. By properly choosing which step to run on which\ntype of machines, the training cost can be significantly reduced.\nDepending on what a machine learning project already has, the starting point of\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\nbuilding a pipeline.\nThe first approach usually applies to the team that hasn't used pipeline before and\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\ntypically have developed some machine learning models on their local environment\nusing their favorite tools. Machine learning engineers need to take data scientists'\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\nGetting  started best practices""]",1870
gpt-35-turbo-ft,41,,3.0,,How can I using MSI with Storage?,v1,"To use Managed Service Identity (MSI) with Azure Storage, you can assign the necessary permissions to the MSI and then access the storage resources using the MSI's credentials. This allows you to avoid storing credentials in your code or configuration files.","[""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data"", 'Service tag Protocol Port\nStorage.<region> TCP 443\nFor more information, see How to secure training environments  and Configure inbound\nand outbound network traffic .\nUse the following steps to enable a storage endpoint for the subnet that contains your\nAzure Machine Learning compute clusters and compute instances:\n1. From the Azure portal , select the Azur e Vir tual Netw ork for your Azure Machine\nLearning workspace.\n2. From the left of the page, select Subnets  and then select the subnet that contains\nyour compute cluster and compute instance.\n3. In the form that appears, expand the Services  dropdown and then enable\nMicrosoft.Storage . Select Save to save these changes.\n4. Apply the service endpoint policy to your workspace subnet.７ Note\nFor the storage outbound, a Service Endpoint P olicy will be applied in a later\nstep to limit outbound traffic.\n3. Enable storage endpoint for the subnet\n\uf80a', ""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data"", 'b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably.', 'b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably.']",1667
gpt-35-turbo-ft,42,,5.0,,How can I browse data in the AML workspace?,v1,"To browse data in your Azure Machine Learning workspace, you can use the Datastores feature. Datastores are references to Azure Storage services or other supported storage services. You can register datastores to your workspace and then use them to access and manage your data.","[""You'll need the following items:\nTo use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\nRun one or more jobs in your workspace to have results available in the\ndashboard. Complete Tutorial: T rain a model in Azure Machine Learning  if you\ndon't have any jobs yet.\nEnable this preview feature via the preview panel.\nNext, let's view your jobs in the studio:\nSelect Jobs on the left side navigation panel.\nSelect either All experiments  to view all the jobs in an experiment or select All jobs\nto view all the jobs submitted in the workspace.\nYou are now on the default dashboard view where you will find your job list\nconsolidated into the left side bar and dashboard content on the right.Items marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nPrerequisites\nExplore the dashboard view"", ""The following features of Azure Machine Learning studio use data profiling:\nDataset: Explore the dataset from the studio.\nDesigner: Visualize module output data.\nAutoML: View a data preview or profile and choose a target column.\nLabeling: Use labels to prepare data for a machine learning project.\nData profiling depends on the ability of the Azure Machine Learning managed service to\naccess the default Azure storage account for your workspace. The managed service\ndoesn 't exist in y our vir tual netw ork, so it can't directly access the storage account in the\nvirtual network. Instead, the workspace uses a service principal to access storage.\nTo allow access to the storage account, configure the storage account to allow a\nresource instance for your workspace or select Allow Azur e ser vices on the trust edScenario: Use AutoML, the designer, the\ndataset, and the datastore from the studio\n\uea80 Tip\nYou can provide a service principal when you're creating the workspace. If you\ndon't, one is created for you and has the same name as your workspace."", ""You'll need the following items:\nTo use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\nRun one or more jobs in your workspace to have results available in the\ndashboard. Complete Tutorial: T rain a model in Azure Machine Learning  if you\ndon't have any jobs yet.\nEnable this preview feature via the preview panel.\nSelect Jobs on the left side navigation panel.\nSelect either All experiments  to view all the jobs in an experiment or select All jobs\nto view all the jobs submitted in the workspace.\nSelect List view  at the top to switch into List view .\nThe job display name is an optional and customizable name that you can provide for\nyour job. Y ou can edit this directly in your jobs list view by selecting the pencil icon\nwhen you move your mouse over a job name.\nCustomizing the name may help you organize and label your training jobs easily.Prerequisites\nView jobs list\nJob display name\nSelect and pin columns"", 'Azure Machine Learning extension to view, update and create workspace assets like\ncomputes, data, environments, jobs and more.\nWhen you use VS Code for the W eb, the latest versions of these extensions are\nautomatically available to you. If you use the desktop application , you may need to\ninstall them.\nWhen you launch VS Code connected to a compute instance  for the first time, make\nsure you follow these steps and take a few moments to orient yourself to the tools in\nyour integrated development environment.\n1. Locate the Azure extension and sign in\n2. Once your subscriptions are listed, you can filter to the ones you use frequently.\nYou can also pin workspaces you use most often within the subscriptions.\n3. The workspace you launched the VS Code remote connection from (the workspace\nthe compute instance is in) should be automatically set as the default. Y ou can\nupdate the default workspace from the VS Code status bar.\n4. If you plan to use the Azure Machine Learning CLI, open a terminal from the menu,\nand sign in to the Azure Machine Learning CLI using az login --identity.', ""2. Expand your workspace node.\n3. Expand the Envir onments  node.\n4. Right-click the environment you want to view and select View Envir onment .\nAlternatively, use the > Azure ML: View Environment command in the command palette.\nThe quickest way to create a job is by clicking the Create Job  icon in the extension's\nactivity bar.\nUsing the resource nodes in the Azure Machine Learning view:\n1. Expand the subscription node that contains your workspace.\n2. Expand your workspace node.\n3. Right-click the Experiments  node in your workspace and select Create Job .\n4. Choose your job type.\n5. A specification file appears. Configure the specification file.\n6. Right-click the specification file and select Azur eML: Ex ecute YAML .\nAlternatively, use the > Azure ML: Create Job command in the command palette.\nTo view your job in Azure Machine Learning studio:\n1. Expand the subscription node that contains your workspace.\n2. Expand the Experiments  node inside your workspace.\n3. Right-click the experiment you want to view and select View Experiment in S tudio .\n4. A prompt appears asking you to open the experiment URL in Azure Machine\nLearning studio. Select Open .\nAlternatively, use the > Azure ML: View Experiment in Studio command respectively in\nthe command palette.\nAs you're running your job, you may want to see its progress. T o track the progress of a\njob in Azure Machine Learning studio from the extension:\n1. Expand the subscription node that contains your workspace.\n2. Expand the Experiments  node inside your workspace.\n3. Expand the job node you want to track progress for.Create job\nView job\nTrack job progress""]",1333
gpt-35-turbo-ft,43,,5.0,,How to use run spark jobs and integrate with ADLS Gen2?,v1,"To run Spark jobs and integrate with ADLS Gen2 in Azure Machine Learning, you need to follow these steps:

1. **Set Up Prerequisites:**
   - Create an Azure Machine Learning workspace.
   - Create an Azure Data Lake Storage Gen2 (ADLS Gen2) storage account.
   - Create an Azure Machine Learning compute instance.

2. **Ensure Access to Data Paths:**
   - Assign Contributor and Storage Blob Data Contributor roles to the user identity for read and write access.

3. **Submit a Spark Job:**
   - Use Azure Machine Learning serverless Spark compute.
   - Specify the ADLS Gen2 storage account as the data source and destination for the job.

4. **Monitor and Manage Jobs:**
   - Track job status and logs through the Azure Machine Learning workspace.

Let me know if you need more detailed information on any of these steps.","[""Quickstart: Apache Spark jobs in Azure\nMachine Lea rning\nArticle •05/23/2023\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides easy\naccess to distributed computing capability - backed by Azure S ynapse - for scaling\nApache Spark jobs on Azure Machine Learning.\nIn this quickstart guide, you learn how to submit a Spark job using Azure Machine\nLearning serverless Spark compute, Azure Data Lake S torage (ADLS) Gen 2 storage\naccount, and user identity passthrough in a few simple steps.\nFor more information about Apache Sp ark in Azur e Machine Learning  concepts, see\nthis resource .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an\nAzure Data Lake S torage (ADLS) Gen 2 storage account .\nCreate an Azure Machine Learning compute instance .\nInstall Azure Machine Learning CLI .\nBefore we submit an Apache Spark job, we must ensure that input, and output, data\npaths are accessible. Assign Contribut or and Storage Blob Data Contribut or roles to\nthe user identity of the logged-in user to enable read and write access.\nTo assign appropriate roles to the user identity:Prerequisites\nCLI\nAdd role assignments in Azure storage\naccounts"", '16. Repeat steps 2-13 for Storage Blob Contribut or role assignment.\nData in the Azure Data Lake S torage (ADLS) Gen 2 storage account should become\naccessible once the user identity has appropriate roles assigned.\nA Spark job requires a Python script that takes arguments, which can be developed by\nmodifying the Python code developed from interactive data wrangling . A sample Python\nscript is shown here.\nPython\n\uf80a\nCreate parametrized Python code\n# titanic.py  \nimport argparse  \nfrom operator import add \nimport pyspark.pandas as pd \nfrom pyspark.ml.feature import Imputer  \nparser = argparse.ArgumentParser()  \nparser.add_argument( ""--titanic_data"" ) \nparser.add_argument( ""--wrangled_data"" ) \nargs = parser.parse_args()  \nprint(args.wrangled_data)  \nprint(args.titanic_data)  \ndf = pd.read_csv(args.titanic_data, index_col= ""PassengerId"" ) \nimputer = Imputer(inputCols=[ ""Age""], outputCol= ""Age"").setStrategy(\n    ""mean"" \n)  # Replace missing values in Age column with the mean value  \ndf.fillna(  \n    value={ ""Cabin"": ""None""}, inplace= True \n)  # Fill Cabin column with value ""None"" if missing', ""Interactive Data Wrangling with Apache\nSpark in Azure Machine Learning\nArticle •10/05/2023\nData wrangling becomes one of the most important steps in machine learning projects.\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides access\nto an Apache Spark pool - backed by Azure S ynapse - for interactive data wrangling\nusing Azure Machine Learning Notebooks.\nIn this article, you'll learn how to perform data wrangling using\nServerless Spark compute\nAttached S ynapse Spark pool\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an Azure\nData Lake S torage (ADLS) Gen 2 storage account .\n(Optional): An Azure K ey Vault. See Create an Azure K ey Vault.\n(Optional): A Service Principal. See Create a Service Principal .\n(Optional): An attached S ynapse Spark pool in the Azure Machine Learning\nworkspace .\nBefore you start your data wrangling tasks, learn about the process of storing secrets\nAzure Blob storage account access key\nShared Access Signature (SAS) token\nAzure Data Lake S torage (ADLS) Gen 2 service principal information\nin the Azure K ey Vault. Y ou also need to know how to handle role assignments in the\nAzure storage accounts. The following sections review these concepts. Then, we'll\nexplore the details of interactive data wrangling using the Spark pools in Azure Machine\nLearning Notebooks.Prerequisites\n\uea80 Tip"", 'Standard_E4s_v3\nStandard_E8s_v3\nStandard_E16s_v3\nStandard_E32s_v3\nStandard_E64s_v3\n4. Input a Spark Session timeout  value, in minutes.\n5. Select whether to Dynamically allocat e executors\n6. Select the number of Executors for the Spark session.\n7. Select Executor size  from the dropdown menu.\n8. Select Driver size  from the dropdown menu.\n9. To use a Conda file to configure a Spark session, check the Uplo ad conda file\ncheckbox. Then, select Browse , and choose the Conda file with the Spark session\nconfiguration you want.\n10. Add Configuration settings  properties, input values in the Proper ty and Value\ntextboxes, and select Add.\n11. Select Apply .\n12. Select Stop session  in the Configur e new session?  pop-up.\nThe session configuration changes persist and become available to another notebook\nsession that is started using the serverless Spark compute.\nYou can access and wrangle data stored in Azure Data Lake S torage (ADLS) Gen 2\nstorage accounts with abfss:// data URIs following one of the two data access\nmechanisms:\nUser identity passthrough\nService principal-based data access\uea80 Tip\nIf you use session-level Conda packages, you can impr ove the Spark session cold\nstart time if you set the configuration variable spark.hadoop.aml.enable_cache to\ntrue.\nImport and wrangle data from Azure Data Lake Storage\n(ADLS) Gen 2\n\uea80 Tip', 'Install and use the CLI (v2)\nSubmit Spark jobs in Azure Machine Learning    outputs:  \n      wrangled_data:  \n        type:  uri_folder  \n        path:  azureml://datastores/workspaceblobstore/paths/data/wrangled/  \n        mode:  direct \n    identity:  \n      type:  user_identity  \n    compute:  mysparkcompute  \nNext steps']",1531
gpt-35-turbo-ft,44,,5.0,,How does batch inferencing work in parallel mode?,v1,"Batch inferencing in parallel mode involves running inference on multiple mini-batches simultaneously across multiple workers. This can be achieved by distributing the workload across multiple nodes or instances, with each worker processing a different mini-batch. The number of workers and the mini-batch size are key factors to consider when configuring parallel batch inferencing.","[""Your deployment configuration controls the size of each mini-batch and the number of\nworkers on each node. T ake into account them when deciding if you want to read the\nentire mini-batch to perform inference, or if you want to run inference file by file, or row\nby row (for tabular). See Running inference at the mini-batch, file or the row level  to see\nthe different approaches.\nWhen running multiple workers on the same instance, take into account that memory is\nshared across all the workers. Usually, increasing the number of workers per node\nshould be accompanied by a decrease in the mini-batch size or by a change in the\nscoring strategy (if data size and compute SKU remains the same).\nBatch endpoints will call the run() function in your scoring script once per mini-batch.\nHowever, you will have the power to decide if you want to run the inference over the\nentire batch, over one file at a time, or over one row at a time (if your data happens to\nbe tabular).\nYou will typically want to run inference over the batch all at once when you want to\nachieve high throughput in your batch scoring process. This is the case for instance if\nyou run inference over a GPU where you want to achieve saturation of the inference\ndevice. Y ou may also be relying on a data loader that can handle the batching itself if\ndata doesn't fit on memory, like TensorFlow or PyTorch data loaders. On those cases,\nyou may want to consider running inference on the entire batch.\nFor an example about how to achieve it, see High throughput deployments . This\nexample processes an entire batch of files at a time.Running inference at the mini-batch, file or the row level\nMini-batch level\n２ Warning\nRunning inference at the batch level may require having high control over the input\ndata size to be able to correctly account for the memory requirements and avoid\nout of memory exceptions. Whether you are able or not of loading the entire mini-\nbatch in memory will depend on the size of the mini-batch, the size of the instances\nin the cluster, the number of workers on each node, and the size of the mini-batch.\nFile level"", 'One of the easiest ways to perform inference is by iterating over all the files in the mini-\nbatch and run your model over it. In some cases, like image processing, this may be a\ngood idea. If your data is tabular, you may need to make a good estimation about the\nnumber of rows on each file to estimate if your model is able to handle the memory\nrequirements to not just load the entire data into memory but also to perform inference\nover it. R emember that some models (specially those based on recurrent neural\nnetworks) will unfold and present a memory footprint that may not be linear with the\nnumber of rows. If your model is expensive in terms of memory, please consider running\ninference at the row level.\nFor an example about how to achieve it see Image processing with batch deployments .\nThis example processes a file at a time.\nFor models that present challenges in the size of their inputs, you may want to consider\nrunning inference at the row level. Y our batch deployment will still provide your scoring\nscript with a mini-batch of files, however, you will read one file, one row at a time. This\nmay look inefficient but for some deep learning models may be the only way to perform\ninference without scaling up your hardware requirements.\nFor an example about how to achieve it see Text processing with batch deployments .\nThis example processes a row at a time.\nThe environment variable AZUREML_MODEL_DIR contains the path to where the selected\nmodel is located and it is typically used in the init() function to load the model into\nmemory. However, some models may contain their files inside of a folder and you may\nneed to account for that when loading them. Y ou can identify the folder structure of\nyour model as follows:\n1. Go to Azure Machine Learning portal .\n2. Go to the section Models .\uea80 Tip\nIf file sizes are too big to be readed even at once, please consider breaking down\nfiles into multiple smaller files to account for better parallelization.\nRow level (tabular)\nUsing models that are folders', ""Batch scoring, or batch infer encing , involves invoking an endpoint with a reference to\ndata. The batch endpoint runs jobs asynchronously to process data in parallel on\ncompute clusters and store the data for further analysis.\nReal-time s coring, or online infer encing , involves invoking an endpoint with one or more\nmodel deployments and receiving a response in near real time via HT TPS. T raffic can be\nsplit across multiple deployments, allowing for testing new model versions by diverting\nsome amount of traffic initially and increasing after confidence in the new model is\nestablished.\nFor more information, see:\nDeploy a model with a real-time managed endpoint\nUse batch endpoints for scoring\nDevOps for ML models, often called ML Ops, is a process for developing models for\nproduction. A model's lifecycle from training to deployment must be auditable if not\nreproducible.\nLearn more about MLOps in Azure Machine Learning .\nMachine Learning is built with the model lifecycle in mind. Y ou can audit the model\nlifecycle down to a specific commit and environment.Real-time and batch scoring (inferencing)\nMLOps: DevOps for machine learning\nML model lifecycle\nIntegrations enabling MLOPs"", ""There are two types of deployments in batch endpoints:\nModel deployments\nPipeline component deployment\nModel deployment allows operationalizing model inference at scale, processing big\namounts of data in a low latency and asynchronous way. Scalability is automatically\ninstrumented by Azure Machine Learning by providing parallelization of the inferencing\nprocesses across multiple nodes in a compute cluster.\nUse Model deployments  when:\nThe main benefit of this kind of deployments is that you can use the very same assets\ndeployed in the online world (Online Endpoints) but now to run at scale in batch. If your\nmodel requires simple pre or pos processing, you can author an scoring script  that\nperforms the data transformations required.\nTo create a model deployment in a batch endpoint, you need to specify the following\nelements:Model deployments\nYou have expensive models that requires a longer time to run inference.＂\nYou need to perform inference over large amounts of data, distributed in multiple\nfiles.＂\nYou don't have low latency requirements.＂\nYou can take advantage of parallelization.＂"", 'You use the updated settings to save the Y AML file.\nPython\nThe batch inference has these steps:\n1. You use the same built-in feature retrieval component for feature retrieval that you\nused in the training pipeline (covered in the third tutorial). For pipeline training,\nyou provided a feature retrieval specification as a component input. For batch\ninference, you pass the registered model as the input. The component looks for\nthe feature retrieval specification in the model artifact.\nAdditionally, for training, the observation data had the target variable. However,\nthe batch inference observation data doesn\'t have the target variable. The feature\nretrieval step joins the observation data with the features and outputs the data for\nbatch inference.\n2. The pipeline uses the batch inference input data from previous step, runs inference\non the model, and appends the predicted value as output.fs_client.feature_sets.begin_create_or_update(transactions_fset_config)\nprint(fs_poller.result())\n(Optional) Save the YAML file for the feature\nset asset\nPython SDK\n## uncomment and run\n# transactions_fset_config.dump(root_dir +  \n""/featurestore/featuresets/transactions/featureset_asset_offline_enabled\n_with_schedule.yaml"")\nRun the batch inference pipeline\n７ Note\nYou use a job for batch inference in this example. Y ou can also use batch\nendpoints in Azure Machine Learning.']",1715
gpt-35-turbo-ft,45,,3.0,,How to deploy custom vision model to real time endpoint?,v1,"To deploy a Custom Vision model to a real-time endpoint, you can use Azure Machine Learning service. First, register the model, then create an inference configuration and deploy it as a web service.","['Deploy a trained model\nDeploy a pipeline with batch endpoints', 'Deploy a trained model', 'With the trained model registered in the Machine learning workspace, you are ready to\ndeploy the model for scoring.\nThis scenario includes prebuilt workflows for two approaches to deploying a trained\nmodel, batch scoring or a deploying a model to an endpoint for real-time scoring. Y ou\nmay run either or both of these workflows to test the performance of the model in your\nAzure Machine Learning workspace.\n1. In your GitHub project repository (ex: taxi-fare-regression), select Actions\n2. Select the deploy-online-endpoint -pipeline  from the workflows listed on the left\nand click Run w orkflow  to execute the online endpoint deployment pipeline\nworkflow. The steps in this pipeline will create an online endpoint in your Machine\nLearning workspace, create a deployment of your model to this endpoint, then\nallocate traffic to the endpoint.７ Note\nIf you want to check the output of each individual step, for example to view output\nof a failed run, click a job output, and then click each step in the job to view any\noutput of that step.\nDeploying the Trained Model\nOnline Endpoint', ""Alternatively Y ou can deploy the model from the Azure Machine Learning studio UI .\nNavigate to the model you wish to deploy in the Models  tab of the automated ML job\nand select on Deploy  and select Deploy t o real-time endpoint  .\n.\nthis is how your review page looks like. we can select instance type, instance count and\nset traffic percentage for the current deployment.\n.az ml online-endpoint update --name 'od-fridge-items-endpoint'  --traffic  \n'od-fridge-items-mlflow-deploy=100'  --workspace-name  \n[YOUR_AZURE_ WORKSPACE]  --resource-group  [YOUR_AZURE_RESOURCE_ GROUP] --\nsubscription  [YOUR_AZURE_ SUBSCRIPTION]"", 'If you directly deploy real-time endpoint from a previous completed real-time inference\npipeline, it may run into errors.\nRecommendation : clone the inference pipeline and submit it again, then deploy to real-\ntime endpoint.\nTutorial: Build a model in designer to predict auto pricesDeploy real-time endpoint from real-time inference\npipeline issue\nNext steps']",511
gpt-35-turbo-ft-nocontext,1,,3.0,,How do you initiate a new Azure ML experiment using the Azure CLI?,v1,"To initiate a new Azure ML experiment using the Azure CLI, you can use the following command:

```bash
az ml experiment submit -c <compute_target> my_experiment.py
```

Replace `<compute_target>` with the name of your compute target and `my_experiment.py` with the name of your experiment script.",[],78
gpt-35-turbo-ft-nocontext,2,,3.0,,Describe the process of creating a new compute instance via the Azure ML CLI. What parameters are necessary?,v1,"To create a new compute instance via the Azure ML CLI, you can use the `az ml computetarget create` command. The necessary parameters include `--name` for the instance name, `--compute-type` to specify the type of compute (e.g., `AmlCompute`), and `--min-nodes` and `--max-nodes` to define the scaling properties.",[],100
gpt-35-turbo-ft-nocontext,3,,3.0,,Explain how to submit a training job using Python and the AML SDK v2. Include an example of specifying a compute target.,v1,"To submit a training job using Python and the AML SDK v2, you need to define a script run configuration, which includes the training script, environment, and compute target. Here's an example:

```python
from azureml.core import Workspace, Experiment, ScriptRunConfig
from azureml.core.compute import AmlCompute, ComputeTarget
from azureml.core.environment import Environment
from azureml.core.conda_dependencies import CondaDependencies

# Load workspace
ws = Workspace.from_config()

# Define compute target
compute_target = ComputeTarget(workspace=ws, name='mycompute')

# Create a Python environment
env = Environment('myenv')
cd = CondaDependencies.create(pip_packages=['azureml-defaults', 'scikit-learn'])
env.python.conda_dependencies = cd

# Create a script run configuration
src = ScriptRunConfig(source_directory='.',
                      script='train.py',
                      compute_target=compute_target,
                      environment=env)

# Submit the experiment
experiment = Experiment(workspace=ws, name='myexperiment')
run = experiment.submit(config=src)
```

In this example, we load the workspace, define a compute target, create a Python environment, and then create a script run configuration with the specified compute target and environment. Finally, we submit the experiment to the workspace.",[],292
gpt-35-turbo-ft-nocontext,4,,3.0,,What is the command to list all the models registered in an Azure ML Workspace using the Azure CLI?,v1,"To list all the models registered in an Azure ML Workspace using the Azure CLI, you can use the following command:

```bash
az ml model list --workspace <workspace_name> --resource-group <resource_group_name>
```

Replace `<workspace_name>` with the name of your Azure ML Workspace and `<resource_group_name>` with the name of the resource group where the workspace is located.",[],98
gpt-35-turbo-ft-nocontext,5,,3.0,,Provide an example of how to use the AML SDK v2 to stream logs from a running experiment in Python.,v1,"To stream logs from a running experiment using the AML SDK v2 in Python, you can use the `stream` method of the `Run` object. Here's an example:

```python
from azureml.core import Workspace, Experiment

# Load workspace
ws = Workspace.from_config()

# Get a running experiment
experiment = Experiment(workspace=ws, name=""your-experiment-name"")
run = next(run for run in experiment.get_runs() if run.status == ""Running"")

# Stream logs
for line in run.get_streaming_log_lines():
    print(line)
```

Replace `""your-experiment-name""` with the name of your experiment.",[],153
gpt-35-turbo-ft-nocontext,6,,3.0,,"How can you configure an AutoML run for a classification task using the AML SDK v2, including specifying primary metric and training data?",v1,"To configure an AutoML run for a classification task using the AML SDK v2, you can define the primary metric and training data in the configuration. Here's an example of how you can do this:

```python
from azureml.core import Workspace, Dataset
from azureml.train.automl import AutoMLConfig

# Define your workspace
ws = Workspace.from_config()

# Define your training data
dataset = Dataset.get_by_name(ws, name='your_dataset_name')

# Define your AutoML configuration
automl_config = AutoMLConfig(task='classification',
                             primary_metric='accuracy',
                             training_data=dataset,
                             label_column_name='your_label_column_name',
                             iterations=10,
                             max_concurrent_iterations=4)
```

In this example, `task` specifies the type of task, `primary_metric` specifies the metric to optimize, `training_data` specifies the dataset to use for training, and `label_column_name` specifies the column containing the labels.",[],228
gpt-35-turbo-ft-nocontext,7,,3.0,,Describe the steps to retrieve the best model from an AutoML run using the AML SDK v2.,v1,"To retrieve the best model from an AutoML run using the AML SDK v2, you can follow these steps:

1. **Get the Best Model**: Use the `get_output` method to retrieve the best model from the AutoML run.
2. **Download the Model**: Once you have the best model, you can download it to your local environment or deploy it to a target environment for inference.

Would you like more detailed information on any of these steps?",[],114
gpt-35-turbo-ft-nocontext,8,,3.0,,What are the AutoML capabilities for handling imbalanced data in Azure ML?,v1,"Azure AutoML provides several capabilities for handling imbalanced data, such as class balancing, which can be specified as a parameter in the AutoML configuration. Additionally, you can specify the evaluation metric to optimize for, such as AUC weighted by prevalence, to account for imbalanced classes.",[],72
gpt-35-turbo-ft-nocontext,9,,3.0,,Explain how to use AutoML for forecasting time series data. What specific settings should be adjusted for time series problems?,v1,"To use AutoML for time series forecasting, you need to specify the time column and the target column in your dataset. Additionally, you should adjust the forecast horizon, the maximum forecast horizon, and the time column name in the AutoML configuration.",[],73
gpt-35-turbo-ft-nocontext,10,,3.0,,How do you create a custom deep learning environment with specific Python packages using the AML SDK v2?,v1,"To create a custom deep learning environment with specific Python packages using the AML SDK v2, you can define a conda or pip environment file and use it to create a Docker image. Then, you can register this image as a custom environment in Azure Machine Learning. Would you like a detailed example of how to do this?",[],87
gpt-35-turbo-ft-nocontext,11,,1.0,,Describe the process of using Azure ML to train a deep learning model on a GPU compute cluster.,v1,"To train a deep learning model on a GPU compute cluster in Azure ML, you would typically follow these steps:

1. **Create or Attach a Workspace**: You need to have an Azure ML workspace to manage your machine learning resources. You can create a new workspace or attach to an existing one.

2. **Prepare your Training Script**: Write your training script, specifying the deep learning framework (e.g., TensorFlow, PyTorch) and the necessary code to utilize GPU resources.

3. **Define your Environment**: Create an environment specification that includes the necessary dependencies and specifies the GPU resource requirements.

4. **Create a Compute Target**: Define a GPU compute cluster as a compute target in your workspace.

5. **Submit the Training Job**: Submit your training job, specifying the compute target and environment. Azure ML will handle provisioning the necessary resources and running your training script.

6. **Monitor the Job**: Monitor the job's progress and resource utilization through the Azure ML interface or SDK.

7. **Retrieve and Register the Model**: Once training is complete, retrieve the trained model and register it in your workspace for later use.

Would you like more detailed information on any of these steps?",[],255
gpt-35-turbo-ft-nocontext,12,,3.0,,Explain how to deploy a TensorFlow model as a web service in Azure ML. What are the key considerations for deployment?,v1,"To deploy a TensorFlow model as a web service in Azure ML, you can use Azure Machine Learning service's deployment capabilities. Here are the key considerations for deployment:

1. **Model Packaging**: Ensure that your TensorFlow model is properly packaged, including all dependencies and the inference script.

2. **Scalability**: Consider the expected load on your service and choose an appropriate compute target for deployment.

3. **Inference Configuration**: Define the inference configuration, specifying the entry script and any necessary dependencies.

4. **Deployment Target**: Choose the deployment target, such as Azure Container Instances or Azure Kubernetes Service, based on your requirements.

5. **Authentication and Security**: Implement appropriate authentication and security measures to protect your deployed model.

6. **Monitoring and Logging**: Set up monitoring and logging to track the performance and usage of your deployed model.

Let me know if you need more detailed information on any of these considerations.",[],206
gpt-35-turbo-ft-nocontext,13,,3.0,,Provide an example of how to use the AML SDK v2 for distributed deep learning training. What configurations are necessary for multi-node training?,v1,"To perform distributed deep learning training using the AML SDK v2, you would typically define a multi-node configuration, specifying the number of nodes, the compute target, and the necessary environment settings. Here's a simplified example:

```python
from azureml.core import Workspace, Experiment, ScriptRunConfig
from azureml.core.compute import AmlCompute
from azureml.core.compute_target import ComputeTarget
from azureml.core import Environment

# Define your workspace and experiment
workspace = Workspace.get(name='myworkspace', subscription_id='12345678-90ab-cdef-1234-567890abcdef', resource_group='myresourcegroup')
experiment = Experiment(workspace, 'myexperiment')

# Define your compute target
compute_target = ComputeTarget(workspace, ""mycompute"")

# Define your environment
env = Environment.from_existing_conda_environment(name=""myenv"", conda_environment_name=""myenv"")

# Define your script run configuration
src = ScriptRunConfig(source_directory='.',
                      script='train.py',
                      compute_target=compute_target,
                      environment=env)

# Submit the experiment
run = experiment.submit(src)
```

In this example, you would need to define the multi-node configuration within the `AmlCompute` object and ensure that your training script is designed to handle distributed training, typically using a framework like TensorFlow or PyTorch with distributed training support.",[],306
gpt-35-turbo-ft-nocontext,14,,3.0,,How can Azure ML's HyperDrive be used to tune hyperparameters of a deep learning model? Give an example of specifying the search space.,v1,"To use Azure ML's HyperDrive to tune hyperparameters of a deep learning model, you would define a search space for the hyperparameters and specify the configuration for the hyperparameter search. Here's an example of specifying the search space for a deep learning model:

```python
from azureml.train.hyperdrive import choice, uniform

param_space = {
    '--batch-size': choice(16, 32, 64),
    '--learning-rate': uniform(0.01, 0.1)
}
```

This example specifies a search space for the batch size and learning rate hyperparameters.",[],148
gpt-35-turbo-ft-nocontext,15,,3.0,,"What is the command to update an existing model's properties, such as description or tags, using the Azure CLI?",v1,"To update an existing model's properties, such as description or tags, using the Azure CLI, you can use the `az ml model update` command. Here's an example of how to update a model's description:

```bash
az ml model update --name <model_name> --description ""New description""
```

Replace `<model_name>` with the name of your model and modify the command accordingly to update other properties.",[],109
gpt-35-turbo-ft-nocontext,16,,1.0,,How does Azure ML integrate with Azure DevOps for implementing CI/CD pipelines for ML models? Describe the steps involved.,v1,"Azure ML integrates with Azure DevOps to enable continuous integration and continuous deployment (CI/CD) for machine learning models. The steps involved typically include:

1. **Setting up Azure DevOps**: Create a project and configure the necessary pipelines for your ML project.

2. **Connecting to Azure ML**: Use Azure ML's SDK or CLI to connect your ML workspace to the Azure DevOps project.

3. **Defining CI/CD workflows**: Define build and release pipelines that include steps for model training, validation, and deployment.

4. **Versioning and tracking**: Leverage Azure ML's capabilities for model versioning and tracking to ensure consistency and traceability in the CI/CD process.

5. **Monitoring and governance**: Implement monitoring and governance practices to ensure the quality and compliance of deployed models.

Would you like more detailed information on any of these steps?",[],196
gpt-35-turbo-ft-nocontext,17,,3.0,,Explain the process of setting up a real-time inferencing endpoint using a deep learning model in Azure ML. What are the performance tuning options available?,v1,"To set up a real-time inferencing endpoint using a deep learning model in Azure ML, you can follow these general steps:

1. **Model Training**: Train your deep learning model using Azure ML's training capabilities, which can include distributed training, hyperparameter tuning, and model versioning.

2. **Model Registration**: Register the trained model in Azure ML to track its versions and metadata.

3. **Deployment Configuration**: Define the deployment configuration, including the compute target, environment, and inference settings.

4. **Deployment**: Deploy the model as a real-time endpoint, which will create a REST API for making predictions.

Regarding performance tuning options, Azure ML provides several options to optimize the performance of your real-time inferencing endpoint:

- **Scaling**: Configure the endpoint to automatically scale based on the incoming request load.
- **Model Optimization**: Optimize the model for inference by quantization, pruning, or other techniques to reduce its size and improve performance.
- **Hardware Selection**: Choose the appropriate hardware for the deployment, such as CPU, GPU, or FPGA, based on the model's requirements and performance goals.
- **Endpoint Monitoring**: Monitor the endpoint's performance and usage to identify bottlenecks and optimize resource allocation.

These options can be configured during the deployment process or adjusted post-deployment to fine-tune the endpoint's performance.",[],298
gpt-35-turbo-ft-nocontext,18,,3.0,,How can you automate the process of model retraining and deployment using Azure ML Pipelines?,v1,"To automate the process of model retraining and deployment using Azure ML Pipelines, you can define a pipeline that includes the steps for data preparation, model training, model evaluation, and deployment. This pipeline can be triggered based on a schedule, a data change, or any other event. Do you want me to provide a few-shot example of how to define such a pipeline in Azure ML?",[],96
gpt-35-turbo-ft-nocontext,19,,3.0,,Describe the approach to use Azure ML for implementing a recommendation system. What kind of data and algorithms are typically involved?,v1,"To implement a recommendation system with Azure ML, you can use collaborative filtering algorithms such as matrix factorization or deep learning models. The data typically involves user-item interactions, such as ratings or purchase history. You can use Azure ML to train, deploy, and manage these recommendation models at scale.",[],81
gpt-35-turbo-ft-nocontext,20,,3.0,,How can you leverage Azure ML's built-in support for ONNX models for interoperability across different frameworks?,v1,"Azure ML's built-in support for ONNX models allows you to easily deploy and manage ONNX models in Azure ML. You can register ONNX models, deploy them as a web service, and use them in your ML pipelines. This interoperability across different frameworks enables you to leverage the strengths of various frameworks while benefiting from Azure ML's management and deployment capabilities.",[],93
gpt-35-turbo-ft-nocontext,21,,3.0,,What Azure storage services can be integrated with Azure Machine Learning for storing datasets?,v1,"Azure Machine Learning can integrate with various Azure storage services such as Azure Blob Storage, Azure File Storage, and Azure Data Lake Storage for storing datasets.",[],44
gpt-35-turbo-ft-nocontext,22,,3.0,,How do you authenticate Azure Machine Learning with Azure Blob Storage to access datasets?,v1,"To authenticate Azure Machine Learning with Azure Blob Storage, you can use the Azure Machine Learning SDK to create a Datastore object that references your Azure Blob Storage account. This can be done by providing the appropriate credentials or using Azure Managed Identity. Would you like to see a code example for this?",[],73
gpt-35-turbo-ft-nocontext,23,,1.0,,What steps are involved in uploading a dataset to Azure Blob Storage and then accessing it from an Azure Machine Learning experiment?,v1,"To upload a dataset to Azure Blob Storage and access it from an Azure Machine Learning experiment, you would typically follow these steps:

1. **Upload Dataset to Azure Blob Storage:**
   - Use Azure Portal, Azure CLI, or Azure SDK to upload your dataset to Azure Blob Storage.

2. **Register the Dataset with Azure Machine Learning:**
   - Register the dataset with Azure Machine Learning using the Azure ML SDK or Azure Portal.

3. **Reference the Dataset in Your Experiment:**
   - In your experiment script, reference the registered dataset by its name or ID to access it during the experiment run.

Would you like more detailed instructions or code examples for any of these steps?",[],160
gpt-35-turbo-ft-nocontext,24,,3.0,,"Can Azure Machine Learning directly access data stored in Azure Data Lake Storage Gen2? If so, how?",v1,"Yes, Azure Machine Learning can directly access data stored in Azure Data Lake Storage Gen2. You can achieve this by registering the data store as a dataset within your Azure Machine Learning workspace. This allows you to reference the data in your training scripts and pipelines without needing to manually copy or move the data.",[],81
gpt-35-turbo-ft-nocontext,25,,3.0,,How does Azure Machine Learning support versioning of datasets stored in Azure Blob Storage?,v1,"Azure Machine Learning supports versioning of datasets stored in Azure Blob Storage by tracking changes to the dataset and maintaining a version history. This allows you to easily access and compare different versions of the dataset, ensuring reproducibility and traceability in your machine learning experiments.",[],68
gpt-35-turbo-ft-nocontext,26,,3.0,,What is the recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage?,v1,"To handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage, you can use the `Datastore` and `Dataset` concepts. First, create a datastore that references your Azure Blob Storage account, and then create a dataset that references the datastore. This allows you to work with large datasets without needing to download them to your local environment.",[],94
gpt-35-turbo-ft-nocontext,27,,3.0,,How do you use the Azure Machine Learning SDK to programmatically download data from Azure Blob Storage?,v1,"To programmatically download data from Azure Blob Storage using the Azure Machine Learning SDK, you can use the `azureml.core.Datastore` class to access the storage. Here's an example of how you can achieve this:

```python
from azureml.core import Workspace, Datastore

# Load the workspace from the saved configuration
ws = Workspace.from_config()

# Get the default datastore
datastore = ws.get_default_datastore()

# Download data from the datastore to a local directory
datastore.download(target_path='local_directory', prefix='data/')
```

In this example, `Workspace.from_config()` loads the workspace from the saved configuration, and `ws.get_default_datastore()` retrieves the default datastore associated with the workspace. The `download` method is then used to download data from the specified prefix in the datastore to a local directory.",[],189
gpt-35-turbo-ft-nocontext,28,,1.0,,What are the benefits of integrating Azure Data Lake Storage with Azure Machine Learning for big data scenarios?,v1,"Azure Data Lake Storage integration with Azure Machine Learning offers several benefits for big data scenarios, including:
1. **Scalability**: Azure Data Lake Storage provides virtually limitless storage capacity, enabling you to handle large volumes of data for machine learning tasks.
2. **Data Management**: It allows you to manage and organize big data effectively, making it easier to access and process data for machine learning experiments.
3. **Data Versioning**: Integration with Azure Machine Learning enables versioning of data, ensuring reproducibility and traceability of machine learning experiments.
4. **Data Access Control**: Azure Data Lake Storage provides fine-grained access control, ensuring that sensitive big data is securely accessed by authorized users and processes.
5. **Data Processing**: It supports parallel processing and distributed computing, which is essential for handling big data workloads efficiently in machine learning pipelines.

Would you like to know more about any specific aspect?",[],200
gpt-35-turbo-ft-nocontext,29,,1.0,,How can you secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces?,v1,"To secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces, you can use the following methods:

1. **Shared Access Signatures (SAS)**: Generate SAS tokens with specific permissions to control access to your data in Azure Storage.

2. **Azure Key Vault Integration**: Store your storage account keys in Azure Key Vault and grant access to the Azure Machine Learning workspace to retrieve the keys when needed.

3. **Private Endpoints**: Use private endpoints to ensure that data is accessed over a private link from your Azure Machine Learning workspace to your Azure Storage account.

4. **Role-Based Access Control (RBAC)**: Use RBAC to control access to your storage account by assigning appropriate roles to the service principal associated with your Azure Machine Learning workspace.

Would you like more details on any of these methods?",[],185
gpt-35-turbo-ft-nocontext,30,,3.0,,What role do Azure Storage accounts play in deploying Azure Machine Learning models as web services?,v1,"Azure Storage accounts play a crucial role in deploying Azure Machine Learning models as web services by providing a centralized location for storing model artifacts, such as model binaries, dependencies, and configuration files. This allows for efficient model versioning, management, and deployment.",[],67
gpt-35-turbo-ft-nocontext,31,,3.0,,How do you monitor data access and usage when integrating Azure Machine Learning with Azure Storage?,v1,"To monitor data access and usage when integrating Azure Machine Learning with Azure Storage, you can utilize Azure Storage Analytics. This feature provides metrics and logs for monitoring the access and usage of your storage account. Additionally, you can set up alerts based on these metrics to be notified of any unusual activity.",[],75
gpt-35-turbo-ft-nocontext,32,,3.0,,"Can Azure Machine Learning use data stored in Azure File Storage? If so, what are the use cases?",v1,"Yes, Azure Machine Learning can use data stored in Azure File Storage. This is useful for scenarios where you need to share data between multiple compute targets, such as training clusters or inference endpoints. Additionally, it allows you to leverage existing data stored in Azure File Storage without needing to move it to a different location.",[],83
gpt-35-turbo-ft-nocontext,33,,3.0,,What are the best practices for managing input and output data for Azure Machine Learning pipelines using Azure Storage?,v1,"To manage input and output data for Azure Machine Learning pipelines using Azure Storage, you should follow these best practices:

1. **Data Registration**: Register your input and output data with Azure Machine Learning to track and version your datasets.

2. **Datastore Configuration**: Configure your Azure Storage account as a datastore in your Azure Machine Learning workspace to access your data.

3. **Data Movement**: Use Azure Data Factory or Azure Data Share to move data between storage accounts and regions.

4. **Data Access Control**: Secure your data by setting up appropriate access control and permissions on your Azure Storage account.

5. **Data Versioning**: Use versioning for your input and output data to track changes and ensure reproducibility.

6. **Data Preprocessing**: Consider preprocessing your data within your pipeline to ensure consistency and reproducibility.

7. **Data Monitoring**: Monitor your data usage and storage to optimize costs and performance.

Would you like to know more about any specific aspect?",[],214
gpt-35-turbo-ft-nocontext,34,,3.0,,How can you optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training?,v1,"To optimize data transfer between Azure Machine Learning and Azure Storage for efficient model training, you can consider the following approaches:

1. **Datastore Configuration**: Use Azure Blob Storage as a datastore for your workspace. This allows you to mount the storage directly to the compute target, reducing data transfer time.

2. **Data Parallelism**: Split large datasets into smaller chunks and distribute them across multiple nodes for parallel processing. This can reduce the time taken for data transfer and improve training efficiency.

3. **Caching**: Cache frequently accessed data in the compute target's local storage to reduce the need for repeated data transfers.

4. **Network Bandwidth**: Choose compute targets and storage accounts in the same region to minimize network latency and maximize bandwidth.

5. **Data Compression**: Compress data before transferring it to reduce transfer times and costs.

Would you like to explore any of these options in more detail?",[],198
gpt-35-turbo-ft-nocontext,35,,3.0,,What considerations should be taken into account for data privacy and compliance when integrating Azure Machine Learning with Azure Storage?,v1,"When integrating Azure Machine Learning with Azure Storage, it's important to consider data privacy and compliance. Here are a few key considerations:

1. **Data Encryption**: Ensure that data at rest and in transit is encrypted to maintain data privacy and security.

2. **Access Control**: Implement role-based access control (RBAC) to restrict access to data and resources based on user roles and permissions.

3. **Compliance Standards**: Understand and adhere to relevant compliance standards such as GDPR, HIPAA, or industry-specific regulations.

4. **Data Residency**: Be aware of data residency requirements and ensure that data is stored in the appropriate geographic locations to comply with local regulations.

5. **Data Anonymization**: Consider anonymizing or pseudonymizing sensitive data to reduce privacy risks.

6. **Audit Trails**: Implement logging and auditing mechanisms to track data access and usage for compliance and security purposes.

7. **Data Retention Policies**: Define and enforce data retention policies to ensure that data is not stored longer than necessary.

By addressing these considerations, you can ensure that your integration of Azure Machine Learning with Azure Storage complies with data privacy and compliance requirements.",[],252
gpt-35-turbo-ft-nocontext,36,,3.0,,How can you use managed identities to securely access data in Azure Storage from Azure Machine Learning?,v1,"To securely access data in Azure Storage from Azure Machine Learning using managed identities, you can configure your workspace to use a system-assigned managed identity. This identity can then be granted access to the Azure Storage account. When the workspace runs jobs or interacts with data in the storage account, it will use the managed identity to authenticate and access the data securely.",[],88
gpt-35-turbo-ft-nocontext,37,,3.0,,What are the implications of storage redundancy options in Azure Storage on data availability for Azure Machine Learning experiments?,v1,"Azure Storage redundancy options in Azure Storage can impact the availability of data for Azure Machine Learning experiments. For instance, choosing a redundancy option such as Zone-redundant storage (ZRS) or Geo-redundant storage (GRS) can ensure data availability in the event of hardware failures or regional outages. However, it's important to consider the trade-offs in terms of cost and performance when selecting a redundancy option.",[],106
gpt-35-turbo-ft-nocontext,38,,3.0,,How do you automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage?,v1,"To automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage, you can use Azure Data Factory to create a pipeline that copies the datasets to another storage account or a backup location. Additionally, you can leverage Azure's built-in redundancy and backup features for storage accounts to ensure data durability and availability.",[],82
gpt-35-turbo-ft-nocontext,39,,3.0,,What tools and techniques are available for analyzing storage costs associated with Azure Machine Learning projects?,v1,"To analyze storage costs associated with Azure Machine Learning projects, you can use Azure Cost Management + Billing. This service provides tools and techniques for monitoring and analyzing costs, including storage costs, associated with your Azure resources. Additionally, you can use Azure Monitor to track and analyze storage usage and costs.",[],75
gpt-35-turbo-ft-nocontext,40,,3.0,,How can data engineers streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics?,v1,"To streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics, data engineers can use Azure Data Factory to orchestrate the movement of data from Azure Storage to Azure Machine Learning. Additionally, they can leverage Azure Event Grid to trigger the ingestion process in real-time based on events in Azure Storage.",[],84
gpt-35-turbo-ft-nocontext,41,,3.0,,How can I using MSI with Storage?,v1,"To use Managed Service Identity (MSI) with Azure Storage, you can assign the MSI to a role with the necessary permissions on the storage account. This allows your application to access the storage account without needing to manage credentials.",[],53
gpt-35-turbo-ft-nocontext,42,,3.0,,How can I browse data in the AML workspace?,v1,"To browse data in your Azure Machine Learning workspace, you can use the Azure Machine Learning studio, which provides a user interface for managing and exploring your data. Additionally, you can use the Azure Machine Learning SDK to programmatically access and manipulate your data.",[],61
gpt-35-turbo-ft-nocontext,43,,3.0,,How to use run spark jobs and integrate with ADLS Gen2?,v1,"To run Spark jobs and integrate with ADLS Gen2, you can use Azure Databricks, which provides a fully managed Apache Spark environment. You can configure Databricks to access data in ADLS Gen2 by mounting the storage account and then use the mounted path in your Spark jobs.",[],73
gpt-35-turbo-ft-nocontext,44,,3.0,,How does batch inferencing work in parallel mode?,v1,"Batch inferencing in parallel mode involves processing multiple inference requests simultaneously, leveraging the available compute resources efficiently. This is achieved by distributing the inference requests across multiple compute nodes or cores, allowing for faster processing of the requests. Azure Machine Learning supports parallel batch inferencing by scaling out the compute resources based on the workload.",[],72
gpt-35-turbo-ft-nocontext,45,,3.0,,How to deploy custom vision model to real time endpoint?,v1,"To deploy a custom vision model to a real-time endpoint, you can use Azure Machine Learning's deployment capabilities. First, register your model, then create an inference configuration and deploy it as a web service. Would you like a detailed example or specific guidance on any of these steps?",[],67
gpt-4-turbo,1,,5.0,,How do you initiate a new Azure ML experiment using the Azure CLI?,v1,"To initiate a new Azure ML experiment using the Azure CLI, you would typically follow these steps:

1. Ensure you have the Azure CLI installed along with the ML extension. If you haven't installed the ML extension, you can add it using the following command:

   ```
   az extension add -n ml
   ```

2. Set your Azure subscription context and configure defaults for your workspace, resource group, and location:

   ```
   az account set --subscription <subscription-id>
   az configure --defaults workspace=<workspace-name> group=<resource-group-name> location=<location>
   ```

3. Create an experiment by using the `az ml` command. However, as of my knowledge cutoff in early 2023, the Azure CLI does not have a direct command to create an experiment. Instead, you would typically run an experiment script or submit a job that implicitly creates an experiment if it does not already exist.

   To submit a job that would create an experiment if it doesn't exist, you would use something like:

   ```
   az ml job create --file <job-definition-file.yaml> --experiment-name <experiment-name>
   ```

   The job definition file (`job-definition-file.yaml`) would contain the details of the job you want to run, including the compute target, inputs, outputs, and the script to be executed.

   When you submit the job with an experiment name, Azure ML will create a new experiment with that name if it doesn't already exist, and then run the job under that experiment.

Please note that the exact commands and parameters might change with updates to the Azure CLI and Azure Machine Learning service. Always refer to the latest Azure ML documentation or use the `az ml -h` command to get help on the latest syntax and options.","[""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\nAzure CLI\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\nso one cluster can host one or many batch deployments (along with other workloads if\ndesired).\nThis article uses a compute created here named batch-cluster. Adjust as needed and\nreference your compute using azureml:<your-compute-name> or create one as shown.\nAzure CLIaz extension add -n ml \nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>  \naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>  \nCreate compute\nAzure CLI\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \n0 --max-instances  5 \n７ Note"", 'Azure CLI\nIf you\'re not on the compute instance, omit --identity and follow the prompt to\nopen a browser window to authenticate.\n3. Make sure you have the most recent versions of the CLI and the ml extension:\nAzure CLI\n4. If you have multiple Azure subscriptions, set the active subscription to the one\nyou\'re using for your workspace. (Y ou can skip this step if you only have access to\na single subscription.) R eplace <SUBSCRIPTION-NAME> with your subscription name.\nAlso remove the brackets <>.\nAzure CLI\n5. Now use CLI to submit the job. If you\'re doing this on a compute instance in your\nworkspace, you can use environment variables for the workspace name and\nresource group as show in the following code. If you aren\'t on a compute instance,\nreplace these values with your workspace name and resource group.\nAzure CLI\nOnce you\'ve submitted the job, you can check the status and results in studio:\n1. Sign in to Azure Machine Learning studio .\n2. Select your workspace if it isn\'t already loaded.\n3. On the left navigation, select Jobs.\n4. Select the Experiment name  that you used to train your model.\n5. Select the Display name  of the job to view details and artifacts of the job,\nincluding metrics, images, child jobs, outputs, logs, and code used in the job.az login --identity\naz upgrade\naz account set --subscription  ""<SUBSCRIPTION-NAME>""\naz ml job create -f job.yml  --workspace-name  $CI_WORKSPACE  --resource-\ngroup $CI_RESOURCE_GROUP\nRegister model', ""Azure CLI\nIf you don't have it, create a cluster called cpu-cluster by running:\nAzure CLI\nNow, create a pipeline job defined in the pipeline.yml file with the following command.\nThe compute target is referenced in the pipeline.yml file as azureml:cpu-cluster. If your\ncompute target uses a different name, remember to update it in the pipeline.yml file.\nAzure CLI\nYou should receive a JSON dictionary with information about the pipeline job including:\nKey Descr iption\nname The GUID-based name of the job.\nexperiment_name The name under which jobs will be organized in studio.\nservices.Studio.endpoint A URL for monitoring and reviewing the pipeline job.\nstatus The status of the job. This will likely be Preparing at this point.\nOpen the services.Studio.endpoint URL to see a graph visualization of the pipeline.az ml compute list\n７ Note\nSkip this step to use serverless comput e.\naz ml compute create -n cpu-cluster  --type amlcompute --min-instances  0 --\nmax-instances  10\naz ml job create --file pipeline.yml\nﾉExpand table"", ""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nIn this example, we are going to create a deployment that can write directly to the\noutput folder of the batch deployment job. The deployment will use this feature to write\ncustom parquet files.Azure CLI\naz extension add -n ml\n７ Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nCreating a batch deployment with a custom\noutput"", ""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\ninference over new data:\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\n７ Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nSteps""]",1618
gpt-4-turbo,2,,5.0,,Describe the process of creating a new compute instance via the Azure ML CLI. What parameters are necessary?,v1,"To create a new compute instance via the Azure Machine Learning CLI, you would typically follow these steps:

1. **Prerequisites**: Ensure you have the Azure CLI installed and the ML extension (v2) added to it. You should also have an Azure Machine Learning workspace created, and the ""Allow storage account key access"" option enabled in the associated storage account.

2. **Login**: Authenticate with Azure using the `az login` command if you haven't already done so.

3. **Set the workspace**: Set the workspace context for the Azure ML CLI using the `az ml workspace set` command with the appropriate parameters for your workspace.

4. **Create the compute instance**: Use the `az ml compute create` command to create a new compute instance. The necessary parameters for creating a compute instance typically include:

   - `--name` or `-n`: The name of the compute instance.
   - `--type`: The type of compute resource, which should be set to `computeinstance` for creating a compute instance.
   - `--size`: The Virtual Machine size for the compute instance.
   - `--workspace-name` or `-w`: The name of the Azure Machine Learning workspace.
   - `--resource-group` or `-g`: The name of the resource group that contains the Azure Machine Learning workspace.

Optional parameters might include:

   - `--location` or `-l`: The Azure region for the compute instance.
   - `--subnet`: The subnet ID if you want to create the compute instance in a virtual network.
   - `--ssh-public-access`: Whether to enable or disable SSH public access.
   - `--admin-user-ssh-key`: The SSH public key for the admin user if SSH access is enabled.

Here's an example command to create a compute instance:

```sh
az ml compute create --name myComputeInstance --type computeinstance --size Standard_DS3_v2 --workspace-name myWorkspace --resource-group myResourceGroup
```

Please replace `myComputeInstance`, `Standard_DS3_v2`, `myWorkspace`, and `myResourceGroup` with the actual names and sizes you intend to use for your compute instance.

Remember to check the Azure ML CLI documentation for the most up-to-date and detailed information on the parameters and their usage.","['Create an Azure Machine Learning\ncompute instance\nArticle •12/08/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nLearn how to create a compute instance  in your Azure Machine Learning workspace.\nUse a compute instance as your fully configured and managed development\nenvironment in the cloud. For development and testing, you can also use the instance as\na training compute target . A compute instance can run multiple jobs in parallel and has\na job queue. As a development environment, a compute instance can\'t be shared with\nother users in your workspace.\nIn this article, you learn how to create a compute instance. See Manage an Azure\nMachine Learning compute instance  for steps to manage start, stop, restart, delete a\ncompute instance.\nYou can also use a setup script  to create the compute instance with your own custom\nenvironment.\nCompute instances can run jobs securely in a virtual network environment , without\nrequiring enterprises to open up SSH ports. The job executes in a containerized\nenvironment and packages your model dependencies in a Docker container.\nAn Azure Machine Learning workspace. For more information, see Create an Azure\nMachine Learning workspace . In the storage account, the ""Allow storage account\nkey access"" option must be enabled for compute instance creation to be\nsuccessful.\nChoose the tab for the environment you\'re using for other prerequisites.\n７ Note\nThis article uses CLI v2 in some examples. If you are still using CLI v1, see Create an\nAzur e Machine Learning comput e clust er CLI v1) .\nPrerequisites', ""For more information on creating a compute cluster and compute cluster, including how\nto do so with Python and the CLI, see the following articles:\nCreate a compute cluster\nCreate a compute instance\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nWhen Azure Container R egistry is behind the virtual network, Azure Machine Learning\ncan't use it to directly build Docker images (used for training and deployment). Instead,\nconfigure the workspace to use the compute cluster you created earlier. Use the\nfollowing steps to create a compute cluster and configure the workspace to use it to\nbuild images:\n1. Navigate to https://shell.azure.com/  to open the Azure Cloud Shell.\n2. From the Cloud Shell, use the following command to install the 2.0 CLI for Azure\nMachine Learning:\nAzure CLIAllow inbound T CP traffic on ports 29876-29877 from the\nBatchNodeManagement service tag.\nAllow inbound T CP traffic on port 44224 from the AzureMachineLearning\nservice tag.\nThe following screenshot shows an example of these rules:\nConfigure image builds\naz extension add -n ml"", ""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\nAzure CLI\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\nso one cluster can host one or many batch deployments (along with other workloads if\ndesired).\nThis article uses a compute created here named batch-cluster. Adjust as needed and\nreference your compute using azureml:<your-compute-name> or create one as shown.\nAzure CLIaz extension add -n ml \nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>  \naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>  \nCreate compute\nAzure CLI\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \n0 --max-instances  5 \n７ Note"", ""To create a workspace using CLI v2, use the following command:\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nBash\nFor more information, see workspace Y AML schema .\nA compute is a designated compute resource where you run your job or host your\nendpoint. Azure Machine Learning supports the following types of compute:\nComput e clust er - a managed-compute infrastructure that allows you to easily\ncreate a cluster of CPU or GPU compute nodes in the cloud.\nComput e instance  - a fully configured and managed development environment in\nthe cloud. Y ou can use the instance as a training or inference compute for\ndevelopment and testing. It's similar to a virtual machine on the cloud.\nInfer ence clust er - used to deploy trained machine learning models to Azure\nKubernetes Service. Y ou can create an Azure Kubernetes Service (AKS) cluster from\nyour Azure Machine Learning workspace, or attach an existing AKS cluster.\nAttached comput e - You can attach your own compute resources to your\nworkspace and use them for training and inference.\nTo create a compute using CLI v2, use the following command:\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)Azure CLI\naz ml workspace create --file my_workspace.yml  \nCompute\n７ Note\nInstead of creating a compute cluster, use serverless comput e (preview) to\noffload compute lifecycle management to Azure Machine Learning.\nAzure CLI"", 'Manage an Azure Machine Learning\ncompute instance\nArticle •07/06/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nLearn how to manage a compute instance  in your Azure Machine Learning workspace.\nUse a compute instance as your fully configured and managed development\nenvironment in the cloud. For development and testing, you can also use the instance as\na training compute target . A compute instance can run multiple jobs in parallel and has\na job queue. As a development environment, a compute instance can\'t be shared with\nother users in your workspace.\nIn this article, you learn how to start, stop, restart, delete) a compute instance. See\nCreate an Azure Machine Learning compute instance  to learn how to create a compute\ninstance.\nAn Azure Machine Learning workspace. For more information, see Create an Azure\nMachine Learning workspace . In the storage account, the ""Allow storage account\nkey access"" option must be enabled for compute instance creation to be\nsuccessful.\nThe Azure CLI extension for Machine Learning service (v2) , Azure Machine\nLearning Python SDK (v2) , or the Azure Machine Learning Visual S tudio Code\nextension .\nIf using the Python SDK, set up your development environment with a workspace .\nOnce your environment is set up, attach to the workspace in your Python script:\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\n７ Note\nThis article shows CLI v2 in the sections below. If you are still using CLI v1, see\nCreate an Azur e Machine Learning comput e clust er CLI v1) .\nPrerequisites']",1920
gpt-4-turbo,3,,5.0,,Explain how to submit a training job using Python and the AML SDK v2. Include an example of specifying a compute target.,v1,"To submit a training job using Python and the Azure Machine Learning (AML) SDK v2, you need to follow these steps:

1. **Set up your environment**:
   - Install the Azure Machine Learning SDK v2 if you haven't already.
   - Import the necessary modules and classes from the SDK.
   - Create or load an existing Azure Machine Learning workspace.

2. **Create or attach a compute target**:
   - Define the compute target where the training job will run. This could be an Azure Machine Learning Compute Instance, Compute Cluster, or other supported compute targets.

3. **Define the training script**:
   - Write a Python script that contains the code for training your model. Save this script in a directory that will be uploaded with your job.

4. **Create a command job**:
   - Use the `Command` class to define the command job, specifying the script to run, the compute target, and any other necessary parameters such as environment and inputs/outputs.

5. **Submit the job**:
   - Use the `Job` class to submit the command job to the workspace.

Here's an example of how you might specify a compute target and submit a training job using the AML SDK v2:

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Workspace, Compute, Command, Environment
from azure.identity import DefaultAzureCredential

# Load the workspace
ws = Workspace.from_config()

# Create MLClient using the workspace and DefaultAzureCredential
ml_client = MLClient.from_config(credential=DefaultAzureCredential(), workspace=ws)

# Define the compute target
compute_name = ""your-compute-cluster""
compute = Compute(name=compute_name, type=""amlcompute"")
compute.size = ""Standard_DS3_v2""
compute.min_instances = 0
compute.max_instances = 4

# Register or get the existing compute target
try:
    # Try to get the existing compute target by name
    compute_target = ml_client.compute.get(compute_name)
except Exception:
    # If it doesn't exist, create it
    compute_target = ml_client.compute.create(compute)

# Define the environment for the job
environment = Environment(
    image=""mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04"",
    conda_file=""path_to_your_conda_file.yml"",
    name=""my-environment"",
    version=""1""
)

# Create the command job
command_job = Command(
    code=""path_to_your_training_script_folder"",
    command=""python train.py"",
    environment=environment.name,
    compute=compute_name,
    distribution={""type"": ""mpi"", ""process_count_per_instance"": 2},
    resources={'instance_count': 2},
    environment_variables={""EXAMPLE_ENV_VAR"": ""example_value""}
)

# Submit the job
returned_job = ml_client.jobs.create_or_update(command_job)

# Get the status of the job
job_status = ml_client.jobs.get(returned_job.name)
print(f""Job status: {job_status.status}"")
```

In this example, replace `""your-compute-cluster""`, `""path_to_your_conda_file.yml""`, `""path_to_your_training_script_folder""`, and `""train.py""` with your actual compute cluster name, conda environment file path, training script directory, and training script filename, respectively.

Please note that the code above is a simplified example and assumes that you have already set up your Azure Machine Learning workspace and have the necessary configurations in place. The actual implementation may vary based on your specific requirements and Azure setup.","[""1. On the top bar above your opened notebook, create a compute instance if you\ndon't already have one.\n2. If the compute instance is stopped, select Start comput e and wait until it is\nrunning.\n3. Make sure that the kernel, found on the top right, is Python 3.10 - SDK v2. If not,\nuse the dropdown to select this kernel.\n4. If you see a banner that says you need to be authenticated, select Authenticat e.\nTo train a model, you need to submit a job. The type of job you'll submit in this tutorial\nis a command job . Azure Machine Learning offers several different types of jobs to train\nmodels. Users can select their method of training based on complexity of the model,\ndata size, and training speed requirements. In this tutorial, you'll learn how to submit a\ncommand job  to run a training s cript.\nA command job is a function that allows you to submit a custom training script to train\nyour model. This can also be defined as a custom training job. A command job in Azure\nMachine Learning is a type of job that runs a script or command in a specified\nenvironment. Y ou can use command jobs to train models, process data, or any other\ncustom code you want to execute in the cloud.Set your kernel\n\uf80a\n\uf80a\n\uf80a\n） Impor tant\nThe rest of this tutorial contains cells of the tutorial notebook. Copy/paste them\ninto your new notebook, or switch to the notebook now if you cloned it.\nUse a command job to train a model in Azure\nMachine Learning"", 'For detailed instructions on how to complete these steps, see Secure a training\nenvironment .\nIn this section, you learn how Azure Machine Learning securely communicates between\nservices to submit a training job. This example shows you how all your configurations\nwork together to secure communication.\n1. The client uploads training scripts and training data to storage accounts that are\nsecured with a service or private endpoint.\n2. The client submits a training job to the Azure Machine Learning workspace\nthrough the private endpoint.\n3. Azure Batch service receives the job from the workspace. It then submits the\ntraining job to the compute environment through the public load balancer for the\ncompute resource.\n4. The compute resource receives the job and begins training. The compute resource\nuses information stored in key vault to access storage accounts to download\ntraining files and upload output.Example training job submission', 'Python\nView more examples of training with serverless compute at:-\nQuick S tart\nTrain Model\nThere\'s no need to specify compute for AutoML jobs. R esources can be optionally\nspecified. If instance count isn\'t specified, then it\'s defaulted based on\nmax_concurrent_trials and max_nodes parameters. If you submit an AutoML image\nclassification or NLP task with no instance type, the GPU VM size is automatically\nselected. It\'s possible to submit AutoML job through CLIs, SDK, or S tudio. T o submit\nAutoML jobs with serverless compute in studio first enable the submit a training job in\nstudio (preview)  feature in the preview panel.Python SDK\nfrom azure.ai.ml import command\nfrom azure.ai.ml import MLClient      # Handle to the workspace\nfrom azure.identity import DefaultAzureCredential     # Authentication  \npackage\nfrom azure.ai.ml.entities import ResourceConfiguration\nfrom azure.ai.ml.entities import UserIdentityConfiguration \ncredential = DefaultAzureCredential()\n# Get a handle to the workspace. You can find the info on the workspace  \ntab on ml.azure.com\nml_client = MLClient(\n    credential=credential,\n    subscription_id= ""<Azure subscription id>"" , \n    resource_group_name= ""<Azure resource group>"" ,\n    workspace_name= ""<Azure Machine Learning Workspace>"" ,\n)\njob = command(\n    command= ""echo \'hello world\'"" ,\n    environment= ""AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest"" ,\n         identity=UserIdentityConfiguration(),\n    queue_settings={\n      ""job_tier"" : ""Standard""   \n    }\n)\njob.resources = ResourceConfiguration(instance_type= ""Standard_E4s_v3"" , \ninstance_count= 1)\n# submit the command job\nml_client.create_or_update(job)\nAutoML job', ""Designer : Azure Machine Learning designer provides an easy entry-point into\nmachine learning for building proof of concepts, or for users with little coding\nexperience. It allows you to train models using a drag and drop web-based UI. Y ou\ncan use Python code as part of the design, or train models without writing any\ncode.\nAzur e CLI : The machine learning CLI provides commands for common tasks with\nAzure Machine Learning, and is often used for scripting and aut omating tasks . For\nexample, once you've created a training script or pipeline, you might use the Azure\nCLI to start a training job on a schedule or when the data files used for training are\nupdated. For training models, it provides commands that submit training jobs. It\ncan submit jobs using run configurations or pipelines.\nEach of these training methods can use different types of compute resources for\ntraining. Collectively, these resources are referred to as comput e targets. A compute\ntarget can be a local machine or a cloud resource, such as an Azure Machine Learning\nCompute, Azure HDInsight, or a remote virtual machine.\nThe Azure Machine Learning SDK for Python allows you to build and run machine\nlearning workflows with Azure Machine Learning. Y ou can interact with the service from\nan interactive Python session, Jupyter Notebooks, Visual S tudio Code, or other IDE.\nInstall/update the SDK\nConfigure a development environment for Azure Machine Learning\nA generic training job with Azure Machine Learning can be defined using the\ncommand() . The command is then used, along with your training script(s) to train a\nmodel on the specified compute target.\nYou may start with a command for your local computer, and then switch to one for a\ncloud-based compute target as needed. When changing the compute target, you only\nchange the compute parameter in the command that you use. A run also logs\ninformation about the training job, such as the inputs, outputs, and logs.\nTutorial: T rain your first ML model\nExamples: Jupyter Notebook and Python examples of training modelsPython SDK\nSubmit a command\nAutomated Machine Learning"", 'An Azure Machine Learning compute cluster is a fully managed compute resource that\ncan be used to run the training job. In the following examples, a compute cluster named\ncpu-compute is created.\nPython\nTo run this script, you\'ll use a command that executes main.py Python script located\nunder ./sdk/python/jobs/single-step/lightgbm/iris/src/. The command will be runworkspace = \'<AZUREML_WORKSPACE_NAME>\'\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \nresource_group, workspace)\n2. Create a compute resource for training\n７ Note\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\ntraining job .\nPython SDK\nfrom azure.ai.ml.entities import AmlCompute\n# specify aml compute name.\ncpu_compute_target = ""cpu-cluster""\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(""Creating a new cpu compute target..."" )\n    compute = AmlCompute(\n        name=cpu_compute_target, size= ""STANDARD_D2_V2"" , min_instances= 0, \nmax_instances= 4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n4. Submit the training job\nPython SDK']",2348
gpt-4-turbo,4,,5.0,,What is the command to list all the models registered in an Azure ML Workspace using the Azure CLI?,v1,"The command to list all the models registered in an Azure ML Workspace using the Azure CLI is:

```
az ml model list
```","['then select Regist er.\nThe SDK and CLI (v2) also allow you to manage the lifecycle of your Azure Machine\nLearning model assets.\nList all the models in your workspace:\ncli\nList all the model versions under a given name:\ncli\n\uf80a\nManage models\nList\nAzure CLI\naz ml model list  \nAzure CLI\naz ml model list --name run-model-example', 'Install the Azure CLI and the ml extension to the Azure CLI. For more\ninformation on installing the CLI, see Install and set up the CLI (v2) .\nFirst, connect to the Azure Machine Learning workspace where you\'ll work.\nAzure CLI\nYou can deploy only registered models to online endpoints. In this case, you already\nhave a local copy of the model in the repository, so you only need to publish the model\nto the registry in the workspace. Y ou can skip this step if the model you\'re trying to\ndeploy is already registered.\nAzure CLI\nIf your model was logged inside of a run, you can register it directly.\nTo register the model, you need to know the location where it is stored. If you\'re using\nMLflow\'s autolog feature, the path to the model depends on the model type and\nframework. Y ou should check the jobs output to identify the name of the model\'s folder.\nThis folder contains a file named MLModel.Connect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nRegister the model\nAzure CLI\nMODEL_NAME= \'sklearn-diabetes\'\naz ml model create --name $MODEL_NAME  --type ""mlflow_model""  --path \n""endpoints/online/ncd/sklearn-diabetes/model""\nWhat if your model was logged inside of a run?', 'Install the Azure CLI and the ml extension to the Azure CLI. For more\ninformation on installing the CLI, see Install and set up the CLI (v2) .\nFirst, connect to the Azure Machine Learning workspace where you\'ll work.\nAzure CLI\nYou can deploy only registered models to online endpoints. In this case, you already\nhave a local copy of the model in the repository, so you only need to publish the model\nto the registry in the workspace. Y ou can skip this step if the model you\'re trying to\ndeploy is already registered.\nAzure CLI\nIf your model was logged inside of a run, you can register it directly.\nTo register the model, you need to know the location where it is stored. If you\'re using\nMLflow\'s autolog feature, the path to the model depends on the model type and\nframework. Y ou should check the jobs output to identify the name of the model\'s folder.\nThis folder contains a file named MLModel.Connect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nRegister the model\nAzure CLI\nMODEL_NAME= \'sklearn-diabetes\'\naz ml model create --name $MODEL_NAME  --type ""mlflow_model""  --path \n""endpoints/online/ncd/sklearn-diabetes/model""\nWhat if your model was logged inside of a run?', 'The az ml model command can be used for managing Azure Machine Learning models.\nExamples are available in the examples GitHub repository . Several are shown below.\nYAML\nYAML\nInstall and use the CLI (v2)Remarks\nExamples\nYAML: local file\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-file-example  \npath: mlflow-model/model.pkl  \ndescription:  Model created from local file. \nYAML: local folder in MLflow format\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-mlflow-example  \npath: mlflow-model  \ntype: mlflow_model  \ndescription:  Model created from local MLflow model directory.', ""The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\ninference over new data:\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\n７ Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nSteps""]",1129
gpt-4-turbo,5,,5.0,,Provide an example of how to use the AML SDK v2 to stream logs from a running experiment in Python.,v1,"To stream logs from a running experiment using Azure Machine Learning SDK v2 in Python, you can use the `stream` method of the `Job` object. Here's an example of how you might do this:

```python
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the workspace
ml_client = MLClient.from_config(DefaultAzureCredential())

# Assuming you have the job name or ID of the running experiment
job_name = ""your_job_name_here""

# Get the job object
job = ml_client.jobs.get(name=job_name)

# Stream the logs
ml_client.jobs.stream(name=job_name)
```

In this example, replace `""your_job_name_here""` with the actual name or ID of the job you want to stream logs from. The `stream` method will continuously output the logs to your console until the job completes or fails. If you want to stop streaming the logs, you can simply interrupt the process (e.g., by pressing Ctrl+C in the console).","['SDK v2\nPython\nFunctionality in SDK v1 Rough mapping in SDK v2\nexperiment.submit MLCLient.jobs.create_or_update\nScriptRunConfig() command()\nFor more information, see:\nV1 - Experiment\nV2 - Command Jobaml_url = run.get_portal_url()  \nprint(aml_url)  \n#import required libraries  \nfrom azure.ai.ml import MLClient, command  \nfrom azure.ai.ml.entities import Environment  \nfrom azure.identity import DefaultAzureCredential  \n#connect to the workspace  \nml_client = MLClient.from_config(DefaultAzureCredential())  \n# set up pytorch environment  \nenv = Environment(  \n    image= ""mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04"" , \n    conda_file= ""pytorch-env.yml"" , \n    name= ""pytorch-env""  \n) \n# define the command  \ncommand_job = command(  \n    code= ""./src"", \n    command= ""train.py"" , \n    environment=env,  \n    compute= ""cpu-cluster"" , \n) \nreturned_job = ml_client.jobs.create_or_update(command_job)  \nreturned_job  \nMapping of key functionality in v1 and v2\nNext steps', ""Explore Azu re Machine Learning with\nJupyter Notebooks\nArticle •02/06/2024\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nThe AzureML-Examples  repository includes the latest (v2) Azure Machine Learning\nPython CLI and SDK samples. For information on the various example types, see the\nreadme .\nThis article shows you how to access the repository from the following environments:\nAzure Machine Learning compute instance\nYour own compute resource\nData Science Virtual Machine\nThe easiest way to get started with the samples is to complete the Create resources to\nget started . Once completed, you'll have a dedicated notebook server preloaded with\nthe SDK and the Azure Machine Learning Notebooks repository. No downloads or\ninstallation necessary.\nTo view example notebooks:\n1. Sign in to studio  and select your workspace if necessary.\n2. Select Notebooks .\n3. Select the Samples  tab. Use the SDK v2  folder for examples using Python SDK v2.\n4. Open the notebook you want to run. Select Clone this not ebook  to create a copy\nin your workspace file share. This action will copy the notebook along with any\ndependent resources.\nIf you'd like to bring your own notebook server for local development, follow these\nsteps on your computer.\n1. Use the instructions at Azure Machine Learning SDK  to install the Azure Machine\nLearning SDK (v2) for Python\nOption 1: Access on Azure Machine Learning\ncompute instance (recommended)\nOption 2: Access on your own notebook server"", ""Query & co mpare experiments and runs\nwith MLflow\nArticle •06/26/2023\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\nYou don't need to install any specific SDK to manage what happens inside of a training\njob, creating a more seamless transition between local runs and the cloud by removing\ncloud-specific dependencies. In this article, you'll learn how to query and compare\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\nin Python.\nMLflow allows you to:\nCreate, query, delete and search for experiments in a workspace.\nQuery, delete, and search for runs in a workspace.\nTrack and retrieve metrics, parameters, artifacts and models from runs.\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\ndetailed comparison between MLflow Open-Source and MLflow when connected to\nAzure Machine Learning.\nQuery and searching experiments and runs is also available using the MLflow REST API.\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\nconsume it.\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\nMLflow azureml-mlflow.７ Note\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\ntracking capabilities. This applies not just for logging but also for querying the\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\nexplains how to use MLflow to manage experiments and runs in Azure Machine\nLearning.\nREST API\nPrerequisites"", ""Query & co mpare experiments and runs\nwith MLflow\nArticle •06/26/2023\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\nYou don't need to install any specific SDK to manage what happens inside of a training\njob, creating a more seamless transition between local runs and the cloud by removing\ncloud-specific dependencies. In this article, you'll learn how to query and compare\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\nin Python.\nMLflow allows you to:\nCreate, query, delete and search for experiments in a workspace.\nQuery, delete, and search for runs in a workspace.\nTrack and retrieve metrics, parameters, artifacts and models from runs.\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\ndetailed comparison between MLflow Open-Source and MLflow when connected to\nAzure Machine Learning.\nQuery and searching experiments and runs is also available using the MLflow REST API.\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\nconsume it.\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\nMLflow azureml-mlflow.７ Note\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\ntracking capabilities. This applies not just for logging but also for querying the\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\nexplains how to use MLflow to manage experiments and runs in Azure Machine\nLearning.\nREST API\nPrerequisites"", 'SDK v2. Full sample link\nPython\'--learning_rate\' , 0.1,\'--model_output\' , model_output],\n    source_directory=script_folder,\n    compute_target=aml_compute,\n    allow_reuse= True,\n)\nscore_output = OutputFileDatasetConfig( \'score_output\' )\nscore_step = PythonScriptStep(\n    name=""score step"" ,\n    script_name= ""score.py"" ,\n    arguments=[ \'--model_input\' ,model_output.as_input( \'model_input\' ), \'-\n-test_data\' , input_ds.as_named_input( \'test_data\' ).as_mount(), \'--\nscore_output\' , score_output],\n    source_directory=script_folder,\n    compute_target=aml_compute,\n    allow_reuse= True,\n)\neval_output = OutputFileDatasetConfig( \'eval_output\' )\neval_step = PythonScriptStep(\n    name=""eval step"" ,\n    script_name= ""eval.py"" ,\n    arguments=[ \'--\nscoring_result\' ,score_output.as_input( \'scoring_result\' ), \'--\neval_output\' , eval_output],\n    source_directory=script_folder,\n    compute_target=aml_compute,\n    allow_reuse= True,\n)\n# built pipeline\nfrom azureml.pipeline.core import Pipeline\npipeline_steps = [train_step, score_step, eval_step]\npipeline = Pipeline(workspace = workspace, steps=pipeline_steps)\nprint(""Pipeline is built."" )\npipeline_run = experiment.submit(pipeline, regenerate_outputs= False)\nprint(""Pipeline submitted for execution."" )\n# import required libraries\nfrom azure.identity import DefaultAzureCredential,  \nInteractiveBrowserCredential\nfrom azure.ai.ml import MLClient, Input\nfrom azure.ai.ml.dsl import pipeline']",1795
gpt-4-turbo,6,,5.0,,"How can you configure an AutoML run for a classification task using the AML SDK v2, including specifying primary metric and training data?",v1,"To configure an AutoML run for a classification task using the Azure Machine Learning (AML) SDK v2, you can follow these steps:

1. **Import necessary modules** from the AML SDK v2:
   ```python
   from azure.ai.ml import automl, Input
   from azure.ai.ml.constants import AssetTypes
   from azure.ai.ml.automl import classification
   ```

2. **Create an MLTable** for your training dataset:
   ```python
   my_training_data_input = Input(
       type=AssetTypes.MLTABLE, 
       path=""./data/training-mltable-folder""
   )
   ```

3. **Create the AutoML classification job** using the `automl.classification` factory function:
   ```python
   classification_job = automl.classification(
       compute=""<compute_name>"", 
       experiment_name=""<exp_name>"", 
       training_data=my_training_data_input, 
       target_column_name=""<name_of_target_column>"", 
       primary_metric=""accuracy"", 
       n_cross_validations=5, 
       enable_model_explainability=True, 
       tags={""my_custom_tag"": ""My custom value""}
   )
   ```

   - `compute` specifies the compute target to run the AutoML job.
   - `experiment_name` is the name of the experiment under which the run will be logged.
   - `training_data` is the input MLTable containing the training data.
   - `target_column_name` is the name of the column in your training data that contains the labels for the classification task.
   - `primary_metric` is the metric that AutoML will optimize for during the model selection process. In this case, it's set to ""accuracy"".
   - `n_cross_validations` specifies the number of cross-validation splits to be used for model validation.
   - `enable_model_explainability` indicates whether to enable explainability features for the models.
   - `tags` allows you to add custom tags to the AutoML run for tracking purposes.

4. **Optionally set limits** for the AutoML job:
   ```python
   classification_job.set_limits(
       timeout_minutes=600,
       trial_timeout_minutes=20,
       max_trials=5,
       max_concurrent_trials=4
   )
   ```

   - `timeout_minutes` is the maximum amount of time in minutes that all trials can take before the job is terminated.
   - `trial_timeout_minutes` is the maximum time in minutes that each trial can run.
   - `max_trials` is the maximum number of trials that AutoML will test in the experiment.
   - `max_concurrent_trials` is the maximum number of trials that will be executed in parallel.

5. **Submit the AutoML run** to the experiment:
   ```python
   from azure.ai.ml import MLClient
   # Assuming you have already created an MLClient instance
   remote_run = ml_client.create_or_update(classification_job)
   ```

6. **Retrieve the URL** for monitoring the AutoML run in Azure Machine Learning Studio:
   ```python
   azureml_url = remote_run.get_portal_url()
   print(azureml_url)
   ```

Make sure to replace placeholder values like `<compute_name>`, `<exp_name>`, and `<name_of_target_column>` with your actual compute target name, experiment name, and target column name, respectively. Also, ensure that the path to the training data is correct.","['SDK v2: Below is a sample AutoML classification task. For the entire code, check\nout our examples repo .\nPython    debug_log= ""automl_errors.log"" , \n    compute_target=compute_target,  \n    training_data=training_data,  \n    label_column_name=label_column_name,  \n    **automl_settings,  \n) \n# Submit run  \nremote_run = experiment.submit(automl_config, show_output= False) \nazureml_url = remote_run.get_portal_url()  \nprint(azureml_url)  \n# Imports  \nfrom azure.ai.ml import automl, Input, MLClient  \nfrom azure.ai.ml.constants import AssetTypes  \nfrom azure.ai.ml.automl import ( \n    classification,  \n    ClassificationPrimaryMetrics,\n    ClassificationModels,  \n) \n# Create MLTables for training dataset  \n# Note that AutoML Job can also take in tabular data  \nmy_training_data_input = Input(  \n    type=AssetTypes.MLTABLE, path= ""./data/training-mltable-folder""  \n) \n# Create the AutoML classification job with the related factory-\nfunction.  \nclassification_job = automl.classification(  \n    compute= ""<compute_name>"" , \n    experiment_name= ""<exp_name?"" , \n    training_data=my_training_data_input,  \n    target_column_name= ""<name_of_target_column>"" , \n    primary_metric= ""accuracy"" , \n    n_cross_validations= 5, \n    enable_model_explainability= True, \n    tags={ ""my_custom_tag"" : ""My custom value"" }, \n) \n# Limits are all optional  \nclassification_job.set_limits(  \n    timeout_minutes= 600, \n    trial_timeout_minutes= 20, \n    max_trials= 5, \n    max_concurrent_trials = 4,', 'Upgrade AutoML to SDK v2\nArticle •04/04/2023\nIn SDK v2, ""experiments"" and ""runs"" are consolidated into jobs.\nIn SDK v1, AutoML was primarily configured and run using the AutoMLConfig class. In\nSDK v2, this class has been converted to an AutoML job. Although there are some\ndifferences in the configuration options, by and large, naming & functionality has been\npreserved in V2.\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\nSDK v1: Below is a sample AutoML classification task. For the entire code, check\nout our examples repo .\nPythonSubmit AutoML run\n# Imports  \nimport azureml.core  \nfrom azureml.core.experiment import Experiment  \nfrom azureml.core.workspace import Workspace  \nfrom azureml.core.dataset import Dataset  \nfrom azureml.train.automl import AutoMLConfig  \nfrom azureml.train.automl.run import AutoMLRun    \n# Load tabular dataset  \ndata = ""<url_to_data>""  \ndataset = Dataset.Tabular.from_delimited_files(data)  \ntraining_data, validation_data = dataset.random_split(percentage= 0.8, \nseed=223) \nlabel_column_name = ""Class"" \n# Configure Auto ML settings  \nautoml_settings = {  \n    ""n_cross_validations"" : 3, \n    ""primary_metric"" : ""average_precision_score_weighted"" , \n    ""enable_early_stopping"" : True, \n    ""max_concurrent_iterations"" : 2,   \n    ""experiment_timeout_hours"" : 0.25,   \n    ""verbosity"" : logging.INFO,  \n} \n# Put together an AutoML job constructor  \nautoml_config = AutoMLConfig(  \n    task= ""classification"" ,', '3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\nMachine Learning in many different ways .\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\niterations over different models, hyperparameter settings, advanced\npreprocessing/featurization, and what metrics to look at when determining the\nbest model.\n5. Submit the training job.\n6. Review the r esults\nThe following diagram illustrates this process. \nYou can also inspect the logged job information, which contains metrics  gathered\nduring the job. The training job produces a Python serialized object ( .pkl file) that\ncontains the model and data preprocessing.\nWhile model building is automated, you can also learn how important or relevant\nfeatures are  to the generated models.\nApply automated ML when you want Azure Machine Learning to train and tune a model\nfor you using the target metric you specify. Automated ML democratizes the machine\nlearning model development process, and empowers its users, no matter their data\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\nML professionals and developers across industries can use automated ML to:\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\nforecasting, computer vision & NLP', 'iii. Select Create to create your compute target.\nThis tak es a couple minut es to complet e.\niv. After creation, select your new compute target from the drop-down list.\ne. Select Next .\n3. On the Select task and settings  form, complete the setup for your automated ML\nexperiment by specifying the machine learning task type and configuration\nsettings.\na. Select Classification  as the machine learning task type.\nb. Select View additional configuration settings  and populate the fields as\nfollows. These settings are to better control the training job. Otherwise, defaults\nare applied based on experiment selection and data.\nAdditional\xa0configurations Descr iption Value\xa0for\xa0tut orial\nPrimary metric Evaluation metric that the\nmachine learning algorithm\nwill be measured by.AUC_weighted\nExplain best model Automatically shows\nexplainability on the bestEnable', ""An Azure subscription. (If you don't have an Azure subscription, create a free\naccount  before you begin)\nAn Azure Machine Learning experiment created with either:\nThe Azure Machine Learning studio  (no code required)\nThe Azure Machine Learning Python SDK\nAfter your automated ML experiment completes, a history of the jobs can be found via:\nA browser with Azure Machine Learning studio\nA Jupyter notebook using the JobDetails Jupyter widget\nThe following steps and video, show you how to view the run history and model\nevaluation metrics and charts in the studio:\n1. Sign into the studio  and navigate to your workspace.\n2. In the left menu, select Jobs.\n3. Select your experiment from the list of experiments.\n4. In the table at the bottom of the page, select an automated ML job.\n5. In the Models  tab, select the Algorithm name  for the model you want to evaluate.\n6. In the Metrics  tab, use the checkboxes on the left to view metrics and charts.\nAutomated ML calculates performance metrics for each classification model generated\nfor your experiment. These metrics are based on the scikit learn implementation.\nMany classification metrics are defined for binary classification on two classes, and\nrequire averaging over classes to produce one score for multi-class classification. Scikit-\nlearn provides several averaging methods, three of which automated ML exposes:\nmacr o, micro, and weight ed.\nMacr o - Calculate the metric for each class and take the unweighted average\nMicro - Calculate the metric globally by counting the total true positives, false\nnegatives, and false positives (independent of classes).\nWeight ed - Calculate the metric for each class and take the weighted average\nbased on the number of samples per class.\nWhile each averaging method has its benefits, one common consideration when\nselecting the appropriate method is class imbalance. If classes have different numbers of\nsamples, it might be more informative to use a macro average where minority classes\nView job results\nClassification metrics""]",2338
gpt-4-turbo,7,,5.0,,Describe the steps to retrieve the best model from an AutoML run using the AML SDK v2.,v1,"To retrieve the best model from an AutoML run using the Azure Machine Learning SDK v2, you would typically follow these steps:

1. **Initialize Workspace**: Import the necessary packages and classes, and create an instance of the `Workspace` class to connect to your Azure Machine Learning workspace.

    ```python
    from azure.ai.ml import MLClient
    from azure.identity import DefaultAzureCredential
    from azure.ai.ml.entities import Workspace

    # Load the workspace
    ws = Workspace.from_config()
    ml_client = MLClient(DefaultAzureCredential(), ws.subscription_id, ws.resource_group, ws.name)
    ```

2. **Retrieve AutoML Run**: Use the `MLClient` to retrieve the specific AutoML run. You will need the run ID for the AutoML run you want to retrieve.

    ```python
    run_id = 'your_automl_run_id'
    automl_run = ml_client.jobs.get(name=run_id)
    ```

3. **Get Best Model**: Once you have the AutoML run, you can access the best model from the run. The best model is typically stored as a child run of the AutoML run.

    ```python
    best_child_run = automl_run.get_best_child()
    ```

4. **Download Model**: After identifying the best child run, you can download the model file associated with it.

    ```python
    best_model_path = best_child_run.download_artifact('model')
    ```

5. **Register Model**: Optionally, you can register the model to your workspace for future deployment and tracking.

    ```python
    from azure.ai.ml.entities import Model

    model = Model(
        path=best_model_path,
        name=""best_automl_model"",
        type=""mlflow_model"",
        description=""Best model from AutoML run""
    )

    registered_model = ml_client.models.create_or_update(model)
    ```

Please note that the code snippets provided are for illustrative purposes and may require adjustments based on the specifics of your AutoML run and the Azure Machine Learning environment setup. Always refer to the latest Azure Machine Learning SDK v2 documentation for the most accurate and up-to-date information.","['Learn more about how and where to deploy a model .\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps', ""Navigate to the Models  tab to see the algorithms (models) tested. By default, the\nmodels are ordered by metric score as they complete. For this tutorial, the model that\nscores the highest based on the chosen Normalized r oot mean squar ed err or metric is\nat the top of the list.\nWhile you wait for all of the experiment models to finish, select the Algorithm name  of\na completed model to explore its performance details.\nThe following example navigates to select a model from the list of models that the job\ncreated. Then, you select the Overview  and the Metrics  tabs to view the selected\nmodel's properties, metrics and performance charts.\nAutomated machine learning in Azure Machine Learning studio allows you to deploy the\nbest model as a web service in a few steps. Deployment is the integration of the model\nso it can predict on new data and identify potential areas of opportunity.\nFor this experiment, deployment to a web service means that the bike share company\nnow has an iterative and scalable web solution for forecasting bike share rental demand.\nOnce the job is complete, navigate back to parent job page by selecting Job 1  at the top\nof your screen.\nIn the Best model summar y section, the best model in the context of this experiment, is\nselected based on the Normalized r oot mean squar ed err or metric.\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\nThe deployment process entails several steps including registering the model,Deploy the model"", ""To run a fine-tuned model job from Machine Learning, in order to deploy an Azure\nOpenAI model:\n1. After you've finished fine-tuning an Azure OpenAI model, find the registered\nmodel in the Models  list with the name provided during fine-tuning and select the\nmodel you want to deploy.\n2. Select Deploy  and name the deployment. The model is deployed to the default\nAzure OpenAI resource linked to your workspace.\nTo enable users to quickly get started with code-based fine-tuning, we've published\nsamples (both Python notebooks and Azure CLI examples) to the azureml-ex amples\nGitHub repo:\nSDK example\nCLI exampleDeploy fine-tuned models\nFine-tuning by using code-based samples"", ""Option 2: T o deploy a specific model iteration from this experiment.\na. Select the desired model from the Models  tab\nb. Select Deploy  on the top left of the window.\n2. Populate the Deploy model  pane.\nField Value\nName Enter a unique name for your deployment.\nDescription Enter a description to better identify what this deployment is for.\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\nService (AKS)  or Azure Container Inst ance (ACI).\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\ndeploy to.\nEnable\nauthenticationSelect to allow for token-based or key-based authentication.\nUse custom\ndeployment assetsEnable this feature if you want to upload your own scoring script and\nenvironment file. Otherwise, automated ML provides these assets for\nyou by default. Learn more about scoring scripts .\nThe Advanced menu offers default deployment features such as data collection  and\nresource utilization settings. If you wish to override these defaults do so in this\nmenu.\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\ndeployment begins, the Model summar y tab appears. See the deployment\nprogress under the Deploy status  section.\nNow you have an operational web service to generate predictions! Y ou can test the\npredictions by querying the service from Power BI's built in Azure Machine Learning\nsupport .ﾉExpand table\n） Impor tant\nFile names must be under 32 characters and must begin and end with\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\nbetween. Spaces are not allowed."", 'The automated machine learning interface allows you to deploy the best model as a\nweb service in a few steps. Deployment is the integration of the model so it can predict\non new data and identify potential areas of opportunity.\nFor this experiment, deployment to a web service means that the financial institution\nnow has an iterative and scalable web solution for identifying potential fixed term\ndeposit customers.\nCheck to see if your experiment run is complete. T o do so, navigate back to the parent\njob page by selecting Job 1  at the top of your screen. A Complet ed status is shown on\nthe top left of the screen.\nOnce the experiment run is complete, the Details  page is populated with a Best model\nsummar y section. In this experiment context, VotingEnsemble  is considered the best\nmodel, based on the AUC_w eight ed metric.\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\nThe deployment process entails several steps including registering the model,\ngenerating resources, and configuring them for the web service.\n1. Select VotingEnsemble  to open the model-specific page.\n2. Select the Deploy  menu in the top-left and select Deploy t o web ser vice.\n3. Populate the Deploy a model  pane as follows:\nField Value\nDeployment name my-automl-deploy\nDeployment\ndescriptionMy first automated machine learning experiment deployment\nCompute type Select Azure Container Instance (A CI)\nEnable\nauthenticationDisable.\nUse custom\ndeploymentsDisable. Allows for the default driver file (scoring script) and\nenvironment file to be auto-generated.\nFor this example, we use the defaults provided in the Advanced menu.\n4. Select Deploy .Deploy the best model']",1656
gpt-4-turbo,8,,5.0,,What are the AutoML capabilities for handling imbalanced data in Azure ML?,v1,"Azure Machine Learning's Automated ML (AutoML) includes several capabilities to handle imbalanced data, which is a common issue in machine learning where one class is significantly more prevalent than others. Here are some of the ways AutoML addresses this challenge:

1. **Data Preprocessing**: AutoML automatically preprocesses the data before training models. This can include techniques such as scaling and normalization, which can help mitigate the impact of imbalanced data.

2. **Sampling Strategies**: AutoML can apply different sampling strategies to balance the dataset. This may involve oversampling the minority class or undersampling the majority class to create a more balanced distribution.

3. **Cost-Sensitive Learning**: AutoML can use cost-sensitive learning where the algorithm assigns a higher cost to misclassifying the minority class. This encourages the model to pay more attention to the minority class during training.

4. **Advanced Metrics**: AutoML provides advanced metrics for evaluating model performance that are suitable for imbalanced datasets, such as the Area Under the Precision-Recall Curve (AUC-PR) instead of just accuracy. These metrics give a better indication of model performance when dealing with imbalanced classes.

5. **Model Selection**: AutoML can select models that are known to handle imbalanced data better. For example, certain tree-based models like Random Forest or Gradient Boosting may perform better on imbalanced datasets.

6. **Hyperparameter Tuning**: AutoML includes hyperparameter tuning capabilities that can optimize for hyperparameters that are particularly effective for imbalanced datasets, such as the class_weight parameter in some models.

By leveraging these capabilities, AutoML in Azure Machine Learning helps users to build models that are more robust and perform better on imbalanced datasets.","[""Prevent overfitting and imbalanced data\nwith Automated ML\nArticle •06/16/2023\nOverfitting and imbalanced data are common pitfalls when you build machine learning\nmodels. By default, Azure Machine Learning's Automated ML provides charts and\nmetrics to help you identify these risks, and implements best practices to help mitigate\nthem.\nOverfitting in machine learning occurs when a model fits the training data too well, and\nas a result can't accurately predict on unseen test data. In other words, the model has\nmemorized specific patterns and noise in the training data, but is not flexible enough to\nmake predictions on real data.\nConsider the following trained models and their corresponding train and test accuracies.\nModel Train accuracy Test accuracy\nA 99.9% 95%\nB 87% 87%\nC 99.9% 45%\nConsider model A, there is a common misconception that if test accuracy on unseen\ndata is lower than training accuracy, the model is overfitted. However, test accuracy\nshould always be less than training accuracy, and the distinction for overfit vs.\nappropriately fit comes down to how much  less accurate.\nCompare models A and B, model A is a better model because it has higher test\naccuracy, and although the test accuracy is slightly lower at 95%, it is not a significant\ndifference that suggests overfitting is present. Y ou wouldn't choose model B because\nthe train and test accuracies are closer together.\nModel C represents a clear case of overfitting; the training accuracy is high but the test\naccuracy isn't anywhere near as high. This distinction is subjective, but comes from\nknowledge of your problem and data, and what magnitudes of error are acceptable.Identify overfitting\nPrevent overfitting"", ""If it ascertains a better performance through this experiment, then this remedy is\napplied.\nUse a performance metric that deals better with imbalanced data. For example, the\nAUC_weighted is a primary metric that calculates the contribution of every class\nbased on the relative number of samples representing that class, hence is more\nrobust against imbalance.\nThe following techniques are additional options to handle imbalanced data outside of\nAutomated ML.\nResampling to even the class imbalance, either by up-sampling the smaller classes\nor down-sampling the larger classes. These methods require expertise to process\nand analyze.\nReview performance metrics for imbalanced data. For example, the F1 score is the\nharmonic mean of precision and recall. Precision measures a classifier's exactness,\nwhere higher precision indicates fewer false positives, while recall measures a\nclassifier's completeness, where higher recall indicates fewer false negatives.\nSee examples and learn how to build models using Automated ML:\nFollow the Tutorial: T rain an object detection model with automated machine\nlearning and Python .\nConfigure the settings for automatic training experiment:\nIn Azure Machine Learning studio, use these steps .\nWith the Python SDK, use these steps .Next steps"", 'Imbalanced data is commonly found in data for machine learning classification\nscenarios, and refers to data that contains a disproportionate ratio of observations in\neach class. This imbalance can lead to a falsely perceived positive effect of a model\'s\naccuracy, because the input data has bias towards one class, which results in the trained\nmodel to mimic that bias.\nIn addition, Automated ML jobs generate the following charts automatically. These\ncharts help you understand the correctness of the classifications of your model, and\nidentify models potentially impacted by imbalanced data.\nChar t Descr iption\nConfusion\nMatrixEvaluates the correctly classified labels against the actual labels of the data.\nPrecision-recall Evaluates the ratio of correct labels against the ratio of found label instances of\nthe data\nROC Curves Evaluates the ratio of correct labels against the ratio of false-positive labels.\nAs part of its goal of simplifying the machine learning workflow, Automated ML has built\nin capabilities to help deal with imbalanced data such as,\nA weight column: Automated ML creates a column of weights as input to cause\nrows in the data to be weighted up or down, which can be used to make a class\nmore or less ""important.""\nThe algorithms used by Automated ML detect imbalance when the number of\nsamples in the minority class is equal to or fewer than 20% of the number of\nsamples in the majority class, where minority class refers to the one with fewest\nsamples and majority class refers to the one with most samples. Subsequently,\nautomated machine learning will run an experiment with subsampled data to\ncheck if using class weights would remedy this problem and improve performance.Cross-validation isn\'t enabled by default; it must be configured in Automated\nmachine learning settings. However, after cross-validation is configured and a\nvalidation data set has been provided, the process is automated for you.\nIdentify models with imbalanced data\nHandle imbalanced data', 'What is automated machine learning\n(AutoML)?\nArticle •04/13/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAutomated machine learning, also referred to as automated ML or AutoML, is the\nprocess of automating the time-consuming, iterative tasks of machine learning model\ndevelopment. It allows data scientists, analysts, and developers to build ML models with\nhigh scale, efficiency, and productivity all while sustaining model quality. Automated ML\nin Azure Machine Learning is based on a breakthrough from our Microsoft R esearch\ndivision .\nFor code-experienced customers, Azure Machine Learning Python SDK . Get\nstarted with Tutorial: T rain an object detection model (preview) with AutoML and\nPython .\nDuring training, Azure Machine Learning creates a number of pipelines in parallel that\ntry different algorithms and parameters for you. The service iterates through ML\nalgorithms paired with feature selections, where each iteration produces a model with a\ntraining score. The better the score for the metric you want to optimize for, the better\nthe model is considered to ""fit"" your data. It will stop once it hits the exit criteria defined\nin the experiment.\nUsing Azur e Machine Learning , you can design and run your automated ML training\nexperiments with these steps:\n1. Identif y the ML pr oblem  to be solved: classification, forecasting, regression,\ncomputer vision or NLP.\n2. Choose whether y ou want a code-fir st experience or a no-code studio w eb\nexperience : Users who prefer a code-first experience can use the Azure Machine\nLearning SDK v2 or the Azure Machine Learning CLIv2 . Get started with Tutorial:\nTrain an object detection model with AutoML and Python . Users who prefer a\nlimited/no-code experience can use the web interface  in Azure Machine Learning\nstudio at https://ml.azure.com . Get started with Tutorial: Create a classification\nmodel with automated ML in Azure Machine Learning .\nHow does AutoML work?', 'Automated ML in Azure Machine Learning supports model training for computer vision\ntasks like image classification and object detection. T o debug AutoML vision models and\nexplain model predictions, AutoML models for computer vision are integrated with\nResponsible AI dashboard. T o generate R esponsible AI insights for AutoML computer\nvision models, register your best AutoML model in the Azure Machine Learning\nworkspace and run it through the R esponsible AI vision insights pipeline. T o learn, see\nhow to set up AutoML to train computer vision models .\nNotebooks related to the AutoML supported computer vision tasks can be found in\nazureml-examples  repository.\nThe R esponsible AI vision Insights pipeline could be submitted through one of the\nfollowing methods\nPython SDK: T o learn how to submit the pipeline through Python, see the AutoML\nImage Classification scenario with RAI Dashboard sample notebook . For\nconstructing the pipeline, refer to section 5.1 in the notebook.\nAzure CLI: T o submit the pipeline via Azure-CLI, see the component Y AML in\nsection 5.2 of the example notebook linked above.\nUI (via Azure Machine Learning studio): From the Designer in Azure Machine\nLearning studio, the RAI-vision insights component can be used to create and\nsubmit a pipeline.\nIn addition to the list of R esponsible AI vision insights parameters provided in the\nprevious section, the following are parameters to set specifically for AutoML models.Integration with AutoML Image\nMode of submitting the Responsible AI vision insights\npipeline\nResponsible AI vision insights component parameter\n(AutoML specific)\n７ Note\nA few parameters are specific to the XAI algorithm chosen and are optional for\nother algorithms.']",2135
gpt-4-turbo,9,,5.0,,Explain how to use AutoML for forecasting time series data. What specific settings should be adjusted for time series problems?,v1,"To use Azure AutoML for forecasting time series data, you should follow these general steps and consider specific settings that are tailored for time series problems:

1. **Set Up Your Environment**:
   - Install the Azure Machine Learning Python SDK v2.
   - Create an Azure Machine Learning workspace if you don't already have one.
   - Set up your development environment, such as a Jupyter notebook, and authenticate to your workspace.

2. **Prepare Your Data**:
   - Ensure your data is in a format suitable for time series forecasting. This typically means having a time column, one or more feature columns, and a target column that you want to forecast.
   - Split your data into training and testing datasets.

3. **Configure Your AutoML Experiment**:
   - Define the settings and configuration for your AutoML run. For time series forecasting, you should adjust the following specific settings:
     - **time_column_name**: Specify the name of the column that holds the time series information.
     - **grain_column_names**: If you have multiple time series (e.g., sales data for different stores), specify the columns that identify the individual series.
     - **max_horizon**: Define the number of periods to forecast into the future.
     - **target_lags**: Set the lags for the target variable, which can help the model understand the relationship between past values and future values.
     - **target_rolling_window_size**: Use a rolling window to aggregate past values of the target variable, which can be useful for capturing trends and seasonality.
     - **forecasting_parameters**: Additional parameters specific to forecasting tasks, such as seasonality, holiday features, etc.

4. **Choose Your Compute Target**:
   - Select a compute target for training your models. For larger datasets or many time series, consider using a compute cluster to distribute the training jobs.

5. **Run the AutoML Experiment**:
   - Submit the AutoML run with the defined settings and data. AutoML will automatically try different models and preprocessing steps to find the best solution for your forecasting problem.

6. **Evaluate Results**:
   - Once the run is complete, evaluate the performance of the models using metrics such as MAE, RMSE, etc.
   - Review the engineered features, model explanations, and any other insights provided by AutoML.

7. **Deploy the Model**:
   - After selecting the best model, deploy it to an Azure Container Instance, Azure Kubernetes Service, or other supported targets for real-time or batch scoring.

For more detailed guidance and examples, you can refer to the Azure Machine Learning documentation, specifically the articles and Jupyter notebooks on forecasting with AutoML, such as the ""Bike share example"" and ""Forecasting using deep learning"". These resources provide hands-on examples and best practices for time series forecasting using Azure AutoML.","[""Frequently asked questions about\nforecasting in AutoML\nArticle •08/01/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nThis article answers common questions about forecasting in automatic machine learning\n(AutoML). For general information about forecasting methodology in AutoML, see the\nOverview of forecasting methods in AutoML  article.\nYou can start by reading the Set up AutoML to train a time-series forecasting model\narticle. Y ou can also find hands-on examples in several Jupyter notebooks:\nBike share example\nForecasting using deep learning\nMany Models solution\nForecasting recipes\nAdvanced forecasting scenarios\nWe're always working to make AutoML faster and more scalable. T o work as a general\nforecasting platform, AutoML does extensive data validations and complex feature\nengineering, and it searches over a large model space. This complexity can require a lot\nof time, depending on the data and the configuration.\nOne common source of slow runtime is training AutoML with default settings on data\nthat contains numerous time series. The cost of many forecasting methods scales with\nthe number of series. For example, methods like Exponential Smoothing and Prophet\ntrain a model for each time series  in the training data.\nThe Many Models feature of AutoML scales to these scenarios by distributing training\njobs across a compute cluster. It has been successfully applied to data with millions of\ntime series. For more information, see the many models  article section. Y ou can also\nread about the success of Many Models  on a high-profile competition dataset.\nHow do I start building forecasting models in\nAutoML?\nWhy is AutoML slow on my data?"", 'Learn more about how and where to deploy a model .\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps', ""If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about calendar features for time series forecasting in AutoML .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about AutoML forecasting for lagged features .Next steps"", ""Set up AutoML to train a time-series\nforecasting model with SDK and CLI\nArticle •08/02/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn this article, you'll learn how to set up AutoML for time-series forecasting with Azure\nMachine Learning automated ML in the Azure Machine Learning Python SDK .\nTo do so, you:\nFor a low code experience, see the Tutorial: Forecast demand with automated machine\nlearning  for a time-series forecasting example using automated ML in the Azure\nMachine Learning studio .\nAutoML uses standard machine learning models along with well-known time series\nmodels to create forecasts. Our approach incorporates historical information about the\ntarget variable, user-provided features in the input data, and automatically engineered\nfeatures. Model search algorithms then work to find a model with the best predictive\naccuracy. For more details, see our articles on forecasting methodology  and model\nsearch .\nFor this article you need,\nAn Azure Machine Learning workspace. T o create the workspace, see Create\nworkspace resources .\nThe ability to launch AutoML training jobs. Follow the how-to guide for setting up\nAutoML  for details.\nPrepare data for training.＂\nConfigure specific time-series parameters in a Forecasting Job . ＂\nOrchestrate training, inference, and model evaluation using components and\npipelines.＂\nPrerequisites\nTraining  and validation data"", ""See the Why is AutoML slow on my data?  answer to understand why AutoML might be\nslow in your case.\nConsider the following configuration changes that might speed up your job:\nBlock time series models  like ARIMA and Prophet.\nTurn off look-back features like lags and rolling windows.\nReduce:\nThe number of trials/iterations.\nTrial/iteration timeout.\nExperiment timeout.\nThe number of cross-validation folds.\nEnsure that early termination is enabled.\nAutoML forecasting supports four basic configurations:\nConfiguration Scenar io Pros Cons\nDefault\nAutoMLRecommended if the dataset\nhas a small number of time\nseries that have roughly\nsimilar historical behavior.- Simple to configure\nfrom code/SDK or\nAzure Machine\nLearning studio.\n- AutoML can learn\nacross different time\nseries because the\nregression models\npool all series together\nin training. For more\ninformation, see\nModel grouping .- Regression models\nmight be less\naccurate if the time\nseries in the training\ndata have divergent\nbehavior.\n- Time series models\nmight take a long\ntime to train if the\ntraining data has a\nlarge number of\nseries. For more\ninformation, see the\nWhy is AutoML slow\non my data?  answer.\nAutoML with\ndeep learningRecommended for datasets\nwith more than 1,000\nobservations and, potentially,\nnumerous time series that\nexhibit complex patterns.\nWhen it's enabled, AutoML\nwill sweep over temporal- Simple to configure\nfrom code/SDK or\nAzure Machine\nLearning studio.\n- Cross-learning\nopportunities, because- Training can take\nmuch longer\nbecause of the\ncomplexity of DNN\nmodels.\n- Series with smallHow can I make AutoML faster?\nWhat modeling configuration should I use?""]",1747
gpt-4-turbo,10,,5.0,,How do you create a custom deep learning environment with specific Python packages using the AML SDK v2?,v1,"To create a custom deep learning environment with specific Python packages using the Azure Machine Learning SDK v2, you can follow these steps:

1. **Define the Environment**: Use the `Environment` class from the `azure.ai.ml.entities` namespace to define your custom environment. You will need to specify the name, description, the path to your Conda environment file (which lists the Python packages and versions you want to include), and optionally, a base Docker image.

2. **Create the Conda Environment File**: Write a `conda.yaml` file that lists all the Python packages and versions you need for your deep learning environment. This file will be used to create the environment.

3. **Register the Environment**: Use the `create_or_update` method of the `ml_client.environments` to register the environment with your Azure Machine Learning workspace.

Here's an example of how you might do this in code:

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Environment
from azure.identity import DefaultAzureCredential
import os

# Set up the MLClient with your Azure subscription and workspace information
subscription_id = 'your-subscription-id'
resource_group = 'your-resource-group'
workspace_name = 'your-workspace-name'
ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace_name)

# Define the custom environment
custom_env_name = ""deep-learning-env""
dependencies_dir = ""path_to_your_dependencies_directory""  # Directory where conda.yaml is located
conda_file_path = os.path.join(dependencies_dir, ""conda.yaml"")

job_env = Environment(
    name=custom_env_name,
    description=""Custom environment for deep learning"",
    conda_file=conda_file_path,
    image=""mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest"",  # Optional base image
)

# Register the environment
job_env = ml_client.environments.create_or_update(job_env)

# Print the environment details
print(f""Environment with name {job_env.name} is registered to workspace, the environment version is {job_env.version}"")
```

In the `conda.yaml` file, you would specify your Python packages like this:

```yaml
name: deep-learning-env
dependencies:
  - python=3.8
  - numpy
  - pandas
  - scikit-learn
  - tensorflow
  - keras
  - pip:
    - some-pip-package
```

Replace the placeholders with your actual subscription ID, resource group, workspace name, and the path to your dependencies directory. Also, customize the `conda.yaml` file with the specific Python packages and versions that you need for your deep learning tasks.","['The specification contains some usual packages (such as numpy and pip) that you\'ll use\nin your job.\nNext, use the Y AML file to create and register this custom environment in your\nworkspace. The environment will be packaged into a Docker container at runtime.\nPython\nFor more information on creating and using environments, see Create and use software\nenvironments in Azure Machine Learning .\nIn this section, we\'ll begin by introducing the data for training. W e\'ll then cover how to\nrun a training job, using a training script that we\'ve provided. Y ou\'ll learn to build the\ntraining job by configuring the command for running the training script. Then, you\'ll\nsubmit the training job to run in Azure Machine Learning.\nYou\'ll use data from the Modified National Institute of S tandards and T echnology\n(MNIST) database of handwritten digits. This data is sourced from Y an LeCun\'s website\nand stored in an Azure storage account.\nPythonfrom azure.ai.ml.entities import Environment  \ncustom_env_name = ""keras-env""  \njob_env = Environment(  \n    name=custom_env_name,  \n    description= ""Custom environment for keras image classification"" , \n    conda_file=os.path.join(dependencies_dir, ""conda.yaml"" ), \n    image= ""mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest"" , \n) \njob_env = ml_client.environments.create_or_update(job_env)  \nprint( \n    f""Environment with name {job_env.name}  is registered to workspace, the  \nenvironment version is {job_env.version} "" \n)\nConfigure and submit your training  job\nObtain the training data\nweb_path = ""wasbs://datasets@azuremlexamples.blob.core.windows.net/mnist/""', ""In the samples deep learning folder on the notebook server, find a\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\ndeploy-with-k eras.\nYour Jupyter notebook server\nInstall the Azure Machine Learning SDK (v2) .\nDownload the training scripts keras_mnist.py  and utils.py .\nYou can also find a completed Jupyter Notebook version  of this guide on the GitHub\nsamples page.\nBefore you can run the code in this article to create a GPU cluster, you'll need to request\na quota increase  for your workspace.\nThis section sets up the job for training by loading the required Python packages,\nconnecting to a workspace, creating a compute resource to run a command job, and\ncreating an environment to run the job.\nFirst, you'll need to connect to your Azure Machine Learning workspace. The Azure\nMachine Learning workspace  is the top-level resource for the service. It provides you\nwith a centralized place to work with all the artifacts you create when you use Azure\nMachine Learning.\nWe're using DefaultAzureCredential to get access to the workspace. This credential\nshould be capable of handling most Azure SDK authentication scenarios.\nIf DefaultAzureCredential doesn't work for you, see azure-identity reference\ndocumentation  or Set up authentication  for more available credentials.\nPython\nSet up the job\nConnect to the workspace\n# Handle to the workspace  \nfrom azure.ai.ml import MLClient  \n# Authentication package  \nfrom azure.identity import DefaultAzureCredential  \ncredential = DefaultAzureCredential()"", 'Azure Machine Learning allows you to either use a curated (or ready-made)\nenvironment or create a custom environment using a Docker image or a Conda\nconfiguration. In this article, you reuse the curated Azure Machine Learning environment\nAzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu. Use the latest version of this\nenvironment using the @latest directive.\nPython\nIn this section, we begin by introducing the data for training. W e then cover how to run\na training job, using a training script that we\'ve provided. Y ou\'ll learn to build the\ntraining job by configuring the command for running the training script. Then, you\'ll\nsubmit the training job to run in Azure Machine Learning.\nYou can use the dataset in this zipped file . This dataset consists of about 120 training\nimages each for two classes (turkeys and chickens), with 100 validation images for each\nclass. The images are a subset of the Open Images v5 Dataset . The training script\npytorch_tr ain.p y downloads and extracts the dataset.\nIn the prerequisites section, we provided the training script pytorch_tr ain.p y. In practice,\nyou should be able to take any custom training script as is and run it with Azure\nMachine Learning without having to modify your code.\nThe provided training script downloads the data, trains a model, and registers the\nmodel.\nNow that you have all the assets required to run your job, it\'s time to build it using the\nAzure Machine Learning Python SDK v2. For this example, we create a command.\nAn Azure Machine Learning command is a resource that specifies all the details needed to\nexecute your training code in the cloud. These details include the inputs and outputs,curated_env_name = ""AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu@latest""\nConfigure and submit your training  job\nObtain the training data\nPrepare the training script\nBuild the training job', ""2. Create an Azure Machine Learning workspace .\n3. Clone the AzureML-Examples repository .\nBash\n4. Start the notebook server from the directory containing your clone.\nBash\nThese instructions install the base SDK packages necessary for the quickstart and tutorial\nnotebooks. Other sample notebooks might require you to install extra components. For\nmore information, see Install the Azure Machine Learning SDK for Python .\nThe Data Science Virtual Machine (DSVM) is a customized VM image built specifically for\ndoing data science. If you create a DSVM , the SDK and notebook server are installed and\nconfigured for you. However, you'll still need to create a workspace and clone the\nsample repository.\n1. Create an Azure Machine Learning workspace .\n2. Clone the the AzureML-Examples repository .\nBash\n3. Start the notebook server from the directory that contains the clone.\nBash\ngit clone https://github.com/Azure/azureml-examples.git --depth 1\njupyter notebook\nOption 3: Access on a DSVM\ngit clone https://github.com/Azure/azureml-examples.git --depth 1\njupyter notebook\nConnect to a workspace"", ""Manage Azu re Machine Learning\nenvironments with the CLI & SDK (v2)\nArticle •01/03/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAzure Machine Learning environments define the execution environments for your jobs\nor deployments and encapsulate the dependencies for your code. Azure Machine\nLearning uses the environment specification to create the Docker container that your\ntraining or scoring code runs in on the specified compute target. Y ou can define an\nenvironment from a conda specification, Docker image, or Docker build context.\nIn this article, learn how to create and manage Azure Machine Learning environments\nusing the SDK & CLI (v2).\nBefore following the steps in this article, make sure you have the following prerequisites:\nAn Azure Machine Learning workspace. If you don't have one, use the steps in the\nQuickstart: Create workspace resources  article to create one.\nThe Azure CLI and the ml extension or the Azure Machine Learning Python SDK v2:\nTo install the Azure CLI and extension, see Install, set up, and use the CLI (v2) .\nTo install the Python SDK v2, use the following command:\nBash\nPrerequisites\n） Impor tant\nThe CLI examples in this article assume that you are using the Bash (or\ncompatible) shell. For example, from a Linux system or Windows\nSubsyst em for Linux .\npip install azure-ai-ml azure-identity""]",2249
gpt-4-turbo,11,,5.0,,Describe the process of using Azure ML to train a deep learning model on a GPU compute cluster.,v1,"To train a deep learning model on a GPU compute cluster using Azure Machine Learning, you would follow these general steps:

1. **Set Up Your Workspace**:
   - Connect to your Azure Machine Learning workspace using the `MLClient` from the Azure ML Python SDK. You'll need your Azure subscription ID, resource group, and the name of the workspace.

   ```python
   from azure.ai.ml import MLClient
   from azure.identity import DefaultAzureCredential

   workspace = '<AZUREML_WORKSPACE_NAME>'
   subscription_id = '<YOUR_SUBSCRIPTION_ID>'
   resource_group = '<YOUR_RESOURCE_GROUP>'

   ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)
   ```

2. **Create or Retrieve a GPU Compute Cluster**:
   - Define a GPU compute target. If it doesn't exist, create a new one using the `AmlCompute` class, specifying the GPU VM size (e.g., `STANDARD_NC6` for a VM with one NVIDIA Tesla K80 GPU).

   ```python
   from azure.ai.ml.entities import AmlCompute

   gpu_compute_target = ""gpu-cluster""
   try:
       ml_client.compute.get(gpu_compute_target)
   except Exception:
       print(""Creating a new GPU compute target..."")
       compute = AmlCompute(
           name=gpu_compute_target,
           size=""STANDARD_NC6"",  # Example GPU VM size
           min_instances=0,
           max_instances=4  # Adjust based on your needs
       )
       ml_client.compute.begin_create_or_update(compute).result()
   ```

3. **Prepare Your Training Script**:
   - Write a Python script (e.g., `train.py`) that defines your deep learning model, the training process, and any other necessary steps like data preprocessing.

4. **Create an Experiment and Submit the Training Job**:
   - Define the training job configuration, including the source directory containing your training script, the compute target, and any necessary environment dependencies.

   ```python
   from azure.ai.ml import command, Input

   # Define the command job with the training script and parameters
   job = command(
       code=""./path_to_your_code_directory"",
       command=""python train.py"",
       compute=gpu_compute_target,
       environment=""<YOUR_ENVIRONMENT_NAME>"",  # Specify your environment
       inputs={""training_data"": Input(type=""uri_folder"", path=""<YOUR_DATA_PATH>"")},
       outputs={""model_output"": ""<YOUR_OUTPUT_PATH>""}
   )

   # Submit the job to the Azure ML workspace
   returned_job = ml_client.create_or_update(job)
   ```

5. **Monitor the Training Job**:
   - Use Azure ML Studio or the SDK to monitor the progress of your training job, view logs, and retrieve the trained model once the job is complete.

Remember to replace placeholders like `<AZUREML_WORKSPACE_NAME>`, `<YOUR_SUBSCRIPTION_ID>`, `<YOUR_RESOURCE_GROUP>`, `<YOUR_ENVIRONMENT_NAME>`, `<YOUR_DATA_PATH>`, and `<YOUR_OUTPUT_PATH>` with your actual values. Also, ensure that the VM size you specify for the compute target is suitable for your deep learning workload and that you have the necessary quota in your Azure subscription for the GPU VMs.","['An Azure Machine Learning compute cluster is a fully managed compute resource that\ncan be used to run the training job. In the following examples, a compute cluster named\ncpu-compute is created.\nPython\nTo run this script, you\'ll use a command that executes main.py Python script located\nunder ./sdk/python/jobs/single-step/lightgbm/iris/src/. The command will be runworkspace = \'<AZUREML_WORKSPACE_NAME>\'\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \nresource_group, workspace)\n2. Create a compute resource for training\n７ Note\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\ntraining job .\nPython SDK\nfrom azure.ai.ml.entities import AmlCompute\n# specify aml compute name.\ncpu_compute_target = ""cpu-cluster""\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(""Creating a new cpu compute target..."" )\n    compute = AmlCompute(\n        name=cpu_compute_target, size= ""STANDARD_D2_V2"" , min_instances= 0, \nmax_instances= 4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n4. Submit the training job\nPython SDK', 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""Manage and optimize Azure Machine\nLearning costs\nArticle •08/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.） Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)"", 'For detailed instructions on how to complete these steps, see Secure a training\nenvironment .\nIn this section, you learn how Azure Machine Learning securely communicates between\nservices to submit a training job. This example shows you how all your configurations\nwork together to secure communication.\n1. The client uploads training scripts and training data to storage accounts that are\nsecured with a service or private endpoint.\n2. The client submits a training job to the Azure Machine Learning workspace\nthrough the private endpoint.\n3. Azure Batch service receives the job from the workspace. It then submits the\ntraining job to the compute environment through the public load balancer for the\ncompute resource.\n4. The compute resource receives the job and begins training. The compute resource\nuses information stored in key vault to access storage accounts to download\ntraining files and upload output.Example training job submission', 'like Spark.\nIn the following example script, we provision a Linux compute cluster . You can see the\nAzure Machine Learning pricing  page for the full list of VM sizes and prices. Since we\nneed a GPU cluster for this example, let\'s pick a STAND ARD_NC6  model and create an\nAzure Machine Learning compute.\nPython\nfrom azure.ai.ml.entities import AmlCompute  \ngpu_compute_target = ""gpu-cluster""  \ntry: \n    # let\'s see if the compute target already exists  \n    gpu_cluster = ml_client.compute.get(gpu_compute_target)  \n    print(  \n        f""You already have a cluster named {gpu_compute_target} , we\'ll reuse  \nit as is.""  \n    ) \nexcept Exception:  \n    print( ""Creating a new gpu compute target..."" ) \n    # Let\'s create the Azure ML compute object with the intended parameters  \n    gpu_cluster = AmlCompute(  \n        # Name assigned to the compute cluster  \n        name= ""gpu-cluster"" , \n        # Azure ML Compute is the on-demand VM service  \n        type= ""amlcompute"" , \n        # VM Family  \n        size= ""STANDARD_NC6"" , \n        # Minimum running nodes when there is no job running  \n        min_instances= 0, \n        # Nodes in cluster  \n        max_instances= 4, \n        # How many seconds will the node running after the job termination  \n        idle_time_before_scale_down= 180, \n        # Dedicated or LowPriority. The latter is cheaper but there is a  \nchance of job termination  \n        tier= ""Dedicated"" , \n    ) \n    # Now, we pass the object to MLClient\'s create_or_update method  \n    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()  \nprint( \n    f""AMLCompute with name {gpu_cluster.name}  is created, the compute size  \nis {gpu_cluster.size} "" \n)']",1908
gpt-4-turbo,12,,5.0,,Explain how to deploy a TensorFlow model as a web service in Azure ML. What are the key considerations for deployment?,v1,"To deploy a TensorFlow model as a web service in Azure Machine Learning, you need to follow these general steps:

1. **Register the Model**: First, you need to register your trained TensorFlow model in your Azure Machine Learning workspace. This can be done using the Azure Machine Learning SDK or the Azure portal by providing the model file and metadata.

2. **Prepare an Inference Configuration**: Create an inference configuration that specifies the environment needed to host the model. This includes the scoring script (`score.py`) that uses the model to make predictions and an environment file (`environment.yml`) that specifies the dependencies, such as TensorFlow and any other required libraries.

3. **Prepare an Entry Script**: The entry script, often named `score.py`, should contain two functions: `init()` which loads the model and runs any one-time initialization, and `run(data)` which is called when new data is submitted for predictions.

4. **Create a Deployment Configuration**: Define the deployment configuration that specifies the compute type where the model will be hosted. This could be an Azure Container Instance (ACI), Azure Kubernetes Service (AKS), or other supported compute targets. The configuration includes details such as CPU cores, memory, and other settings.

5. **Deploy the Model**: Use the Azure Machine Learning SDK to deploy the model to the chosen compute target. This will create a web service endpoint that can be used to make predictions.

6. **Test the Web Service**: After deployment, test the web service endpoint to ensure it is working correctly by sending sample requests and verifying the responses.

Key considerations for deployment include:

- **Scalability**: Choose a compute target that can scale to meet your expected load. AKS is typically used for high-scale production deployments, while ACI is often used for testing or low-scale applications.

- **Security**: Ensure that the endpoint is secure. Use features like authentication and virtual networks to protect your endpoint.

- **Monitoring and Logging**: Set up monitoring and logging to track the health and usage of your web service. Azure Machine Learning provides capabilities to monitor your endpoints.

- **Cost**: Consider the cost implications of the compute resources you choose for hosting your model. Different compute targets have different pricing models.

- **Latency**: The choice of compute target can affect the latency of your predictions. For low-latency requirements, you may need to choose a more powerful compute target or optimize your model and scoring script.

- **Model Versioning**: Use Azure Machine Learning's model versioning capabilities to manage and deploy different versions of your model.

- **Environment Reproducibility**: Ensure that the environment for your model is reproducible, especially if you are moving from a development to a production setting.

Remember to consult the Azure Machine Learning documentation for the most up-to-date and detailed instructions, as the platform and best practices may evolve over time.","['What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps', 'Tutorial Descr iption\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'To learn more, review these articles:\nDeploy models with REST\nCreate and use managed online endpoints in the studio\nSafe rollout for online endpoints\nHow to autoscale managed online endpoints\nView costs for an Azure Machine Learning managed online endpoint\nAccess Azure resources with a managed online endpoint and managed identity\nTroubleshoot managed online endpoints deployment', ""Option 2: T o deploy a specific model iteration from this experiment.\na. Select the desired model from the Models  tab\nb. Select Deploy  on the top left of the window.\n2. Populate the Deploy model  pane.\nField Value\nName Enter a unique name for your deployment.\nDescription Enter a description to better identify what this deployment is for.\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\nService (AKS)  or Azure Container Inst ance (ACI).\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\ndeploy to.\nEnable\nauthenticationSelect to allow for token-based or key-based authentication.\nUse custom\ndeployment assetsEnable this feature if you want to upload your own scoring script and\nenvironment file. Otherwise, automated ML provides these assets for\nyou by default. Learn more about scoring scripts .\nThe Advanced menu offers default deployment features such as data collection  and\nresource utilization settings. If you wish to override these defaults do so in this\nmenu.\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\ndeployment begins, the Model summar y tab appears. See the deployment\nprogress under the Deploy status  section.\nNow you have an operational web service to generate predictions! Y ou can test the\npredictions by querying the service from Power BI's built in Azure Machine Learning\nsupport .ﾉExpand table\n） Impor tant\nFile names must be under 32 characters and must begin and end with\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\nbetween. Spaces are not allowed."", 'In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\nmodel to an online endpoint. See these other articles to learn more about Azure\nMachine Learning.\nTrack run metrics during training\nTune hyperparameters\nReference architecture for distributed deep learning training in AzureNext steps']",1128
gpt-4-turbo,13,,5.0,,Provide an example of how to use the AML SDK v2 for distributed deep learning training. What configurations are necessary for multi-node training?,v1,"To use Azure Machine Learning SDK v2 for distributed deep learning training, you would typically follow these steps and include the necessary configurations for multi-node training:

1. **Set up your environment**: Install the Azure Machine Learning SDK v2 and configure your environment with the necessary dependencies for your deep learning framework (PyTorch, TensorFlow, etc.).

2. **Create or attach a compute target**: Define a multi-node compute cluster in Azure Machine Learning. This is where your model will be trained.

3. **Prepare your training script**: Write a training script that is compatible with distributed training. For PyTorch, you might use `torch.distributed.launch` or `torch.nn.parallel.DistributedDataParallel`. For TensorFlow, you might use `tf.distribute.Strategy`.

4. **Configure the distributed training settings**: Use the `MpiConfiguration` for MPI-based frameworks like Horovod, or use the `TensorflowConfiguration` or `PyTorchConfiguration` for the respective frameworks. Specify the number of nodes and the number of GPUs per node.

5. **Submit the training job**: Use the `CommandJob` class to submit your training job to the compute target you configured.

Here is an example of how you might set up a distributed training job using the Azure Machine Learning SDK v2:

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import CommandJob, Environment, MpiDistribution, CodeConfiguration
from azure.identity import DefaultAzureCredential

# Set up the MLClient with your Azure subscription and resource details
ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace_name)

# Define the compute target (assuming it's already created in your workspace)
compute_target = ""your-compute-cluster""

# Define the environment for your training job
environment = Environment(
    image=""mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04"",
    conda_file=""path_to_your_conda_file.yml"",
    name=""my-distributed-training-env"",
)

# Define the distribution configuration for MPI-based distributed training
distribution = MpiDistribution()
distribution.process_count_per_instance = 4  # Number of GPUs per node

# Define the code configuration
code_configuration = CodeConfiguration(
    code=""path_to_your_training_script_folder"",
    command=""python train.py""  # Your training script
)

# Create the command job for distributed training
command_job = CommandJob(
    compute=compute_target,
    environment=environment,
    distribution=distribution,
    code_configuration=code_configuration,
    resources=ResourceConfiguration(instance_count=2),  # Number of nodes
)

# Submit the job
returned_job = ml_client.jobs.create_or_update(command_job)
```

In this example, replace `subscription_id`, `resource_group`, `workspace_name`, `your-compute-cluster`, `path_to_your_conda_file.yml`, `path_to_your_training_script_folder`, and `train.py` with your specific details. The `ResourceConfiguration` specifies that the job should run on 2 nodes, and the `MpiDistribution` specifies that there should be 4 processes per instance, which typically corresponds to the number of GPUs per node.

Please note that the Azure ML SDK v2 is continuously evolving, and the exact classes and methods may change. Always refer to the latest Azure ML SDK v2 documentation for the most up-to-date information.","[""Distributed training with Azure Machine\nLearning\nArticle •03/27/2023\nIn this article, you learn about distributed training and how Azure Machine Learning\nsupports it for deep learning models.\nIn distributed training the workload to train a model is split up and shared among\nmultiple mini processors, called worker nodes. These worker nodes work in parallel to\nspeed up model training. Distributed training can be used for traditional ML models, but\nis better suited for compute and time intensive tasks, like deep learning  for training\ndeep neural networks.\nThere are two main types of distributed training: data parallelism  and model parallelism .\nFor distributed training on deep learning models, the Azure Machine Learning SDK in\nPython  supports integrations with popular frameworks, PyT orch and T ensorFlow. Both\nframeworks employ data parallelism for distributed training, and can leverage\nhorovod  for optimizing compute speeds.\nDistributed training with PyT orch\nDistributed training with T ensorFlow\nFor ML models that don't require distributed training, see train models with Azure\nMachine Learning  for the different ways to train models using the Python SDK.\nData parallelism is the easiest to implement of the two distributed training approaches,\nand is sufficient for most use cases.\nIn this approach, the data is divided into partitions, where the number of partitions is\nequal to the total number of available nodes, in the compute cluster or serverless\ncompute . The model is copied in each of these worker nodes, and each worker operates\non its own subset of the data. K eep in mind that each node has to have the capacity to\nsupport the model that's being trained, that is the model has to entirely fit on each\nnode. The following diagram provides a visual demonstration of this approach.Deep learning and distributed training\nData parallelism"", ""Distributed GPU training guide (SDK v2)\nArticle •02/02/2024\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nLearn more about using distributed GPU training code in Azure Machine Learning. This\narticle helps you run your existing distributed training code, and offers tips and\nexamples for you to follow for each framework:\nMessage P assing Interface (MPI)\nHorovod\nEnvironment variables from Open MPI\nPyTorch\nTensorFlow\nAccelerate GPU training with InfiniBand\nReview the basic concepts of distributed GPU training , such as data parallelism ,\ndistributed dat a parallelism , and model p arallelism .\nAzure Machine Learning offers an MPI job  to launch a given number of processes in\neach node. Azure Machine Learning constructs the full MPI launch command ( mpirun)\nbehind the scenes. Y ou can't provide your own full head-node-launcher commands like\nmpirun or DeepSpeed launcher.\nPrerequisites\n\uea80 Tip\nIf you don't know which type of parallelism to use, more than 90% of the time you\nshould use distribut ed data p arallelism .\nMPI\n\uea80 Tip\nThe base Docker image used by an Azure Machine Learning MPI job needs to have\nan MPI library installed. Open MPI  is included in all the Azur e Machine Learning\nGPU b ase images . When you use a custom Docker image, you are responsible\nfor making sure the image includes an MPI library. Open MPI is recommended, but"", 'Distributed DNN training achieves scalability using a data partitioning algorithm that\nrespects time series boundaries. The following diagram illustrates a simple example with\ntwo partitions:\nDuring training, the DNN data loaders on each compute load just what they need to\ncomplete an iteration of back-propagation; the whole dataset is nev er read int o\nmemor y. The partitions are further distributed across multiple compute cores (usually\nGPUs) on possibly multiple nodes to accelerate training. Coordination across computes\nis provided by the Horovod  framework.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about deep learning models  for forecasting in AutoML\nNext steps', 'For more information, see What is automated machine learning? .\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\nLearning can automate this task for arbitrary parameterized commands with little\nmodification to your job definition. R esults are visualized in the studio.\nFor more information, see Tune hyperparameters .\nEfficiency of training for deep learning and sometimes classical machine learning\ntraining jobs can be drastically improved via multinode distributed training. Azure\nMachine Learning compute clusters and serverless compute  offer the latest GPU\noptions.\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\nclusters, and serverless compute :\nPyTorch\nTensorFlow\nMPI\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\nAzure S ynapse Analytics Spark clusters.\nFor more information, see Distributed training with Azure Machine Learning .\nScaling an ML project might require scaling embarrassingly parallel model training. This\npattern is common for scenarios like forecasting demand, where a model might be\ntrained for many stores.\nTo bring a model into production, you deploy the model. The Azure Machine Learning\nmanaged endpoints abstract the required infrastructure for both batch or real-time\n(online) model scoring (inferencing).Hyperparameter optimization\nMultinode distributed training\nEmbarrassingly parallel training\nDeploy models', ""Train TensorFlow models at scale with\nAzure Machine Lea rning\nArticle •04/04/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this article, learn how to run your TensorFlow  training scripts at scale using Azure\nMachine Learning Python SDK v2.\nThe example code in this article train a T ensorFlow model to classify handwritten digits,\nusing a deep neural network (DNN); register the model; and deploy it to an online\nendpoint.\nWhether you're developing a T ensorFlow model from the ground-up or you're bringing\nan existing model into the cloud, you can use Azure Machine Learning to scale out\nopen-source training jobs using elastic cloud compute resources. Y ou can build, deploy,\nversion, and monitor production-grade models with Azure Machine Learning.\nTo benefit from this article, you'll need to:\nAccess an Azure subscription. If you don't have one already, create a free\naccount .\nRun the code in this article using either an Azure Machine Learning compute\ninstance or your own Jupyter notebook.\nAzure Machine Learning compute instance—no downloads or installation\nnecessary\nComplete the Create resources to get started  to create a dedicated notebook\nserver pre-loaded with the SDK and the sample repository.\nIn the samples deep learning folder on the notebook server, find a\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\ndeploy-with-t ensor flow.\nYour Jupyter notebook server\nInstall the Azure Machine Learning SDK (v2) .\nDownload the following files:\ntraining script tf_mnist.py\nscoring script score.py\nsample request file sample-request.json\nPrerequisites""]",2241
gpt-4-turbo,14,,5.0,,How can Azure ML's HyperDrive be used to tune hyperparameters of a deep learning model? Give an example of specifying the search space.,v1,"Azure ML's HyperDrive can be used to tune hyperparameters of a deep learning model by automating the process of selecting the best hyperparameters for your model. HyperDrive runs multiple training experiments in parallel, each with different hyperparameter configurations. It then compares the performance of each configuration and provides the best performing hyperparameters based on the defined objective metric.

Here's an example of how you can specify the search space for a deep learning model using HyperDrive in Azure Machine Learning:

```python
from azureml.train.hyperdrive import RandomParameterSampling, HyperDriveConfig, PrimaryMetricGoal
from azureml.train.hyperdrive import choice, loguniform

# Define the parameter search space
param_sampling = RandomParameterSampling({
    'learning_rate': loguniform(-6, -1),  # e.g., 0.0001 to 0.1 on a log scale
    'batch_size': choice(16, 32, 64, 128),  # discrete choices for batch size
    'momentum': uniform(0.85, 0.95),  # a range for momentum values
    # Add other hyperparameters you want to tune
})

# Specify a primary metric to optimize
primary_metric_name = 'accuracy'
primary_metric_goal = PrimaryMetricGoal.MAXIMIZE

# Create a HyperDriveConfig using the estimator (assuming `estimator` is predefined), parameter sampler, and policy
hyperdrive_config = HyperDriveConfig(estimator=estimator,
                                     hyperparameter_sampling=param_sampling,
                                     policy=None,  # No early termination policy
                                     primary_metric_name=primary_metric_name,
                                     primary_metric_goal=primary_metric_goal,
                                     max_total_runs=100,
                                     max_concurrent_runs=4)

# Submit the HyperDrive run to the experiment
hyperdrive_run = experiment.submit(hyperdrive_config)
```

In this example, `RandomParameterSampling` is used to define a random search space for the hyperparameters. The `learning_rate` is searched over a log-uniform distribution, `batch_size` is chosen from a set of discrete options, and `momentum` is sampled from a uniform distribution within the specified range. The `HyperDriveConfig` object is then created with the defined search space, primary metric, and other configurations. Finally, the HyperDrive run is submitted to the experiment for execution.

Please note that the actual code may vary depending on the specific version of the Azure ML SDK and the details of the deep learning framework you are using (e.g., TensorFlow, PyTorch).","['For more information, see What is automated machine learning? .\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\nLearning can automate this task for arbitrary parameterized commands with little\nmodification to your job definition. R esults are visualized in the studio.\nFor more information, see Tune hyperparameters .\nEfficiency of training for deep learning and sometimes classical machine learning\ntraining jobs can be drastically improved via multinode distributed training. Azure\nMachine Learning compute clusters and serverless compute  offer the latest GPU\noptions.\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\nclusters, and serverless compute :\nPyTorch\nTensorFlow\nMPI\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\nAzure S ynapse Analytics Spark clusters.\nFor more information, see Distributed training with Azure Machine Learning .\nScaling an ML project might require scaling embarrassingly parallel model training. This\npattern is common for scenarios like forecasting demand, where a model might be\ntrained for many stores.\nTo bring a model into production, you deploy the model. The Azure Machine Learning\nmanaged endpoints abstract the required infrastructure for both batch or real-time\n(online) model scoring (inferencing).Hyperparameter optimization\nMultinode distributed training\nEmbarrassingly parallel training\nDeploy models', 'Hyperparameter tuning a model (v2)\nArticle •04/04/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAutomate efficient hyperparameter tuning using Azure Machine Learning SDK v2 and\nCLI v2 by way of the S weepJob type.\n1. Define the parameter search space for your trial\n2. Specify the sampling algorithm for your sweep job\n3. Specify the objective to optimize\n4. Specify early termination policy for low-performing jobs\n5. Define limits for the sweep job\n6. Launch an experiment with the defined configuration\n7. Visualize the training jobs\n8. Select the best configuration for your model\nHyperp aramet ers are adjustable parameters that let you control the model training\nprocess. For example, with neural networks, you decide the number of hidden layers and\nthe number of nodes in each layer. Model performance depends heavily on\nhyperparameters.\nHyperp aramet er tuning , also called hyperp aramet er optimization , is the process of\nfinding the configuration of hyperparameters that results in the best performance. The\nprocess is typically computationally expensive and manual.\nAzure Machine Learning lets you automate hyperparameter tuning and run experiments\nin parallel to efficiently optimize hyperparameters.\nTune hyperparameters by exploring the range of values defined for each\nhyperparameter.\nHyperparameters can be discrete or continuous, and has a distribution of values\ndescribed by a parameter expression .\nWhat is hyperparameter tuning ?\nDefine the search space', 'When training computer vision models, model performance depends heavily on the\nhyperparameter values selected. Often, you might want to tune the hyperparameters to\nget optimal performance. For computer vision tasks, you can sweep hyperparameters to\nfind the optimal settings for your model. This feature applies the hyperparameter tuning\ncapabilities in Azure Machine Learning. Learn how to tune hyperparameters .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nYAML\nYou can define the model architectures and hyperparameters to sweep in the parameter\nspace. Y ou can either specify a single model architecture or multiple ones.\nSee Individual trials  for the list of supported model architectures for each task type.\nSee Hyperparameters for computer vision tasks  hyperparameters for each\ncomputer vision task type.Azure CLI\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value:  0.0001\n      max_value:  0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value:  0.0001\n      max_value:  0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\nDefine the parameter search space', 'To tune the model\'s hyperparameters, define the parameter space in which to search\nduring training. Y ou\'ll do this by replacing some of the parameters ( batch_size,\nfirst_layer_neurons, second_layer_neurons, and learning_rate) passed to the training\njob with special inputs from the azure.ml.sweep package.\nPython\nThen, you\'ll configure sweep on the command job, using some sweep-specific\nparameters, such as the primary metric to watch and the sampling algorithm to use.\nIn the following code, we use random sampling to try different configuration sets of\nhyperparameters in an attempt to maximize our primary metric, validation_acc.\nWe also define an early termination policy—the BanditPolicy. This policy operates by\nchecking the job every two iterations. If the primary metric, validation_acc, falls outside\nthe top ten percent range, Azure Machine Learning will terminate the job. This saves the\nmodel from continuing to explore hyperparameters that show no promise of helping to\nreach the target metric.\nPythonfrom azure.ai.ml.sweep import Choice, LogUniform  \n# we will reuse the command_job created before. we call it as a function so  \nthat we can apply inputs  \n# we do not apply the \'iris_csv\' input again -- we will just use what was  \nalready defined earlier  \njob_for_sweep = job(  \n    batch_size=Choice(values=[ 25, 50, 100]), \n    first_layer_neurons=Choice(values=[ 10, 50, 200, 300, 500]), \n    second_layer_neurons=Choice(values=[ 10, 50, 200, 500]), \n    learning_rate=LogUniform(min_value= -6, max_value= -1), \n)\nfrom azure.ai.ml.sweep import BanditPolicy  \nsweep_job = job_for_sweep.sweep(  \n    compute=gpu_compute_target,  \n    sampling_algorithm= ""random"" , \n    primary_metric= ""Accuracy"" , \n    goal= ""Maximize"" , \n    max_total_trials= 20, \n    max_concurrent_trials= 4, \n    early_termination_policy=BanditPolicy(slack_factor= 0.1, \nevaluation_interval= 2), \n)', 'You trained the model with one set of parameters, let\'s now see if you can further\nimprove the accuracy of your model. Y ou can tune and optimize your model\'s\nhyperparameters using Azure Machine Learning\'s sweep  capabilities.\nTo tune the model\'s hyperparameters, define the parameter space in which to search\nduring training. Y ou do this by replacing some of the parameters passed to the training\njob with special inputs from the azure.ml.sweep package.\nSince the training script uses a learning rate schedule to decay the learning rate every\nseveral epochs, you can tune the initial learning rate and the momentum parameters.\nPython\nThen, you can configure sweep on the command job, using some sweep-specific\nparameters, such as the primary metric to watch and the sampling algorithm to use.\nIn the following code, we use random sampling to try different configuration sets of\nhyperparameters in an attempt to maximize our primary metric, best_val_acc.\nWe also define an early termination policy, the BanditPolicy, to terminate poorly\nperforming runs early. The BanditPolicy terminates any run that doesn\'t fall within the\nslack factor of our primary evaluation metric. Y ou apply this policy every epoch (since we\nreport our best_val_acc metric every epoch and evaluation_interval=1). Notice we\ndelay the first policy evaluation until after the first 10 epochs ( delay_evaluation=10).\nPythonfrom azure.ai.ml.sweep import Uniform\n# we will reuse the command_job created before. we call it as a function so  \nthat we can apply inputs\njob_for_sweep = job(\n    learning_rate=Uniform(min_value= 0.0005, max_value= 0.005),\n    momentum=Uniform(min_value= 0.9, max_value= 0.99),\n)\nfrom azure.ai.ml.sweep import BanditPolicy\nsweep_job = job_for_sweep.sweep(\n    compute= ""gpu-cluster"" ,\n    sampling_algorithm= ""random"" ,\n    primary_metric= ""best_val_acc"" ,\n    goal=""Maximize"" ,\n    max_total_trials= 8,\n    max_concurrent_trials= 4,\n    early_termination_policy=BanditPolicy(\n        slack_factor= 0.15, evaluation_interval= 1, delay_evaluation= 10']",2418
gpt-4-turbo,15,,5.0,,"What is the command to update an existing model's properties, such as description or tags, using the Azure CLI?",v1,"The command to update an existing model's properties, such as description or tags, using the Azure CLI is:

```azurecli
az ml model update --name run-model-example --version 1 --set description=""This is an updated description."" --set tags.stage=""Prod""
```

Replace `run-model-example` with the name of your model, `1` with the version of the model you want to update, and the description and tags with the values you wish to set.","['Get the details of a specific model:\ncli\nUpdate mutable properties of a specific model:\ncli\nArchiving a model will hide it by default from list queries ( az ml model list). You can\nstill continue to reference and use an archived model in your workflows. Y ou can archive\neither all versions of a model or only a specific version.\nIf you don\'t specify a version, all versions of the model under that given name will be\narchived. If you create a new model version under an archived model container, that\nnew version will automatically be set as archived as well.\nArchive all versions of a model:Show\nAzure CLI\naz ml model show --name run-model-example --version 1  \nUpdate\nAzure CLI\naz ml model update --name  run-model-example --version 1 --set  \ndescription=""This is an updated description."" --set tags.stage=""Prod""  \n） Impor tant\nFor model, only description and tags can be updated. All other properties are\nimmutable; if you need to change any of those properties you should create a new\nversion of the model.\nArchive', 'Execute the following command in the Azure CLI, and update the <> placeholders\nwith your data asset name, version and key-value pair for the tag.\nAzure CLI\nTypically, your ETL processes organize your folder structure on Azure storage by time,\nfor example:\ntext\nThe combination of time/version structured folders and Azure Machine Learning T ables\n(MLTable) allow you to construct versioned datasets. T o show how to achieve versioned\ndata with Azure Machine Learning T ables, we use a hypothetical ex ample . Suppose you\nhave a process that uploads camera images to Azure Blob storage every week, in the\nfollowing structure:\ntextaz ml data update --name <DATA ASSET NAME>  --version  <VERSION>  --set \ntags.<KEY>=<VALUE> \nVersioning best practices\n/ \n└── 📁 mydata \n    ├── 📁  year=2022  \n    │   ├── 📁  month=11  \n    │   │   └── 📄  file1 \n    │   │   └── 📄  file2 \n    │   └── 📁  month=12  \n    │       └── 📄  file1 \n    │   │   └── 📄  file2 \n    └── 📁  year=2023  \n        └── 📁  month=1  \n            └── 📄  file1 \n    │   │   └── 📄  file2 \n/myimages  \n└── 📁 year=2022  \n    ├── 📁  week52 \n    │   ├── 📁  camera1  \n    │   │   └── 🖼  file1.jpeg  \n    │   │   └── 🖼  file2.jpeg  \n    │   └── 📁  camera2  \n    │       └── 🖼  file1.jpeg  \n    │   │   └── 🖼  file2.jpeg', 'The az ml model command can be used for managing Azure Machine Learning models.\nExamples are available in the examples GitHub repository . Several are shown below.\nYAML\nYAML\nInstall and use the CLI (v2)Remarks\nExamples\nYAML: local file\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-file-example  \npath: mlflow-model/model.pkl  \ndescription:  Model created from local file. \nYAML: local folder in MLflow format\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-mlflow-example  \npath: mlflow-model  \ntype: mlflow_model  \ndescription:  Model created from local MLflow model directory.', 'If you want to update the code, model, or environment, update the Y AML file, and\nthen run the az ml online-endpoint update command.\nTo understand how update works:\n1. Open the file online/model-1/onlines coring/s core.py.\n2. Change the last line of the init() function: After logging.info(""Init\ncomplete""), add logging.info(""Updated successfully"").\n3. Save the file.\n4. Run this command:\nAzure CLI(Optional) Update the deployment\nAzure CLI\n７ Note\nIf you update instance count (to scale your deployment) along with other\nmodel settings (such as code, model, or environment) in a single update\ncommand, the scaling operation will be performed first, then the other updates\nwill be applied. It\'s a good practice to perform these operations separately in a\nproduction environment.\naz ml online-deployment update -n blue --endpoint  $ENDPOINT_NAME  -f \nendpoints/online/managed/sample/blue -deployment .yml\n７ Note\nUpdating by using Y AML is declarative. That is, changes in the Y AML are\nreflected in the underlying Azure R esource Manager resources (endpoints\nand deployments). A declarative approach facilitates GitOps : All\nchanges to endpoints and deployments (even instance_count) go\nthrough the Y AML.\n\uea80 Tip', 'then select Regist er.\nThe SDK and CLI (v2) also allow you to manage the lifecycle of your Azure Machine\nLearning model assets.\nList all the models in your workspace:\ncli\nList all the model versions under a given name:\ncli\n\uf80a\nManage models\nList\nAzure CLI\naz ml model list  \nAzure CLI\naz ml model list --name run-model-example']",1299
gpt-4-turbo,16,,5.0,,How does Azure ML integrate with Azure DevOps for implementing CI/CD pipelines for ML models? Describe the steps involved.,v1,"Azure Machine Learning integrates with Azure DevOps to implement CI/CD pipelines for ML models by allowing you to automate the entire lifecycle of machine learning models from training to deployment and management. Here are the general steps involved in setting up CI/CD pipelines for ML models with Azure ML and Azure DevOps:

1. **Repository Setup**: Start by setting up a repository for your machine learning project in Azure Repos or GitHub. This repository will contain your machine learning code, training scripts, inference scripts, environment configurations, and other necessary files.

2. **Azure ML Workspace**: Create an Azure Machine Learning workspace, which is the foundational block in Azure ML that provides a centralized place to work with all the artifacts you create when you use Azure Machine Learning.

3. **Azure Pipelines Configuration**: Configure Azure Pipelines within Azure DevOps. This involves creating a pipeline that references your repository and includes steps to train the model, evaluate it, and, if the evaluation is successful, deploy the model.

4. **Training Pipeline**: Create a training pipeline in Azure ML that can be triggered by Azure Pipelines. This pipeline will run the training script, log metrics, and save the trained model.

5. **Model Evaluation**: After training, evaluate the model against a validation dataset to ensure it meets the required performance criteria.

6. **Model Registration**: If the model passes the evaluation step, register the model in the Azure ML workspace. Model registration allows you to store and version your models in the workspace.

7. **Release Pipeline**: Set up a release pipeline in Azure DevOps that is triggered once the model is registered. This pipeline can include steps to deploy the model to a target environment, such as Azure Kubernetes Service (AKS) or Azure Container Instances (ACI).

8. **Model Deployment**: Deploy the model as a web service to an Azure ML online endpoint or an edge device using Azure ML. This step involves creating an inference configuration that specifies the environment and scripts needed for the web service.

9. **Monitoring and Management**: After deployment, set up monitoring and management for the deployed models. Azure ML provides capabilities to monitor the health, performance, and usage of your models in production.

10. **Feedback Loop**: Implement a feedback loop to continuously improve the model. This can involve retraining the model with new data, tuning hyperparameters, or updating the model based on performance metrics.

By integrating Azure ML with Azure DevOps, you can automate these steps to create a robust MLOps workflow that ensures consistent quality and efficiency in deploying and managing machine learning models.","['Continuous integration and deployment of machine learning models with Azure\nPipelines\nMachine Learning ML Ops  repository\nLearn more by reading and exploring the following resources:\nSet up ML Ops with Azure DevOps\nLearning path: End-to-end ML Ops with Azure Machine Learning\nHow to deploy a model to an online endpoint  with Machine Learning\nTutorial: T rain and deploy a model\nCI/CD of machine learning models with Azure Pipelines\nMachine learning at scale\nAzure AI reference architectures and best practices repo\nNext steps', 'Some key features enabling ML Ops include:\ngit integration.\nMLflow integration.\nMachine learning pipeline scheduling.\nAzure Event Grid integration for custom triggers.\nEase of use with CI/CD tools like GitHub Actions or Azure DevOps.\nMachine Learning also includes features for monitoring and auditing:\nJob artifacts, such as code snapshots, logs, and other outputs.\nLineage between jobs and assets, such as containers, data, and compute resources.\nIf you use Apache Airflow, the airflow-provider-azure-machinelearning  package is a\nprovider that enables you to submit workflows to Azure Machine Learning from Apache\nAirFlow.\nStart using Azure Machine Learning:\nSet up an Azure Machine Learning workspace\nTutorial: Build a first machine learning project\nRun training jobs\nNext steps', 'What are Azu re Machine Learning\npipelines?\nArticle •01/31/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAn Azure Machine Learning pipeline is an independently executable workflow of a\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\nstandardize the best practices of producing a machine learning model, enables the team\nto execute at scale, and improves the model building efficiency.\nThe core of a machine learning pipeline is to split a complete machine learning task into\na multistep workflow. Each step is a manageable component that can be developed,\noptimized, configured, and automated individually. S teps are connected through well-\ndefined interfaces. The Azure Machine Learning pipeline service automatically\norchestrates all the dependencies between pipeline steps. This modular approach brings\ntwo key benefits:\nStandardize the Machine learning operation (ML Ops) practice and support scalable\nteam collaboration\nTraining efficiency and cost reduction\nMachine learning operation (ML Ops) automates the process of building machine\nlearning models and taking the model to production. This is a complex process. It\nusually requires collaboration from different teams with different skills. A well-defined\nmachine learning pipeline can abstract this complex process into a multiple steps\nworkflow, mapping each step to a specific task such that each team can work\nindependently.\nFor example, a typical machine learning project includes the steps of data collection,\ndata preparation, model training, model evaluation, and model deployment. Usually, the\ndata engineers concentrate on data steps, data scientists spend most time on model\nWhy are Azure Machine Learning pipelines\nneeded?\nStandardize the MLOps practice and support scalable\nteam collaboration', 'What are Azu re Machine Learning\npipelines?\nArticle •01/31/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAn Azure Machine Learning pipeline is an independently executable workflow of a\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\nstandardize the best practices of producing a machine learning model, enables the team\nto execute at scale, and improves the model building efficiency.\nThe core of a machine learning pipeline is to split a complete machine learning task into\na multistep workflow. Each step is a manageable component that can be developed,\noptimized, configured, and automated individually. S teps are connected through well-\ndefined interfaces. The Azure Machine Learning pipeline service automatically\norchestrates all the dependencies between pipeline steps. This modular approach brings\ntwo key benefits:\nStandardize the Machine learning operation (ML Ops) practice and support scalable\nteam collaboration\nTraining efficiency and cost reduction\nMachine learning operation (ML Ops) automates the process of building machine\nlearning models and taking the model to production. This is a complex process. It\nusually requires collaboration from different teams with different skills. A well-defined\nmachine learning pipeline can abstract this complex process into a multiple steps\nworkflow, mapping each step to a specific task such that each team can work\nindependently.\nFor example, a typical machine learning project includes the steps of data collection,\ndata preparation, model training, model evaluation, and model deployment. Usually, the\ndata engineers concentrate on data steps, data scientists spend most time on model\nWhy are Azure Machine Learning pipelines\nneeded?\nStandardize the MLOps practice and support scalable\nteam collaboration', 'Use Azu re Pipelines with Azure Machine\nLearning\nArticle •09/29/2023\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\nof the operations you can automate are:\nData preparation (extract, transform, load operations)\nTraining machine learning models with on-demand scale-out and scale-up\nDeployment of machine learning models as public or private web services\nMonitoring deployed machine learning models (such as for performance or data-\ndrift analysis)\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\nmachine learning model to Azure Machine Learning .\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\nv2.\nComplete the Create resources to get started  to:\nCreate a workspace\nCreate a cloud-based compute cluster  to use for training your model\nAzure Machine Learning extension for Azure Pipelines. This extension can be\ninstalled from the Visual S tudio marketplace at\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-\nv2.\nFork the following repo at GitHub:Prerequisites\nStep 1: Get the code\nhttps://github.com/azure/azureml-examples']",1786
gpt-4-turbo,17,,5.0,,Explain the process of setting up a real-time inferencing endpoint using a deep learning model in Azure ML. What are the performance tuning options available?,v1,"To set up a real-time inferencing endpoint using a deep learning model in Azure Machine Learning, you would typically follow these steps:

1. **Prepare Your Model**: Make sure your deep learning model is trained and ready for deployment. It should be saved in a format that Azure ML can work with, such as ONNX or a framework-specific format like TensorFlow's SavedModel or PyTorch's state dictionary.

2. **Register the Model**: Upload and register your model in the Azure ML workspace. This allows you to manage and version your model.

3. **Define the Scoring Script**: Create a scoring script (entry script) that loads the model and defines how the model will process incoming requests. This script should include two main functions: `init()` to load the model and `run(data)` to run the model on the input data.

4. **Create an Environment**: Define an environment that specifies all the Python packages and dependencies required by your model and scoring script. This can be done using a Conda environment file or a Docker image.

5. **Configure the Endpoint**: Define the endpoint by specifying the endpoint name and the authentication mode (key-based or token-based). You may also need to configure other settings such as compute type (CPU/GPU), instance count, and instance type depending on the load and performance requirements.

6. **Deploy the Model**: Deploy the model to the endpoint. Azure ML will set up a web service that can receive HTTP requests, load the model, and return predictions.

7. **Test the Endpoint**: Once the endpoint is deployed, test it by sending HTTP requests with input data and verifying that you receive the expected predictions.

For performance tuning, Azure Machine Learning offers several options:

- **Instance Type Selection**: Choose the right virtual machine size or compute cluster that matches your model's resource requirements (CPU/GPU, memory).

- **Scaling**: Configure the endpoint to automatically scale the number of instances based on the load. This helps in handling varying numbers of inference requests.

- **Batch Inference**: If real-time requirements are flexible, batch processing can be used to process multiple requests at once, which can be more efficient.

- **Optimized Model Formats**: Convert your model to an optimized format like ONNX, which can provide faster inferencing times.

- **Concurrency**: Adjust the concurrency level to define how many requests can be processed in parallel by a single instance.

- **Caching**: Implement caching for frequent requests to reduce latency and compute overhead.

- **Monitoring and Diagnostics**: Use Azure ML's built-in monitoring and diagnostics to identify bottlenecks and performance issues, which can then be addressed through tuning.

Remember to follow best practices for security, monitoring, and maintaining your endpoints to ensure they remain reliable and performant.","['Online endpoints and deployments for\nreal-time inference\nArticle •10/24/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAzure Machine Learning allows you to perform real-time inferencing on data by using\nmodels that are deployed to online endpoints . Inferencing is the process of applying new\ninput data to a machine learning model to generate outputs. While these outputs are\ntypically referred to as ""predictions,"" inferencing can be used to generate outputs for\nother machine learning tasks, such as classification and clustering.\nOnline endpoints  deploy models to a web server that can return predictions under the\nHTTP protocol. Use online endpoints to operationalize models for real-time inference in\nsynchronous low-latency requests. W e recommend using them when:\nTo define an endpoint, you need to specify:\nEndpoint name : This name must be unique in the Azure region. For more\ninformation on the naming rules, see endpoint limits .\nAuthentication mode : You can choose between key-based authentication mode\nand Azure Machine Learning token-based authentication mode for the endpoint. A\nkey doesn\'t expire, but a token does expire. For more information on\nauthenticating, see Authenticate to an online endpoint .\nAzure Machine Learning provides the convenience of using managed online endpoints\nfor deploying your ML models in a turnkey manner. This is the recommended  way to use\nonline endpoints in Azure Machine Learning. Managed online endpoints work with\npowerful CPU and GPU machines in Azure in a scalable, fully managed way. These\nendpoints also take care of serving, scaling, securing, and monitoring your models, to\nfree you from the overhead of setting up and managing the underlying infrastructure. T o\nOnline  endpoints\nYou have low-latency requirements＂\nYour model can answer the request in a relatively short amount of time＂\nYour model\'s inputs fit on the HT TP payload of the request ＂\nYou need to scale up in terms of number of requests＂', 'In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\nmodel to an online endpoint. See these other articles to learn more about Azure\nMachine Learning.\nTrack run metrics during training\nTune hyperparameters\nReference architecture for distributed deep learning training in AzureNext steps', ""Batch scoring, or batch infer encing , involves invoking an endpoint with a reference to\ndata. The batch endpoint runs jobs asynchronously to process data in parallel on\ncompute clusters and store the data for further analysis.\nReal-time s coring, or online infer encing , involves invoking an endpoint with one or more\nmodel deployments and receiving a response in near real time via HT TPS. T raffic can be\nsplit across multiple deployments, allowing for testing new model versions by diverting\nsome amount of traffic initially and increasing after confidence in the new model is\nestablished.\nFor more information, see:\nDeploy a model with a real-time managed endpoint\nUse batch endpoints for scoring\nDevOps for ML models, often called ML Ops, is a process for developing models for\nproduction. A model's lifecycle from training to deployment must be auditable if not\nreproducible.\nLearn more about MLOps in Azure Machine Learning .\nMachine Learning is built with the model lifecycle in mind. Y ou can audit the model\nlifecycle down to a specific commit and environment.Real-time and batch scoring (inferencing)\nMLOps: DevOps for machine learning\nML model lifecycle\nIntegrations enabling MLOPs"", 'What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps', ""Currently, Azure Machine Learning supports fine-tuning models for the following\nlanguage tasks:\nText classification\nToken classification\nQuestion answering\nSummarization\nTranslation\nTo enable users to quickly get started with fine-tuning, we have published samples (both\nPython notebooks and CLI examples) for each task in the azureml-examples git repo\nFinetune samples . Each model card also links to fine-tuning samples for supported\nfine-tuning tasks.\nYou can deploy foundation models (both pre-trained models from the model catalog,\nand fine-tuned models, once they're registered to your workspace) to an endpoint that\ncan then be used for inferencing. Deployment to both real time endpoints and batch\nendpoints is supported. Y ou can deploy these models by using either the Deploy UI\nwizard or by using the code based samples linked from the model card.\nYou can invoke the Deploy UI form by selecting the Deploy  button on the model card\nfor any foundation model, and selecting either R eal-time endpoint or Batch endpoint\nDeploying foundation models to endpoints for\ninferencing\nDeploying using the studio\n\uf80a\nDeployment settings""]",1568
gpt-4-turbo,18,,5.0,,How can you automate the process of model retraining and deployment using Azure ML Pipelines?,v1,"To automate the process of model retraining and deployment using Azure ML Pipelines, you can follow these general steps:

1. **Create an Azure ML Workspace**: This is the foundational block for any machine learning operations in Azure. It provides a centralized place to work with all the artifacts you create when you use Azure Machine Learning.

2. **Set up a Compute Target**: Create a cloud-based compute cluster that can scale out for training your model. This is where your model will be trained.

3. **Prepare Your Training Script**: Write a training script in Python that defines how your model should be trained. This script is run during the pipeline execution.

4. **Define a Pipeline**: Use the Azure Machine Learning Python SDK v2 to define a pipeline. A pipeline is a workflow of a complete machine learning task. Subtasks are encapsulated as a series of steps within the pipeline.

5. **Create a Pipeline Step for Training**: Define a step in your pipeline to train the model. This step will use the compute resources you've set up to run the training script.

6. **Create a Pipeline Step for Model Registration**: After the model is trained, create another step that registers the model to your Azure ML Workspace. Model registration allows you to store and version your trained models in your workspace.

7. **Create a Pipeline Step for Deployment**: Define a step to deploy the trained model as a web service on Azure. You can deploy the model to either Azure Kubernetes Service (AKS) for high-scale production deployments or Azure Container Instances (ACI) for low-scale testing and development.

8. **Set up Continuous Integration and Delivery (CI/CD)**: Use Azure DevOps Services to set up CI/CD pipelines that trigger the Azure ML Pipeline. You can automate the entire process from code commit to model deployment.

9. **Monitor the Deployed Model**: Implement monitoring for the deployed model to track its performance and data drift. This can be done using Azure ML's monitoring capabilities.

10. **Install Azure Machine Learning Extension for Azure Pipelines**: To integrate Azure Pipelines with Azure Machine Learning, install the Azure Machine Learning extension from the Visual Studio Marketplace.

11. **Fork and Clone the Necessary Code**: Fork and clone the example repository provided in the Azure ML documentation to get started with predefined templates and scripts.

By following these steps, you can create an end-to-end automated workflow for retraining and deploying machine learning models using Azure ML Pipelines. This allows for consistent, repeatable processes that can be triggered on a schedule or in response to data changes, ensuring that your models are always up-to-date and performing optimally.","['Use Azu re Pipelines with Azure Machine\nLearning\nArticle •09/29/2023\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\nof the operations you can automate are:\nData preparation (extract, transform, load operations)\nTraining machine learning models with on-demand scale-out and scale-up\nDeployment of machine learning models as public or private web services\nMonitoring deployed machine learning models (such as for performance or data-\ndrift analysis)\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\nmachine learning model to Azure Machine Learning .\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\nv2.\nComplete the Create resources to get started  to:\nCreate a workspace\nCreate a cloud-based compute cluster  to use for training your model\nAzure Machine Learning extension for Azure Pipelines. This extension can be\ninstalled from the Visual S tudio marketplace at\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-\nv2.\nFork the following repo at GitHub:Prerequisites\nStep 1: Get the code\nhttps://github.com/azure/azureml-examples', 'Continuous integration and deployment of machine learning models with Azure\nPipelines\nMachine Learning ML Ops  repository\nLearn more by reading and exploring the following resources:\nSet up ML Ops with Azure DevOps\nLearning path: End-to-end ML Ops with Azure Machine Learning\nHow to deploy a model to an online endpoint  with Machine Learning\nTutorial: T rain and deploy a model\nCI/CD of machine learning models with Azure Pipelines\nMachine learning at scale\nAzure AI reference architectures and best practices repo\nNext steps', ""6. Select main as a branch and choose /mlops/devops-pipelines/deploy-model-\ntraining-pipeline.yml, then select Continue .\n7. Save and Run  the pipeline\nThis scenario includes prebuilt workflows for two approaches to deploying a trained\nmodel, batch scoring or a deploying a model to an endpoint for real-time scoring. Y ou\nmay run either or both of these workflows to test the performance of the model in your\nAzure ML workspace. IN this example we will be using real-time scoring.\n1. Go to ADO pipelines\n2. Select New Pipeline .\n3. Select Azur e Repos Git .７ Note\nAt this point, the infrastructure is configured and the Prototyping Loop of the\nMLOps Architecture is deployed. you're ready to move to our trained model to\nproduction.\nDeploying the Trained model\nDeploy ML model endpoint"", '3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\nMachine Learning in many different ways .\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\niterations over different models, hyperparameter settings, advanced\npreprocessing/featurization, and what metrics to look at when determining the\nbest model.\n5. Submit the training job.\n6. Review the r esults\nThe following diagram illustrates this process. \nYou can also inspect the logged job information, which contains metrics  gathered\nduring the job. The training job produces a Python serialized object ( .pkl file) that\ncontains the model and data preprocessing.\nWhile model building is automated, you can also learn how important or relevant\nfeatures are  to the generated models.\nApply automated ML when you want Azure Machine Learning to train and tune a model\nfor you using the target metric you specify. Automated ML democratizes the machine\nlearning model development process, and empowers its users, no matter their data\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\nML professionals and developers across industries can use automated ML to:\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\nforecasting, computer vision & NLP', '4. When the pipeline has complete successfully, you can find your Azure Machine\nLearning W orkspace and associated resources by logging in to the Azure P ortal.\nNext, a model training and scoring pipelines will be deployed into the new\nMachine Learning environment.\nThe solution accelerator includes code and data for a sample end-to-end machine\nlearning pipeline which runs a linear regression to predict taxi fares in NY C. The pipeline\nis made up of components, each serving different functions, which can be registered\nwith the workspace, versioned, and reused with various inputs and outputs. Sample\npipelines and workflows for the Computer Vision and NLP scenarios will have different\nsteps and deployment steps.\nThis training pipeline contains the following steps:\nPrepare Data\nThis component takes multiple taxi datasets (yellow and green) and merges/filters\nthe data, and prepare the train/val and evaluation datasets.\nInput: Local data under ./data/ (multiple .csv files)\nOutput: Single prepared dataset (.csv) and train/val/test datasets.\nTrain Model\nThis component trains a Linear R egressor with the training set.\nInput: T raining dataset\nOutput: T rained model (pickle format)\nEvaluat e Model\nThis component uses the trained model to predict taxi fares on the test set.\nInput: ML model and T est dataset\nOutput: P erformance of model and a deploy flag whether to deploy or not.Sample Training  and Deployment Scenario']",1678
gpt-4-turbo,19,,5.0,,Describe the approach to use Azure ML for implementing a recommendation system. What kind of data and algorithms are typically involved?,v1,"To implement a recommendation system using Azure Machine Learning (Azure ML), you would typically follow these steps:

1. **Data Upload and Exploration**:
   - You would start by uploading your data to Azure ML. This data could be user-item interactions, user or item features, or a combination of both.
   - You would then explore and preprocess your data using Azure ML notebooks or scripts. This might involve normalizing data, handling missing values, and encoding categorical variables.

2. **Model Development**:
   - On a cloud workstation provided by Azure ML, you would prototype and develop your recommendation model. Azure ML supports various machine learning frameworks and libraries, such as TensorFlow, PyTorch, and scikit-learn, which you can use to build your model.

3. **Model Training**:
   - You would train your recommendation model using Azure ML's training capabilities. This involves selecting an algorithm suitable for recommendation systems, such as collaborative filtering, content-based filtering, or hybrid methods.
   - Azure ML allows you to train models on various scales of compute resources, from a single VM to a cluster of VMs for larger datasets.

4. **Model Deployment**:
   - Once the model is trained and evaluated, you can deploy it as an online endpoint using Azure ML. This endpoint can then serve real-time recommendations to users.

5. **Machine Learning Pipelines**:
   - For production systems, you would create machine learning pipelines in Azure ML. These pipelines would automate the workflow of data preprocessing, model training, evaluation, and deployment.

**Data Involved**:
- User-item interaction data, which could be explicit (e.g., ratings) or implicit (e.g., clicks, purchases).
- User demographic data or profile information.
- Item attributes or content information.

**Algorithms Typically Involved**:
- **Collaborative Filtering**: Algorithms like matrix factorization (e.g., SVD, ALS) that make recommendations based on user-item interactions.
- **Content-Based Filtering**: Algorithms that use item features to recommend items similar to what the user likes.
- **Hybrid Methods**: Combining collaborative and content-based approaches to leverage both interaction data and item/user features.
- **Deep Learning**: Neural network architectures like autoencoders or RNNs for more complex recommendation systems.

Azure ML provides a robust environment to manage the entire lifecycle of a recommendation system, from data handling to model deployment and management.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Tell us about y our PDF experience.\nAzur e Machine Learning documentation\nLearn how to train and deploy models and manage the ML lifecycle (ML Ops) with Azure\nMachine Learning. Tutorials, code examples, API references, and more.\nOverview\nｅOVERVIE W\nWhat is Azure Machine Learning?\nSetup & quickstar t\nｆQUICKST ART\nCreate resources\nGet started with Azure Machine Learning\nStart with the b asics\nｇTUTORIAL\nPrepare and explore data\nDevelop on a cloud workstation\nTrain a model\nDeploy a model\nSet up a reusable pipeline\nWork with data\nｃHOW-T O GUIDE\nUse Apache Spark in Azure Machine Learning\nCreate data assets', ""What is Azure Machine Learning?\nArticle •01/29/2024\nAzure Machine Learning is a cloud service for accelerating and managing the machine\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\nin their day-to-day workflows to train and deploy models and manage machine learning\noperations (ML Ops).\nYou can create a model in Machine Learning or use a model built from an open-source\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\nretrain, and redeploy models.\nMachine Learning is for individuals and teams implementing ML Ops within their\norganization to bring ML models into production in a secure and auditable production\nenvironment.\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\nday workflows. Application developers can use tools for integrating models into\napplications or services. Platform developers can use a robust set of tools, backed by\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\nbased access control for infrastructure. Y ou can set up a project to deny access to\nprotected data and select operations.\nML projects often require a team with a varied skill set to build and maintain. Machine\nLearning has tools that help enable you to:\uea80 Tip\nFree trial!  If you don't have an Azure subscription, create a free account before you\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\nto spend on Azure services. After they're used up, you can keep the account and\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\nchange your settings and ask to be charged.\nWho is Azure Machine Learning for?\nProductivity for everyone on the team"", ""If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about calendar features for time series forecasting in AutoML .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about AutoML forecasting for lagged features .Next steps"", 'For training in developing AI and Machine Learning solutions with Microsoft, see\nMicrosoft Learn training .\nChoose a Microsoft cognitive services technology\nArtificial intelligence (AI) architecture design\nHow Azure Machine Learning works: resources and assetsRelated resources']",1283
gpt-4-turbo,20,,5.0,,How can you leverage Azure ML's built-in support for ONNX models for interoperability across different frameworks?,v1,"Azure Machine Learning (Azure ML) provides built-in support for ONNX (Open Neural Network Exchange) models, which facilitates interoperability across different machine learning frameworks. Here's how you can leverage this support:

1. **Model Conversion to ONNX**: You can convert models from various frameworks such as TensorFlow, PyTorch, Scikit-Learn, Keras, and others into the ONNX format. Azure ML supports the conversion process through its integration with the ONNX converters available for these frameworks.

2. **Model Deployment**: Once you have your model in ONNX format, you can deploy it using Azure ML services. Azure ML allows you to deploy models as web services on Azure Container Instances (ACI), Azure Kubernetes Service (AKS), or on IoT devices and edge computing platforms.

3. **Inference Optimization**: Azure ML leverages ONNX Runtime for optimized model inference. ONNX Runtime is an inference engine that provides high performance across a variety of platforms and hardware, including cloud and edge devices. It supports integration with hardware-specific accelerators like NVIDIA TensorRT, Intel OpenVINO, and DirectML for Windows.

4. **MLOps**: Azure ML supports MLOps practices, allowing you to manage, version, and monitor ONNX models just like any other model format. This includes the ability to track model performance, data drift, and retraining pipelines.

5. **Scalability and High Availability**: By deploying ONNX models on Azure ML, you can take advantage of Azure's infrastructure for scalability and high availability. This is crucial for production-grade machine learning workloads.

6. **Integration with Azure Services**: ONNX models deployed on Azure ML can easily integrate with other Azure services such as Azure Functions, Azure Logic Apps, and Azure IoT Edge, allowing for seamless workflows and data processing pipelines.

By using Azure ML's support for ONNX, you can train your model in your preferred framework, convert it to ONNX format, and then deploy and manage it efficiently, while also ensuring compatibility and performance across different platforms and devices.","[""ONNX and Azure Machine Learning:\nCreate and accelerate ML models\nArticle •06/13/2023\nLearn how using the Open Neural Network Exchange  (ONNX) can help optimize the\ninference of your machine learning model. Inference, or model scoring, is the phase\nwhere the deployed model is used for prediction, most commonly on production data.\nOptimizing machine learning models for inference (or model scoring) is difficult since\nyou need to tune the model and the inference library to make the most of the hardware\ncapabilities. The problem becomes extremely hard if you want to get optimal\nperformance on different kinds of platforms (cloud/edge, CPU/GPU, etc.), since each one\nhas different capabilities and characteristics. The complexity increases if you have\nmodels from a variety of frameworks that need to run on a variety of platforms. It's very\ntime consuming to optimize all the different combinations of frameworks and hardware.\nA solution to train once in your preferred framework and run anywhere on the cloud or\nedge is needed. This is where ONNX comes in.\nMicrosoft and a community of partners created ONNX as an open standard for\nrepresenting machine learning models. Models from many frameworks  including\nTensorFlow, PyT orch, SciKit-Learn, K eras, Chainer, MXNet, MA TLAB, and SparkML can be\nexported or converted to the standard ONNX format. Once the models are in the ONNX\nformat, they can be run on a variety of platforms and devices.\nONNX Runtime  is a high-performance inference engine for deploying ONNX models\nto production. It's optimized for both cloud and edge and works on Linux, Windows,\nand Mac. Written in C++, it also has C, Python, C#, Java, and JavaScript (Node.js) APIs\nfor usage in a variety of environments. ONNX Runtime supports both DNN and\ntraditional ML models and integrates with accelerators on different hardware such as\nTensorR T on NVidia GPUs, OpenVINO on Intel processors, DirectML on Windows, and\nmore. By using ONNX Runtime, you can benefit from the extensive production-grade\noptimizations, testing, and ongoing improvements.\nONNX Runtime is used in high-scale Microsoft services such as Bing, Office, and Azure\nAI. Performance gains are dependent on a number of factors, but these Microsoft\nservices have seen an average 2x per formance gain on CPU . In addition to Azure\nMachine Learning services, ONNX Runtime also runs in other products that support\nMachine Learning workloads, including:\nWindows: The runtime is built into Windows as part of Windows Machine Learning\nand runs on hundreds of millions of devices."", 'Azure SQL product family: Run native scoring on data in Azure SQL Edge  and\nAzure SQL Managed Instance .\nML.NET: Run ONNX models in ML.NET .\nYou can obtain ONNX models in several ways:\nTrain a new ONNX model in Azure Machine Learning (see examples at the bottom\nof this article) or by using automated Machine Learning capabilities\nConvert existing model from another format to ONNX (see the tutorials )\nGet a pre-trained ONNX model from the ONNX Model Zoo\nGenerate a customized ONNX model from Azure Custom Vision service\nMany models including image classification, object detection, and text processing can\nbe represented as ONNX models. If you run into an issue with a model that cannot be\nconverted successfully, please file an issue in the GitHub of the respective converter that\nyou used. Y ou can continue using your existing format model until the issue is\naddressed.\nWith Azure Machine Learning, you can deploy, manage, and monitor your ONNX\nmodels. Using the standard deployment workflow  and ONNX Runtime, you can create a\nREST endpoint hosted in the cloud. See example Jupyter notebooks at the end of this\narticle to try it out for yourself.\n\uf80a\nGet ONNX models\nDeploy ONNX models in Azure\nInstall and use ONNX Runtime with Python', 'See the AutoML package  for changing default ensemble settings in automated machine\nlearning.\nWith Azure Machine Learning, you can use automated ML to build a Python model and\nhave it converted to the ONNX format. Once the models are in the ONNX format, they\ncan be run on a variety of platforms and devices. Learn more about accelerating ML\nmodels with ONNX .\nSee how to convert to ONNX format in this Jupyter notebook example . Learn which\nalgorithms are supported in ONNX .\nThe ONNX runtime also supports C#, so you can use the model built automatically in\nyour C# apps without any need for recoding or any of the network latencies that REST\nendpoints introduce. Learn more about using an AutoML ONNX model in a .NET\napplication with ML.NET  and inferencing ONNX models with the ONNX runtime C#\nAPI .\nThere are multiple resources to get you up and running with AutoML.\nTutorials are end-to-end introductory examples of AutoML scenarios.\nFor a code fir st experience , follow the Tutorial: T rain an object detection model\nwith AutoML and Python\nFor a low or no-code experience , see the Tutorial: T rain a classification model with\nno-code AutoML in Azure Machine Learning studio .\nHow-to articles provide additional detail into what functionality automated ML offers.\nFor example,\nConfigure the settings for automatic training experiments\nWithout code in the Azure Machine Learning studio .\nWith the Python SDK .\nLearn how to train computer vision models with Python .\nLearn how to view the generated code from your automated ML models (SDK v1) .AutoML & ONNX\nNext steps\nTutorials/ how-tos', ""Make predictions with ONNX on\ncomputer vision models from AutoML\nArticle •04/04/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this article, you will learn how to use Open Neural Network Exchange (ONNX) to\nmake predictions on computer vision models generated from automated machine\nlearning (AutoML) in Azure Machine Learning.\nTo use ONNX for predictions, you need to:\n1. Download ONNX model files from an AutoML training run.\n2. Understand the inputs and outputs of an ONNX model.\n3. Preprocess your data so that it's in the required format for input images.\n4. Perform inference with ONNX Runtime for Python.\n5. Visualize predictions for object detection and instance segmentation tasks.\nONNX  is an open standard for machine learning and deep learning models. It enables\nmodel import and export (interoperability) across the popular AI frameworks. For more\ndetails, explore the ONNX GitHub project .\nONNX Runtime  is an open-source project that supports cross-platform inference.\nONNX Runtime provides APIs across programming languages (including Python, C++,\nC#, C, Java, and JavaScript). Y ou can use these APIs to perform inference on input\nimages. After you have the model that has been exported to ONNX format, you can use\nthese APIs on any programming language that your project needs.\nIn this guide, you'll learn how to use Python APIs for ONNX Runtime  to make\npredictions on images for popular vision tasks. Y ou can use these ONNX exported\nmodels across languages.\nGet an AutoML-trained computer vision model for any of the supported image\ntasks: classification, object detection, or instance segmentation. Learn more about\nAutoML support for computer vision tasks .\nInstall the onnxruntime  package. The methods in this article have been tested\nwith versions 1.3.0 to 1.8.0.\nPrerequisites"", ""runs from experimentation in the studio user interface. Or they can use versioned assets\nfor jobs like environments and storage references.\nFor more information, see Manage Azure Machine Learning workspaces .\nWhen a project is ready for operationalization, users' work can be automated in an ML\npipeline and triggered on a schedule or HT TPS request.\nYou can deploy models to the managed inferencing solution, for both real-time and\nbatch deployments, abstracting away the infrastructure management typically required\nfor deploying models.\nIn Azure Machine Learning, you can run your training script in the cloud or build a\nmodel from scratch. Customers often bring models they've built and trained in open-\nsource frameworks so that they can operationalize them in the cloud.\nData scientists can use models in Azure Machine Learning that they've created in\ncommon Python frameworks, such as:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nLightGBM\nOther languages and frameworks are also supported:\nR\n.NET\nFor more information, see Open-source integration with Azure Machine Learning .\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\nexperience and intuition to select the right data featurization and algorithm for training.\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\nLearning studio UI or the Python SDK.Train models\nOpen and interoperable\nAutomated featurization and algorithm selection""]",2293
gpt-4-turbo,21,,5.0,,What Azure storage services can be integrated with Azure Machine Learning for storing datasets?,v1,"Azure Machine Learning can be integrated with several Azure storage services for storing datasets. These include:

1. Azure Blob Storage: This is a scalable, object storage solution for unstructured data. It is often used to store large amounts of data for machine learning workloads.

2. Azure Data Lake Storage: This storage service is designed for big data analytics and is optimized for analytics workloads. It provides a hierarchical file system and is suitable for storing large datasets used in machine learning.

3. Azure File Storage: This service offers shared storage for applications using the standard SMB protocol. It can be used for smaller datasets or when a traditional file system structure is required.

4. Azure SQL Database: For structured data, Azure SQL Database can be used to store and retrieve data via SQL queries, which can then be used for machine learning purposes.

These storage services can be accessed from within Azure Machine Learning notebooks, scripts, and pipelines to store, retrieve, and manage datasets used for training and deploying machine learning models.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore"", ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", ""Service Usage Example datanumber of nodes; datastore names and tags, and\ndescriptions on assets like models; and data label names.\nAzure AI\nSearchStores indexes that\nhelp with querying\nyour machine learning\ncontent.These indexes are built on top of the data stored in Azure\nCosmos DB.\nAzure\nStorageStores metadata\nrelated to Azure\nMachine Learning\npipeline data.Data can include designer pipeline names, pipeline layout,\nand execution properties.\nFrom the perspective of data lifecycle management, data in the preceding resources is\ncreated and deleted as you create and delete corresponding objects in Azure Machine\nLearning.\nYour Azure Machine Learning workspace reads and writes data by using its managed\nidentity. This identity is granted access to the resources through a role assignment\n(Azure role-based access control) on the data resources. The encryption key that you\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\nused to create indexes for Azure AI Search at runtime.\nExtra networking controls are configured when you create a private link endpoint on\nyour workspace to allow for inbound connectivity. This configuration includes the\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\nNetwork access is restricted to only trusted Microsoft services.\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\nthe data.\nWhen you use a customer-managed key, the resources are in your Azure subscription\nand encrypted with your key. While these resources exist in your subscription, Microsoft\nmanages them. They're automatically created and configured when you create your\nAzure Machine Learning workspace.\nThese Microsoft-managed resources are located in a new Azure resource group that's\ncreated in your subscription. This resource group is separate from the resource group\nfor your workspace. It contains the Microsoft-managed resources that your key is used\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\nresource group name><GUID>.Customer-managed keys""]",1759
gpt-4-turbo,22,,5.0,,How do you authenticate Azure Machine Learning with Azure Blob Storage to access datasets?,v1,"To authenticate Azure Machine Learning with Azure Blob Storage for accessing datasets, you can use identity-based data access by connecting to an Azure Machine Learning datastore. Here are the steps to set up identity-based authentication:

1. **Managed Identity**: Ensure that the Azure Machine Learning workspace or the specific compute resource (like a compute instance or compute cluster) is assigned a managed identity. A managed identity allows Azure services to authenticate to cloud services without storing credentials in code.

2. **Assign Role**: Assign the appropriate role to the managed identity for the Azure Blob Storage account. At a minimum, you would need to assign the ""Storage Blob Data Reader"" role to the managed identity to read datasets from the blob storage.

3. **Create Datastore**: In Azure Machine Learning, create a datastore that references the Azure Blob Storage. When creating the datastore, select the option to use the managed identity for authentication.

4. **Access Data**: When you access data in your training jobs or Jupyter Notebooks, Azure Machine Learning will use the managed identity to authenticate against the Azure Blob Storage, and you won't need to provide any credentials.

By following these steps, you can securely connect to Azure Blob Storage from Azure Machine Learning without storing credentials in the datastore, and manage data access at the storage level.","['You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity', 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.', 'means that in order to successfully read data from storage, the managed identity of the\ncompute cluster where the deployment is running must have at least Storage Blob Data\nReader  access to the storage account.\nTo configure the compute cluster for data access, follow these steps:\n1. Go to Azure Machine Learning studio .\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\ndeployment is using.\n3. Assign a managed identity to the compute cluster:\na. In the Managed identity  section, verify if the compute has a managed identity\nassigned. If not, select the option Edit.\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\nusing a S ystem-Assigned Managed Identity, it is named as ""[workspace\nname]/computes/[compute cluster name]"".\nc. Save the changes.\n4. Go to the Azure portal  and navigate to the associated storage account where the\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\nstorage account where those assets are placed.', ""When you use an Azure S torage Account from Azure Machine Learning studio, you must\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\naccount:\nBlob Data R eader\nIf the storage account uses a private endpoint to connect to the VNet, you must\ngrant the Reader  role for the storage account private endpoint to the managed\nidentity.\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\nNetwork .\nThe following sections explain the limitations of using an Azure S torage Account, with\nyour workspace, in a VNet.\nTo secure communication between Azure Machine Learning and Azure S torage\nAccounts, configure the storage to Grant access to trusted Azure services .\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\ncan normally be used to allow your client to directly connect over the internet. However,\nwhen using studio, your client doesn't connect to the storage account. The Azure\nMachine Learning service that makes the request connects to the storage account. The\nIP address of the service isn't documented, and it changes frequently. Enabling the\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\nconfiguration .\uea80 Tip\nTo access data from outside Azure Machine Learning, for example with Azure\nStorage Explorer, that access probably relies on the user identity. For specific\ninformation, review the documentation for the tool or service you're using. For\nmore information about how Azure Machine Learning works with data, see Setup\nauthentication betw een Azur e Machine Learning and other ser vices .\nAzure Storage Account\nSecure communication with Azure Storage Account\nAzure Storage firewall"", ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.""]",1780
gpt-4-turbo,23,,5.0,,What steps are involved in uploading a dataset to Azure Blob Storage and then accessing it from an Azure Machine Learning experiment?,v1,"To upload a dataset to Azure Blob Storage and then access it from an Azure Machine Learning experiment, you can follow these steps:

1. **Upload Data to Azure Blob Storage:**
   - Use Azure Storage Explorer to transfer your data to Azure Blob Storage. This is recommended for a large amount of data due to its robustness and speed.
   - Alternatively, you can upload files directly through the Azure portal or programmatically using Azure Storage SDKs.

2. **Create a Dataset in Azure Machine Learning:**
   - In the Azure Machine Learning Studio, select **Create** to create a new dataset.
   - Enter a **Name** for your dataset and, optionally, a description.
   - Choose the **Dataset type**. Select **Tabular** if you're using a .csv or .tsv file where each row contains a response. Select **File** if you're using separate .txt files for each response.
   - Select **Next**.

3. **Specify Datastore and Path:**
   - Select **From Azure storage**, then select **Next**.
   - Choose the datastore that points to your Azure Blob Storage.
   - If your data is in a subfolder within Blob Storage, use **Browse** to select the path. To include all files in subfolders, append `/**` to the path. To include all data in the current container and its subfolders, append `**/*.*` to the path.
   - Select **Next**.

4. **Finalize Dataset Creation:**
   - Review your settings and then select **Create** to finalize the creation of the dataset.

5. **Access the Dataset in an Experiment:**
   - In your Azure Machine Learning experiment, access the dataset by referencing the dataset name you created.
   - Use the Azure Machine Learning SDK to load the dataset into your experiment code. For example, you can use `Dataset.get_by_name(workspace, name='your_dataset_name')` to retrieve the dataset in your experiment script.

By following these steps, you will have successfully uploaded your data to Azure Blob Storage, created a dataset in Azure Machine Learning, and accessed it from an experiment.","[""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data"", '2. Select your subscription and the workspace you created.\n3. Select Get star ted.\n4. In the left pane, select Automat ed ML  under the Author  section.\n5. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning dataset. Doing so, allows you to ensure that your\ndata is formatted appropriately for your experiment.\n1. On the Select dataset  form, select From local files  from the +Create dataset  drop-\ndown.\na. On the Basic info  form, give your dataset a name and provide an optional\ndescription. The dataset type should default to Tabular , since automated ML in\nAzure Machine Learning studio currently only supports tabular datasets.\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is the storage location where you upload your data\nfile.\nd. Select Uplo ad files  from the Uplo ad drop-down.\ne. Choose the bike-no.csv  file on your local computer. This is the file you\ndownloaded as a prerequisite .\nf. Select Next\nWhen the upload is complete, the Settings and preview form is pre-populated\nbased on the file type.\ng. Verify that the Settings and pr eview  form is populated as follows and select\nNext .Create and load dataset\nﾉExpand table', ""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data"", ""Since this is your first automated ML experiment, you'll see an empty list and links\nto documentation.\n3. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning data asset. In the case of this tutorial, you can think\nof a data asset as your dataset for the AutoML job. Doing so, allows you to ensure that\nyour data is formatted appropriately for your experiment.\n1. Create a new data asset by selecting From local files  from the +Create data asset\ndrop-down.\na. On the Basic info  form, give your data asset a name and provide an optional\ndescription. The automated ML interface currently only supports\nTabularDatasets, so the dataset type should default to Tabular .\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is where you'll upload your data file to make it\navailable to your workspace.\nd. Select Uplo ad files  from the Uplo ad drop-down.Create and load a dataset as a data asset"", 'b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably.']",2164
gpt-4-turbo,24,,5.0,,"Can Azure Machine Learning directly access data stored in Azure Data Lake Storage Gen2? If so, how?",v1,"Yes, Azure Machine Learning can directly access data stored in Azure Data Lake Storage Gen2. Azure Data Lake Storage Gen2 is a large-scale data lake solution for big data analytics, which integrates with Azure Blob Storage and is commonly used as a datastore for Azure Machine Learning.

To access data in Azure Data Lake Storage Gen2 from Azure Machine Learning, you can follow these general steps:

1. **Set up authentication**: Ensure that your Azure Machine Learning workspace has the appropriate permissions to access the Azure Data Lake Storage Gen2 account. This typically involves configuring a service principal with the necessary roles and permissions and ensuring that it is registered with your Azure Machine Learning workspace.

2. **Configure datastore**: In Azure Machine Learning, you can register an Azure Data Lake Storage Gen2 account as a datastore. A datastore in Azure Machine Learning is a place where data can be stored and then accessed during training or scoring.

3. **Access data in your experiments**: Once the datastore is registered, you can access the data within your Azure Machine Learning experiments by referencing the datastore and the specific paths to the data you need.

4. **Use Azure Machine Learning SDK**: You can use the Azure Machine Learning SDK for Python to interact with the datastore programmatically. The SDK provides methods to upload, download, and manage data within the datastore.

Here is an example of how you might register an Azure Data Lake Storage Gen2 account as a datastore using the Azure Machine Learning SDK for Python:

```python
from azureml.core import Workspace, Datastore

# Get the workspace
workspace = Workspace.from_config()

# Register a new datastore
datastore = Datastore.register_azure_data_lake_gen2(
    workspace=workspace,
    datastore_name='my_datastore',
    account_name='myadlsaccount',  # Replace with your account name
    filesystem='myfilesystem',  # Replace with your filesystem name
    tenant_id='my-tenant-id',  # Replace with your tenant ID
    client_id='my-client-id',  # Replace with your service principal's client ID
    client_secret='my-client-secret'  # Replace with your service principal's client secret
)

# Now you can use the datastore in your experiments
```

Remember to replace the placeholders with your actual account details and credentials. It's important to manage and secure your credentials properly, especially when dealing with access keys and service principals.","[""For information on regenerating the access keys, see Regenerate storage account access\nkeys.\nAzure Data Lake S torage Gen2 is built on top of Azure Blob S torage and is designed for\nbig data analytics in enterprises. Data Lake S torage Gen2 is used as a datastore for\nAzure Machine Learning. Like Azure Blob S torage, the data at rest is encrypted with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Data Lake\nStorage, see Azure S torage encryption with customer-managed keys in Azure K ey Vault.\nThe Azure Machine Learning service supports data from the following data sources.\nTransparent data encryption helps protect Azure SQL Database against the threat of\nmalicious offline activity by encrypting data at rest. By default, transparent data\nencryption is enabled for all newly deployed SQL databases that use Microsoft-\nmanaged keys.\nFor information on how to use customer-managed keys for transparent data encryption,\nsee Azure SQL Database transparent data encryption .Azure Data Lake Storage\n７ Note\nOn Feb 29, 2024 , Azure Data Lake S torage Gen1 will be retired. For more\ninformation, see the official announcement . If you use Azure Data Lake S torage\nGen1, make sure to migrate to Azure Data Lake S torage Gen2 before that date. T o\nlearn how, see Migrat e Azur e Data Lak e Storage fr om Gen1 t o Gen2 by using the\nAzur e por tal.\nUnless you already have an Azure Data Lake S torage Gen1 account, you can't create\nnew ones.\nAzure relational databases\nAzure SQL Database\nAzure Database for PostgreSQL"", 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity', 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore""]",1889
gpt-4-turbo,25,,5.0,,How does Azure Machine Learning support versioning of datasets stored in Azure Blob Storage?,v1,"Azure Machine Learning supports dataset versioning by allowing users to create and manage versions of datasets that are registered in the Azure Machine Learning workspace. When you register a dataset in Azure Machine Learning, you can specify a version number. If you do not specify a version, Azure Machine Learning will automatically assign a version number starting with 1 and incrementing with each new registration of the dataset.

When datasets are stored in Azure Blob Storage, you can use Azure Machine Learning to track changes and versions of these datasets. Each time you register a new version of the dataset with updated data or changes, it is tracked within the Azure Machine Learning workspace. This allows for reproducibility and traceability in machine learning experiments, as you can reference specific versions of datasets when training models.

To manage dataset versions in Azure Machine Learning, you can use the Azure Machine Learning SDK or the Azure Machine Learning studio. The SDK provides methods for registering, retrieving, and managing different versions of datasets. In the studio, you can view and manage dataset versions through the user interface.

It's important to note that while Azure Machine Learning provides the versioning mechanism, the actual data is stored in Azure Blob Storage, and the storage service itself does not provide versioning of the data. Instead, the versioning is a feature of the dataset registration within the Azure Machine Learning service.","[""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'You now have two versions of the data, where the name of the version corresponds to\nthe date the images were uploaded to storage:\n1. 20230108:  The images up to 2023-Jan-08.\n2. 20230115:  The images up to 2023-Jan-15.\nIn both cases, ML Table constructs a table of paths that only include the images up t o\nthose dat es.\nIn an Azure Machine Learning job you can mount or download those paths in the\nversioned ML Table to your compute target using either the eval_download or\neval_mount modes:\nPython\nfrom azure.ai.ml import MLClient, command, Input  \nfrom azure.ai.ml.entities import Environment  \nfrom azure.identity import DefaultAzureCredential  \nfrom azure.ai.ml.constants import InputOutputModes  \n# connect to the AzureML workspace  \nml_client = MLClient.from_config(\n    DefaultAzureCredential()  \n) \n# Get the 20230115 version of the data  \ndata_asset = ml_client.data.get(name= ""myimages"" , version= ""20230115"" ) \ninput = {  \n    ""images"" : Input(type= ""mltable"" , \n                   path=data_asset.id,  \n                   mode=InputOutputModes.EVAL_MOUNT  \n            )  \n} \ncmd = """""" \nls ${{inputs.images}}/**  \n"""""" \njob = command(  \n    command=cmd,  \n    inputs=input,  \n    compute= ""cpu-cluster"" , \n    environment= ""azureml://registries/azureml/environments/sklearn-\n1.1/versions/4""  \n) \nml_client.jobs.create_or_update(job)', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore"", ""Only Azure Blob S torage Accounts are supported for outputs. For instance, Azure Data Lake\nStorage Gen2 isn't supported as output in batch deployment jobs. If you need to output the\ndata to a different location/sink, use the Azure Data F actory Copy activity after the execution of\nthe batch job.\nUse low priority VMs in batch deployments\nAuthorization on batch endpoints\nNetwork isolation in batch endpointsNext steps""]",1529
gpt-4-turbo,26,,5.0,,What is the recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage?,v1,The recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage is to use Azure Storage Explorer. It provides a faster and more robust method for transferring a large amount of data compared to uploading local files directly. Azure Storage Explorer should be used as the default method for moving files into Azure Machine Learning.,"[""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", ""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data"", 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity']",1700
gpt-4-turbo,27,,5.0,,How do you use the Azure Machine Learning SDK to programmatically download data from Azure Blob Storage?,v1,"To programmatically download data from Azure Blob Storage using the Azure Machine Learning SDK, you would typically follow these steps:

1. **Set up your Azure Machine Learning workspace**: Before you can interact with Azure Blob Storage, you need to have an Azure Machine Learning workspace set up. This workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create.

2. **Create a datastore**: In Azure Machine Learning, a datastore is an abstraction over an Azure Storage Account. The datastore can point to either an Azure Blob container or an Azure File Share. You can register a datastore using the SDK like this:

    ```python
    from azureml.core import Workspace, Datastore
    
    workspace = Workspace.from_config()  # Assuming you have a config.json file with workspace details
    
    # Register a new datastore
    blob_datastore = Datastore.register_azure_blob_container(workspace=workspace,
                                                            datastore_name='my_blob_datastore',
                                                            container_name='my-container-name',
                                                            account_name='my-storage-account-name',
                                                            account_key='my-storage-account-key')
    ```

3. **Access the datastore**: Once the datastore is registered, you can access it using the SDK:

    ```python
    datastore = Datastore.get(workspace, datastore_name='my_blob_datastore')
    ```

4. **Download data from the datastore**: You can download files from the datastore to your local environment using the `download` method:

    ```python
    datastore.download(target_path='local/path',
                       prefix='folder-or-file-prefix',  # Use this to filter which files to download
                       overwrite=False,  # Overwrite existing files or not
                       show_progress=True)  # Show download progress
    ```

5. **Use the data in your machine learning code**: After downloading, you can use the data as needed for training or inference in your machine learning code.

Please note that the actual code and parameters might vary based on your specific setup and requirements. Always refer to the latest Azure Machine Learning SDK documentation for the most up-to-date information and methods.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""Tutorial: Upload, access and explore\nyour data in Azure Machine Learning\nArticle •12/27/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this tutorial you learn how to:\nThe start of a machine learning project typically involves exploratory data analysis (ED A),\ndata-preprocessing (cleaning, feature engineering), and the building of Machine\nLearning model prototypes to validate hypotheses. This prototyping  project phase is\nhighly interactive. It lends itself to development in an IDE or a Jupyter notebook, with a\nPython int eractive console. This tutorial describes these ideas.\nThis video shows how to get started in Azure Machine Learning studio so that you can\nfollow the steps in the tutorial. The video shows how to create a notebook, clone the\nnotebook, create a compute instance, and download the data needed for the tutorial.\nThe steps are also described in the following sections.\n1. To use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\n2. Sign in to studio  and select your workspace if it's not already open.\n3. Open or create a notebook in your workspace:\nCreate a new notebook , if you want to copy/paste code into cells.\nOr, open tutorials/get -star ted-not ebooks/explor e-data.ipynb  from the\nSamples  section of studio. Then select Clone  to add the notebook to your\nFiles. (See where to find Samples .)\nUpload your data to cloud storage＂\nCreate an Azure Machine Learning data asset＂\nAccess your data in a notebook for interactive development＂\nCreate new versions of data assets＂\nhttps://learn-video.azurefd.net/vod/player?id=514a29e2-0ae7-4a5d-a537-\n8f10681f5545&locale=en-us&embedUrl=%2F azure%2Fmachine-learning%2Ftutorial-\nexplore-data\nPrerequisites"", ""Data from Azure Machine Learning registered data stores can be directly referenced by\nbatch deployments jobs. In this example, you first upload some data to the default data\nstore in the Azure Machine Learning workspace and then run a batch deployment on it.\nFollow these steps to run a batch endpoint job using data stored in a data store.\n1. Access the default data store in the Azure Machine Learning workspace. If your\ndata is in a different store, you can use that store instead. Y ou're not required to\nuse the default data store.\nAzure CLI\n2. You need to upload some sample data to the data store. This example assumes\nyou already uploaded the sample data included in the repo in the folder\nsdk/python/endpoints/batch/deploy-models/heart-classifier-mlflow/data in the\nfolder heart-disease-uci-unlabeled in the blob storage account. Ensure you've\ndone that before moving forward.\n3. Create the input or request:\nPlace the file path in the following variable:Azure CLI\nDATASTORE_ID=$(az ml datastore show -n workspaceblobstore | jq -r \n'.id')\n７ Note\nData stores ID would look like\n/subscriptions/<subscription>/resourceGroups/<resource-\ngroup>/providers/Microsoft.MachineLearningServices/workspaces/<worksp\nace>/datastores/<data-store>.\n\uea80 Tip\nThe default blob data store in a workspace is called worksp aceblobst ore. You\ncan skip this step if you already know the resource ID of the default data store\nin your workspace.\nAzure CLI"", ""Acces s data in a job\nArticle •02/06/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2 (current)\nIn this article you learn:\nAn Azure subscription. If you don't have an Azure subscription, create a free account before you begin.\nTry the free or paid version of Azure Machine Learning .\nThe Azure Machine Learning SDK for Python v2 .\nAn Azure Machine Learning workspace\nBefore you explore the detailed options available to you when you access data, we first describe the relevant\ncode snippets for data access.\nIn this example, you submit an Azure Machine Learning job that accesses data from a public  blob storage\naccount. However, you can adapt the snippet to access your own data in a private Azure S torage account.\nUpdate the path as described here. Azure Machine Learning seamlessly handles authentication to cloud\nstorage, with Microsoft Entra passthrough. When you submit a job, you can choose:\nUser identity:  Passthrough your Microsoft Entra identity to access the data\nManaged identity:  Use the managed identity of the compute target to access data\nNone:  Don't specify an identity to access the data. Use None when using credential-based (key/SAS\ntoken) datastores or when accessing public data\nHow to read data from Azure storage in an Azure Machine Learning job.＂\nHow to write data from your Azure Machine Learning job to Azure S torage. ＂\nThe difference between mount  and downlo ad modes. ＂\nHow to use user identity and managed identity to access data.＂\nMount settings available in a job.＂\nOptimum mount settings for common scenarios.＂\nHow to access V1 data assets.＂\nPrerequisites\nQuickstart\nRead data from Azure storage in an Azure Machine Learning job\n\uea80 Tip\nIf you use keys or SAS tokens to authenticate, we suggest that you create an Azur e Machine Learning\ndatast ore, because the runtime will automatically connect to storage without exposure of the key/token.\nPython SDK"", ""Acces s data from Azure cloud storage during\ninteractive development\nArticle •09/13/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nA machine learning project typically starts with exploratory data analysis (ED A), data-preprocessing\n(cleaning, feature engineering), and includes building prototypes of ML models to validate hypotheses.\nThis prototyping  project phase is highly interactive in nature, and it lends itself to development in a\nJupyter notebook, or an IDE with a Python int eractive console. In this article you'll learn how to:\nAn Azure Machine Learning workspace. For more information, see Manage Azure Machine Learning\nworkspaces in the portal or with the Python SDK (v2) .\nAn Azure Machine Learning Datastore. For more information, see Create datastores .\nAn Azure Machine Learning datastore is a reference to an existing  Azure storage account. The benefits of\ndatastore creation and use include:\nAccess data from a Azure Machine Learning Datastores URI as if it were a file system.＂\nMaterialize data into P andas using mltable Python library. ＂\nMaterialize Azure Machine Learning data assets into P andas using mltable Python library. ＂\nMaterialize data through an explicit download with the azcopy utility. ＂\nPrerequisites\n\uea80 Tip\nThe guidance in this article describes data access during interactive development. It applies to any\nhost that can run a Python session. This can include your local machine, a cloud VM, a GitHub\nCodespace, etc. W e recommend use of an Azure Machine Learning compute instance - a fully\nmanaged and pre-configured cloud workstation. For more information, see Create an Azur e\nMachine Learning comput e instance .\n） Impor tant\nEnsure you have the latest azure-fsspec and mltable python libraries installed in your python\nenvironment:\nBash\npip install -U azureml-fsspec mltable\nAccess data from a datastore URI, like a filesystem\nA common, easy-to-use API to interact with different storage types (Blob/Files/ADLS).＂""]",2181
gpt-4-turbo,28,,5.0,,What are the benefits of integrating Azure Data Lake Storage with Azure Machine Learning for big data scenarios?,v1,"Integrating Azure Data Lake Storage with Azure Machine Learning offers several benefits for big data scenarios:

1. **Scalability**: Azure Data Lake Storage is designed to handle large volumes of data, scaling to petabytes of information. This makes it suitable for big data scenarios where data volume can grow rapidly.

2. **Data Accessibility**: By using Azure Data Lake Storage, you can store data in a central repository that is easily accessible from Azure Machine Learning. This simplifies the management of your data and allows for seamless integration with the machine learning workflows.

3. **Performance**: Azure Data Lake Storage provides high-performance file system operations, which are essential when dealing with big data. This ensures that data can be read and written quickly, which is crucial for machine learning tasks that require frequent data access.

4. **Advanced Analytics**: With Azure Machine Learning, you can build and train advanced analytics models on the data stored in Azure Data Lake Storage. This allows you to leverage the full potential of your big data by applying machine learning algorithms to uncover insights.

5. **Security and Compliance**: Azure Data Lake Storage offers robust security features, including encryption and access control, which are important for protecting sensitive big data. When integrated with Azure Machine Learning, you can maintain compliance with industry standards and regulations.

6. **Cost-Effectiveness**: Storing large volumes of data can be cost-effective with Azure Data Lake Storage due to its optimized storage solutions. When combined with Azure Machine Learning, you can also optimize the compute resources used for training models, leading to a more efficient use of resources.

7. **Streamlined Data Processing Pipelines**: You can create production machine learning pipelines that leverage Azure Data Lake Storage for data input and output. This allows for a structured and efficient way to process data, train models, and deploy them into production.

By integrating Azure Data Lake Storage with Azure Machine Learning, organizations can build robust, scalable, and efficient big data solutions that harness the power of machine learning to extract valuable insights from their data.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Azur e Machine Learning designer : Use the designer to train and deploy ML\nmodels without writing any code. Drag and drop datasets and components to\ncreate ML pipelines.\nAutomat ed machine learning UI : Learn how to create automated ML experiments\nwith an easy-to-use interface.\nData labeling : Use Machine Learning data labeling to efficiently coordinate image\nlabeling  or text labeling  projects.\nMachine Learning integrates with the Azure cloud platform to add security to ML\nprojects.\nSecurity integrations include:\nAzure Virtual Networks with network security groups.\nAzure K ey Vault, where you can save security secrets, such as access information\nfor storage accounts.\nAzure Container R egistry set up behind a virtual network.\nFor more information, see Tutorial: Set up a secure workspace .\nOther integrations with Azure services support an ML project from end to end. They\ninclude:Enterprise-readiness and security\nAzure integrations for complete solutio ns', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore"", ""What is Azure Machine Learning?\nArticle •01/29/2024\nAzure Machine Learning is a cloud service for accelerating and managing the machine\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\nin their day-to-day workflows to train and deploy models and manage machine learning\noperations (ML Ops).\nYou can create a model in Machine Learning or use a model built from an open-source\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\nretrain, and redeploy models.\nMachine Learning is for individuals and teams implementing ML Ops within their\norganization to bring ML models into production in a secure and auditable production\nenvironment.\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\nday workflows. Application developers can use tools for integrating models into\napplications or services. Platform developers can use a robust set of tools, backed by\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\nbased access control for infrastructure. Y ou can set up a project to deny access to\nprotected data and select operations.\nML projects often require a team with a varied skill set to build and maintain. Machine\nLearning has tools that help enable you to:\uea80 Tip\nFree trial!  If you don't have an Azure subscription, create a free account before you\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\nto spend on Azure services. After they're used up, you can keep the account and\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\nchange your settings and ask to be charged.\nWho is Azure Machine Learning for?\nProductivity for everyone on the team"", 'Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .']",1438
gpt-4-turbo,29,,5.0,,How can you secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces?,v1,"To secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces, you can use the following methods:

1. **Encryption at Rest**: By default, all data stored in Azure Blob Storage is encrypted at rest with Microsoft-managed keys. This ensures that your data is secure when it is not being accessed or moved.

2. **Customer-Managed Keys**: For additional control, you can use your own encryption keys for data stored in Azure Blob Storage. This is done through Azure Storage encryption with customer-managed keys in Azure Key Vault. By using customer-managed keys, you have the ability to manage the lifecycle of the encryption keys, including rotation and revocation.

3. **Secure Data Access**: When setting up your Azure Machine Learning workspace, ensure that you configure the storage account and other data services with proper access controls. Use Azure role-based access control (RBAC) to grant the minimum necessary permissions to users and services that need to access the data.

4. **Datastore Configuration**: In Azure Machine Learning, you can register Azure Storage services as datastores. When you use datastores, you can manage data connections securely without having to hard-code your credentials.

5. **Network Security**: Use Azure Virtual Network to isolate your Azure Machine Learning compute resources and storage services. You can also use service endpoints or private endpoints to secure and limit access to your storage account.

6. **Monitoring and Auditing**: Enable Azure Monitor and Azure Storage logging to track access and usage of your storage services. This helps in identifying any unauthorized access or anomalies in data usage.

By combining these methods, you can ensure that your data stored in Azure Storage services is secure when accessed by Azure Machine Learning workspaces.","[""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", 'This article is part of a series on securing an Azure Machine Learning workflow. See the\nother articles in this series:\nSecure the workspace resources\nSecure machine learning registries\nSecure the training environment\nSecure the inference environment\nEnable studio functionality\nUse custom DNS\nUse a firewall\nAPI platform network isolationNext steps', ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."", ""When you use an Azure S torage Account from Azure Machine Learning studio, you must\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\naccount:\nBlob Data R eader\nIf the storage account uses a private endpoint to connect to the VNet, you must\ngrant the Reader  role for the storage account private endpoint to the managed\nidentity.\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\nNetwork .\nThe following sections explain the limitations of using an Azure S torage Account, with\nyour workspace, in a VNet.\nTo secure communication between Azure Machine Learning and Azure S torage\nAccounts, configure the storage to Grant access to trusted Azure services .\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\ncan normally be used to allow your client to directly connect over the internet. However,\nwhen using studio, your client doesn't connect to the storage account. The Azure\nMachine Learning service that makes the request connects to the storage account. The\nIP address of the service isn't documented, and it changes frequently. Enabling the\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\nconfiguration .\uea80 Tip\nTo access data from outside Azure Machine Learning, for example with Azure\nStorage Explorer, that access probably relies on the user identity. For specific\ninformation, review the documentation for the tool or service you're using. For\nmore information about how Azure Machine Learning works with data, see Setup\nauthentication betw een Azur e Machine Learning and other ser vices .\nAzure Storage Account\nSecure communication with Azure Storage Account\nAzure Storage firewall"", 'You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity']",1777
gpt-4-turbo,30,,5.0,,What role do Azure Storage accounts play in deploying Azure Machine Learning models as web services?,v1,"Azure Storage accounts play a crucial role in deploying Azure Machine Learning models as web services in several ways:

1. **Model Storage**: The storage accounts in your Azure subscription are used to store your machine learning models. When you train a model using Azure Machine Learning, the model is saved in your Azure Storage account.

2. **Data Storage**: Training data, training logs, and Python scripts are stored in Azure Storage accounts. This data can be accessed by the compute resources during the training process and when deploying models as web services.

3. **Operational Artifacts**: When deploying a model as a web service, operational artifacts such as scoring scripts, environment configuration files, and other necessary files are stored in Azure Storage accounts.

4. **Web Service Assets**: For web services, the storage account may also store assets that the web service needs to function, such as additional scripts or files that are required at runtime.

5. **Endpoint Accessibility**: When using a compute instance or compute cluster, the storage account is mounted as a file share via the SMB protocol. This allows the deployed web service to access the necessary files and models securely.

6. **Private Endpoint**: The compute instance and cluster use the private endpoint when they access the storage account, ensuring secure and private connectivity to the storage resources.

7. **Blob Storage**: Azure Blob storage is often used to store large amounts of unstructured data that can be accessed by the deployed web service for inference tasks.

In summary, Azure Storage accounts provide a secure and scalable way to store and manage the data, models, and artifacts that are essential for deploying and running Azure Machine Learning models as web services.","[""Scenar io Requir ed\ninboundRequir ed outbound Additional\nconfiguration\nUse Azure\nKubernetes\nServiceNot applicable For information on the outbound\nconfiguration for AKS, see Secure\nAzure Kubernetes Service\ninferencing environment .\nUse Docker\nimages that\nAzure\nMachine\nLearning\nmanagesNot applicable Microsoft Artifact R egistry\nviennaglobal.azurecr.io\nglobal container registryIf the container registry\nfor your workspace is\nbehind the virtual\nnetwork, configure the\nworkspace to use a\ncompute cluster to build\nimages. For more\ninformation, see Secure\nan Azure Machine\nLearning workspace with\nvirtual networks .\nAzure Machine Learning uses multiple storage accounts. Each stores different data and\nhas a different purpose:\nYour st orage : The storage accounts in your Azure subscription store your data and\nartifacts, such as models, training data, training logs, and Python scripts. For\nexample, the default  storage account for your workspace is in your subscription.\nThe Azure Machine Learning compute instance and compute cluster access file and\nblob data in this storage over ports 445 (SMB) and 443 (HT TPS).\nWhen you're using a compute instance or compute cluster, your storage account is\nmounted as a file share via the SMB protocol. The compute instance and cluster\nuse this file share to store items like the data, models, Jupyter notebooks, and\ndatasets. The compute instance and cluster use the private endpoint when they\naccess the storage account.\nMicrosoft storage : The Azure Machine Learning compute instance and compute\ncluster rely on Azure Batch. They access storage located in a Microsoft\nsubscription. This storage is used only for the management of the compute\ninstance or cluster. None of your data is stored here. The compute instance and\ncompute cluster access the blob, table, and queue data in this storage, by using\nport 443 (HT TPS).Purposes of storage accounts"", 'Azure Machine Learning supports storage accounts configured to use either a private\nendpoint or service endpoint.\n1. In the Azure portal, select the Azure S torage Account.\n2. Use the information in Use private endpoints for Azure S torage  to add private\nendpoints for the following storage resources:\nBlob\nFile\nQueue  - Only needed if you plan to use Batch endpoints  or the\nParallelRunS tep in an Azure Machine Learning pipeline.\nTable - Only needed if you plan to use Batch endpoints  or the\nParallelRunS tep in an Azure Machine Learning pipeline.Securing a workspace with private endpoints does not ensure end-to-end security\nby itself. Y ou must follow the steps in the rest of this article, and the VNet series, to\nsecure individual components of your solution. For example, if you use a private\nendpoint for the workspace, but your Azure S torage Account is not behind the\nVNet, traffic between the workspace and storage does not use the VNet for\nsecurity.\nSecure Azure storage accounts\nPrivate endpoint', 'What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps', ""Azure Machine Learning offers many ways to deploy MLflow models to online and batch\nendpoints. Y ou can deploy models, using the following tools:\nEach workflow has different capabilities, particularly around which type of compute they\ncan target. The following table shows the different capabilities.\nScenar io MLflow SDK Azur e Machine\nLearning CLI/SDKAzur e Machine\nLearning studio\nDeploy to managed online\nendpointsSee example See example See example\nDeploy to managed online\nendpoints (with a scoring script)Not\nsupportedSee example See example\nDeploy to batch endpoints Not\nsupportedSee example See example\nDeploy to batch endpoints (with\na scoring script)Not\nsupportedSee example See example\nDeploy to web services\n(ACI/AKS)Legacy\nsupportNot supported Not supported\nDeploy to web services (A CI/AKS\n- with a scoring script)Not\nsupportedLegacy support Legacy support\n Deployment to online endpoints that are in workspaces with private link enabled\nrequires you to package models before deployment (preview) .\n We recommend switching to managed online endpoints  instead.\n MLflow (OSS) doesn't have the concept of a scoring script and doesn't support batch\nexecution currently.\nUse the MLflow SDK if both of these conditions apply:Deployment tools\nMLflow SDK＂\nAzure Machine Learning CLI＂\nAzure Machine Learning SDK for Python＂\nAzure Machine Learning studio＂\nﾉExpand table\n1 1 1\n3\n3\n3\n22 2\n32 2\n1\n2\n3\nWhich deployment tool to use?"", 'region basis. When you sign up for Azure Machine Learning studio, you receive a default\nVM quota for several VM families available in the region. Y ou can continue to create\ndeployments until you reach your quota limit. Once you reach this limit, you can request\na quota increase.\nModels deployed as a service with pay-as-you-go are protected by Azure AI content\nsafety. When deployed to real-time endpoints, you can opt out of this capability. With\nAzure AI content safety enabled, both the prompt and completion pass through an\nensemble of classification models aimed at detecting and preventing the output of\nharmful content. The content filtering system detects and takes action on specific\ncategories of potentially harmful content in both input prompts and output\ncompletions. Learn more about Azure AI Content Safety .\nModel Catalog and Collections\nDeploy and score a machine learning model by using an online endpoint\nPlan and manage costs for Azure AI S tudioContent filtering\nRelated content']",1500
gpt-4-turbo,31,,5.0,,How do you monitor data access and usage when integrating Azure Machine Learning with Azure Storage?,v1,"To monitor data access and usage when integrating Azure Machine Learning with Azure Storage, you can use Azure Monitor, which provides comprehensive monitoring of your Azure resources and the applications you run on Azure. Here are the steps you can follow:

1. **Enable Diagnostic Settings in Azure Storage**: Configure the diagnostic settings in your Azure Storage account to send logs and metrics to Azure Monitor. This can include read, write, and delete operations on your storage blobs or files, which can be indicative of data access and usage.

2. **Log Analytics Workspace**: Set up a Log Analytics workspace in Azure Monitor to query and visualize the logs sent from your Azure Storage account. You can write queries to analyze the data access patterns or look for specific access events.

3. **Metrics**: Review the metrics provided by Azure Storage in the Azure portal. These metrics can give you insights into the performance and usage of your storage account.

4. **Alerts**: Create alerts in Azure Monitor based on metrics or log query results. For example, you can set an alert for when there's an unusual amount of data read or written, which could indicate a spike in usage or a potential security issue.

5. **Azure Storage Analytics**: Use Azure Storage Analytics to track, analyze, and debug your applications' storage usage.

6. **Azure Machine Learning Monitoring**: Within Azure Machine Learning, you can monitor data drift, model performance, and other operational metrics that can indirectly indicate how your models are using the data stored in Azure Storage.

7. **Role-Based Access Control (RBAC)**: Ensure that you have proper RBAC policies in place to control who has access to your Azure Storage data. Monitoring these access policies can help you understand who is accessing your data and for what purpose.

8. **Audit Logs**: Review the audit logs for any access to the Azure Storage account. These logs can provide detailed information about who accessed the data and what operations were performed.

Remember to consult the Azure documentation for the most up-to-date information on monitoring and logging, as Azure services are frequently updated with new features and capabilities.","['To learn how to monitor resources and logs from Azure Machine Learning, see\nMonitoring Azure Machine Learning .', 'Monitor Azure Machine Learning\nArticle •11/06/2023\nWhen you have critical applications and business processes relying on Azure resources, you\nwant to monitor those resources for their availability, performance, and operation. This\narticle describes the monitoring data generated by Azure Machine Learning and how to\nanalyze and alert on this data with Azure Monitor.\nAzure Machine Learning creates monitoring data using Azure Monitor , which is a full stack\nmonitoring service in Azure. Azure Monitor provides a complete set of features to monitor\nyour Azure resources. It can also monitor resources in other clouds and on-premises.\nStart with the article Monitoring Azure resources with Azure Monitor , which describes the\nfollowing concepts:\nWhat is Azure Monitor?\nCosts associated with monitoring\nMonitoring data collected in Azure\nConfiguring data collection\nStandard tools in Azure for analyzing and alerting on monitoring data\nThe following sections build on this article by describing the specific data gathered for\nAzure Machine Learning. These sections also provide examples for configuring data\ncollection and analyzing this data with Azure tools.\uea80 Tip\nThe information in this document is primarily for administrat ors, as it describes\nmonitoring for the Azure Machine Learning service and associated Azure services. If\nyou are a data scientist  or developer , and want to monitor information specific to your\nmodel tr aining r uns, see the following documents:\nStart, monit or, and cancel training runs\nLog metrics for training runs\nTrack experiments with MLflow\nIf you want to monitor information generated by models deployed to online\nendpoints, see Monit or online endpoints .\nWhat is Azure Monitor?', ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."", 'Monitor online endpoints\nArticle •10/24/2023\nAzure Machine Learning uses integration with Azure Monitor to track and monitor\nmetrics and logs for online endpoints . You can view metrics in charts, compare between\nendpoints and deployments, pin to Azure portal dashboards, configure alerts, query\nfrom log tables, and push logs to supported targets. Y ou can also use Application\nInsights to analyze events from user containers.\nMetrics : For endpoint-level metrics such as request latency, requests per minute,\nnew connections per second, and network bytes, you can drill down to see details\nat the deployment level or status level. Deployment-level metrics such as CPU/GPU\nutilization and memory or disk utilization can also be drilled down to instance\nlevel. Azure Monitor allows tracking these metrics in charts and setting up\ndashboards and alerts for further analysis.\nLogs : You can send metrics to the Log Analytics workspace where you can query\nthe logs using Kusto query syntax. Y ou can also send metrics to Azure S torage\naccounts and/or Event Hubs for further processing. In addition, you can use\ndedicated log tables for online endpoint related events, traffic, and console\n(container) logs. Kusto query allows complex analysis and joining of multiple\ntables.\nApplication insights : Curated environments include integration with Application\nInsights, and you can enable or disable this integration when you create an online\ndeployment. Built-in metrics and logs are sent to Application Insights, and you can\nuse the built-in features of Application Insights (such as Live metrics, T ransaction\nsearch, F ailures, and P erformance) for further analysis.\nIn this article you learn how to:\nDeploy an Azure Machine Learning online endpoint.Choose the right method to view and track metrics and logs＂\nView metrics for your online endpoint＂\nCreate a dashboard for your metrics＂\nCreate a metric alert＂\nView logs for your online endpoint＂\nUse Application Insights to track metrics and logs＂\nPrerequisites', ""Use monitoring view in the Azure Machine Learning studio  to check the CPU and memory utilization of\nyour job. If you're low on CPU and memory, consider increasing the compute target VM size.\nConsider increasing RSLEX_DOWNLOADER_THREADS if you're downloading and you don't utilize the CPU and\nmemory. If you use mount, you should increase DATASET_MOUNT_READ_BUFFER_BLOCK_COUNT to do more\nprefetching, and increase DATASET_MOUNT_READ_THREADS for more read threads.\nIf the metrics show low SuccessE2ELat ency and low SuccessSer verLat ency, but the client experiences high\nlatency , you have a delay in the storage request that reaches the service. Y ou should check:\nWhether the number of threads used for mount/download\n(DATASET_MOUNT_READ_THREADS/RSLEX_DOWNLOADER_THREADS) is set too low, relative to the number of cores\navailable on the compute target. If the setting is too low, increase the number of threads.\nWhether the number of retries for downloading ( AZUREML_DATASET_HTTP_RETRY_COUNT) is set too high. If\nso, decrease the number of retries.\nFrom the Azure Machine Learning studio, you can also monitor the compute target disk IO and usage during\nyour job execution. Navigate to your job, and select the Monit oring  tab. This tab provides insights on the\nresources of your job, on a 30 day rolling basis. For example:\nAzure Machine Learning data runtime doesn't use the last RESERVED_FREE_DISK_SPACE bytes of disk space, to\nkeep the compute healthy (the default value is 150MB). If your disk is full, your code is writing files to disk\nwithout declaring the files as an output. Therefore, check your code to make sure that data isn't being written\nerroneously to temporary disk. If you must write files to temporary disk, and that resource is becoming full,\nconsider:\nIncreasing the VM Size to one that has a larger temporary disk\nSetting a T TL on the cached data ( DATASET_MOUNT_ATTRIBUTE_CACHE_TTL), to purge your data from diskMonitor disk usage during a job\n\uf80a\n７ Note\nJob monitoring supports only compute resources that Azure Machine Learning manages. Jobs with a\nruntime of less than 5 minutes will not have enough data to populate this view.""]",1958
gpt-4-turbo,32,,5.0,,"Can Azure Machine Learning use data stored in Azure File Storage? If so, what are the use cases?",v1,"Yes, Azure Machine Learning can use data stored in Azure File Storage. Azure File Storage provides shared storage that can be accessed via the SMB protocol, and it can be mounted onto Azure Machine Learning compute instances or pipelines. Here are some use cases for using Azure File Storage with Azure Machine Learning:

1. **Data Access**: You can store your datasets in Azure File Storage and access them from your Azure Machine Learning workspace. This is useful for scenarios where you have large datasets that need to be shared across multiple experiments or users.

2. **Persistent Storage**: When using Azure Machine Learning compute instances, you can mount Azure File Shares to ensure that your data persists even when the compute instance is shut down. This is particularly useful for saving intermediate results or datasets that are used frequently.

3. **Collaboration**: Since Azure File Storage can be accessed from anywhere, it facilitates collaboration among data scientists who are working on the same project but may be located in different geographies.

4. **Training and Model Development**: You can use the data stored in Azure File Storage for training machine learning models directly within Azure Machine Learning. The data can be accessed from notebooks or training scripts.

5. **Pipeline Storage**: In machine learning pipelines, you might need to store output from one step to be used as input in another. Azure File Storage can be used to store these intermediate outputs.

Remember to configure the appropriate permissions and network settings to ensure secure and efficient access to the file shares from your Azure Machine Learning environment.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore"", ""Attached comput e: You can attach your own compute resources to your\nworkspace and use them for training and inference.\nMachine Learning allows you to work with different types of data:\nURIs (a location in local or cloud storage):\nuri_folder\nuri_file\nTables (a tabular data abstraction):\nmltable\nPrimitives:\nstring\nboolean\nnumber\nFor most scenarios, you use URIs ( uri_folder and uri_file) to identify a location in\nstorage that can be easily mapped to the file system of a compute node in a job by\neither mounting or downloading the storage to the node.\nThe mltable parameter is an abstraction for tabular data that's used for automated\nmachine learning (AutoML) jobs, parallel jobs, and some advanced scenarios. If you're\nstarting to use Machine Learning and aren't using AutoML, we strongly encourage you\nto begin with URIs.\nMachine Learning datastores securely keep the connection information to your data\nstorage on Azure so that you don't have to code it in your scripts. Y ou can register and\ncreate a datastore to easily connect to your storage account and access the data in your\nunderlying storage service. The Azure Machine Learning CLI v2 and SDK v2 support the\nfollowing types of cloud-based storage services:\nAzure Blob S torage container\nAzure Files share\nAzure Data Lake S torage\nAzure Data Lake S torage Gen2Data\nDatastore\nEnvironment"", ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.""]",1727
gpt-4-turbo,33,,5.0,,What are the best practices for managing input and output data for Azure Machine Learning pipelines using Azure Storage?,v1,"When managing input and output data for Azure Machine Learning pipelines using Azure Storage, consider the following best practices:

1. **Use Azure Machine Learning Datastores**: Register your Azure Storage accounts (Azure Blob Storage, Azure Data Lake Storage Gen1, and Azure Data Lake Storage Gen2) as Azure Machine Learning datastores. This allows you to manage and access data within your Azure Machine Learning workspace easily.

2. **Flatten Nested Folder Structures**: Azure Machine Learning batch endpoint jobs do not support nested folder structures. If your data is organized in nested folders, you will need to flatten the structure before the batch job can process it.

3. **Data Movement with Azure Data Factory**: If your input data resides in a storage service not directly supported by Azure Machine Learning, use Azure Data Factory to copy the data to a compatible Azure Storage account before executing the batch job.

4. **Scoring Script Compatibility**: Ensure that the scoring script used in your deployment is capable of handling the data format and structure as it will be provided to the batch job. This includes being aware of any limitations related to MLflow models if you are using them.

5. **Register Data Stores**: It is recommended to register the storage account you are using with Azure Data Factory as a data store in Azure Machine Learning. This allows you to read from and write back to the same storage account seamlessly.

6. **Monitor Data Access and Permissions**: Make sure that the necessary permissions are set on your Azure Storage account so that the Azure Machine Learning service can access the data. Use Shared Access Signatures (SAS) or Azure Active Directory (Azure AD) authentication to control access.

7. **Optimize Data Transfer**: To reduce latency and potential costs, consider the location of your storage account relative to your Azure Machine Learning workspace and compute resources. Placing them in the same region can improve performance.

8. **Data Output Considerations**: For outputs, ensure that the data store you are writing to is registered with Azure Machine Learning and that your pipeline is configured to write the output to the correct location.

By following these best practices, you can effectively manage data inputs and outputs for your Azure Machine Learning pipelines, ensuring smooth and efficient operation.","[""7. Your pipeline is ready to be used.\nWhen calling Azure Machine Learning batch deployments consider the following limitations:\nOnly Azure Machine Learning data stores or Azure S torage Accounts (Azure Blob S torage, Azure\nData Lake S torage Gen1, Azure Data Lake S torage Gen2) are supported as inputs. If your input\ndata is in another source, use the Azure Data F actory Copy activity before the execution of the\nbatch job to sink the data to a compatible store.\nBatch endpoint jobs don't explore nested folders and hence can't work with nested folder\nstructures. If your data is distributed in multiple folders, notice that you will have to flatten the\nstructure.\nMake sure that your scoring script provided in the deployment can handle the data as it is\nexpected to be fed into the job. If the model is MLflow, read the limitation in terms of the file\ntype supported by the moment at Using MLflow models in batch deployments .\nOnly registered Azure Machine Learning data stores are supported by the moment. W e\nrecommend you to register the storage account your Azure Data F actory is using as a Data S tore\nin Azure Machine Learning. In that way, you will be able to write back to the same storage\naccount from where you are reading.\nLimitations\nData inputs\nData outputs"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.', ""original notebook or Python code, changes the training input from local data to\nparameterized values, split the training code into multiple steps as needed, perform unit\ntest of each step, and finally wraps all steps into a pipeline.\nOnce the teams get familiar with pipelines and want to do more machine learning\nprojects using pipelines, they'll find the first approach is hard to scale. The second\napproach is set up a few pipeline templates, each try to solve one specific machine\nlearning problem. The template predefines the pipeline structure including how many\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\nlearning project, the team first forks one template repo. The team leader then assigns\nmembers which step they need to work on. The data scientists and data engineers do\ntheir regular work. When they're happy with their result, they structure their code to fit\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\nexecuted or automated. If there's any change, each member only needs to work on their\npiece of code without touching the rest of the pipeline code.\nOnce a team has built a collection of machine learnings pipelines and reusable\ncomponents, they could start to build the machine learning pipeline from cloning\nprevious pipeline or tie existing reusable component together. At this stage, the team's\noverall productivity will be improved significantly.\nAzure Machine Learning offers different methods to build a pipeline. For users who are\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\npipelines by using registered components .\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\nfollowing table lists the different pipelines and what they're used for:\nScenar io Primar y\npersonaAzur e\nofferingOSS\nofferingCanonical\npipeStrengths\nModel\norchestration\n(Machine\nlearning)Data\nscientistAzure\nMachine\nLearning\nPipelinesKubeflow\nPipelinesData ->\nModelDistribution,\ncaching, code-first,\nreuse\nData\norchestrationData\nengineerAzure Data\nFactoryApache\nAirflowData -> Data Strongly typed\nmovement, data-Which Azure pipeline technology should I use?\nﾉExpand table"", ""original notebook or Python code, changes the training input from local data to\nparameterized values, split the training code into multiple steps as needed, perform unit\ntest of each step, and finally wraps all steps into a pipeline.\nOnce the teams get familiar with pipelines and want to do more machine learning\nprojects using pipelines, they'll find the first approach is hard to scale. The second\napproach is set up a few pipeline templates, each try to solve one specific machine\nlearning problem. The template predefines the pipeline structure including how many\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\nlearning project, the team first forks one template repo. The team leader then assigns\nmembers which step they need to work on. The data scientists and data engineers do\ntheir regular work. When they're happy with their result, they structure their code to fit\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\nexecuted or automated. If there's any change, each member only needs to work on their\npiece of code without touching the rest of the pipeline code.\nOnce a team has built a collection of machine learnings pipelines and reusable\ncomponents, they could start to build the machine learning pipeline from cloning\nprevious pipeline or tie existing reusable component together. At this stage, the team's\noverall productivity will be improved significantly.\nAzure Machine Learning offers different methods to build a pipeline. For users who are\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\npipelines by using registered components .\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\nfollowing table lists the different pipelines and what they're used for:\nScenar io Primar y\npersonaAzur e\nofferingOSS\nofferingCanonical\npipeStrengths\nModel\norchestration\n(Machine\nlearning)Data\nscientistAzure\nMachine\nLearning\nPipelinesKubeflow\nPipelinesData ->\nModelDistribution,\ncaching, code-first,\nreuse\nData\norchestrationData\nengineerAzure Data\nFactoryApache\nAirflowData -> Data Strongly typed\nmovement, data-Which Azure pipeline technology should I use?\nﾉExpand table""]",2161
gpt-4-turbo,34,,5.0,,How can you optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training?,v1,"To optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training, you can consider the following strategies:

1. **Proximity of Resources**: Ensure that your Azure Machine Learning workspace and Azure Storage account are in the same Azure region to reduce latency and data transfer times.

2. **Datastore Mounting**: Use Azure Machine Learning Datastores to mount your Azure Storage (Blob or File Storage) as a file system in your Azure Machine Learning compute. This allows you to access your data without the need for transferring it, which can be more efficient for large datasets.

3. **Data Caching**: When using Azure Machine Learning Compute Instances or Compute Clusters, you can cache the data on the VMs to reduce the data transfer time for subsequent runs.

4. **Optimized Data Formats**: Store your data in optimized formats such as Apache Parquet or ORC, which are designed for efficient data storage and retrieval.

5. **Incremental Data Transfer**: If your data changes incrementally, consider transferring only the new or changed data instead of the entire dataset.

6. **Pipeline Data Transfer**: Use Azure Machine Learning Pipelines to manage data transfer within the steps of your machine learning workflow. This can help to ensure that data is transferred efficiently between different stages of the pipeline.

7. **Azure Blob Storage's Hot and Cool Access Tiers**: Use the appropriate access tier in Azure Blob Storage based on how frequently you access the data. Hot access tier is optimized for data that is accessed frequently, while the cool access tier is cost-effective for data that is infrequently accessed.

8. **Network Optimization**: Use Azure's high-bandwidth networking options if you are transferring large amounts of data to ensure that network capacity does not become a bottleneck.

9. **Parallelization**: When transferring data, use parallelism to speed up the process. Azure Storage SDKs and tools like AzCopy support parallel data transfer.

10. **Compression**: Compress your data before transferring it to reduce the amount of data that needs to be moved. This can be particularly effective for text-based data formats.

By implementing these strategies, you can reduce the time and cost associated with transferring data between Azure Machine Learning and Azure Storage services, leading to more efficient model training processes.","['Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps', ""12,000 Mbps) would load the same data in ~33 secs (assuming you saturate the NIC at 1500-MB/s\nthroughput).\nStorage tier : For most scenarios - including Large Language Models (LLM) - standard storage provides\nthe best cost/performance profile. However, if you have many small files , premium  storage offers a\nbetter cost/performance profile. For more information, read Azure S torage options .\nStorage lo ad: If the storage account is under high load - for example, many GPU nodes in a cluster\nrequesting data - then you risk hitting the egress capacity of storage. For more information, read\nStorage load . If you have many small files that need access in parallel, you might hit the request limits\nof storage. R ead up-to-date information on the limits for both egress capacity and storage requests in\nScale targets for standard storage accounts .\nData access p attern in user code : When you use mount mode, data is fetched based on the open/read\nactions in your code. For example, when reading random sections of a large file, the default data\nprefetching settings of mounts can lead to downloads of blocks that won't be read. Y ou might need to\ntune some settings to reach maximum throughput. For more information, read Optimum mount\nsettings for common scenarios .\nTo access the logs of the data runtime from your job:\n1. Select Outputs+Logs  tab from the job page.\n2. Select the system_logs  folder, followed by data_cap ability  folder.\n3. You should see two log files:\nThe log file data-cap ability .log shows the high-level information about the time spent on key data loading\ntasks. For example, when you download data, the runtime logs the download activity start and finish times:\nlog\nIf the download throughput is a fraction of the expected network bandwidth for the VM size, you can inspect\nthe log file rslex.log.<TIMEST AMP> . This file contains all the fine-grain logging from the Rust-based\nruntime; for example, parallelization:\nlogUsing logs to diagnose issues\n\uf80a\nINFO 2023-05-18 17:14:47,790 sdk_logger.py:44 [28] - ActivityStarted, download\nINFO 2023-05-18 17:14:50,295 sdk_logger.py:44 [28] - ActivityCompleted: Activity=download,  \nHowEnded=Success, Duration=2504.39 [ms]"", ""Manage and optimize Azure Machine\nLearning costs\nArticle •08/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.） Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)"", ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage""]",1950
gpt-4-turbo,35,,5.0,,What considerations should be taken into account for data privacy and compliance when integrating Azure Machine Learning with Azure Storage?,v1,"When integrating Azure Machine Learning with Azure Storage for data privacy and compliance, the following considerations should be taken into account:

1. **Encryption at Rest**: Ensure that all data stored in Azure Blob Storage is encrypted at rest. Azure Storage encrypts data with Microsoft-managed keys by default. For additional control, you can use customer-managed keys in Azure Key Vault.

2. **Encryption in Transit**: Data should be encrypted in transit to and from Azure Storage. Azure Machine Learning and Azure Storage use secure protocols such as HTTPS to protect data as it moves between services.

3. **Access Control**: Implement proper access control mechanisms. Use Azure Role-Based Access Control (RBAC) to define who has access to your Azure Storage data. Ensure that only authorized Azure Machine Learning compute resources and users can access the data.

4. **Datastore Configuration**: When configuring datastores in Azure Machine Learning, ensure that they are linked to secure storage accounts and that the data is accessed securely.

5. **Monitoring and Auditing**: Enable monitoring and auditing features to track access and usage of data. Azure provides tools like Azure Monitor and Azure Security Center to help with this.

6. **Compliance Standards**: Be aware of the compliance standards relevant to your industry and region, such as GDPR, HIPAA, or CCPA. Azure has many certifications that can help meet these compliance requirements.

7. **Key Management**: Regularly rotate and manage encryption keys. If you are using customer-managed keys, you should have a process in place for key rotation and revocation. Azure Key Vault can assist with this process.

8. **Data Residency**: Consider the location of your Azure Storage resources to comply with data residency requirements. Data should be stored in a region that aligns with the legal and regulatory requirements pertinent to the data.

9. **Data Retention and Deletion**: Implement policies for data retention and deletion in line with compliance requirements. Ensure that you can securely delete data from Azure Storage when it is no longer needed or when required by compliance regulations.

10. **Secure Development Practices**: When developing applications that use Azure Machine Learning and Azure Storage, follow secure development practices to prevent data leaks or unauthorized access.

By considering these points, you can help ensure that your integration of Azure Machine Learning with Azure Storage is secure and compliant with relevant data privacy regulations.","[""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.', 'Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps', 'Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .', ""Data concepts in Azure Machine\nLearning\nArticle •07/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container ✓ ✓\nAzure File Share ✓\nAzure Data Lake Gen1 ✓ ✓\nAzure Data Lake Gen2 ✓ ✓Datastore""]",1886
gpt-4-turbo,36,,5.0,,How can you use managed identities to securely access data in Azure Storage from Azure Machine Learning?,v1,"To securely access data in Azure Storage from Azure Machine Learning using managed identities, you can follow these steps:

1. **Enable Managed Identity on Azure Machine Learning Workspace:**
   - In the Azure portal, navigate to your Azure Machine Learning workspace.
   - Under the ""Settings"" section, find and click on ""Identity"".
   - In the ""System assigned"" tab, switch the ""Status"" to ""On"" to enable a system-assigned managed identity for the workspace. Save the changes.

2. **Grant Permissions to Managed Identity:**
   - Navigate to the Azure Storage account you want to access from Azure Machine Learning.
   - Go to the ""Access control (IAM)"" section.
   - Click on ""Add role assignment"" to grant the managed identity the necessary permissions (e.g., ""Storage Blob Data Reader"" for read-only access).

3. **Create a Datastore in Azure Machine Learning:**
   - In the Azure Machine Learning studio, go to ""Datastores"" under the ""Data"" section.
   - Click on ""New datastore"" and select the appropriate storage type (e.g., Azure Blob Storage, Azure Data Lake Storage Gen1/Gen2).
   - Provide a name for the datastore and select the storage account.
   - For authentication, choose ""Managed Identity"" instead of providing access keys or SAS tokens.

4. **Use the Datastore in Training Jobs:**
   - When submitting training jobs, reference the datastore you created. Azure Machine Learning will use the managed identity to authenticate and access the data in the storage account.

By using managed identities, you avoid the need to store sensitive credentials in your code or configuration files, and you can manage access centrally through Azure role-based access control (RBAC). This approach enhances security and simplifies the management of credentials.","['You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .７ Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n２ Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity', ""Certain machine learning scenarios involve working with private data. In such cases, data\nscientists may not have direct access to data as Microsoft Entra users. In this scenario,\nthe managed identity of a compute can be used for data access authentication. In this\nscenario, the data can only be accessed from a compute instance or a machine learning\ncompute cluster executing a training job. With this approach, the admin grants the\ncompute instance or compute cluster managed identity S torage Blob Data R eader\npermissions on the storage. The individual data scientists don't need to be granted\naccess.\nTo enable authentication with compute managed identity:\nCreate compute with managed identity enabled. See the compute cluster  section,\nor for compute instance, the Assign managed identity  section.\nGrant compute managed identity at least S torage Blob Data R eader role on the\nstorage account.\nCreate any datastores with identity-based authentication enabled. See Create\ndatastores .\nOnce the identity-based authentication is enabled, the compute managed identity is\nused by default when accessing data within your training jobs. Optionally, you can\nauthenticate with user identity using the steps described in next section.\nFor information on using configuring Azure RBA C for the storage, see role-based access\ncontrols .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nWhen training on Azure Machine Learning compute clusters , you can authenticate to\nstorage with your user Microsoft Entra token.\nThis authentication mode allows you to:７ Note\nThe name of the created system managed identity for compute instance or cluster\nwill be in the format /workspace-name/computes/compute-name in your Microsoft\nEntra ID.\nAccess data for training jobs on compute clusters using\nuser identity"", 'means that in order to successfully read data from storage, the managed identity of the\ncompute cluster where the deployment is running must have at least Storage Blob Data\nReader  access to the storage account.\nTo configure the compute cluster for data access, follow these steps:\n1. Go to Azure Machine Learning studio .\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\ndeployment is using.\n3. Assign a managed identity to the compute cluster:\na. In the Managed identity  section, verify if the compute has a managed identity\nassigned. If not, select the option Edit.\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\nusing a S ystem-Assigned Managed Identity, it is named as ""[workspace\nname]/computes/[compute cluster name]"".\nc. Save the changes.\n4. Go to the Azure portal  and navigate to the associated storage account where the\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\nstorage account where those assets are placed.', 'Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .', 'Data administration\nArticle •09/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n） Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.']",1624
gpt-4-turbo,37,,5.0,,What are the implications of storage redundancy options in Azure Storage on data availability for Azure Machine Learning experiments?,v1,"Storage redundancy options in Azure Storage can have significant implications on data availability for Azure Machine Learning experiments. Here are some key points to consider:

1. **Locally Redundant Storage (LRS)**: This option replicates your data within a single storage scale unit in a single data center. While LRS is the least expensive redundancy option, it also poses the highest risk for data availability because if the data center fails, you could lose access to your data. For Azure Machine Learning, this could mean that your experiments may not run or complete if the data is not accessible.

2. **Zone-Redundant Storage (ZRS)**: ZRS replicates your data across multiple data centers or zones within a region. This increases data availability since even if one zone goes down, the other zones can still provide access to your data. For Azure Machine Learning experiments, this means a higher likelihood that your data will be available when needed, even in the event of a partial regional outage.

3. **Geo-Redundant Storage (GRS)** and **Geo-Zone-Redundant Storage (GZRS)**: These options provide the highest level of data availability by replicating data to a secondary region far away from the primary region. GRS and GZRS protect against regional outages, ensuring that your data is still accessible for Azure Machine Learning experiments even if the primary region experiences a significant disruption.

Choosing the right storage redundancy option depends on your specific requirements for data availability, recovery, and cost. For critical Azure Machine Learning experiments where data availability is paramount, it is advisable to opt for higher redundancy options like ZRS, GRS, or GZRS. However, these options come with higher costs compared to LRS.

It's also important to consider the location of your Azure Machine Learning workspace and compute resources in relation to your storage. To minimize latency and data transfer costs, it's recommended to deploy all resources in the same region. If your storage redundancy option involves replication across regions (like GRS or GZRS), you should still ensure that your primary region for storage aligns with the region of your Azure Machine Learning resources to optimize for performance and cost.","['setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps', 'Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps', ""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", ""Service Usage Example datanumber of nodes; datastore names and tags, and\ndescriptions on assets like models; and data label names.\nAzure AI\nSearchStores indexes that\nhelp with querying\nyour machine learning\ncontent.These indexes are built on top of the data stored in Azure\nCosmos DB.\nAzure\nStorageStores metadata\nrelated to Azure\nMachine Learning\npipeline data.Data can include designer pipeline names, pipeline layout,\nand execution properties.\nFrom the perspective of data lifecycle management, data in the preceding resources is\ncreated and deleted as you create and delete corresponding objects in Azure Machine\nLearning.\nYour Azure Machine Learning workspace reads and writes data by using its managed\nidentity. This identity is granted access to the resources through a role assignment\n(Azure role-based access control) on the data resources. The encryption key that you\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\nused to create indexes for Azure AI Search at runtime.\nExtra networking controls are configured when you create a private link endpoint on\nyour workspace to allow for inbound connectivity. This configuration includes the\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\nNetwork access is restricted to only trusted Microsoft services.\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\nthe data.\nWhen you use a customer-managed key, the resources are in your Azure subscription\nand encrypted with your key. While these resources exist in your subscription, Microsoft\nmanages them. They're automatically created and configured when you create your\nAzure Machine Learning workspace.\nThese Microsoft-managed resources are located in a new Azure resource group that's\ncreated in your subscription. This resource group is separate from the resource group\nfor your workspace. It contains the Microsoft-managed resources that your key is used\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\nresource group name><GUID>.Customer-managed keys"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.']",1892
gpt-4-turbo,38,,5.0,,How do you automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage?,v1,"To automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage, you can use Azure services and features that are designed for backup and disaster recovery. Here are the steps you can follow:

1. **Azure Backup Service:**
   - Use Azure Backup to create backup policies for Azure Blob Storage.
   - Schedule regular backups according to your requirements.
   - Azure Backup will manage the lifecycle of the backups, including retention policies.

2. **Azure Data Factory:**
   - Create a data pipeline in Azure Data Factory that copies data from your primary Azure Blob Storage to a secondary location for backup.
   - Schedule the pipeline to run at regular intervals.
   - Use the monitoring features to track the pipeline's performance and ensure backups are completed successfully.

3. **Azure Blob Storage Snapshots:**
   - Create snapshots of your Azure Blob Storage blobs to maintain recoverable points in time.
   - Automate snapshot creation using Azure Automation or Azure Functions with a time-triggered schedule.
   - Manage snapshot lifecycle to delete old snapshots and maintain storage costs.

4. **Azure Site Recovery:**
   - Although typically used for virtual machines, you can use Azure Site Recovery to replicate your storage accounts to a secondary region for geo-redundancy.
   - Configure replication and failover policies as per your business continuity plans.

5. **Azure Storage Account Geo-Replication:**
   - Enable geo-redundant storage (GRS) or read-access geo-redundant storage (RA-GRS) on your Azure Storage account to automatically replicate your data to a secondary region.

6. **Azure Automation:**
   - Use Azure Automation to run runbooks that can automate the backup process.
   - Create scripts to manage the backup lifecycle, including the creation and deletion of backups based on a schedule.

7. **Azure Logic Apps:**
   - Create logic apps that trigger on a schedule or specific events to manage the backup process.
   - Integrate with other Azure services and custom logic to ensure datasets are backed up and recoverable.

8. **Monitoring and Alerts:**
   - Set up monitoring and alerts using Azure Monitor to track the status of your backups.
   - Receive notifications in case of failures or issues with the backup process.

Remember to test your backup and recovery process regularly to ensure that it meets your recovery time objectives (RTO) and recovery point objectives (RPO). Additionally, consider the cost implications of the backup strategy you choose, as data storage and transfer can incur costs.","[""Data encryption with Azure Machine\nLearning\nArticle •01/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', 'Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps', 'setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps', ""Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n７ Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.""]",1836
gpt-4-turbo,39,,5.0,,What tools and techniques are available for analyzing storage costs associated with Azure Machine Learning projects?,v1,"To analyze storage costs associated with Azure Machine Learning projects, you can use the following tools and techniques:

1. **Azure Pricing Calculator**: Before adding any resources, you can use the Azure Pricing Calculator to estimate the costs of the storage you plan to use with Azure Machine Learning.

2. **Cost Management Features**: After you've started using Azure Machine Learning resources, you can utilize Azure Cost Management features to monitor and manage your costs. This includes setting budgets, reviewing forecasted costs, and identifying spending trends.

3. **Cost Analysis in Azure Cost Management**: This tool supports most Azure account types and allows you to analyze your spending on Azure services, including storage used by Azure Machine Learning. You can view and analyze your costs, usage patterns, and trends over time.

4. **Assign Access to Cost Data**: To view cost data, you need at least read access to an Azure account. You can assign access to Azure Cost Management data to ensure the right team members can monitor and manage costs.

Remember to review the Azure documentation for any updates or changes to these tools and techniques, as Azure services are continuously evolving. Additionally, consider applying similar cost management methods to other Azure services you use in conjunction with Azure Machine Learning to get a comprehensive view of your overall costs.","[""Plan to manage costs for Azure Machine\nLearning\nArticle •03/31/2023\nThis article describes how to plan and manage costs for Azure Machine Learning. First,\nyou use the Azure pricing calculator to help plan for costs before you add any resources.\nNext, as you add the Azure resources, review the estimated costs.\nAfter you've started using Azure Machine Learning resources, use the cost management\nfeatures to set budgets and monitor costs. Also review the forecasted costs and identify\nspending trends to identify areas where you might want to act.\nUnderstand that the costs for Azure Machine Learning are only a portion of the monthly\ncosts in your Azure bill. If you are using other Azure services, you're billed for all the\nAzure services and resources used in your Azure subscription, including the third-party\nservices. This article explains how to plan for and manage costs for Azure Machine\nLearning. After you're familiar with managing costs for Azure Machine Learning, apply\nsimilar methods to manage costs for all the Azure services used in your subscription.\nFor more information on optimizing costs, see how to manage and optimize cost in\nAzure Machine Learning .\nCost analysis in Cost Management supports most Azure account types, but not all of\nthem. T o view the full list of supported account types, see Understand Cost\nManagement data .\nTo view cost data, you need at least read access for an Azure account. For information\nabout assigning access to Azure Cost Management data, see Assign access to data .） Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nPrerequisites"", ""Use the Azure pricing calculator  to estimate costs before you create the\nresources in an Azure Machine Learning workspace. On the left, select AI +\nMachine Learning , then select Azur e Machine Learning  to begin.\nThe following screenshot shows the cost estimation by using the calculator:\nAs you add new resources to your workspace, return to this calculator and add the same\nresource here to update your cost estimates.\nFor more information, see Azure Machine Learning pricing .\nAzure Machine Learning runs on Azure infrastructure that accrues costs along with\nAzure Machine Learning when you deploy the new resource. It's important toEstimate costs before using Azure Machine\nLearning\nUnderstand the full billing  model for Azure\nMachine Learning"", ""Manage and optimize Azure Machine\nLearning costs\nArticle •08/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.） Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)"", 'monthly schedule and set a custom date range. Exporting cost data is the\nrecommended way to retrieve cost datasets.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nDelete instances and clusters if you do not plan on using them in the near future.\nFor more information, see manage and optimize costs in Azure Machine Learning .\nManage and optimize costs in Azure Machine Learning .\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscale\nLearn how to optimize your cloud investment with Azure Cost Management .\nLearn more about managing costs with cost analysis .\nLearn about how to prevent unexpected costs .\nTake the Cost Management  guided learning course.Other ways to manage and reduce costs for\nAzure Machine Learning\nNext steps', ""To view Azure Machine Learning costs in cost analysis:\n1. Sign in to the Azure portal.\n2. Open the scope in the Azure portal and select Cost analysis  in the menu. For\nexample, go to Subscriptions , select a subscription from the list, and then select\nCost analysis  in the menu. Select Scope  to switch to a different scope in cost\nanalysis.\n3. By default, cost for services are shown in the first donut chart. Select the area in the\nchart labeled Azure Machine Learning.\nActual monthly costs are shown when you initially open cost analysis. Here's an example\nshowing all monthly usage costs.\nTo narrow costs for a single service, like Azure Machine Learning, select Add filt er and\nthen select Service name . Then, select virtual machines .\nHere's an example showing costs for just Azure Machine Learning.\n\uf80a""]",1467
gpt-4-turbo,40,,5.0,,How can data engineers streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics?,v1,"Data engineers can streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics by leveraging several Azure services and features that work together to facilitate a smooth and efficient data pipeline. Here are some steps and best practices:

1. **Azure Data Lake Storage (ADLS) Gen 2**: Utilize ADLS Gen 2 as the primary data storage, which provides a scalable and secure data lake that supports high-throughput analytics.

2. **Azure Synapse Analytics**: Integrate Azure Machine Learning with Azure Synapse Analytics to access an Apache Spark pool. This allows for interactive data wrangling and preprocessing within Azure Machine Learning Notebooks.

3. **Azure Machine Learning Pipelines**: Create Azure Machine Learning pipelines that can automate the data ingestion and preprocessing steps. These pipelines can be triggered on a schedule or in response to events, ensuring that the data is ready for real-time analytics.

4. **Real-time Ingestion with Azure Event Hubs or Azure IoT Hub**: For real-time analytics, use Azure Event Hubs or Azure IoT Hub to ingest streaming data directly into Azure Machine Learning.

5. **Azure Functions**: Use Azure Functions to create serverless components that can process or transform data as it arrives, reducing latency for real-time analytics.

6. **Azure Key Vault**: Store sensitive information such as connection strings, access keys, and secrets securely in Azure Key Vault. This ensures that the data ingestion process is secure and that credentials are managed properly.

7. **Role Assignments and Access Control**: Manage role assignments and access control for Azure Storage accounts to ensure that only authorized services and users can access the data.

8. **Monitoring and Logging**: Implement monitoring and logging using Azure Monitor and Azure Log Analytics to track the performance of the data ingestion process and to quickly identify and troubleshoot any issues.

By following these steps, data engineers can create a robust and efficient data ingestion pipeline that feeds into Azure Machine Learning for real-time analytics, ensuring that the data is always up-to-date and ready for analysis.","[""Interactive Data Wrangling with Apache\nSpark in Azure Machine Learning\nArticle •10/05/2023\nData wrangling becomes one of the most important steps in machine learning projects.\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides access\nto an Apache Spark pool - backed by Azure S ynapse - for interactive data wrangling\nusing Azure Machine Learning Notebooks.\nIn this article, you'll learn how to perform data wrangling using\nServerless Spark compute\nAttached S ynapse Spark pool\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an Azure\nData Lake S torage (ADLS) Gen 2 storage account .\n(Optional): An Azure K ey Vault. See Create an Azure K ey Vault.\n(Optional): A Service Principal. See Create a Service Principal .\n(Optional): An attached S ynapse Spark pool in the Azure Machine Learning\nworkspace .\nBefore you start your data wrangling tasks, learn about the process of storing secrets\nAzure Blob storage account access key\nShared Access Signature (SAS) token\nAzure Data Lake S torage (ADLS) Gen 2 service principal information\nin the Azure K ey Vault. Y ou also need to know how to handle role assignments in the\nAzure storage accounts. The following sections review these concepts. Then, we'll\nexplore the details of interactive data wrangling using the Spark pools in Azure Machine\nLearning Notebooks.Prerequisites\n\uea80 Tip"", 'Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.', ""runs from experimentation in the studio user interface. Or they can use versioned assets\nfor jobs like environments and storage references.\nFor more information, see Manage Azure Machine Learning workspaces .\nWhen a project is ready for operationalization, users' work can be automated in an ML\npipeline and triggered on a schedule or HT TPS request.\nYou can deploy models to the managed inferencing solution, for both real-time and\nbatch deployments, abstracting away the infrastructure management typically required\nfor deploying models.\nIn Azure Machine Learning, you can run your training script in the cloud or build a\nmodel from scratch. Customers often bring models they've built and trained in open-\nsource frameworks so that they can operationalize them in the cloud.\nData scientists can use models in Azure Machine Learning that they've created in\ncommon Python frameworks, such as:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nLightGBM\nOther languages and frameworks are also supported:\nR\n.NET\nFor more information, see Open-source integration with Azure Machine Learning .\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\nexperience and intuition to select the right data featurization and algorithm for training.\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\nLearning studio UI or the Python SDK.Train models\nOpen and interoperable\nAutomated featurization and algorithm selection"", ""training and evaluation, the machine learning engineers focus on model deployment\nand automation of the entire workflow. By leveraging machine learning pipeline, each\nteam only needs to work on building their own steps. The best way of building steps is\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\none step in a machine learning pipeline. All these steps built by different users are finally\nintegrated into one workflow through the pipeline definition. The pipeline is a\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\nits steps can be standardized by each company's preferred DevOps practice. The\npipeline can be further versioned and automated. If the ML projects are described as a\npipeline, then the best ML Ops practice is already applied.\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\nimproves large model training's efficiency and reduces cost. T aking modern natural\nlanguage model training as an example. It requires pre-processing large amounts of\ndata and GPU intensive transformer model training. It takes hours to days to train a\nmodel each time. When the model is being built, the data scientist wants to test\ndifferent training code or hyperparameters and run the training many times to get the\nbest model performance. For most of these trainings, there's usually small changes from\none training to another one. It will be a significant waste if every time the full training\nfrom data processing to model training takes place. By using machine learning pipeline,\nit can automatically calculate which steps result is unchanged and reuse outputs from\nprevious training. Additionally, the machine learning pipeline supports running each\nstep on different computation resources. Such that, the memory heavy data processing\nwork and run-on high memory CPU machines, and the computation intensive training\ncan run on expensive GPU machines. By properly choosing which step to run on which\ntype of machines, the training cost can be significantly reduced.\nDepending on what a machine learning project already has, the starting point of\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\nbuilding a pipeline.\nThe first approach usually applies to the team that hasn't used pipeline before and\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\ntypically have developed some machine learning models on their local environment\nusing their favorite tools. Machine learning engineers need to take data scientists'\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\nGetting  started best practices"", ""training and evaluation, the machine learning engineers focus on model deployment\nand automation of the entire workflow. By leveraging machine learning pipeline, each\nteam only needs to work on building their own steps. The best way of building steps is\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\none step in a machine learning pipeline. All these steps built by different users are finally\nintegrated into one workflow through the pipeline definition. The pipeline is a\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\nits steps can be standardized by each company's preferred DevOps practice. The\npipeline can be further versioned and automated. If the ML projects are described as a\npipeline, then the best ML Ops practice is already applied.\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\nimproves large model training's efficiency and reduces cost. T aking modern natural\nlanguage model training as an example. It requires pre-processing large amounts of\ndata and GPU intensive transformer model training. It takes hours to days to train a\nmodel each time. When the model is being built, the data scientist wants to test\ndifferent training code or hyperparameters and run the training many times to get the\nbest model performance. For most of these trainings, there's usually small changes from\none training to another one. It will be a significant waste if every time the full training\nfrom data processing to model training takes place. By using machine learning pipeline,\nit can automatically calculate which steps result is unchanged and reuse outputs from\nprevious training. Additionally, the machine learning pipeline supports running each\nstep on different computation resources. Such that, the memory heavy data processing\nwork and run-on high memory CPU machines, and the computation intensive training\ncan run on expensive GPU machines. By properly choosing which step to run on which\ntype of machines, the training cost can be significantly reduced.\nDepending on what a machine learning project already has, the starting point of\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\nbuilding a pipeline.\nThe first approach usually applies to the team that hasn't used pipeline before and\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\ntypically have developed some machine learning models on their local environment\nusing their favorite tools. Machine learning engineers need to take data scientists'\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\nGetting  started best practices""]",2207
gpt-4-turbo,41,,3.0,,How can I using MSI with Storage?,v1,"To use Managed Service Identity (MSI) with Azure Storage within the context of Azure Machine Learning, you would typically set up an identity for your Azure Machine Learning workspace and grant it access to your Azure Storage account. Here's a general outline of the steps you would follow:

1. **Enable Managed Identity for Azure Machine Learning Workspace:**
   - In the Azure portal, navigate to your Azure Machine Learning workspace.
   - Under the ""Settings"" section, find and click on ""Identity"".
   - In the ""System assigned"" tab, switch the Status to ""On"" to enable a system-assigned managed identity for the workspace. Save the changes.

2. **Grant Access to Azure Storage:**
   - Navigate to your Azure Storage account in the Azure portal.
   - Under the ""Access control (IAM)"" section, click on ""Add role assignment"".
   - Choose the appropriate role that you want to assign to the Azure Machine Learning workspace (e.g., Storage Blob Data Contributor).
   - In the ""Assign access to"" dropdown, select ""Managed Identity"".
   - Search for your Azure Machine Learning workspace and select it.
   - Click ""Save"" to grant the managed identity access to the storage account.

3. **Configure Azure Machine Learning to Use the Managed Identity:**
   - When setting up datastores or other storage-related configurations in Azure Machine Learning, you can specify that the managed identity should be used for authentication.

4. **Access Storage from Azure Machine Learning:**
   - In your Azure Machine Learning code or pipeline, when accessing the Azure Storage, the managed identity will be used automatically for authentication, assuming that the environment is correctly configured to use the identity.

Please note that the exact steps and configurations might vary based on the specific requirements of your project and the current features available in Azure Machine Learning and Azure Storage services. Always refer to the latest Azure documentation for the most accurate and up-to-date instructions.","[""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data"", 'Service tag Protocol Port\nStorage.<region> TCP 443\nFor more information, see How to secure training environments  and Configure inbound\nand outbound network traffic .\nUse the following steps to enable a storage endpoint for the subnet that contains your\nAzure Machine Learning compute clusters and compute instances:\n1. From the Azure portal , select the Azur e Vir tual Netw ork for your Azure Machine\nLearning workspace.\n2. From the left of the page, select Subnets  and then select the subnet that contains\nyour compute cluster and compute instance.\n3. In the form that appears, expand the Services  dropdown and then enable\nMicrosoft.Storage . Select Save to save these changes.\n4. Apply the service endpoint policy to your workspace subnet.７ Note\nFor the storage outbound, a Service Endpoint P olicy will be applied in a later\nstep to limit outbound traffic.\n3. Enable storage endpoint for the subnet\n\uf80a', ""In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data"", 'b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably.', 'b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably.']",2008
gpt-4-turbo,42,,5.0,,How can I browse data in the AML workspace?,v1,"To browse data in an Azure Machine Learning (AML) workspace, you can follow these steps:

1. **Access Azure Machine Learning Studio**: Open Azure Machine Learning Studio by navigating to https://ml.azure.com. You will need to sign in with the credentials associated with your Azure subscription.

2. **Select Your Workspace**: Once you are in the Azure Machine Learning Studio, select the appropriate workspace from the top right corner if it's not already selected.

3. **Navigate to the Datasets**: On the left side navigation panel, click on ""Datasets"" to view the datasets that have been registered or uploaded to your workspace.

4. **Explore Datasets**: In the Datasets section, you can explore the list of datasets. You can click on any dataset to see more details, such as the schema, profile, and explore the data if the dataset is not too large.

5. **Preview Data**: For many datasets, you can preview the data directly in the Azure Machine Learning Studio by selecting the dataset and clicking on the ""Preview"" tab.

6. **Use Notebooks**: If you want to interact with the data programmatically, you can use Azure Notebooks. Create a new notebook or open an existing one and use the Azure ML SDK to load and work with your data.

Remember that the exact steps and UI elements might change over time as Azure Machine Learning continues to evolve. Always refer to the latest Azure Machine Learning documentation for the most current information.","[""You'll need the following items:\nTo use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\nRun one or more jobs in your workspace to have results available in the\ndashboard. Complete Tutorial: T rain a model in Azure Machine Learning  if you\ndon't have any jobs yet.\nEnable this preview feature via the preview panel.\nNext, let's view your jobs in the studio:\nSelect Jobs on the left side navigation panel.\nSelect either All experiments  to view all the jobs in an experiment or select All jobs\nto view all the jobs submitted in the workspace.\nYou are now on the default dashboard view where you will find your job list\nconsolidated into the left side bar and dashboard content on the right.Items marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nPrerequisites\nExplore the dashboard view"", ""The following features of Azure Machine Learning studio use data profiling:\nDataset: Explore the dataset from the studio.\nDesigner: Visualize module output data.\nAutoML: View a data preview or profile and choose a target column.\nLabeling: Use labels to prepare data for a machine learning project.\nData profiling depends on the ability of the Azure Machine Learning managed service to\naccess the default Azure storage account for your workspace. The managed service\ndoesn 't exist in y our vir tual netw ork, so it can't directly access the storage account in the\nvirtual network. Instead, the workspace uses a service principal to access storage.\nTo allow access to the storage account, configure the storage account to allow a\nresource instance for your workspace or select Allow Azur e ser vices on the trust edScenario: Use AutoML, the designer, the\ndataset, and the datastore from the studio\n\uea80 Tip\nYou can provide a service principal when you're creating the workspace. If you\ndon't, one is created for you and has the same name as your workspace."", ""You'll need the following items:\nTo use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\nRun one or more jobs in your workspace to have results available in the\ndashboard. Complete Tutorial: T rain a model in Azure Machine Learning  if you\ndon't have any jobs yet.\nEnable this preview feature via the preview panel.\nSelect Jobs on the left side navigation panel.\nSelect either All experiments  to view all the jobs in an experiment or select All jobs\nto view all the jobs submitted in the workspace.\nSelect List view  at the top to switch into List view .\nThe job display name is an optional and customizable name that you can provide for\nyour job. Y ou can edit this directly in your jobs list view by selecting the pencil icon\nwhen you move your mouse over a job name.\nCustomizing the name may help you organize and label your training jobs easily.Prerequisites\nView jobs list\nJob display name\nSelect and pin columns"", 'Azure Machine Learning extension to view, update and create workspace assets like\ncomputes, data, environments, jobs and more.\nWhen you use VS Code for the W eb, the latest versions of these extensions are\nautomatically available to you. If you use the desktop application , you may need to\ninstall them.\nWhen you launch VS Code connected to a compute instance  for the first time, make\nsure you follow these steps and take a few moments to orient yourself to the tools in\nyour integrated development environment.\n1. Locate the Azure extension and sign in\n2. Once your subscriptions are listed, you can filter to the ones you use frequently.\nYou can also pin workspaces you use most often within the subscriptions.\n3. The workspace you launched the VS Code remote connection from (the workspace\nthe compute instance is in) should be automatically set as the default. Y ou can\nupdate the default workspace from the VS Code status bar.\n4. If you plan to use the Azure Machine Learning CLI, open a terminal from the menu,\nand sign in to the Azure Machine Learning CLI using az login --identity.', ""2. Expand your workspace node.\n3. Expand the Envir onments  node.\n4. Right-click the environment you want to view and select View Envir onment .\nAlternatively, use the > Azure ML: View Environment command in the command palette.\nThe quickest way to create a job is by clicking the Create Job  icon in the extension's\nactivity bar.\nUsing the resource nodes in the Azure Machine Learning view:\n1. Expand the subscription node that contains your workspace.\n2. Expand your workspace node.\n3. Right-click the Experiments  node in your workspace and select Create Job .\n4. Choose your job type.\n5. A specification file appears. Configure the specification file.\n6. Right-click the specification file and select Azur eML: Ex ecute YAML .\nAlternatively, use the > Azure ML: Create Job command in the command palette.\nTo view your job in Azure Machine Learning studio:\n1. Expand the subscription node that contains your workspace.\n2. Expand the Experiments  node inside your workspace.\n3. Right-click the experiment you want to view and select View Experiment in S tudio .\n4. A prompt appears asking you to open the experiment URL in Azure Machine\nLearning studio. Select Open .\nAlternatively, use the > Azure ML: View Experiment in Studio command respectively in\nthe command palette.\nAs you're running your job, you may want to see its progress. T o track the progress of a\njob in Azure Machine Learning studio from the extension:\n1. Expand the subscription node that contains your workspace.\n2. Expand the Experiments  node inside your workspace.\n3. Expand the job node you want to track progress for.Create job\nView job\nTrack job progress""]",1577
gpt-4-turbo,43,,5.0,,How to use run spark jobs and integrate with ADLS Gen2?,v1,"To run Apache Spark jobs and integrate with Azure Data Lake Storage (ADLS) Gen 2 using Azure Machine Learning, you can follow these steps:

1. **Prerequisites**:
   - Ensure you have an Azure subscription. If you don't, you can create a free account.
   - Create an Azure Machine Learning workspace. You can find instructions on how to do this in the Azure documentation.
   - Set up an ADLS Gen 2 storage account. Instructions for creating one are available in the Azure documentation.
   - Create an Azure Machine Learning compute instance if you don't already have one.
   - Install the Azure Machine Learning CLI (Command Line Interface) on your local machine or on a cloud-based compute instance.

2. **Assign Roles**:
   - Before submitting a Spark job, make sure that the input and output data paths in your ADLS Gen 2 storage account are accessible.
   - Assign the 'Contributor' and 'Storage Blob Data Contributor' roles to the user identity of the logged-in user. This will enable read and write access to the storage account.

3. **Submit a Spark Job**:
   - Use the Azure Machine Learning CLI or SDK to configure and submit your Spark job.
   - When configuring your Spark job, specify the ADLS Gen 2 storage account details as part of the input and output data paths.
   - If you are using the Azure Machine Learning CLI, you would typically use the `az ml job create` command with a job YAML file that defines the Spark job configuration, including the compute target, which would be the Azure Machine Learning serverless Spark compute in this case.

4. **User Identity Passthrough**:
   - For security and governance, you may want to use user identity passthrough. This ensures that the job runs under the identity of the user who submitted it, leveraging their permissions for data access.

5. **Monitor and Manage the Job**:
   - After submitting the job, you can monitor its progress using the Azure Machine Learning studio or the CLI.
   - Once the job is completed, you can review the outputs and logs to verify the results.

For more detailed instructions and examples, you should refer to the Azure Machine Learning documentation, specifically the section on running Apache Spark jobs. The documentation will provide step-by-step guidance and examples of the necessary configurations and commands.","[""Quickstart: Apache Spark jobs in Azure\nMachine Lea rning\nArticle •05/23/2023\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides easy\naccess to distributed computing capability - backed by Azure S ynapse - for scaling\nApache Spark jobs on Azure Machine Learning.\nIn this quickstart guide, you learn how to submit a Spark job using Azure Machine\nLearning serverless Spark compute, Azure Data Lake S torage (ADLS) Gen 2 storage\naccount, and user identity passthrough in a few simple steps.\nFor more information about Apache Sp ark in Azur e Machine Learning  concepts, see\nthis resource .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an\nAzure Data Lake S torage (ADLS) Gen 2 storage account .\nCreate an Azure Machine Learning compute instance .\nInstall Azure Machine Learning CLI .\nBefore we submit an Apache Spark job, we must ensure that input, and output, data\npaths are accessible. Assign Contribut or and Storage Blob Data Contribut or roles to\nthe user identity of the logged-in user to enable read and write access.\nTo assign appropriate roles to the user identity:Prerequisites\nCLI\nAdd role assignments in Azure storage\naccounts"", '16. Repeat steps 2-13 for Storage Blob Contribut or role assignment.\nData in the Azure Data Lake S torage (ADLS) Gen 2 storage account should become\naccessible once the user identity has appropriate roles assigned.\nA Spark job requires a Python script that takes arguments, which can be developed by\nmodifying the Python code developed from interactive data wrangling . A sample Python\nscript is shown here.\nPython\n\uf80a\nCreate parametrized Python code\n# titanic.py  \nimport argparse  \nfrom operator import add \nimport pyspark.pandas as pd \nfrom pyspark.ml.feature import Imputer  \nparser = argparse.ArgumentParser()  \nparser.add_argument( ""--titanic_data"" ) \nparser.add_argument( ""--wrangled_data"" ) \nargs = parser.parse_args()  \nprint(args.wrangled_data)  \nprint(args.titanic_data)  \ndf = pd.read_csv(args.titanic_data, index_col= ""PassengerId"" ) \nimputer = Imputer(inputCols=[ ""Age""], outputCol= ""Age"").setStrategy(\n    ""mean"" \n)  # Replace missing values in Age column with the mean value  \ndf.fillna(  \n    value={ ""Cabin"": ""None""}, inplace= True \n)  # Fill Cabin column with value ""None"" if missing', ""Interactive Data Wrangling with Apache\nSpark in Azure Machine Learning\nArticle •10/05/2023\nData wrangling becomes one of the most important steps in machine learning projects.\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides access\nto an Apache Spark pool - backed by Azure S ynapse - for interactive data wrangling\nusing Azure Machine Learning Notebooks.\nIn this article, you'll learn how to perform data wrangling using\nServerless Spark compute\nAttached S ynapse Spark pool\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an Azure\nData Lake S torage (ADLS) Gen 2 storage account .\n(Optional): An Azure K ey Vault. See Create an Azure K ey Vault.\n(Optional): A Service Principal. See Create a Service Principal .\n(Optional): An attached S ynapse Spark pool in the Azure Machine Learning\nworkspace .\nBefore you start your data wrangling tasks, learn about the process of storing secrets\nAzure Blob storage account access key\nShared Access Signature (SAS) token\nAzure Data Lake S torage (ADLS) Gen 2 service principal information\nin the Azure K ey Vault. Y ou also need to know how to handle role assignments in the\nAzure storage accounts. The following sections review these concepts. Then, we'll\nexplore the details of interactive data wrangling using the Spark pools in Azure Machine\nLearning Notebooks.Prerequisites\n\uea80 Tip"", 'Standard_E4s_v3\nStandard_E8s_v3\nStandard_E16s_v3\nStandard_E32s_v3\nStandard_E64s_v3\n4. Input a Spark Session timeout  value, in minutes.\n5. Select whether to Dynamically allocat e executors\n6. Select the number of Executors for the Spark session.\n7. Select Executor size  from the dropdown menu.\n8. Select Driver size  from the dropdown menu.\n9. To use a Conda file to configure a Spark session, check the Uplo ad conda file\ncheckbox. Then, select Browse , and choose the Conda file with the Spark session\nconfiguration you want.\n10. Add Configuration settings  properties, input values in the Proper ty and Value\ntextboxes, and select Add.\n11. Select Apply .\n12. Select Stop session  in the Configur e new session?  pop-up.\nThe session configuration changes persist and become available to another notebook\nsession that is started using the serverless Spark compute.\nYou can access and wrangle data stored in Azure Data Lake S torage (ADLS) Gen 2\nstorage accounts with abfss:// data URIs following one of the two data access\nmechanisms:\nUser identity passthrough\nService principal-based data access\uea80 Tip\nIf you use session-level Conda packages, you can impr ove the Spark session cold\nstart time if you set the configuration variable spark.hadoop.aml.enable_cache to\ntrue.\nImport and wrangle data from Azure Data Lake Storage\n(ADLS) Gen 2\n\uea80 Tip', 'Install and use the CLI (v2)\nSubmit Spark jobs in Azure Machine Learning    outputs:  \n      wrangled_data:  \n        type:  uri_folder  \n        path:  azureml://datastores/workspaceblobstore/paths/data/wrangled/  \n        mode:  direct \n    identity:  \n      type:  user_identity  \n    compute:  mysparkcompute  \nNext steps']",1829
gpt-4-turbo,44,,5.0,,How does batch inferencing work in parallel mode?,v1,"In Azure Machine Learning, batch inferencing in parallel mode involves processing multiple mini-batches of data simultaneously across different workers and nodes. Here's how it generally works:

1. **Deployment Configuration**: You configure the deployment settings to control the size of each mini-batch and the number of workers on each node. This configuration is crucial for parallel processing.

2. **Mini-Batch Processing**: The `run()` function in your scoring script is called once per mini-batch. You have the option to process the entire mini-batch at once or to process the data file by file or row by row, depending on your data type (e.g., tabular).

3. **Multiple Workers**: When running multiple workers on the same instance, it's important to remember that they share memory. To ensure efficient memory usage and to prevent out-of-memory exceptions, you may need to adjust the mini-batch size or change the scoring strategy if you increase the number of workers per node.

4. **Parallel Execution**: Each worker can process a mini-batch of data independently. If you have multiple workers across different nodes, they can all work in parallel, each handling a separate mini-batch, thus speeding up the overall inferencing process.

5. **Memory Management**: Running inference at the batch level requires careful control over the input data size to manage memory requirements effectively. The ability to load the entire mini-batch into memory depends on various factors, including the mini-batch size, the size of the instances in the cluster, the number of workers on each node, and the memory capacity.

6. **High Throughput**: For high throughput scenarios, such as when using GPUs, you might want to process the entire batch at once to saturate the inference device. Some data loaders, like those provided by TensorFlow or PyTorch, can handle batching themselves, which can be beneficial when data doesn't fit in memory.

In summary, batch inferencing in parallel mode in Azure Machine Learning is about distributing the workload across multiple workers and nodes, with each worker handling a portion of the data to achieve faster processing times. The specific approach to parallel processing will depend on the deployment configuration and the characteristics of the data and compute resources.","[""Your deployment configuration controls the size of each mini-batch and the number of\nworkers on each node. T ake into account them when deciding if you want to read the\nentire mini-batch to perform inference, or if you want to run inference file by file, or row\nby row (for tabular). See Running inference at the mini-batch, file or the row level  to see\nthe different approaches.\nWhen running multiple workers on the same instance, take into account that memory is\nshared across all the workers. Usually, increasing the number of workers per node\nshould be accompanied by a decrease in the mini-batch size or by a change in the\nscoring strategy (if data size and compute SKU remains the same).\nBatch endpoints will call the run() function in your scoring script once per mini-batch.\nHowever, you will have the power to decide if you want to run the inference over the\nentire batch, over one file at a time, or over one row at a time (if your data happens to\nbe tabular).\nYou will typically want to run inference over the batch all at once when you want to\nachieve high throughput in your batch scoring process. This is the case for instance if\nyou run inference over a GPU where you want to achieve saturation of the inference\ndevice. Y ou may also be relying on a data loader that can handle the batching itself if\ndata doesn't fit on memory, like TensorFlow or PyTorch data loaders. On those cases,\nyou may want to consider running inference on the entire batch.\nFor an example about how to achieve it, see High throughput deployments . This\nexample processes an entire batch of files at a time.Running inference at the mini-batch, file or the row level\nMini-batch level\n２ Warning\nRunning inference at the batch level may require having high control over the input\ndata size to be able to correctly account for the memory requirements and avoid\nout of memory exceptions. Whether you are able or not of loading the entire mini-\nbatch in memory will depend on the size of the mini-batch, the size of the instances\nin the cluster, the number of workers on each node, and the size of the mini-batch.\nFile level"", 'One of the easiest ways to perform inference is by iterating over all the files in the mini-\nbatch and run your model over it. In some cases, like image processing, this may be a\ngood idea. If your data is tabular, you may need to make a good estimation about the\nnumber of rows on each file to estimate if your model is able to handle the memory\nrequirements to not just load the entire data into memory but also to perform inference\nover it. R emember that some models (specially those based on recurrent neural\nnetworks) will unfold and present a memory footprint that may not be linear with the\nnumber of rows. If your model is expensive in terms of memory, please consider running\ninference at the row level.\nFor an example about how to achieve it see Image processing with batch deployments .\nThis example processes a file at a time.\nFor models that present challenges in the size of their inputs, you may want to consider\nrunning inference at the row level. Y our batch deployment will still provide your scoring\nscript with a mini-batch of files, however, you will read one file, one row at a time. This\nmay look inefficient but for some deep learning models may be the only way to perform\ninference without scaling up your hardware requirements.\nFor an example about how to achieve it see Text processing with batch deployments .\nThis example processes a row at a time.\nThe environment variable AZUREML_MODEL_DIR contains the path to where the selected\nmodel is located and it is typically used in the init() function to load the model into\nmemory. However, some models may contain their files inside of a folder and you may\nneed to account for that when loading them. Y ou can identify the folder structure of\nyour model as follows:\n1. Go to Azure Machine Learning portal .\n2. Go to the section Models .\uea80 Tip\nIf file sizes are too big to be readed even at once, please consider breaking down\nfiles into multiple smaller files to account for better parallelization.\nRow level (tabular)\nUsing models that are folders', ""Batch scoring, or batch infer encing , involves invoking an endpoint with a reference to\ndata. The batch endpoint runs jobs asynchronously to process data in parallel on\ncompute clusters and store the data for further analysis.\nReal-time s coring, or online infer encing , involves invoking an endpoint with one or more\nmodel deployments and receiving a response in near real time via HT TPS. T raffic can be\nsplit across multiple deployments, allowing for testing new model versions by diverting\nsome amount of traffic initially and increasing after confidence in the new model is\nestablished.\nFor more information, see:\nDeploy a model with a real-time managed endpoint\nUse batch endpoints for scoring\nDevOps for ML models, often called ML Ops, is a process for developing models for\nproduction. A model's lifecycle from training to deployment must be auditable if not\nreproducible.\nLearn more about MLOps in Azure Machine Learning .\nMachine Learning is built with the model lifecycle in mind. Y ou can audit the model\nlifecycle down to a specific commit and environment.Real-time and batch scoring (inferencing)\nMLOps: DevOps for machine learning\nML model lifecycle\nIntegrations enabling MLOPs"", ""There are two types of deployments in batch endpoints:\nModel deployments\nPipeline component deployment\nModel deployment allows operationalizing model inference at scale, processing big\namounts of data in a low latency and asynchronous way. Scalability is automatically\ninstrumented by Azure Machine Learning by providing parallelization of the inferencing\nprocesses across multiple nodes in a compute cluster.\nUse Model deployments  when:\nThe main benefit of this kind of deployments is that you can use the very same assets\ndeployed in the online world (Online Endpoints) but now to run at scale in batch. If your\nmodel requires simple pre or pos processing, you can author an scoring script  that\nperforms the data transformations required.\nTo create a model deployment in a batch endpoint, you need to specify the following\nelements:Model deployments\nYou have expensive models that requires a longer time to run inference.＂\nYou need to perform inference over large amounts of data, distributed in multiple\nfiles.＂\nYou don't have low latency requirements.＂\nYou can take advantage of parallelization.＂"", 'You use the updated settings to save the Y AML file.\nPython\nThe batch inference has these steps:\n1. You use the same built-in feature retrieval component for feature retrieval that you\nused in the training pipeline (covered in the third tutorial). For pipeline training,\nyou provided a feature retrieval specification as a component input. For batch\ninference, you pass the registered model as the input. The component looks for\nthe feature retrieval specification in the model artifact.\nAdditionally, for training, the observation data had the target variable. However,\nthe batch inference observation data doesn\'t have the target variable. The feature\nretrieval step joins the observation data with the features and outputs the data for\nbatch inference.\n2. The pipeline uses the batch inference input data from previous step, runs inference\non the model, and appends the predicted value as output.fs_client.feature_sets.begin_create_or_update(transactions_fset_config)\nprint(fs_poller.result())\n(Optional) Save the YAML file for the feature\nset asset\nPython SDK\n## uncomment and run\n# transactions_fset_config.dump(root_dir +  \n""/featurestore/featuresets/transactions/featureset_asset_offline_enabled\n_with_schedule.yaml"")\nRun the batch inference pipeline\n７ Note\nYou use a job for batch inference in this example. Y ou can also use batch\nendpoints in Azure Machine Learning.']",2095
gpt-4-turbo,45,,5.0,,How to deploy custom vision model to real time endpoint?,v1,"To deploy a custom vision model to a real-time endpoint in Azure Machine Learning, you can follow these general steps:

1. **Register the Model**: First, you need to register your trained custom vision model in Azure Machine Learning. You can do this through the Azure Machine Learning Studio or by using the SDK. Here's an example of how to register a model using the SDK:

    ```python
    from azureml.core import Workspace, Model

    workspace = Workspace.from_config()
    model = Model.register(workspace=workspace,
                           model_name='your-model-name',                # Name of the registered model in your workspace.
                           model_path='path/to/your/model',             # Local path to the model
                           description='Your custom vision model')
    ```

2. **Create Scoring Script**: Create a scoring script (`score.py`) that will be used to load your model and make predictions. This script must have two required functions, `init()` and `run(raw_data)`. The `init()` function is called when the deployment starts and is typically used to load the model. The `run(raw_data)` function is called when new data is submitted to the endpoint.

    ```python
    def init():
        # Load the model from a file
        global model
        model = Model.get_model_path('your-model-name')

    def run(raw_data):
        # Make predictions using the model
        data = ... # Parse the raw_data
        result = model.predict(data)
        return result
    ```

3. **Create Inference Configuration**: Create an inference configuration that specifies the environment and the scoring script.

    ```python
    from azureml.core.model import InferenceConfig
    from azureml.core.environment import Environment

    environment = Environment(name='your-environment-name')
    environment.docker.enabled = True
    environment.python.conda_dependencies = ... # Define your conda dependencies

    inference_config = InferenceConfig(entry_script='score.py',
                                       environment=environment)
    ```

4. **Deploy the Model**: Deploy the model as a web service to an Azure Container Instance (ACI) or Azure Kubernetes Service (AKS) for real-time scoring.

    ```python
    from azureml.core.webservice import AciWebservice, Webservice

    deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)
    service = Model.deploy(workspace=workspace,
                           name='your-service-name',
                           models=[model],
                           inference_config=inference_config,
                           deployment_config=deployment_config)
    service.wait_for_deployment(show_output=True)
    ```

5. **Test the Endpoint**: Once the service is deployed, you can test the real-time endpoint by sending data to it and getting predictions back.

    ```python
    import requests

    url = service.scoring_uri
    data = {...}  # Your input data
    headers = {'Content-Type': 'application/json'}
    response = requests.post(url, json=data, headers=headers)
    print(response.json())
    ```

Please note that the actual code and parameters will depend on the specifics of your model and environment. Make sure to replace placeholders like `'your-model-name'`, `'path/to/your/model'`, and `'your-environment-name'` with your actual model and environment details.","['Deploy a trained model\nDeploy a pipeline with batch endpoints', 'Deploy a trained model', 'With the trained model registered in the Machine learning workspace, you are ready to\ndeploy the model for scoring.\nThis scenario includes prebuilt workflows for two approaches to deploying a trained\nmodel, batch scoring or a deploying a model to an endpoint for real-time scoring. Y ou\nmay run either or both of these workflows to test the performance of the model in your\nAzure Machine Learning workspace.\n1. In your GitHub project repository (ex: taxi-fare-regression), select Actions\n2. Select the deploy-online-endpoint -pipeline  from the workflows listed on the left\nand click Run w orkflow  to execute the online endpoint deployment pipeline\nworkflow. The steps in this pipeline will create an online endpoint in your Machine\nLearning workspace, create a deployment of your model to this endpoint, then\nallocate traffic to the endpoint.７ Note\nIf you want to check the output of each individual step, for example to view output\nof a failed run, click a job output, and then click each step in the job to view any\noutput of that step.\nDeploying the Trained Model\nOnline Endpoint', ""Alternatively Y ou can deploy the model from the Azure Machine Learning studio UI .\nNavigate to the model you wish to deploy in the Models  tab of the automated ML job\nand select on Deploy  and select Deploy t o real-time endpoint  .\n.\nthis is how your review page looks like. we can select instance type, instance count and\nset traffic percentage for the current deployment.\n.az ml online-endpoint update --name 'od-fridge-items-endpoint'  --traffic  \n'od-fridge-items-mlflow-deploy=100'  --workspace-name  \n[YOUR_AZURE_ WORKSPACE]  --resource-group  [YOUR_AZURE_RESOURCE_ GROUP] --\nsubscription  [YOUR_AZURE_ SUBSCRIPTION]"", 'If you directly deploy real-time endpoint from a previous completed real-time inference\npipeline, it may run into errors.\nRecommendation : clone the inference pipeline and submit it again, then deploy to real-\ntime endpoint.\nTutorial: Build a model in designer to predict auto pricesDeploy real-time endpoint from real-time inference\npipeline issue\nNext steps']",1148
