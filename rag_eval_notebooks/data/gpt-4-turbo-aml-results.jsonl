{"question": "How do you initiate a new Azure ML experiment using the Azure CLI?", "variant_id": "v1", "line_number": 1, "answer": "To initiate a new Azure ML experiment using the Azure CLI, you would typically follow these steps:\n\n1. Ensure you have the Azure CLI installed along with the ML extension. If you haven't installed the ML extension, you can add it using the following command:\n\n   ```\n   az extension add -n ml\n   ```\n\n2. Set your Azure subscription context and configure defaults for your workspace, resource group, and location:\n\n   ```\n   az account set --subscription <subscription-id>\n   az configure --defaults workspace=<workspace-name> group=<resource-group-name> location=<location>\n   ```\n\n3. Create an experiment by using the `az ml` command. However, as of my knowledge cutoff in early 2023, the Azure CLI does not have a direct command to create an experiment. Instead, you would typically run an experiment script or submit a job that implicitly creates an experiment if it does not already exist.\n\n   To submit a job that would create an experiment if it doesn't exist, you would use something like:\n\n   ```\n   az ml job create --file <job-definition-file.yaml> --experiment-name <experiment-name>\n   ```\n\n   The job definition file (`job-definition-file.yaml`) would contain the details of the job you want to run, including the compute target, inputs, outputs, and the script to be executed.\n\n   When you submit the job with an experiment name, Azure ML will create a new experiment with that name if it doesn't already exist, and then run the job under that experiment.\n\nPlease note that the exact commands and parameters might change with updates to the Azure CLI and Azure Machine Learning service. Always refer to the latest Azure ML documentation or use the `az ml -h` command to get help on the latest syntax and options.", "context": ["The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\nAzure CLI\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\nso one cluster can host one or many batch deployments (along with other workloads if\ndesired).\nThis article uses a compute created here named batch-cluster. Adjust as needed and\nreference your compute using azureml:<your-compute-name> or create one as shown.\nAzure CLIaz extension add -n ml \nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>  \naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>  \nCreate compute\nAzure CLI\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \n0 --max-instances  5 \n\uff17 Note", "Azure CLI\nIf you're not on the compute instance, omit --identity and follow the prompt to\nopen a browser window to authenticate.\n3. Make sure you have the most recent versions of the CLI and the ml extension:\nAzure CLI\n4. If you have multiple Azure subscriptions, set the active subscription to the one\nyou're using for your workspace. (Y ou can skip this step if you only have access to\na single subscription.) R eplace <SUBSCRIPTION-NAME> with your subscription name.\nAlso remove the brackets <>.\nAzure CLI\n5. Now use CLI to submit the job. If you're doing this on a compute instance in your\nworkspace, you can use environment variables for the workspace name and\nresource group as show in the following code. If you aren't on a compute instance,\nreplace these values with your workspace name and resource group.\nAzure CLI\nOnce you've submitted the job, you can check the status and results in studio:\n1. Sign in to Azure Machine Learning studio .\n2. Select your workspace if it isn't already loaded.\n3. On the left navigation, select Jobs.\n4. Select the Experiment name  that you used to train your model.\n5. Select the Display name  of the job to view details and artifacts of the job,\nincluding metrics, images, child jobs, outputs, logs, and code used in the job.az login --identity\naz upgrade\naz account set --subscription  \"<SUBSCRIPTION-NAME>\"\naz ml job create -f job.yml  --workspace-name  $CI_WORKSPACE  --resource-\ngroup $CI_RESOURCE_GROUP\nRegister model", "Azure CLI\nIf you don't have it, create a cluster called cpu-cluster by running:\nAzure CLI\nNow, create a pipeline job defined in the pipeline.yml file with the following command.\nThe compute target is referenced in the pipeline.yml file as azureml:cpu-cluster. If your\ncompute target uses a different name, remember to update it in the pipeline.yml file.\nAzure CLI\nYou should receive a JSON dictionary with information about the pipeline job including:\nKey Descr iption\nname The GUID-based name of the job.\nexperiment_name The name under which jobs will be organized in studio.\nservices.Studio.endpoint A URL for monitoring and reviewing the pipeline job.\nstatus The status of the job. This will likely be Preparing at this point.\nOpen the services.Studio.endpoint URL to see a graph visualization of the pipeline.az ml compute list\n\uff17 Note\nSkip this step to use serverless comput e.\naz ml compute create -n cpu-cluster  --type amlcompute --min-instances  0 --\nmax-instances  10\naz ml job create --file pipeline.yml\n\uff89Expand table", "The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nIn this example, we are going to create a deployment that can write directly to the\noutput folder of the batch deployment job. The deployment will use this feature to write\ncustom parquet files.Azure CLI\naz extension add -n ml\n\uff17 Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nCreating a batch deployment with a custom\noutput", "The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\ninference over new data:\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\n\uff17 Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nSteps"]}
{"question": "Describe the process of creating a new compute instance via the Azure ML CLI. What parameters are necessary?", "variant_id": "v1", "line_number": 2, "answer": "To create a new compute instance via the Azure Machine Learning CLI, you would typically follow these steps:\n\n1. **Prerequisites**: Ensure you have the Azure CLI installed and the ML extension (v2) added to it. You should also have an Azure Machine Learning workspace created, and the \"Allow storage account key access\" option enabled in the associated storage account.\n\n2. **Login**: Authenticate with Azure using the `az login` command if you haven't already done so.\n\n3. **Set the workspace**: Set the workspace context for the Azure ML CLI using the `az ml workspace set` command with the appropriate parameters for your workspace.\n\n4. **Create the compute instance**: Use the `az ml compute create` command to create a new compute instance. The necessary parameters for creating a compute instance typically include:\n\n   - `--name` or `-n`: The name of the compute instance.\n   - `--type`: The type of compute resource, which should be set to `computeinstance` for creating a compute instance.\n   - `--size`: The Virtual Machine size for the compute instance.\n   - `--workspace-name` or `-w`: The name of the Azure Machine Learning workspace.\n   - `--resource-group` or `-g`: The name of the resource group that contains the Azure Machine Learning workspace.\n\nOptional parameters might include:\n\n   - `--location` or `-l`: The Azure region for the compute instance.\n   - `--subnet`: The subnet ID if you want to create the compute instance in a virtual network.\n   - `--ssh-public-access`: Whether to enable or disable SSH public access.\n   - `--admin-user-ssh-key`: The SSH public key for the admin user if SSH access is enabled.\n\nHere's an example command to create a compute instance:\n\n```sh\naz ml compute create --name myComputeInstance --type computeinstance --size Standard_DS3_v2 --workspace-name myWorkspace --resource-group myResourceGroup\n```\n\nPlease replace `myComputeInstance`, `Standard_DS3_v2`, `myWorkspace`, and `myResourceGroup` with the actual names and sizes you intend to use for your compute instance.\n\nRemember to check the Azure ML CLI documentation for the most up-to-date and detailed information on the parameters and their usage.", "context": ["Create an Azure Machine Learning\ncompute instance\nArticle \u202212/08/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nLearn how to create a compute instance  in your Azure Machine Learning workspace.\nUse a compute instance as your fully configured and managed development\nenvironment in the cloud. For development and testing, you can also use the instance as\na training compute target . A compute instance can run multiple jobs in parallel and has\na job queue. As a development environment, a compute instance can't be shared with\nother users in your workspace.\nIn this article, you learn how to create a compute instance. See Manage an Azure\nMachine Learning compute instance  for steps to manage start, stop, restart, delete a\ncompute instance.\nYou can also use a setup script  to create the compute instance with your own custom\nenvironment.\nCompute instances can run jobs securely in a virtual network environment , without\nrequiring enterprises to open up SSH ports. The job executes in a containerized\nenvironment and packages your model dependencies in a Docker container.\nAn Azure Machine Learning workspace. For more information, see Create an Azure\nMachine Learning workspace . In the storage account, the \"Allow storage account\nkey access\" option must be enabled for compute instance creation to be\nsuccessful.\nChoose the tab for the environment you're using for other prerequisites.\n\uff17 Note\nThis article uses CLI v2 in some examples. If you are still using CLI v1, see Create an\nAzur e Machine Learning comput e clust er CLI v1) .\nPrerequisites", "For more information on creating a compute cluster and compute cluster, including how\nto do so with Python and the CLI, see the following articles:\nCreate a compute cluster\nCreate a compute instance\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nWhen Azure Container R egistry is behind the virtual network, Azure Machine Learning\ncan't use it to directly build Docker images (used for training and deployment). Instead,\nconfigure the workspace to use the compute cluster you created earlier. Use the\nfollowing steps to create a compute cluster and configure the workspace to use it to\nbuild images:\n1. Navigate to https://shell.azure.com/  to open the Azure Cloud Shell.\n2. From the Cloud Shell, use the following command to install the 2.0 CLI for Azure\nMachine Learning:\nAzure CLIAllow inbound T CP traffic on ports 29876-29877 from the\nBatchNodeManagement service tag.\nAllow inbound T CP traffic on port 44224 from the AzureMachineLearning\nservice tag.\nThe following screenshot shows an example of these rules:\nConfigure image builds\naz extension add -n ml", "The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nFirst, let's connect to Azure Machine Learning workspace where we're going to work on.\nAzure CLI\nBatch endpoints run on compute clusters. They support both Azure Machine Learning\nCompute clusters (AmlCompute)  or Kubernetes clusters . Clusters are a shared resource\nso one cluster can host one or many batch deployments (along with other workloads if\ndesired).\nThis article uses a compute created here named batch-cluster. Adjust as needed and\nreference your compute using azureml:<your-compute-name> or create one as shown.\nAzure CLIaz extension add -n ml \nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>  \naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>  \nCreate compute\nAzure CLI\naz ml compute create -n batch-cluster  --type amlcompute --min-instances  \n0 --max-instances  5 \n\uff17 Note", "To create a workspace using CLI v2, use the following command:\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nBash\nFor more information, see workspace Y AML schema .\nA compute is a designated compute resource where you run your job or host your\nendpoint. Azure Machine Learning supports the following types of compute:\nComput e clust er - a managed-compute infrastructure that allows you to easily\ncreate a cluster of CPU or GPU compute nodes in the cloud.\nComput e instance  - a fully configured and managed development environment in\nthe cloud. Y ou can use the instance as a training or inference compute for\ndevelopment and testing. It's similar to a virtual machine on the cloud.\nInfer ence clust er - used to deploy trained machine learning models to Azure\nKubernetes Service. Y ou can create an Azure Kubernetes Service (AKS) cluster from\nyour Azure Machine Learning workspace, or attach an existing AKS cluster.\nAttached comput e - You can attach your own compute resources to your\nworkspace and use them for training and inference.\nTo create a compute using CLI v2, use the following command:\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)Azure CLI\naz ml workspace create --file my_workspace.yml  \nCompute\n\uff17 Note\nInstead of creating a compute cluster, use serverless comput e (preview) to\noffload compute lifecycle management to Azure Machine Learning.\nAzure CLI", "Manage an Azure Machine Learning\ncompute instance\nArticle \u202207/06/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nLearn how to manage a compute instance  in your Azure Machine Learning workspace.\nUse a compute instance as your fully configured and managed development\nenvironment in the cloud. For development and testing, you can also use the instance as\na training compute target . A compute instance can run multiple jobs in parallel and has\na job queue. As a development environment, a compute instance can't be shared with\nother users in your workspace.\nIn this article, you learn how to start, stop, restart, delete) a compute instance. See\nCreate an Azure Machine Learning compute instance  to learn how to create a compute\ninstance.\nAn Azure Machine Learning workspace. For more information, see Create an Azure\nMachine Learning workspace . In the storage account, the \"Allow storage account\nkey access\" option must be enabled for compute instance creation to be\nsuccessful.\nThe Azure CLI extension for Machine Learning service (v2) , Azure Machine\nLearning Python SDK (v2) , or the Azure Machine Learning Visual S tudio Code\nextension .\nIf using the Python SDK, set up your development environment with a workspace .\nOnce your environment is set up, attach to the workspace in your Python script:\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\n\uff17 Note\nThis article shows CLI v2 in the sections below. If you are still using CLI v1, see\nCreate an Azur e Machine Learning comput e clust er CLI v1) .\nPrerequisites"]}
{"question": "Explain how to submit a training job using Python and the AML SDK v2. Include an example of specifying a compute target.", "variant_id": "v1", "line_number": 3, "answer": "To submit a training job using Python and the Azure Machine Learning (AML) SDK v2, you need to follow these steps:\n\n1. **Set up your environment**:\n   - Install the Azure Machine Learning SDK v2 if you haven't already.\n   - Import the necessary modules and classes from the SDK.\n   - Create or load an existing Azure Machine Learning workspace.\n\n2. **Create or attach a compute target**:\n   - Define the compute target where the training job will run. This could be an Azure Machine Learning Compute Instance, Compute Cluster, or other supported compute targets.\n\n3. **Define the training script**:\n   - Write a Python script that contains the code for training your model. Save this script in a directory that will be uploaded with your job.\n\n4. **Create a command job**:\n   - Use the `Command` class to define the command job, specifying the script to run, the compute target, and any other necessary parameters such as environment and inputs/outputs.\n\n5. **Submit the job**:\n   - Use the `Job` class to submit the command job to the workspace.\n\nHere's an example of how you might specify a compute target and submit a training job using the AML SDK v2:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import Workspace, Compute, Command, Environment\nfrom azure.identity import DefaultAzureCredential\n\n# Load the workspace\nws = Workspace.from_config()\n\n# Create MLClient using the workspace and DefaultAzureCredential\nml_client = MLClient.from_config(credential=DefaultAzureCredential(), workspace=ws)\n\n# Define the compute target\ncompute_name = \"your-compute-cluster\"\ncompute = Compute(name=compute_name, type=\"amlcompute\")\ncompute.size = \"Standard_DS3_v2\"\ncompute.min_instances = 0\ncompute.max_instances = 4\n\n# Register or get the existing compute target\ntry:\n    # Try to get the existing compute target by name\n    compute_target = ml_client.compute.get(compute_name)\nexcept Exception:\n    # If it doesn't exist, create it\n    compute_target = ml_client.compute.create(compute)\n\n# Define the environment for the job\nenvironment = Environment(\n    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n    conda_file=\"path_to_your_conda_file.yml\",\n    name=\"my-environment\",\n    version=\"1\"\n)\n\n# Create the command job\ncommand_job = Command(\n    code=\"path_to_your_training_script_folder\",\n    command=\"python train.py\",\n    environment=environment.name,\n    compute=compute_name,\n    distribution={\"type\": \"mpi\", \"process_count_per_instance\": 2},\n    resources={'instance_count': 2},\n    environment_variables={\"EXAMPLE_ENV_VAR\": \"example_value\"}\n)\n\n# Submit the job\nreturned_job = ml_client.jobs.create_or_update(command_job)\n\n# Get the status of the job\njob_status = ml_client.jobs.get(returned_job.name)\nprint(f\"Job status: {job_status.status}\")\n```\n\nIn this example, replace `\"your-compute-cluster\"`, `\"path_to_your_conda_file.yml\"`, `\"path_to_your_training_script_folder\"`, and `\"train.py\"` with your actual compute cluster name, conda environment file path, training script directory, and training script filename, respectively.\n\nPlease note that the code above is a simplified example and assumes that you have already set up your Azure Machine Learning workspace and have the necessary configurations in place. The actual implementation may vary based on your specific requirements and Azure setup.", "context": ["1. On the top bar above your opened notebook, create a compute instance if you\ndon't already have one.\n2. If the compute instance is stopped, select Start comput e and wait until it is\nrunning.\n3. Make sure that the kernel, found on the top right, is Python 3.10 - SDK v2. If not,\nuse the dropdown to select this kernel.\n4. If you see a banner that says you need to be authenticated, select Authenticat e.\nTo train a model, you need to submit a job. The type of job you'll submit in this tutorial\nis a command job . Azure Machine Learning offers several different types of jobs to train\nmodels. Users can select their method of training based on complexity of the model,\ndata size, and training speed requirements. In this tutorial, you'll learn how to submit a\ncommand job  to run a training s cript.\nA command job is a function that allows you to submit a custom training script to train\nyour model. This can also be defined as a custom training job. A command job in Azure\nMachine Learning is a type of job that runs a script or command in a specified\nenvironment. Y ou can use command jobs to train models, process data, or any other\ncustom code you want to execute in the cloud.Set your kernel\n\uf80a\n\uf80a\n\uf80a\n\uff09 Impor tant\nThe rest of this tutorial contains cells of the tutorial notebook. Copy/paste them\ninto your new notebook, or switch to the notebook now if you cloned it.\nUse a command job to train a model in Azure\nMachine Learning", "For detailed instructions on how to complete these steps, see Secure a training\nenvironment .\nIn this section, you learn how Azure Machine Learning securely communicates between\nservices to submit a training job. This example shows you how all your configurations\nwork together to secure communication.\n1. The client uploads training scripts and training data to storage accounts that are\nsecured with a service or private endpoint.\n2. The client submits a training job to the Azure Machine Learning workspace\nthrough the private endpoint.\n3. Azure Batch service receives the job from the workspace. It then submits the\ntraining job to the compute environment through the public load balancer for the\ncompute resource.\n4. The compute resource receives the job and begins training. The compute resource\nuses information stored in key vault to access storage accounts to download\ntraining files and upload output.Example training job submission", "Python\nView more examples of training with serverless compute at:-\nQuick S tart\nTrain Model\nThere's no need to specify compute for AutoML jobs. R esources can be optionally\nspecified. If instance count isn't specified, then it's defaulted based on\nmax_concurrent_trials and max_nodes parameters. If you submit an AutoML image\nclassification or NLP task with no instance type, the GPU VM size is automatically\nselected. It's possible to submit AutoML job through CLIs, SDK, or S tudio. T o submit\nAutoML jobs with serverless compute in studio first enable the submit a training job in\nstudio (preview)  feature in the preview panel.Python SDK\nfrom azure.ai.ml import command\nfrom azure.ai.ml import MLClient      # Handle to the workspace\nfrom azure.identity import DefaultAzureCredential     # Authentication  \npackage\nfrom azure.ai.ml.entities import ResourceConfiguration\nfrom azure.ai.ml.entities import UserIdentityConfiguration \ncredential = DefaultAzureCredential()\n# Get a handle to the workspace. You can find the info on the workspace  \ntab on ml.azure.com\nml_client = MLClient(\n    credential=credential,\n    subscription_id= \"<Azure subscription id>\" , \n    resource_group_name= \"<Azure resource group>\" ,\n    workspace_name= \"<Azure Machine Learning Workspace>\" ,\n)\njob = command(\n    command= \"echo 'hello world'\" ,\n    environment= \"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\" ,\n         identity=UserIdentityConfiguration(),\n    queue_settings={\n      \"job_tier\" : \"Standard\"   \n    }\n)\njob.resources = ResourceConfiguration(instance_type= \"Standard_E4s_v3\" , \ninstance_count= 1)\n# submit the command job\nml_client.create_or_update(job)\nAutoML job", "Designer : Azure Machine Learning designer provides an easy entry-point into\nmachine learning for building proof of concepts, or for users with little coding\nexperience. It allows you to train models using a drag and drop web-based UI. Y ou\ncan use Python code as part of the design, or train models without writing any\ncode.\nAzur e CLI : The machine learning CLI provides commands for common tasks with\nAzure Machine Learning, and is often used for scripting and aut omating tasks . For\nexample, once you've created a training script or pipeline, you might use the Azure\nCLI to start a training job on a schedule or when the data files used for training are\nupdated. For training models, it provides commands that submit training jobs. It\ncan submit jobs using run configurations or pipelines.\nEach of these training methods can use different types of compute resources for\ntraining. Collectively, these resources are referred to as comput e targets. A compute\ntarget can be a local machine or a cloud resource, such as an Azure Machine Learning\nCompute, Azure HDInsight, or a remote virtual machine.\nThe Azure Machine Learning SDK for Python allows you to build and run machine\nlearning workflows with Azure Machine Learning. Y ou can interact with the service from\nan interactive Python session, Jupyter Notebooks, Visual S tudio Code, or other IDE.\nInstall/update the SDK\nConfigure a development environment for Azure Machine Learning\nA generic training job with Azure Machine Learning can be defined using the\ncommand() . The command is then used, along with your training script(s) to train a\nmodel on the specified compute target.\nYou may start with a command for your local computer, and then switch to one for a\ncloud-based compute target as needed. When changing the compute target, you only\nchange the compute parameter in the command that you use. A run also logs\ninformation about the training job, such as the inputs, outputs, and logs.\nTutorial: T rain your first ML model\nExamples: Jupyter Notebook and Python examples of training modelsPython SDK\nSubmit a command\nAutomated Machine Learning", "An Azure Machine Learning compute cluster is a fully managed compute resource that\ncan be used to run the training job. In the following examples, a compute cluster named\ncpu-compute is created.\nPython\nTo run this script, you'll use a command that executes main.py Python script located\nunder ./sdk/python/jobs/single-step/lightgbm/iris/src/. The command will be runworkspace = '<AZUREML_WORKSPACE_NAME>'\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \nresource_group, workspace)\n2. Create a compute resource for training\n\uff17 Note\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\ntraining job .\nPython SDK\nfrom azure.ai.ml.entities import AmlCompute\n# specify aml compute name.\ncpu_compute_target = \"cpu-cluster\"\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(\"Creating a new cpu compute target...\" )\n    compute = AmlCompute(\n        name=cpu_compute_target, size= \"STANDARD_D2_V2\" , min_instances= 0, \nmax_instances= 4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n4. Submit the training job\nPython SDK"]}
{"question": "What is the command to list all the models registered in an Azure ML Workspace using the Azure CLI?", "variant_id": "v1", "line_number": 4, "answer": "The command to list all the models registered in an Azure ML Workspace using the Azure CLI is:\n\n```\naz ml model list\n```", "context": ["then select Regist er.\nThe SDK and CLI (v2) also allow you to manage the lifecycle of your Azure Machine\nLearning model assets.\nList all the models in your workspace:\ncli\nList all the model versions under a given name:\ncli\n\uf80a\nManage models\nList\nAzure CLI\naz ml model list  \nAzure CLI\naz ml model list --name run-model-example", "Install the Azure CLI and the ml extension to the Azure CLI. For more\ninformation on installing the CLI, see Install and set up the CLI (v2) .\nFirst, connect to the Azure Machine Learning workspace where you'll work.\nAzure CLI\nYou can deploy only registered models to online endpoints. In this case, you already\nhave a local copy of the model in the repository, so you only need to publish the model\nto the registry in the workspace. Y ou can skip this step if the model you're trying to\ndeploy is already registered.\nAzure CLI\nIf your model was logged inside of a run, you can register it directly.\nTo register the model, you need to know the location where it is stored. If you're using\nMLflow's autolog feature, the path to the model depends on the model type and\nframework. Y ou should check the jobs output to identify the name of the model's folder.\nThis folder contains a file named MLModel.Connect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nRegister the model\nAzure CLI\nMODEL_NAME= 'sklearn-diabetes'\naz ml model create --name $MODEL_NAME  --type \"mlflow_model\"  --path \n\"endpoints/online/ncd/sklearn-diabetes/model\"\nWhat if your model was logged inside of a run?", "Install the Azure CLI and the ml extension to the Azure CLI. For more\ninformation on installing the CLI, see Install and set up the CLI (v2) .\nFirst, connect to the Azure Machine Learning workspace where you'll work.\nAzure CLI\nYou can deploy only registered models to online endpoints. In this case, you already\nhave a local copy of the model in the repository, so you only need to publish the model\nto the registry in the workspace. Y ou can skip this step if the model you're trying to\ndeploy is already registered.\nAzure CLI\nIf your model was logged inside of a run, you can register it directly.\nTo register the model, you need to know the location where it is stored. If you're using\nMLflow's autolog feature, the path to the model depends on the model type and\nframework. Y ou should check the jobs output to identify the name of the model's folder.\nThis folder contains a file named MLModel.Connect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nRegister the model\nAzure CLI\nMODEL_NAME= 'sklearn-diabetes'\naz ml model create --name $MODEL_NAME  --type \"mlflow_model\"  --path \n\"endpoints/online/ncd/sklearn-diabetes/model\"\nWhat if your model was logged inside of a run?", "The az ml model command can be used for managing Azure Machine Learning models.\nExamples are available in the examples GitHub repository . Several are shown below.\nYAML\nYAML\nInstall and use the CLI (v2)Remarks\nExamples\nYAML: local file\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-file-example  \npath: mlflow-model/model.pkl  \ndescription:  Model created from local file. \nYAML: local folder in MLflow format\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-mlflow-example  \npath: mlflow-model  \ntype: mlflow_model  \ndescription:  Model created from local MLflow model directory.", "The Azure CLI  and the ml extension for Azure Machine Learning .\nAzure CLI\nThe workspace is the top-level resource for Azure Machine Learning, providing a\ncentralized place to work with all the artifacts you create when you use Azure Machine\nLearning. In this section, we'll connect to the workspace in which you'll perform\ndeployment tasks.\nPass in the values for your subscription ID, workspace, location, and resource group\nin the following code:\nAzure CLI\nFollow these steps to deploy an MLflow model to a batch endpoint for running batch\ninference over new data:\n1. Batch Endpoint can only deploy registered models. In this case, we already have a\nlocal copy of the model in the repository, so we only need to publish the model toaz extension add -n ml\n\uff17 Note\nPipeline component deployments for Batch Endpoints were introduced in\nversion 2.7 of the ml extension for Azure CLI. Use az extension update --\nname ml to get the last version of it.\nConnect to your workspace\nAzure CLI\naz account set --subscription  <subscription>\naz configure --defaults  workspace= <workspace>  group=<resource-group>  \nlocation= <location>\nSteps"]}
{"question": "Provide an example of how to use the AML SDK v2 to stream logs from a running experiment in Python.", "variant_id": "v1", "line_number": 5, "answer": "To stream logs from a running experiment using Azure Machine Learning SDK v2 in Python, you can use the `stream` method of the `Job` object. Here's an example of how you might do this:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\n\n# Connect to the workspace\nml_client = MLClient.from_config(DefaultAzureCredential())\n\n# Assuming you have the job name or ID of the running experiment\njob_name = \"your_job_name_here\"\n\n# Get the job object\njob = ml_client.jobs.get(name=job_name)\n\n# Stream the logs\nml_client.jobs.stream(name=job_name)\n```\n\nIn this example, replace `\"your_job_name_here\"` with the actual name or ID of the job you want to stream logs from. The `stream` method will continuously output the logs to your console until the job completes or fails. If you want to stop streaming the logs, you can simply interrupt the process (e.g., by pressing Ctrl+C in the console).", "context": ["SDK v2\nPython\nFunctionality in SDK v1 Rough mapping in SDK v2\nexperiment.submit MLCLient.jobs.create_or_update\nScriptRunConfig() command()\nFor more information, see:\nV1 - Experiment\nV2 - Command Jobaml_url = run.get_portal_url()  \nprint(aml_url)  \n#import required libraries  \nfrom azure.ai.ml import MLClient, command  \nfrom azure.ai.ml.entities import Environment  \nfrom azure.identity import DefaultAzureCredential  \n#connect to the workspace  \nml_client = MLClient.from_config(DefaultAzureCredential())  \n# set up pytorch environment  \nenv = Environment(  \n    image= \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\" , \n    conda_file= \"pytorch-env.yml\" , \n    name= \"pytorch-env\"  \n) \n# define the command  \ncommand_job = command(  \n    code= \"./src\", \n    command= \"train.py\" , \n    environment=env,  \n    compute= \"cpu-cluster\" , \n) \nreturned_job = ml_client.jobs.create_or_update(command_job)  \nreturned_job  \nMapping of key functionality in v1 and v2\nNext steps", "Explore Azu re Machine Learning with\nJupyter Notebooks\nArticle \u202202/06/2024\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nThe AzureML-Examples  repository includes the latest (v2) Azure Machine Learning\nPython CLI and SDK samples. For information on the various example types, see the\nreadme .\nThis article shows you how to access the repository from the following environments:\nAzure Machine Learning compute instance\nYour own compute resource\nData Science Virtual Machine\nThe easiest way to get started with the samples is to complete the Create resources to\nget started . Once completed, you'll have a dedicated notebook server preloaded with\nthe SDK and the Azure Machine Learning Notebooks repository. No downloads or\ninstallation necessary.\nTo view example notebooks:\n1. Sign in to studio  and select your workspace if necessary.\n2. Select Notebooks .\n3. Select the Samples  tab. Use the SDK v2  folder for examples using Python SDK v2.\n4. Open the notebook you want to run. Select Clone this not ebook  to create a copy\nin your workspace file share. This action will copy the notebook along with any\ndependent resources.\nIf you'd like to bring your own notebook server for local development, follow these\nsteps on your computer.\n1. Use the instructions at Azure Machine Learning SDK  to install the Azure Machine\nLearning SDK (v2) for Python\nOption 1: Access on Azure Machine Learning\ncompute instance (recommended)\nOption 2: Access on your own notebook server", "Query & co mpare experiments and runs\nwith MLflow\nArticle \u202206/26/2023\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\nYou don't need to install any specific SDK to manage what happens inside of a training\njob, creating a more seamless transition between local runs and the cloud by removing\ncloud-specific dependencies. In this article, you'll learn how to query and compare\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\nin Python.\nMLflow allows you to:\nCreate, query, delete and search for experiments in a workspace.\nQuery, delete, and search for runs in a workspace.\nTrack and retrieve metrics, parameters, artifacts and models from runs.\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\ndetailed comparison between MLflow Open-Source and MLflow when connected to\nAzure Machine Learning.\nQuery and searching experiments and runs is also available using the MLflow REST API.\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\nconsume it.\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\nMLflow azureml-mlflow.\uff17 Note\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\ntracking capabilities. This applies not just for logging but also for querying the\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\nexplains how to use MLflow to manage experiments and runs in Azure Machine\nLearning.\nREST API\nPrerequisites", "Query & co mpare experiments and runs\nwith MLflow\nArticle \u202206/26/2023\nExperiments and jobs (or runs) in Azure Machine Learning can be queried using MLflow.\nYou don't need to install any specific SDK to manage what happens inside of a training\njob, creating a more seamless transition between local runs and the cloud by removing\ncloud-specific dependencies. In this article, you'll learn how to query and compare\nexperiments and runs in your workspace using Azure Machine Learning and MLflow SDK\nin Python.\nMLflow allows you to:\nCreate, query, delete and search for experiments in a workspace.\nQuery, delete, and search for runs in a workspace.\nTrack and retrieve metrics, parameters, artifacts and models from runs.\nSee Support matrix for querying runs and experiments in Azure Machine Learning  for a\ndetailed comparison between MLflow Open-Source and MLflow when connected to\nAzure Machine Learning.\nQuery and searching experiments and runs is also available using the MLflow REST API.\nSee Using MLflow REST with Azure Machine Learning  for an example about how to\nconsume it.\nInstall Mlflow SDK package mlflow and Azure Machine Learning plug-in for\nMLflow azureml-mlflow.\uff17 Note\nThe Azure Machine Learning Python SDK v2 does not provide native logging or\ntracking capabilities. This applies not just for logging but also for querying the\nmetrics logged. Instead, use MLflow to manage experiments and runs. This article\nexplains how to use MLflow to manage experiments and runs in Azure Machine\nLearning.\nREST API\nPrerequisites", "SDK v2. Full sample link\nPython'--learning_rate' , 0.1,'--model_output' , model_output],\n    source_directory=script_folder,\n    compute_target=aml_compute,\n    allow_reuse= True,\n)\nscore_output = OutputFileDatasetConfig( 'score_output' )\nscore_step = PythonScriptStep(\n    name=\"score step\" ,\n    script_name= \"score.py\" ,\n    arguments=[ '--model_input' ,model_output.as_input( 'model_input' ), '-\n-test_data' , input_ds.as_named_input( 'test_data' ).as_mount(), '--\nscore_output' , score_output],\n    source_directory=script_folder,\n    compute_target=aml_compute,\n    allow_reuse= True,\n)\neval_output = OutputFileDatasetConfig( 'eval_output' )\neval_step = PythonScriptStep(\n    name=\"eval step\" ,\n    script_name= \"eval.py\" ,\n    arguments=[ '--\nscoring_result' ,score_output.as_input( 'scoring_result' ), '--\neval_output' , eval_output],\n    source_directory=script_folder,\n    compute_target=aml_compute,\n    allow_reuse= True,\n)\n# built pipeline\nfrom azureml.pipeline.core import Pipeline\npipeline_steps = [train_step, score_step, eval_step]\npipeline = Pipeline(workspace = workspace, steps=pipeline_steps)\nprint(\"Pipeline is built.\" )\npipeline_run = experiment.submit(pipeline, regenerate_outputs= False)\nprint(\"Pipeline submitted for execution.\" )\n# import required libraries\nfrom azure.identity import DefaultAzureCredential,  \nInteractiveBrowserCredential\nfrom azure.ai.ml import MLClient, Input\nfrom azure.ai.ml.dsl import pipeline"]}
{"question": "How can you configure an AutoML run for a classification task using the AML SDK v2, including specifying primary metric and training data?", "variant_id": "v1", "line_number": 6, "answer": "To configure an AutoML run for a classification task using the Azure Machine Learning (AML) SDK v2, you can follow these steps:\n\n1. **Import necessary modules** from the AML SDK v2:\n   ```python\n   from azure.ai.ml import automl, Input\n   from azure.ai.ml.constants import AssetTypes\n   from azure.ai.ml.automl import classification\n   ```\n\n2. **Create an MLTable** for your training dataset:\n   ```python\n   my_training_data_input = Input(\n       type=AssetTypes.MLTABLE, \n       path=\"./data/training-mltable-folder\"\n   )\n   ```\n\n3. **Create the AutoML classification job** using the `automl.classification` factory function:\n   ```python\n   classification_job = automl.classification(\n       compute=\"<compute_name>\", \n       experiment_name=\"<exp_name>\", \n       training_data=my_training_data_input, \n       target_column_name=\"<name_of_target_column>\", \n       primary_metric=\"accuracy\", \n       n_cross_validations=5, \n       enable_model_explainability=True, \n       tags={\"my_custom_tag\": \"My custom value\"}\n   )\n   ```\n\n   - `compute` specifies the compute target to run the AutoML job.\n   - `experiment_name` is the name of the experiment under which the run will be logged.\n   - `training_data` is the input MLTable containing the training data.\n   - `target_column_name` is the name of the column in your training data that contains the labels for the classification task.\n   - `primary_metric` is the metric that AutoML will optimize for during the model selection process. In this case, it's set to \"accuracy\".\n   - `n_cross_validations` specifies the number of cross-validation splits to be used for model validation.\n   - `enable_model_explainability` indicates whether to enable explainability features for the models.\n   - `tags` allows you to add custom tags to the AutoML run for tracking purposes.\n\n4. **Optionally set limits** for the AutoML job:\n   ```python\n   classification_job.set_limits(\n       timeout_minutes=600,\n       trial_timeout_minutes=20,\n       max_trials=5,\n       max_concurrent_trials=4\n   )\n   ```\n\n   - `timeout_minutes` is the maximum amount of time in minutes that all trials can take before the job is terminated.\n   - `trial_timeout_minutes` is the maximum time in minutes that each trial can run.\n   - `max_trials` is the maximum number of trials that AutoML will test in the experiment.\n   - `max_concurrent_trials` is the maximum number of trials that will be executed in parallel.\n\n5. **Submit the AutoML run** to the experiment:\n   ```python\n   from azure.ai.ml import MLClient\n   # Assuming you have already created an MLClient instance\n   remote_run = ml_client.create_or_update(classification_job)\n   ```\n\n6. **Retrieve the URL** for monitoring the AutoML run in Azure Machine Learning Studio:\n   ```python\n   azureml_url = remote_run.get_portal_url()\n   print(azureml_url)\n   ```\n\nMake sure to replace placeholder values like `<compute_name>`, `<exp_name>`, and `<name_of_target_column>` with your actual compute target name, experiment name, and target column name, respectively. Also, ensure that the path to the training data is correct.", "context": ["SDK v2: Below is a sample AutoML classification task. For the entire code, check\nout our examples repo .\nPython    debug_log= \"automl_errors.log\" , \n    compute_target=compute_target,  \n    training_data=training_data,  \n    label_column_name=label_column_name,  \n    **automl_settings,  \n) \n# Submit run  \nremote_run = experiment.submit(automl_config, show_output= False) \nazureml_url = remote_run.get_portal_url()  \nprint(azureml_url)  \n# Imports  \nfrom azure.ai.ml import automl, Input, MLClient  \nfrom azure.ai.ml.constants import AssetTypes  \nfrom azure.ai.ml.automl import ( \n    classification,  \n    ClassificationPrimaryMetrics,\n    ClassificationModels,  \n) \n# Create MLTables for training dataset  \n# Note that AutoML Job can also take in tabular data  \nmy_training_data_input = Input(  \n    type=AssetTypes.MLTABLE, path= \"./data/training-mltable-folder\"  \n) \n# Create the AutoML classification job with the related factory-\nfunction.  \nclassification_job = automl.classification(  \n    compute= \"<compute_name>\" , \n    experiment_name= \"<exp_name?\" , \n    training_data=my_training_data_input,  \n    target_column_name= \"<name_of_target_column>\" , \n    primary_metric= \"accuracy\" , \n    n_cross_validations= 5, \n    enable_model_explainability= True, \n    tags={ \"my_custom_tag\" : \"My custom value\" }, \n) \n# Limits are all optional  \nclassification_job.set_limits(  \n    timeout_minutes= 600, \n    trial_timeout_minutes= 20, \n    max_trials= 5, \n    max_concurrent_trials = 4,", "Upgrade AutoML to SDK v2\nArticle \u202204/04/2023\nIn SDK v2, \"experiments\" and \"runs\" are consolidated into jobs.\nIn SDK v1, AutoML was primarily configured and run using the AutoMLConfig class. In\nSDK v2, this class has been converted to an AutoML job. Although there are some\ndifferences in the configuration options, by and large, naming & functionality has been\npreserved in V2.\nThis article gives a comparison of scenario(s) in SDK v1 and SDK v2.\nSDK v1: Below is a sample AutoML classification task. For the entire code, check\nout our examples repo .\nPythonSubmit AutoML run\n# Imports  \nimport azureml.core  \nfrom azureml.core.experiment import Experiment  \nfrom azureml.core.workspace import Workspace  \nfrom azureml.core.dataset import Dataset  \nfrom azureml.train.automl import AutoMLConfig  \nfrom azureml.train.automl.run import AutoMLRun    \n# Load tabular dataset  \ndata = \"<url_to_data>\"  \ndataset = Dataset.Tabular.from_delimited_files(data)  \ntraining_data, validation_data = dataset.random_split(percentage= 0.8, \nseed=223) \nlabel_column_name = \"Class\" \n# Configure Auto ML settings  \nautoml_settings = {  \n    \"n_cross_validations\" : 3, \n    \"primary_metric\" : \"average_precision_score_weighted\" , \n    \"enable_early_stopping\" : True, \n    \"max_concurrent_iterations\" : 2,   \n    \"experiment_timeout_hours\" : 0.25,   \n    \"verbosity\" : logging.INFO,  \n} \n# Put together an AutoML job constructor  \nautoml_config = AutoMLConfig(  \n    task= \"classification\" ,", "3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\nMachine Learning in many different ways .\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\niterations over different models, hyperparameter settings, advanced\npreprocessing/featurization, and what metrics to look at when determining the\nbest model.\n5. Submit the training job.\n6. Review the r esults\nThe following diagram illustrates this process. \nYou can also inspect the logged job information, which contains metrics  gathered\nduring the job. The training job produces a Python serialized object ( .pkl file) that\ncontains the model and data preprocessing.\nWhile model building is automated, you can also learn how important or relevant\nfeatures are  to the generated models.\nApply automated ML when you want Azure Machine Learning to train and tune a model\nfor you using the target metric you specify. Automated ML democratizes the machine\nlearning model development process, and empowers its users, no matter their data\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\nML professionals and developers across industries can use automated ML to:\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\nforecasting, computer vision & NLP", "iii. Select Create to create your compute target.\nThis tak es a couple minut es to complet e.\niv. After creation, select your new compute target from the drop-down list.\ne. Select Next .\n3. On the Select task and settings  form, complete the setup for your automated ML\nexperiment by specifying the machine learning task type and configuration\nsettings.\na. Select Classification  as the machine learning task type.\nb. Select View additional configuration settings  and populate the fields as\nfollows. These settings are to better control the training job. Otherwise, defaults\nare applied based on experiment selection and data.\nAdditional\u00a0configurations Descr iption Value\u00a0for\u00a0tut orial\nPrimary metric Evaluation metric that the\nmachine learning algorithm\nwill be measured by.AUC_weighted\nExplain best model Automatically shows\nexplainability on the bestEnable", "An Azure subscription. (If you don't have an Azure subscription, create a free\naccount  before you begin)\nAn Azure Machine Learning experiment created with either:\nThe Azure Machine Learning studio  (no code required)\nThe Azure Machine Learning Python SDK\nAfter your automated ML experiment completes, a history of the jobs can be found via:\nA browser with Azure Machine Learning studio\nA Jupyter notebook using the JobDetails Jupyter widget\nThe following steps and video, show you how to view the run history and model\nevaluation metrics and charts in the studio:\n1. Sign into the studio  and navigate to your workspace.\n2. In the left menu, select Jobs.\n3. Select your experiment from the list of experiments.\n4. In the table at the bottom of the page, select an automated ML job.\n5. In the Models  tab, select the Algorithm name  for the model you want to evaluate.\n6. In the Metrics  tab, use the checkboxes on the left to view metrics and charts.\nAutomated ML calculates performance metrics for each classification model generated\nfor your experiment. These metrics are based on the scikit learn implementation.\nMany classification metrics are defined for binary classification on two classes, and\nrequire averaging over classes to produce one score for multi-class classification. Scikit-\nlearn provides several averaging methods, three of which automated ML exposes:\nmacr o, micro, and weight ed.\nMacr o - Calculate the metric for each class and take the unweighted average\nMicro - Calculate the metric globally by counting the total true positives, false\nnegatives, and false positives (independent of classes).\nWeight ed - Calculate the metric for each class and take the weighted average\nbased on the number of samples per class.\nWhile each averaging method has its benefits, one common consideration when\nselecting the appropriate method is class imbalance. If classes have different numbers of\nsamples, it might be more informative to use a macro average where minority classes\nView job results\nClassification metrics"]}
{"question": "Describe the steps to retrieve the best model from an AutoML run using the AML SDK v2.", "variant_id": "v1", "line_number": 7, "answer": "To retrieve the best model from an AutoML run using the Azure Machine Learning SDK v2, you would typically follow these steps:\n\n1. **Initialize Workspace**: Import the necessary packages and classes, and create an instance of the `Workspace` class to connect to your Azure Machine Learning workspace.\n\n    ```python\n    from azure.ai.ml import MLClient\n    from azure.identity import DefaultAzureCredential\n    from azure.ai.ml.entities import Workspace\n\n    # Load the workspace\n    ws = Workspace.from_config()\n    ml_client = MLClient(DefaultAzureCredential(), ws.subscription_id, ws.resource_group, ws.name)\n    ```\n\n2. **Retrieve AutoML Run**: Use the `MLClient` to retrieve the specific AutoML run. You will need the run ID for the AutoML run you want to retrieve.\n\n    ```python\n    run_id = 'your_automl_run_id'\n    automl_run = ml_client.jobs.get(name=run_id)\n    ```\n\n3. **Get Best Model**: Once you have the AutoML run, you can access the best model from the run. The best model is typically stored as a child run of the AutoML run.\n\n    ```python\n    best_child_run = automl_run.get_best_child()\n    ```\n\n4. **Download Model**: After identifying the best child run, you can download the model file associated with it.\n\n    ```python\n    best_model_path = best_child_run.download_artifact('model')\n    ```\n\n5. **Register Model**: Optionally, you can register the model to your workspace for future deployment and tracking.\n\n    ```python\n    from azure.ai.ml.entities import Model\n\n    model = Model(\n        path=best_model_path,\n        name=\"best_automl_model\",\n        type=\"mlflow_model\",\n        description=\"Best model from AutoML run\"\n    )\n\n    registered_model = ml_client.models.create_or_update(model)\n    ```\n\nPlease note that the code snippets provided are for illustrative purposes and may require adjustments based on the specifics of your AutoML run and the Azure Machine Learning environment setup. Always refer to the latest Azure Machine Learning SDK v2 documentation for the most accurate and up-to-date information.", "context": ["Learn more about how and where to deploy a model .\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps", "Navigate to the Models  tab to see the algorithms (models) tested. By default, the\nmodels are ordered by metric score as they complete. For this tutorial, the model that\nscores the highest based on the chosen Normalized r oot mean squar ed err or metric is\nat the top of the list.\nWhile you wait for all of the experiment models to finish, select the Algorithm name  of\na completed model to explore its performance details.\nThe following example navigates to select a model from the list of models that the job\ncreated. Then, you select the Overview  and the Metrics  tabs to view the selected\nmodel's properties, metrics and performance charts.\nAutomated machine learning in Azure Machine Learning studio allows you to deploy the\nbest model as a web service in a few steps. Deployment is the integration of the model\nso it can predict on new data and identify potential areas of opportunity.\nFor this experiment, deployment to a web service means that the bike share company\nnow has an iterative and scalable web solution for forecasting bike share rental demand.\nOnce the job is complete, navigate back to parent job page by selecting Job 1  at the top\nof your screen.\nIn the Best model summar y section, the best model in the context of this experiment, is\nselected based on the Normalized r oot mean squar ed err or metric.\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\nThe deployment process entails several steps including registering the model,Deploy the model", "To run a fine-tuned model job from Machine Learning, in order to deploy an Azure\nOpenAI model:\n1. After you've finished fine-tuning an Azure OpenAI model, find the registered\nmodel in the Models  list with the name provided during fine-tuning and select the\nmodel you want to deploy.\n2. Select Deploy  and name the deployment. The model is deployed to the default\nAzure OpenAI resource linked to your workspace.\nTo enable users to quickly get started with code-based fine-tuning, we've published\nsamples (both Python notebooks and Azure CLI examples) to the azureml-ex amples\nGitHub repo:\nSDK example\nCLI exampleDeploy fine-tuned models\nFine-tuning by using code-based samples", "Option 2: T o deploy a specific model iteration from this experiment.\na. Select the desired model from the Models  tab\nb. Select Deploy  on the top left of the window.\n2. Populate the Deploy model  pane.\nField Value\nName Enter a unique name for your deployment.\nDescription Enter a description to better identify what this deployment is for.\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\nService (AKS)  or Azure Container Inst ance (ACI).\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\ndeploy to.\nEnable\nauthenticationSelect to allow for token-based or key-based authentication.\nUse custom\ndeployment assetsEnable this feature if you want to upload your own scoring script and\nenvironment file. Otherwise, automated ML provides these assets for\nyou by default. Learn more about scoring scripts .\nThe Advanced menu offers default deployment features such as data collection  and\nresource utilization settings. If you wish to override these defaults do so in this\nmenu.\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\ndeployment begins, the Model summar y tab appears. See the deployment\nprogress under the Deploy status  section.\nNow you have an operational web service to generate predictions! Y ou can test the\npredictions by querying the service from Power BI's built in Azure Machine Learning\nsupport .\uff89Expand table\n\uff09 Impor tant\nFile names must be under 32 characters and must begin and end with\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\nbetween. Spaces are not allowed.", "The automated machine learning interface allows you to deploy the best model as a\nweb service in a few steps. Deployment is the integration of the model so it can predict\non new data and identify potential areas of opportunity.\nFor this experiment, deployment to a web service means that the financial institution\nnow has an iterative and scalable web solution for identifying potential fixed term\ndeposit customers.\nCheck to see if your experiment run is complete. T o do so, navigate back to the parent\njob page by selecting Job 1  at the top of your screen. A Complet ed status is shown on\nthe top left of the screen.\nOnce the experiment run is complete, the Details  page is populated with a Best model\nsummar y section. In this experiment context, VotingEnsemble  is considered the best\nmodel, based on the AUC_w eight ed metric.\nWe deploy this model, but be advised, deployment takes about 20 minutes to complete.\nThe deployment process entails several steps including registering the model,\ngenerating resources, and configuring them for the web service.\n1. Select VotingEnsemble  to open the model-specific page.\n2. Select the Deploy  menu in the top-left and select Deploy t o web ser vice.\n3. Populate the Deploy a model  pane as follows:\nField Value\nDeployment name my-automl-deploy\nDeployment\ndescriptionMy first automated machine learning experiment deployment\nCompute type Select Azure Container Instance (A CI)\nEnable\nauthenticationDisable.\nUse custom\ndeploymentsDisable. Allows for the default driver file (scoring script) and\nenvironment file to be auto-generated.\nFor this example, we use the defaults provided in the Advanced menu.\n4. Select Deploy .Deploy the best model"]}
{"question": "What are the AutoML capabilities for handling imbalanced data in Azure ML?", "variant_id": "v1", "line_number": 8, "answer": "Azure Machine Learning's Automated ML (AutoML) includes several capabilities to handle imbalanced data, which is a common issue in machine learning where one class is significantly more prevalent than others. Here are some of the ways AutoML addresses this challenge:\n\n1. **Data Preprocessing**: AutoML automatically preprocesses the data before training models. This can include techniques such as scaling and normalization, which can help mitigate the impact of imbalanced data.\n\n2. **Sampling Strategies**: AutoML can apply different sampling strategies to balance the dataset. This may involve oversampling the minority class or undersampling the majority class to create a more balanced distribution.\n\n3. **Cost-Sensitive Learning**: AutoML can use cost-sensitive learning where the algorithm assigns a higher cost to misclassifying the minority class. This encourages the model to pay more attention to the minority class during training.\n\n4. **Advanced Metrics**: AutoML provides advanced metrics for evaluating model performance that are suitable for imbalanced datasets, such as the Area Under the Precision-Recall Curve (AUC-PR) instead of just accuracy. These metrics give a better indication of model performance when dealing with imbalanced classes.\n\n5. **Model Selection**: AutoML can select models that are known to handle imbalanced data better. For example, certain tree-based models like Random Forest or Gradient Boosting may perform better on imbalanced datasets.\n\n6. **Hyperparameter Tuning**: AutoML includes hyperparameter tuning capabilities that can optimize for hyperparameters that are particularly effective for imbalanced datasets, such as the class_weight parameter in some models.\n\nBy leveraging these capabilities, AutoML in Azure Machine Learning helps users to build models that are more robust and perform better on imbalanced datasets.", "context": ["Prevent overfitting and imbalanced data\nwith Automated ML\nArticle \u202206/16/2023\nOverfitting and imbalanced data are common pitfalls when you build machine learning\nmodels. By default, Azure Machine Learning's Automated ML provides charts and\nmetrics to help you identify these risks, and implements best practices to help mitigate\nthem.\nOverfitting in machine learning occurs when a model fits the training data too well, and\nas a result can't accurately predict on unseen test data. In other words, the model has\nmemorized specific patterns and noise in the training data, but is not flexible enough to\nmake predictions on real data.\nConsider the following trained models and their corresponding train and test accuracies.\nModel Train accuracy Test accuracy\nA 99.9% 95%\nB 87% 87%\nC 99.9% 45%\nConsider model A, there is a common misconception that if test accuracy on unseen\ndata is lower than training accuracy, the model is overfitted. However, test accuracy\nshould always be less than training accuracy, and the distinction for overfit vs.\nappropriately fit comes down to how much  less accurate.\nCompare models A and B, model A is a better model because it has higher test\naccuracy, and although the test accuracy is slightly lower at 95%, it is not a significant\ndifference that suggests overfitting is present. Y ou wouldn't choose model B because\nthe train and test accuracies are closer together.\nModel C represents a clear case of overfitting; the training accuracy is high but the test\naccuracy isn't anywhere near as high. This distinction is subjective, but comes from\nknowledge of your problem and data, and what magnitudes of error are acceptable.Identify overfitting\nPrevent overfitting", "If it ascertains a better performance through this experiment, then this remedy is\napplied.\nUse a performance metric that deals better with imbalanced data. For example, the\nAUC_weighted is a primary metric that calculates the contribution of every class\nbased on the relative number of samples representing that class, hence is more\nrobust against imbalance.\nThe following techniques are additional options to handle imbalanced data outside of\nAutomated ML.\nResampling to even the class imbalance, either by up-sampling the smaller classes\nor down-sampling the larger classes. These methods require expertise to process\nand analyze.\nReview performance metrics for imbalanced data. For example, the F1 score is the\nharmonic mean of precision and recall. Precision measures a classifier's exactness,\nwhere higher precision indicates fewer false positives, while recall measures a\nclassifier's completeness, where higher recall indicates fewer false negatives.\nSee examples and learn how to build models using Automated ML:\nFollow the Tutorial: T rain an object detection model with automated machine\nlearning and Python .\nConfigure the settings for automatic training experiment:\nIn Azure Machine Learning studio, use these steps .\nWith the Python SDK, use these steps .Next steps", "Imbalanced data is commonly found in data for machine learning classification\nscenarios, and refers to data that contains a disproportionate ratio of observations in\neach class. This imbalance can lead to a falsely perceived positive effect of a model's\naccuracy, because the input data has bias towards one class, which results in the trained\nmodel to mimic that bias.\nIn addition, Automated ML jobs generate the following charts automatically. These\ncharts help you understand the correctness of the classifications of your model, and\nidentify models potentially impacted by imbalanced data.\nChar t Descr iption\nConfusion\nMatrixEvaluates the correctly classified labels against the actual labels of the data.\nPrecision-recall Evaluates the ratio of correct labels against the ratio of found label instances of\nthe data\nROC Curves Evaluates the ratio of correct labels against the ratio of false-positive labels.\nAs part of its goal of simplifying the machine learning workflow, Automated ML has built\nin capabilities to help deal with imbalanced data such as,\nA weight column: Automated ML creates a column of weights as input to cause\nrows in the data to be weighted up or down, which can be used to make a class\nmore or less \"important.\"\nThe algorithms used by Automated ML detect imbalance when the number of\nsamples in the minority class is equal to or fewer than 20% of the number of\nsamples in the majority class, where minority class refers to the one with fewest\nsamples and majority class refers to the one with most samples. Subsequently,\nautomated machine learning will run an experiment with subsampled data to\ncheck if using class weights would remedy this problem and improve performance.Cross-validation isn't enabled by default; it must be configured in Automated\nmachine learning settings. However, after cross-validation is configured and a\nvalidation data set has been provided, the process is automated for you.\nIdentify models with imbalanced data\nHandle imbalanced data", "What is automated machine learning\n(AutoML)?\nArticle \u202204/13/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAutomated machine learning, also referred to as automated ML or AutoML, is the\nprocess of automating the time-consuming, iterative tasks of machine learning model\ndevelopment. It allows data scientists, analysts, and developers to build ML models with\nhigh scale, efficiency, and productivity all while sustaining model quality. Automated ML\nin Azure Machine Learning is based on a breakthrough from our Microsoft R esearch\ndivision .\nFor code-experienced customers, Azure Machine Learning Python SDK . Get\nstarted with Tutorial: T rain an object detection model (preview) with AutoML and\nPython .\nDuring training, Azure Machine Learning creates a number of pipelines in parallel that\ntry different algorithms and parameters for you. The service iterates through ML\nalgorithms paired with feature selections, where each iteration produces a model with a\ntraining score. The better the score for the metric you want to optimize for, the better\nthe model is considered to \"fit\" your data. It will stop once it hits the exit criteria defined\nin the experiment.\nUsing Azur e Machine Learning , you can design and run your automated ML training\nexperiments with these steps:\n1. Identif y the ML pr oblem  to be solved: classification, forecasting, regression,\ncomputer vision or NLP.\n2. Choose whether y ou want a code-fir st experience or a no-code studio w eb\nexperience : Users who prefer a code-first experience can use the Azure Machine\nLearning SDK v2 or the Azure Machine Learning CLIv2 . Get started with Tutorial:\nTrain an object detection model with AutoML and Python . Users who prefer a\nlimited/no-code experience can use the web interface  in Azure Machine Learning\nstudio at https://ml.azure.com . Get started with Tutorial: Create a classification\nmodel with automated ML in Azure Machine Learning .\nHow does AutoML work?", "Automated ML in Azure Machine Learning supports model training for computer vision\ntasks like image classification and object detection. T o debug AutoML vision models and\nexplain model predictions, AutoML models for computer vision are integrated with\nResponsible AI dashboard. T o generate R esponsible AI insights for AutoML computer\nvision models, register your best AutoML model in the Azure Machine Learning\nworkspace and run it through the R esponsible AI vision insights pipeline. T o learn, see\nhow to set up AutoML to train computer vision models .\nNotebooks related to the AutoML supported computer vision tasks can be found in\nazureml-examples  repository.\nThe R esponsible AI vision Insights pipeline could be submitted through one of the\nfollowing methods\nPython SDK: T o learn how to submit the pipeline through Python, see the AutoML\nImage Classification scenario with RAI Dashboard sample notebook . For\nconstructing the pipeline, refer to section 5.1 in the notebook.\nAzure CLI: T o submit the pipeline via Azure-CLI, see the component Y AML in\nsection 5.2 of the example notebook linked above.\nUI (via Azure Machine Learning studio): From the Designer in Azure Machine\nLearning studio, the RAI-vision insights component can be used to create and\nsubmit a pipeline.\nIn addition to the list of R esponsible AI vision insights parameters provided in the\nprevious section, the following are parameters to set specifically for AutoML models.Integration with AutoML Image\nMode of submitting the Responsible AI vision insights\npipeline\nResponsible AI vision insights component parameter\n(AutoML specific)\n\uff17 Note\nA few parameters are specific to the XAI algorithm chosen and are optional for\nother algorithms."]}
{"question": "Explain how to use AutoML for forecasting time series data. What specific settings should be adjusted for time series problems?", "variant_id": "v1", "line_number": 9, "answer": "To use Azure AutoML for forecasting time series data, you should follow these general steps and consider specific settings that are tailored for time series problems:\n\n1. **Set Up Your Environment**:\n   - Install the Azure Machine Learning Python SDK v2.\n   - Create an Azure Machine Learning workspace if you don't already have one.\n   - Set up your development environment, such as a Jupyter notebook, and authenticate to your workspace.\n\n2. **Prepare Your Data**:\n   - Ensure your data is in a format suitable for time series forecasting. This typically means having a time column, one or more feature columns, and a target column that you want to forecast.\n   - Split your data into training and testing datasets.\n\n3. **Configure Your AutoML Experiment**:\n   - Define the settings and configuration for your AutoML run. For time series forecasting, you should adjust the following specific settings:\n     - **time_column_name**: Specify the name of the column that holds the time series information.\n     - **grain_column_names**: If you have multiple time series (e.g., sales data for different stores), specify the columns that identify the individual series.\n     - **max_horizon**: Define the number of periods to forecast into the future.\n     - **target_lags**: Set the lags for the target variable, which can help the model understand the relationship between past values and future values.\n     - **target_rolling_window_size**: Use a rolling window to aggregate past values of the target variable, which can be useful for capturing trends and seasonality.\n     - **forecasting_parameters**: Additional parameters specific to forecasting tasks, such as seasonality, holiday features, etc.\n\n4. **Choose Your Compute Target**:\n   - Select a compute target for training your models. For larger datasets or many time series, consider using a compute cluster to distribute the training jobs.\n\n5. **Run the AutoML Experiment**:\n   - Submit the AutoML run with the defined settings and data. AutoML will automatically try different models and preprocessing steps to find the best solution for your forecasting problem.\n\n6. **Evaluate Results**:\n   - Once the run is complete, evaluate the performance of the models using metrics such as MAE, RMSE, etc.\n   - Review the engineered features, model explanations, and any other insights provided by AutoML.\n\n7. **Deploy the Model**:\n   - After selecting the best model, deploy it to an Azure Container Instance, Azure Kubernetes Service, or other supported targets for real-time or batch scoring.\n\nFor more detailed guidance and examples, you can refer to the Azure Machine Learning documentation, specifically the articles and Jupyter notebooks on forecasting with AutoML, such as the \"Bike share example\" and \"Forecasting using deep learning\". These resources provide hands-on examples and best practices for time series forecasting using Azure AutoML.", "context": ["Frequently asked questions about\nforecasting in AutoML\nArticle \u202208/01/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nThis article answers common questions about forecasting in automatic machine learning\n(AutoML). For general information about forecasting methodology in AutoML, see the\nOverview of forecasting methods in AutoML  article.\nYou can start by reading the Set up AutoML to train a time-series forecasting model\narticle. Y ou can also find hands-on examples in several Jupyter notebooks:\nBike share example\nForecasting using deep learning\nMany Models solution\nForecasting recipes\nAdvanced forecasting scenarios\nWe're always working to make AutoML faster and more scalable. T o work as a general\nforecasting platform, AutoML does extensive data validations and complex feature\nengineering, and it searches over a large model space. This complexity can require a lot\nof time, depending on the data and the configuration.\nOne common source of slow runtime is training AutoML with default settings on data\nthat contains numerous time series. The cost of many forecasting methods scales with\nthe number of series. For example, methods like Exponential Smoothing and Prophet\ntrain a model for each time series  in the training data.\nThe Many Models feature of AutoML scales to these scenarios by distributing training\njobs across a compute cluster. It has been successfully applied to data with millions of\ntime series. For more information, see the many models  article section. Y ou can also\nread about the success of Many Models  on a high-profile competition dataset.\nHow do I start building forecasting models in\nAutoML?\nWhy is AutoML slow on my data?", "Learn more about how and where to deploy a model .\nLearn more about how to set up AutoML to train a time-series forecasting model .Next steps", "If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about calendar features for time series forecasting in AutoML .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about AutoML forecasting for lagged features .Next steps", "Set up AutoML to train a time-series\nforecasting model with SDK and CLI\nArticle \u202208/02/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn this article, you'll learn how to set up AutoML for time-series forecasting with Azure\nMachine Learning automated ML in the Azure Machine Learning Python SDK .\nTo do so, you:\nFor a low code experience, see the Tutorial: Forecast demand with automated machine\nlearning  for a time-series forecasting example using automated ML in the Azure\nMachine Learning studio .\nAutoML uses standard machine learning models along with well-known time series\nmodels to create forecasts. Our approach incorporates historical information about the\ntarget variable, user-provided features in the input data, and automatically engineered\nfeatures. Model search algorithms then work to find a model with the best predictive\naccuracy. For more details, see our articles on forecasting methodology  and model\nsearch .\nFor this article you need,\nAn Azure Machine Learning workspace. T o create the workspace, see Create\nworkspace resources .\nThe ability to launch AutoML training jobs. Follow the how-to guide for setting up\nAutoML  for details.\nPrepare data for training.\uff02\nConfigure specific time-series parameters in a Forecasting Job . \uff02\nOrchestrate training, inference, and model evaluation using components and\npipelines.\uff02\nPrerequisites\nTraining  and validation data", "See the Why is AutoML slow on my data?  answer to understand why AutoML might be\nslow in your case.\nConsider the following configuration changes that might speed up your job:\nBlock time series models  like ARIMA and Prophet.\nTurn off look-back features like lags and rolling windows.\nReduce:\nThe number of trials/iterations.\nTrial/iteration timeout.\nExperiment timeout.\nThe number of cross-validation folds.\nEnsure that early termination is enabled.\nAutoML forecasting supports four basic configurations:\nConfiguration Scenar io Pros Cons\nDefault\nAutoMLRecommended if the dataset\nhas a small number of time\nseries that have roughly\nsimilar historical behavior.- Simple to configure\nfrom code/SDK or\nAzure Machine\nLearning studio.\n- AutoML can learn\nacross different time\nseries because the\nregression models\npool all series together\nin training. For more\ninformation, see\nModel grouping .- Regression models\nmight be less\naccurate if the time\nseries in the training\ndata have divergent\nbehavior.\n- Time series models\nmight take a long\ntime to train if the\ntraining data has a\nlarge number of\nseries. For more\ninformation, see the\nWhy is AutoML slow\non my data?  answer.\nAutoML with\ndeep learningRecommended for datasets\nwith more than 1,000\nobservations and, potentially,\nnumerous time series that\nexhibit complex patterns.\nWhen it's enabled, AutoML\nwill sweep over temporal- Simple to configure\nfrom code/SDK or\nAzure Machine\nLearning studio.\n- Cross-learning\nopportunities, because- Training can take\nmuch longer\nbecause of the\ncomplexity of DNN\nmodels.\n- Series with smallHow can I make AutoML faster?\nWhat modeling configuration should I use?"]}
{"question": "How do you create a custom deep learning environment with specific Python packages using the AML SDK v2?", "variant_id": "v1", "line_number": 10, "answer": "To create a custom deep learning environment with specific Python packages using the Azure Machine Learning SDK v2, you can follow these steps:\n\n1. **Define the Environment**: Use the `Environment` class from the `azure.ai.ml.entities` namespace to define your custom environment. You will need to specify the name, description, the path to your Conda environment file (which lists the Python packages and versions you want to include), and optionally, a base Docker image.\n\n2. **Create the Conda Environment File**: Write a `conda.yaml` file that lists all the Python packages and versions you need for your deep learning environment. This file will be used to create the environment.\n\n3. **Register the Environment**: Use the `create_or_update` method of the `ml_client.environments` to register the environment with your Azure Machine Learning workspace.\n\nHere's an example of how you might do this in code:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import Environment\nfrom azure.identity import DefaultAzureCredential\nimport os\n\n# Set up the MLClient with your Azure subscription and workspace information\nsubscription_id = 'your-subscription-id'\nresource_group = 'your-resource-group'\nworkspace_name = 'your-workspace-name'\nml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace_name)\n\n# Define the custom environment\ncustom_env_name = \"deep-learning-env\"\ndependencies_dir = \"path_to_your_dependencies_directory\"  # Directory where conda.yaml is located\nconda_file_path = os.path.join(dependencies_dir, \"conda.yaml\")\n\njob_env = Environment(\n    name=custom_env_name,\n    description=\"Custom environment for deep learning\",\n    conda_file=conda_file_path,\n    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",  # Optional base image\n)\n\n# Register the environment\njob_env = ml_client.environments.create_or_update(job_env)\n\n# Print the environment details\nprint(f\"Environment with name {job_env.name} is registered to workspace, the environment version is {job_env.version}\")\n```\n\nIn the `conda.yaml` file, you would specify your Python packages like this:\n\n```yaml\nname: deep-learning-env\ndependencies:\n  - python=3.8\n  - numpy\n  - pandas\n  - scikit-learn\n  - tensorflow\n  - keras\n  - pip:\n    - some-pip-package\n```\n\nReplace the placeholders with your actual subscription ID, resource group, workspace name, and the path to your dependencies directory. Also, customize the `conda.yaml` file with the specific Python packages and versions that you need for your deep learning tasks.", "context": ["The specification contains some usual packages (such as numpy and pip) that you'll use\nin your job.\nNext, use the Y AML file to create and register this custom environment in your\nworkspace. The environment will be packaged into a Docker container at runtime.\nPython\nFor more information on creating and using environments, see Create and use software\nenvironments in Azure Machine Learning .\nIn this section, we'll begin by introducing the data for training. W e'll then cover how to\nrun a training job, using a training script that we've provided. Y ou'll learn to build the\ntraining job by configuring the command for running the training script. Then, you'll\nsubmit the training job to run in Azure Machine Learning.\nYou'll use data from the Modified National Institute of S tandards and T echnology\n(MNIST) database of handwritten digits. This data is sourced from Y an LeCun's website\nand stored in an Azure storage account.\nPythonfrom azure.ai.ml.entities import Environment  \ncustom_env_name = \"keras-env\"  \njob_env = Environment(  \n    name=custom_env_name,  \n    description= \"Custom environment for keras image classification\" , \n    conda_file=os.path.join(dependencies_dir, \"conda.yaml\" ), \n    image= \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\" , \n) \njob_env = ml_client.environments.create_or_update(job_env)  \nprint( \n    f\"Environment with name {job_env.name}  is registered to workspace, the  \nenvironment version is {job_env.version} \" \n)\nConfigure and submit your training  job\nObtain the training data\nweb_path = \"wasbs://datasets@azuremlexamples.blob.core.windows.net/mnist/\"", "In the samples deep learning folder on the notebook server, find a\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\ndeploy-with-k eras.\nYour Jupyter notebook server\nInstall the Azure Machine Learning SDK (v2) .\nDownload the training scripts keras_mnist.py  and utils.py .\nYou can also find a completed Jupyter Notebook version  of this guide on the GitHub\nsamples page.\nBefore you can run the code in this article to create a GPU cluster, you'll need to request\na quota increase  for your workspace.\nThis section sets up the job for training by loading the required Python packages,\nconnecting to a workspace, creating a compute resource to run a command job, and\ncreating an environment to run the job.\nFirst, you'll need to connect to your Azure Machine Learning workspace. The Azure\nMachine Learning workspace  is the top-level resource for the service. It provides you\nwith a centralized place to work with all the artifacts you create when you use Azure\nMachine Learning.\nWe're using DefaultAzureCredential to get access to the workspace. This credential\nshould be capable of handling most Azure SDK authentication scenarios.\nIf DefaultAzureCredential doesn't work for you, see azure-identity reference\ndocumentation  or Set up authentication  for more available credentials.\nPython\nSet up the job\nConnect to the workspace\n# Handle to the workspace  \nfrom azure.ai.ml import MLClient  \n# Authentication package  \nfrom azure.identity import DefaultAzureCredential  \ncredential = DefaultAzureCredential()", "Azure Machine Learning allows you to either use a curated (or ready-made)\nenvironment or create a custom environment using a Docker image or a Conda\nconfiguration. In this article, you reuse the curated Azure Machine Learning environment\nAzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu. Use the latest version of this\nenvironment using the @latest directive.\nPython\nIn this section, we begin by introducing the data for training. W e then cover how to run\na training job, using a training script that we've provided. Y ou'll learn to build the\ntraining job by configuring the command for running the training script. Then, you'll\nsubmit the training job to run in Azure Machine Learning.\nYou can use the dataset in this zipped file . This dataset consists of about 120 training\nimages each for two classes (turkeys and chickens), with 100 validation images for each\nclass. The images are a subset of the Open Images v5 Dataset . The training script\npytorch_tr ain.p y downloads and extracts the dataset.\nIn the prerequisites section, we provided the training script pytorch_tr ain.p y. In practice,\nyou should be able to take any custom training script as is and run it with Azure\nMachine Learning without having to modify your code.\nThe provided training script downloads the data, trains a model, and registers the\nmodel.\nNow that you have all the assets required to run your job, it's time to build it using the\nAzure Machine Learning Python SDK v2. For this example, we create a command.\nAn Azure Machine Learning command is a resource that specifies all the details needed to\nexecute your training code in the cloud. These details include the inputs and outputs,curated_env_name = \"AzureML-pytorch-1.9-ubuntu18.04-py37-cuda11-gpu@latest\"\nConfigure and submit your training  job\nObtain the training data\nPrepare the training script\nBuild the training job", "2. Create an Azure Machine Learning workspace .\n3. Clone the AzureML-Examples repository .\nBash\n4. Start the notebook server from the directory containing your clone.\nBash\nThese instructions install the base SDK packages necessary for the quickstart and tutorial\nnotebooks. Other sample notebooks might require you to install extra components. For\nmore information, see Install the Azure Machine Learning SDK for Python .\nThe Data Science Virtual Machine (DSVM) is a customized VM image built specifically for\ndoing data science. If you create a DSVM , the SDK and notebook server are installed and\nconfigured for you. However, you'll still need to create a workspace and clone the\nsample repository.\n1. Create an Azure Machine Learning workspace .\n2. Clone the the AzureML-Examples repository .\nBash\n3. Start the notebook server from the directory that contains the clone.\nBash\ngit clone https://github.com/Azure/azureml-examples.git --depth 1\njupyter notebook\nOption 3: Access on a DSVM\ngit clone https://github.com/Azure/azureml-examples.git --depth 1\njupyter notebook\nConnect to a workspace", "Manage Azu re Machine Learning\nenvironments with the CLI & SDK (v2)\nArticle \u202201/03/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAzure Machine Learning environments define the execution environments for your jobs\nor deployments and encapsulate the dependencies for your code. Azure Machine\nLearning uses the environment specification to create the Docker container that your\ntraining or scoring code runs in on the specified compute target. Y ou can define an\nenvironment from a conda specification, Docker image, or Docker build context.\nIn this article, learn how to create and manage Azure Machine Learning environments\nusing the SDK & CLI (v2).\nBefore following the steps in this article, make sure you have the following prerequisites:\nAn Azure Machine Learning workspace. If you don't have one, use the steps in the\nQuickstart: Create workspace resources  article to create one.\nThe Azure CLI and the ml extension or the Azure Machine Learning Python SDK v2:\nTo install the Azure CLI and extension, see Install, set up, and use the CLI (v2) .\nTo install the Python SDK v2, use the following command:\nBash\nPrerequisites\n\uff09 Impor tant\nThe CLI examples in this article assume that you are using the Bash (or\ncompatible) shell. For example, from a Linux system or Windows\nSubsyst em for Linux .\npip install azure-ai-ml azure-identity"]}
{"question": "Describe the process of using Azure ML to train a deep learning model on a GPU compute cluster.", "variant_id": "v1", "line_number": 11, "answer": "To train a deep learning model on a GPU compute cluster using Azure Machine Learning, you would follow these general steps:\n\n1. **Set Up Your Workspace**:\n   - Connect to your Azure Machine Learning workspace using the `MLClient` from the Azure ML Python SDK. You'll need your Azure subscription ID, resource group, and the name of the workspace.\n\n   ```python\n   from azure.ai.ml import MLClient\n   from azure.identity import DefaultAzureCredential\n\n   workspace = '<AZUREML_WORKSPACE_NAME>'\n   subscription_id = '<YOUR_SUBSCRIPTION_ID>'\n   resource_group = '<YOUR_RESOURCE_GROUP>'\n\n   ml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace)\n   ```\n\n2. **Create or Retrieve a GPU Compute Cluster**:\n   - Define a GPU compute target. If it doesn't exist, create a new one using the `AmlCompute` class, specifying the GPU VM size (e.g., `STANDARD_NC6` for a VM with one NVIDIA Tesla K80 GPU).\n\n   ```python\n   from azure.ai.ml.entities import AmlCompute\n\n   gpu_compute_target = \"gpu-cluster\"\n   try:\n       ml_client.compute.get(gpu_compute_target)\n   except Exception:\n       print(\"Creating a new GPU compute target...\")\n       compute = AmlCompute(\n           name=gpu_compute_target,\n           size=\"STANDARD_NC6\",  # Example GPU VM size\n           min_instances=0,\n           max_instances=4  # Adjust based on your needs\n       )\n       ml_client.compute.begin_create_or_update(compute).result()\n   ```\n\n3. **Prepare Your Training Script**:\n   - Write a Python script (e.g., `train.py`) that defines your deep learning model, the training process, and any other necessary steps like data preprocessing.\n\n4. **Create an Experiment and Submit the Training Job**:\n   - Define the training job configuration, including the source directory containing your training script, the compute target, and any necessary environment dependencies.\n\n   ```python\n   from azure.ai.ml import command, Input\n\n   # Define the command job with the training script and parameters\n   job = command(\n       code=\"./path_to_your_code_directory\",\n       command=\"python train.py\",\n       compute=gpu_compute_target,\n       environment=\"<YOUR_ENVIRONMENT_NAME>\",  # Specify your environment\n       inputs={\"training_data\": Input(type=\"uri_folder\", path=\"<YOUR_DATA_PATH>\")},\n       outputs={\"model_output\": \"<YOUR_OUTPUT_PATH>\"}\n   )\n\n   # Submit the job to the Azure ML workspace\n   returned_job = ml_client.create_or_update(job)\n   ```\n\n5. **Monitor the Training Job**:\n   - Use Azure ML Studio or the SDK to monitor the progress of your training job, view logs, and retrieve the trained model once the job is complete.\n\nRemember to replace placeholders like `<AZUREML_WORKSPACE_NAME>`, `<YOUR_SUBSCRIPTION_ID>`, `<YOUR_RESOURCE_GROUP>`, `<YOUR_ENVIRONMENT_NAME>`, `<YOUR_DATA_PATH>`, and `<YOUR_OUTPUT_PATH>` with your actual values. Also, ensure that the VM size you specify for the compute target is suitable for your deep learning workload and that you have the necessary quota in your Azure subscription for the GPU VMs.", "context": ["An Azure Machine Learning compute cluster is a fully managed compute resource that\ncan be used to run the training job. In the following examples, a compute cluster named\ncpu-compute is created.\nPython\nTo run this script, you'll use a command that executes main.py Python script located\nunder ./sdk/python/jobs/single-step/lightgbm/iris/src/. The command will be runworkspace = '<AZUREML_WORKSPACE_NAME>'\n#connect to the workspace\nml_client = MLClient(DefaultAzureCredential(), subscription_id,  \nresource_group, workspace)\n2. Create a compute resource for training\n\uff17 Note\nTo try serverless comput e (pr eview) , skip this step and proceed to 4. Submit the\ntraining job .\nPython SDK\nfrom azure.ai.ml.entities import AmlCompute\n# specify aml compute name.\ncpu_compute_target = \"cpu-cluster\"\ntry:\n    ml_client.compute.get(cpu_compute_target)\nexcept Exception:\n    print(\"Creating a new cpu compute target...\" )\n    compute = AmlCompute(\n        name=cpu_compute_target, size= \"STANDARD_D2_V2\" , min_instances= 0, \nmax_instances= 4\n    )\n    ml_client.compute.begin_create_or_update(compute).result()\n4. Submit the training job\nPython SDK", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Manage and optimize Azure Machine\nLearning costs\nArticle \u202208/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.\uff09 Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)", "For detailed instructions on how to complete these steps, see Secure a training\nenvironment .\nIn this section, you learn how Azure Machine Learning securely communicates between\nservices to submit a training job. This example shows you how all your configurations\nwork together to secure communication.\n1. The client uploads training scripts and training data to storage accounts that are\nsecured with a service or private endpoint.\n2. The client submits a training job to the Azure Machine Learning workspace\nthrough the private endpoint.\n3. Azure Batch service receives the job from the workspace. It then submits the\ntraining job to the compute environment through the public load balancer for the\ncompute resource.\n4. The compute resource receives the job and begins training. The compute resource\nuses information stored in key vault to access storage accounts to download\ntraining files and upload output.Example training job submission", "like Spark.\nIn the following example script, we provision a Linux compute cluster . You can see the\nAzure Machine Learning pricing  page for the full list of VM sizes and prices. Since we\nneed a GPU cluster for this example, let's pick a STAND ARD_NC6  model and create an\nAzure Machine Learning compute.\nPython\nfrom azure.ai.ml.entities import AmlCompute  \ngpu_compute_target = \"gpu-cluster\"  \ntry: \n    # let's see if the compute target already exists  \n    gpu_cluster = ml_client.compute.get(gpu_compute_target)  \n    print(  \n        f\"You already have a cluster named {gpu_compute_target} , we'll reuse  \nit as is.\"  \n    ) \nexcept Exception:  \n    print( \"Creating a new gpu compute target...\" ) \n    # Let's create the Azure ML compute object with the intended parameters  \n    gpu_cluster = AmlCompute(  \n        # Name assigned to the compute cluster  \n        name= \"gpu-cluster\" , \n        # Azure ML Compute is the on-demand VM service  \n        type= \"amlcompute\" , \n        # VM Family  \n        size= \"STANDARD_NC6\" , \n        # Minimum running nodes when there is no job running  \n        min_instances= 0, \n        # Nodes in cluster  \n        max_instances= 4, \n        # How many seconds will the node running after the job termination  \n        idle_time_before_scale_down= 180, \n        # Dedicated or LowPriority. The latter is cheaper but there is a  \nchance of job termination  \n        tier= \"Dedicated\" , \n    ) \n    # Now, we pass the object to MLClient's create_or_update method  \n    gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()  \nprint( \n    f\"AMLCompute with name {gpu_cluster.name}  is created, the compute size  \nis {gpu_cluster.size} \" \n)"]}
{"question": "Explain how to deploy a TensorFlow model as a web service in Azure ML. What are the key considerations for deployment?", "variant_id": "v1", "line_number": 12, "answer": "To deploy a TensorFlow model as a web service in Azure Machine Learning, you need to follow these general steps:\n\n1. **Register the Model**: First, you need to register your trained TensorFlow model in your Azure Machine Learning workspace. This can be done using the Azure Machine Learning SDK or the Azure portal by providing the model file and metadata.\n\n2. **Prepare an Inference Configuration**: Create an inference configuration that specifies the environment needed to host the model. This includes the scoring script (`score.py`) that uses the model to make predictions and an environment file (`environment.yml`) that specifies the dependencies, such as TensorFlow and any other required libraries.\n\n3. **Prepare an Entry Script**: The entry script, often named `score.py`, should contain two functions: `init()` which loads the model and runs any one-time initialization, and `run(data)` which is called when new data is submitted for predictions.\n\n4. **Create a Deployment Configuration**: Define the deployment configuration that specifies the compute type where the model will be hosted. This could be an Azure Container Instance (ACI), Azure Kubernetes Service (AKS), or other supported compute targets. The configuration includes details such as CPU cores, memory, and other settings.\n\n5. **Deploy the Model**: Use the Azure Machine Learning SDK to deploy the model to the chosen compute target. This will create a web service endpoint that can be used to make predictions.\n\n6. **Test the Web Service**: After deployment, test the web service endpoint to ensure it is working correctly by sending sample requests and verifying the responses.\n\nKey considerations for deployment include:\n\n- **Scalability**: Choose a compute target that can scale to meet your expected load. AKS is typically used for high-scale production deployments, while ACI is often used for testing or low-scale applications.\n\n- **Security**: Ensure that the endpoint is secure. Use features like authentication and virtual networks to protect your endpoint.\n\n- **Monitoring and Logging**: Set up monitoring and logging to track the health and usage of your web service. Azure Machine Learning provides capabilities to monitor your endpoints.\n\n- **Cost**: Consider the cost implications of the compute resources you choose for hosting your model. Different compute targets have different pricing models.\n\n- **Latency**: The choice of compute target can affect the latency of your predictions. For low-latency requirements, you may need to choose a more powerful compute target or optimize your model and scoring script.\n\n- **Model Versioning**: Use Azure Machine Learning's model versioning capabilities to manage and deploy different versions of your model.\n\n- **Environment Reproducibility**: Ensure that the environment for your model is reproducible, especially if you are moving from a development to a production setting.\n\nRemember to consult the Azure Machine Learning documentation for the most up-to-date and detailed instructions, as the platform and best practices may evolve over time.", "context": ["What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps", "Tutorial Descr iption\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "To learn more, review these articles:\nDeploy models with REST\nCreate and use managed online endpoints in the studio\nSafe rollout for online endpoints\nHow to autoscale managed online endpoints\nView costs for an Azure Machine Learning managed online endpoint\nAccess Azure resources with a managed online endpoint and managed identity\nTroubleshoot managed online endpoints deployment", "Option 2: T o deploy a specific model iteration from this experiment.\na. Select the desired model from the Models  tab\nb. Select Deploy  on the top left of the window.\n2. Populate the Deploy model  pane.\nField Value\nName Enter a unique name for your deployment.\nDescription Enter a description to better identify what this deployment is for.\nCompute type Select the type of endpoint you want to deploy: Azure Kuber netes\nService (AKS)  or Azure Container Inst ance (ACI).\nCompute name Applies t o AKS only:  Select the name of the AKS cluster you wish to\ndeploy to.\nEnable\nauthenticationSelect to allow for token-based or key-based authentication.\nUse custom\ndeployment assetsEnable this feature if you want to upload your own scoring script and\nenvironment file. Otherwise, automated ML provides these assets for\nyou by default. Learn more about scoring scripts .\nThe Advanced menu offers default deployment features such as data collection  and\nresource utilization settings. If you wish to override these defaults do so in this\nmenu.\n3. Select Deploy . Deployment can take about 20 minutes to complete. Once\ndeployment begins, the Model summar y tab appears. See the deployment\nprogress under the Deploy status  section.\nNow you have an operational web service to generate predictions! Y ou can test the\npredictions by querying the service from Power BI's built in Azure Machine Learning\nsupport .\uff89Expand table\n\uff09 Impor tant\nFile names must be under 32 characters and must begin and end with\nalphanumerics. May include dashes, underscores, dots, and alphanumerics\nbetween. Spaces are not allowed.", "In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\nmodel to an online endpoint. See these other articles to learn more about Azure\nMachine Learning.\nTrack run metrics during training\nTune hyperparameters\nReference architecture for distributed deep learning training in AzureNext steps"]}
{"question": "Provide an example of how to use the AML SDK v2 for distributed deep learning training. What configurations are necessary for multi-node training?", "variant_id": "v1", "line_number": 13, "answer": "To use Azure Machine Learning SDK v2 for distributed deep learning training, you would typically follow these steps and include the necessary configurations for multi-node training:\n\n1. **Set up your environment**: Install the Azure Machine Learning SDK v2 and configure your environment with the necessary dependencies for your deep learning framework (PyTorch, TensorFlow, etc.).\n\n2. **Create or attach a compute target**: Define a multi-node compute cluster in Azure Machine Learning. This is where your model will be trained.\n\n3. **Prepare your training script**: Write a training script that is compatible with distributed training. For PyTorch, you might use `torch.distributed.launch` or `torch.nn.parallel.DistributedDataParallel`. For TensorFlow, you might use `tf.distribute.Strategy`.\n\n4. **Configure the distributed training settings**: Use the `MpiConfiguration` for MPI-based frameworks like Horovod, or use the `TensorflowConfiguration` or `PyTorchConfiguration` for the respective frameworks. Specify the number of nodes and the number of GPUs per node.\n\n5. **Submit the training job**: Use the `CommandJob` class to submit your training job to the compute target you configured.\n\nHere is an example of how you might set up a distributed training job using the Azure Machine Learning SDK v2:\n\n```python\nfrom azure.ai.ml import MLClient\nfrom azure.ai.ml.entities import CommandJob, Environment, MpiDistribution, CodeConfiguration\nfrom azure.identity import DefaultAzureCredential\n\n# Set up the MLClient with your Azure subscription and resource details\nml_client = MLClient(DefaultAzureCredential(), subscription_id, resource_group, workspace_name)\n\n# Define the compute target (assuming it's already created in your workspace)\ncompute_target = \"your-compute-cluster\"\n\n# Define the environment for your training job\nenvironment = Environment(\n    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04\",\n    conda_file=\"path_to_your_conda_file.yml\",\n    name=\"my-distributed-training-env\",\n)\n\n# Define the distribution configuration for MPI-based distributed training\ndistribution = MpiDistribution()\ndistribution.process_count_per_instance = 4  # Number of GPUs per node\n\n# Define the code configuration\ncode_configuration = CodeConfiguration(\n    code=\"path_to_your_training_script_folder\",\n    command=\"python train.py\"  # Your training script\n)\n\n# Create the command job for distributed training\ncommand_job = CommandJob(\n    compute=compute_target,\n    environment=environment,\n    distribution=distribution,\n    code_configuration=code_configuration,\n    resources=ResourceConfiguration(instance_count=2),  # Number of nodes\n)\n\n# Submit the job\nreturned_job = ml_client.jobs.create_or_update(command_job)\n```\n\nIn this example, replace `subscription_id`, `resource_group`, `workspace_name`, `your-compute-cluster`, `path_to_your_conda_file.yml`, `path_to_your_training_script_folder`, and `train.py` with your specific details. The `ResourceConfiguration` specifies that the job should run on 2 nodes, and the `MpiDistribution` specifies that there should be 4 processes per instance, which typically corresponds to the number of GPUs per node.\n\nPlease note that the Azure ML SDK v2 is continuously evolving, and the exact classes and methods may change. Always refer to the latest Azure ML SDK v2 documentation for the most up-to-date information.", "context": ["Distributed training with Azure Machine\nLearning\nArticle \u202203/27/2023\nIn this article, you learn about distributed training and how Azure Machine Learning\nsupports it for deep learning models.\nIn distributed training the workload to train a model is split up and shared among\nmultiple mini processors, called worker nodes. These worker nodes work in parallel to\nspeed up model training. Distributed training can be used for traditional ML models, but\nis better suited for compute and time intensive tasks, like deep learning  for training\ndeep neural networks.\nThere are two main types of distributed training: data parallelism  and model parallelism .\nFor distributed training on deep learning models, the Azure Machine Learning SDK in\nPython  supports integrations with popular frameworks, PyT orch and T ensorFlow. Both\nframeworks employ data parallelism for distributed training, and can leverage\nhorovod  for optimizing compute speeds.\nDistributed training with PyT orch\nDistributed training with T ensorFlow\nFor ML models that don't require distributed training, see train models with Azure\nMachine Learning  for the different ways to train models using the Python SDK.\nData parallelism is the easiest to implement of the two distributed training approaches,\nand is sufficient for most use cases.\nIn this approach, the data is divided into partitions, where the number of partitions is\nequal to the total number of available nodes, in the compute cluster or serverless\ncompute . The model is copied in each of these worker nodes, and each worker operates\non its own subset of the data. K eep in mind that each node has to have the capacity to\nsupport the model that's being trained, that is the model has to entirely fit on each\nnode. The following diagram provides a visual demonstration of this approach.Deep learning and distributed training\nData parallelism", "Distributed GPU training guide (SDK v2)\nArticle \u202202/02/2024\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nLearn more about using distributed GPU training code in Azure Machine Learning. This\narticle helps you run your existing distributed training code, and offers tips and\nexamples for you to follow for each framework:\nMessage P assing Interface (MPI)\nHorovod\nEnvironment variables from Open MPI\nPyTorch\nTensorFlow\nAccelerate GPU training with InfiniBand\nReview the basic concepts of distributed GPU training , such as data parallelism ,\ndistributed dat a parallelism , and model p arallelism .\nAzure Machine Learning offers an MPI job  to launch a given number of processes in\neach node. Azure Machine Learning constructs the full MPI launch command ( mpirun)\nbehind the scenes. Y ou can't provide your own full head-node-launcher commands like\nmpirun or DeepSpeed launcher.\nPrerequisites\n\uea80 Tip\nIf you don't know which type of parallelism to use, more than 90% of the time you\nshould use distribut ed data p arallelism .\nMPI\n\uea80 Tip\nThe base Docker image used by an Azure Machine Learning MPI job needs to have\nan MPI library installed. Open MPI  is included in all the Azur e Machine Learning\nGPU b ase images . When you use a custom Docker image, you are responsible\nfor making sure the image includes an MPI library. Open MPI is recommended, but", "Distributed DNN training achieves scalability using a data partitioning algorithm that\nrespects time series boundaries. The following diagram illustrates a simple example with\ntwo partitions:\nDuring training, the DNN data loaders on each compute load just what they need to\ncomplete an iteration of back-propagation; the whole dataset is nev er read int o\nmemor y. The partitions are further distributed across multiple compute cores (usually\nGPUs) on possibly multiple nodes to accelerate training. Coordination across computes\nis provided by the Horovod  framework.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about deep learning models  for forecasting in AutoML\nNext steps", "For more information, see What is automated machine learning? .\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\nLearning can automate this task for arbitrary parameterized commands with little\nmodification to your job definition. R esults are visualized in the studio.\nFor more information, see Tune hyperparameters .\nEfficiency of training for deep learning and sometimes classical machine learning\ntraining jobs can be drastically improved via multinode distributed training. Azure\nMachine Learning compute clusters and serverless compute  offer the latest GPU\noptions.\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\nclusters, and serverless compute :\nPyTorch\nTensorFlow\nMPI\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\nAzure S ynapse Analytics Spark clusters.\nFor more information, see Distributed training with Azure Machine Learning .\nScaling an ML project might require scaling embarrassingly parallel model training. This\npattern is common for scenarios like forecasting demand, where a model might be\ntrained for many stores.\nTo bring a model into production, you deploy the model. The Azure Machine Learning\nmanaged endpoints abstract the required infrastructure for both batch or real-time\n(online) model scoring (inferencing).Hyperparameter optimization\nMultinode distributed training\nEmbarrassingly parallel training\nDeploy models", "Train TensorFlow models at scale with\nAzure Machine Lea rning\nArticle \u202204/04/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this article, learn how to run your TensorFlow  training scripts at scale using Azure\nMachine Learning Python SDK v2.\nThe example code in this article train a T ensorFlow model to classify handwritten digits,\nusing a deep neural network (DNN); register the model; and deploy it to an online\nendpoint.\nWhether you're developing a T ensorFlow model from the ground-up or you're bringing\nan existing model into the cloud, you can use Azure Machine Learning to scale out\nopen-source training jobs using elastic cloud compute resources. Y ou can build, deploy,\nversion, and monitor production-grade models with Azure Machine Learning.\nTo benefit from this article, you'll need to:\nAccess an Azure subscription. If you don't have one already, create a free\naccount .\nRun the code in this article using either an Azure Machine Learning compute\ninstance or your own Jupyter notebook.\nAzure Machine Learning compute instance\u2014no downloads or installation\nnecessary\nComplete the Create resources to get started  to create a dedicated notebook\nserver pre-loaded with the SDK and the sample repository.\nIn the samples deep learning folder on the notebook server, find a\ncompleted and expanded notebook by navigating to this directory: v2 > sdk\n> py thon > jobs > single-st ep > t ensor flow > train-hyperp aramet er-tune-\ndeploy-with-t ensor flow.\nYour Jupyter notebook server\nInstall the Azure Machine Learning SDK (v2) .\nDownload the following files:\ntraining script tf_mnist.py\nscoring script score.py\nsample request file sample-request.json\nPrerequisites"]}
{"question": "How can Azure ML's HyperDrive be used to tune hyperparameters of a deep learning model? Give an example of specifying the search space.", "variant_id": "v1", "line_number": 14, "answer": "Azure ML's HyperDrive can be used to tune hyperparameters of a deep learning model by automating the process of selecting the best hyperparameters for your model. HyperDrive runs multiple training experiments in parallel, each with different hyperparameter configurations. It then compares the performance of each configuration and provides the best performing hyperparameters based on the defined objective metric.\n\nHere's an example of how you can specify the search space for a deep learning model using HyperDrive in Azure Machine Learning:\n\n```python\nfrom azureml.train.hyperdrive import RandomParameterSampling, HyperDriveConfig, PrimaryMetricGoal\nfrom azureml.train.hyperdrive import choice, loguniform\n\n# Define the parameter search space\nparam_sampling = RandomParameterSampling({\n    'learning_rate': loguniform(-6, -1),  # e.g., 0.0001 to 0.1 on a log scale\n    'batch_size': choice(16, 32, 64, 128),  # discrete choices for batch size\n    'momentum': uniform(0.85, 0.95),  # a range for momentum values\n    # Add other hyperparameters you want to tune\n})\n\n# Specify a primary metric to optimize\nprimary_metric_name = 'accuracy'\nprimary_metric_goal = PrimaryMetricGoal.MAXIMIZE\n\n# Create a HyperDriveConfig using the estimator (assuming `estimator` is predefined), parameter sampler, and policy\nhyperdrive_config = HyperDriveConfig(estimator=estimator,\n                                     hyperparameter_sampling=param_sampling,\n                                     policy=None,  # No early termination policy\n                                     primary_metric_name=primary_metric_name,\n                                     primary_metric_goal=primary_metric_goal,\n                                     max_total_runs=100,\n                                     max_concurrent_runs=4)\n\n# Submit the HyperDrive run to the experiment\nhyperdrive_run = experiment.submit(hyperdrive_config)\n```\n\nIn this example, `RandomParameterSampling` is used to define a random search space for the hyperparameters. The `learning_rate` is searched over a log-uniform distribution, `batch_size` is chosen from a set of discrete options, and `momentum` is sampled from a uniform distribution within the specified range. The `HyperDriveConfig` object is then created with the defined search space, primary metric, and other configurations. Finally, the HyperDrive run is submitted to the experiment for execution.\n\nPlease note that the actual code may vary depending on the specific version of the Azure ML SDK and the details of the deep learning framework you are using (e.g., TensorFlow, PyTorch).", "context": ["For more information, see What is automated machine learning? .\nHyperparameter optimization, or hyperparameter tuning, can be a tedious task. Machine\nLearning can automate this task for arbitrary parameterized commands with little\nmodification to your job definition. R esults are visualized in the studio.\nFor more information, see Tune hyperparameters .\nEfficiency of training for deep learning and sometimes classical machine learning\ntraining jobs can be drastically improved via multinode distributed training. Azure\nMachine Learning compute clusters and serverless compute  offer the latest GPU\noptions.\nSupported via Azure Machine Learning Kubernetes, Azure Machine Learning compute\nclusters, and serverless compute :\nPyTorch\nTensorFlow\nMPI\nYou can use MPI distribution for Horovod or custom multinode logic. Apache Spark is\nsupported via serverless Spark compute and attached S ynapse Spark pool  that use\nAzure S ynapse Analytics Spark clusters.\nFor more information, see Distributed training with Azure Machine Learning .\nScaling an ML project might require scaling embarrassingly parallel model training. This\npattern is common for scenarios like forecasting demand, where a model might be\ntrained for many stores.\nTo bring a model into production, you deploy the model. The Azure Machine Learning\nmanaged endpoints abstract the required infrastructure for both batch or real-time\n(online) model scoring (inferencing).Hyperparameter optimization\nMultinode distributed training\nEmbarrassingly parallel training\nDeploy models", "Hyperparameter tuning a model (v2)\nArticle \u202204/04/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nAutomate efficient hyperparameter tuning using Azure Machine Learning SDK v2 and\nCLI v2 by way of the S weepJob type.\n1. Define the parameter search space for your trial\n2. Specify the sampling algorithm for your sweep job\n3. Specify the objective to optimize\n4. Specify early termination policy for low-performing jobs\n5. Define limits for the sweep job\n6. Launch an experiment with the defined configuration\n7. Visualize the training jobs\n8. Select the best configuration for your model\nHyperp aramet ers are adjustable parameters that let you control the model training\nprocess. For example, with neural networks, you decide the number of hidden layers and\nthe number of nodes in each layer. Model performance depends heavily on\nhyperparameters.\nHyperp aramet er tuning , also called hyperp aramet er optimization , is the process of\nfinding the configuration of hyperparameters that results in the best performance. The\nprocess is typically computationally expensive and manual.\nAzure Machine Learning lets you automate hyperparameter tuning and run experiments\nin parallel to efficiently optimize hyperparameters.\nTune hyperparameters by exploring the range of values defined for each\nhyperparameter.\nHyperparameters can be discrete or continuous, and has a distribution of values\ndescribed by a parameter expression .\nWhat is hyperparameter tuning ?\nDefine the search space", "When training computer vision models, model performance depends heavily on the\nhyperparameter values selected. Often, you might want to tune the hyperparameters to\nget optimal performance. For computer vision tasks, you can sweep hyperparameters to\nfind the optimal settings for your model. This feature applies the hyperparameter tuning\ncapabilities in Azure Machine Learning. Learn how to tune hyperparameters .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nYAML\nYou can define the model architectures and hyperparameters to sweep in the parameter\nspace. Y ou can either specify a single model architecture or multiple ones.\nSee Individual trials  for the list of supported model architectures for each task type.\nSee Hyperparameters for computer vision tasks  hyperparameters for each\ncomputer vision task type.Azure CLI\nsearch_space:\n  - model_name:\n      type: choice\n      values: [yolov5]\n    learning_rate:\n      type: uniform\n      min_value:  0.0001\n      max_value:  0.01\n    model_size:\n      type: choice\n      values: [small, medium]\n  - model_name:\n      type: choice\n      values: [fasterrcnn_resnet50_fpn]\n    learning_rate:\n      type: uniform\n      min_value:  0.0001\n      max_value:  0.001\n    optimizer:\n      type: choice\n      values: [sgd, adam, adamw]\n    min_size:\n      type: choice\n      values: [600, 800]\nDefine the parameter search space", "To tune the model's hyperparameters, define the parameter space in which to search\nduring training. Y ou'll do this by replacing some of the parameters ( batch_size,\nfirst_layer_neurons, second_layer_neurons, and learning_rate) passed to the training\njob with special inputs from the azure.ml.sweep package.\nPython\nThen, you'll configure sweep on the command job, using some sweep-specific\nparameters, such as the primary metric to watch and the sampling algorithm to use.\nIn the following code, we use random sampling to try different configuration sets of\nhyperparameters in an attempt to maximize our primary metric, validation_acc.\nWe also define an early termination policy\u2014the BanditPolicy. This policy operates by\nchecking the job every two iterations. If the primary metric, validation_acc, falls outside\nthe top ten percent range, Azure Machine Learning will terminate the job. This saves the\nmodel from continuing to explore hyperparameters that show no promise of helping to\nreach the target metric.\nPythonfrom azure.ai.ml.sweep import Choice, LogUniform  \n# we will reuse the command_job created before. we call it as a function so  \nthat we can apply inputs  \n# we do not apply the 'iris_csv' input again -- we will just use what was  \nalready defined earlier  \njob_for_sweep = job(  \n    batch_size=Choice(values=[ 25, 50, 100]), \n    first_layer_neurons=Choice(values=[ 10, 50, 200, 300, 500]), \n    second_layer_neurons=Choice(values=[ 10, 50, 200, 500]), \n    learning_rate=LogUniform(min_value= -6, max_value= -1), \n)\nfrom azure.ai.ml.sweep import BanditPolicy  \nsweep_job = job_for_sweep.sweep(  \n    compute=gpu_compute_target,  \n    sampling_algorithm= \"random\" , \n    primary_metric= \"Accuracy\" , \n    goal= \"Maximize\" , \n    max_total_trials= 20, \n    max_concurrent_trials= 4, \n    early_termination_policy=BanditPolicy(slack_factor= 0.1, \nevaluation_interval= 2), \n)", "You trained the model with one set of parameters, let's now see if you can further\nimprove the accuracy of your model. Y ou can tune and optimize your model's\nhyperparameters using Azure Machine Learning's sweep  capabilities.\nTo tune the model's hyperparameters, define the parameter space in which to search\nduring training. Y ou do this by replacing some of the parameters passed to the training\njob with special inputs from the azure.ml.sweep package.\nSince the training script uses a learning rate schedule to decay the learning rate every\nseveral epochs, you can tune the initial learning rate and the momentum parameters.\nPython\nThen, you can configure sweep on the command job, using some sweep-specific\nparameters, such as the primary metric to watch and the sampling algorithm to use.\nIn the following code, we use random sampling to try different configuration sets of\nhyperparameters in an attempt to maximize our primary metric, best_val_acc.\nWe also define an early termination policy, the BanditPolicy, to terminate poorly\nperforming runs early. The BanditPolicy terminates any run that doesn't fall within the\nslack factor of our primary evaluation metric. Y ou apply this policy every epoch (since we\nreport our best_val_acc metric every epoch and evaluation_interval=1). Notice we\ndelay the first policy evaluation until after the first 10 epochs ( delay_evaluation=10).\nPythonfrom azure.ai.ml.sweep import Uniform\n# we will reuse the command_job created before. we call it as a function so  \nthat we can apply inputs\njob_for_sweep = job(\n    learning_rate=Uniform(min_value= 0.0005, max_value= 0.005),\n    momentum=Uniform(min_value= 0.9, max_value= 0.99),\n)\nfrom azure.ai.ml.sweep import BanditPolicy\nsweep_job = job_for_sweep.sweep(\n    compute= \"gpu-cluster\" ,\n    sampling_algorithm= \"random\" ,\n    primary_metric= \"best_val_acc\" ,\n    goal=\"Maximize\" ,\n    max_total_trials= 8,\n    max_concurrent_trials= 4,\n    early_termination_policy=BanditPolicy(\n        slack_factor= 0.15, evaluation_interval= 1, delay_evaluation= 10"]}
{"question": "What is the command to update an existing model's properties, such as description or tags, using the Azure CLI?", "variant_id": "v1", "line_number": 15, "answer": "The command to update an existing model's properties, such as description or tags, using the Azure CLI is:\n\n```azurecli\naz ml model update --name run-model-example --version 1 --set description=\"This is an updated description.\" --set tags.stage=\"Prod\"\n```\n\nReplace `run-model-example` with the name of your model, `1` with the version of the model you want to update, and the description and tags with the values you wish to set.", "context": ["Get the details of a specific model:\ncli\nUpdate mutable properties of a specific model:\ncli\nArchiving a model will hide it by default from list queries ( az ml model list). You can\nstill continue to reference and use an archived model in your workflows. Y ou can archive\neither all versions of a model or only a specific version.\nIf you don't specify a version, all versions of the model under that given name will be\narchived. If you create a new model version under an archived model container, that\nnew version will automatically be set as archived as well.\nArchive all versions of a model:Show\nAzure CLI\naz ml model show --name run-model-example --version 1  \nUpdate\nAzure CLI\naz ml model update --name  run-model-example --version 1 --set  \ndescription=\"This is an updated description.\" --set tags.stage=\"Prod\"  \n\uff09 Impor tant\nFor model, only description and tags can be updated. All other properties are\nimmutable; if you need to change any of those properties you should create a new\nversion of the model.\nArchive", "Execute the following command in the Azure CLI, and update the <> placeholders\nwith your data asset name, version and key-value pair for the tag.\nAzure CLI\nTypically, your ETL processes organize your folder structure on Azure storage by time,\nfor example:\ntext\nThe combination of time/version structured folders and Azure Machine Learning T ables\n(MLTable) allow you to construct versioned datasets. T o show how to achieve versioned\ndata with Azure Machine Learning T ables, we use a hypothetical ex ample . Suppose you\nhave a process that uploads camera images to Azure Blob storage every week, in the\nfollowing structure:\ntextaz ml data update --name <DATA ASSET NAME>  --version  <VERSION>  --set \ntags.<KEY>=<VALUE> \nVersioning best practices\n/ \n\u2514\u2500\u2500 \ud83d\udcc1 mydata \n    \u251c\u2500\u2500 \ud83d\udcc1  year=2022  \n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1  month=11  \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4  file1 \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4  file2 \n    \u2502   \u2514\u2500\u2500 \ud83d\udcc1  month=12  \n    \u2502       \u2514\u2500\u2500 \ud83d\udcc4  file1 \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4  file2 \n    \u2514\u2500\u2500 \ud83d\udcc1  year=2023  \n        \u2514\u2500\u2500 \ud83d\udcc1  month=1  \n            \u2514\u2500\u2500 \ud83d\udcc4  file1 \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\udcc4  file2 \n/myimages  \n\u2514\u2500\u2500 \ud83d\udcc1 year=2022  \n    \u251c\u2500\u2500 \ud83d\udcc1  week52 \n    \u2502   \u251c\u2500\u2500 \ud83d\udcc1  camera1  \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\uddbc  file1.jpeg  \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\uddbc  file2.jpeg  \n    \u2502   \u2514\u2500\u2500 \ud83d\udcc1  camera2  \n    \u2502       \u2514\u2500\u2500 \ud83d\uddbc  file1.jpeg  \n    \u2502   \u2502   \u2514\u2500\u2500 \ud83d\uddbc  file2.jpeg", "The az ml model command can be used for managing Azure Machine Learning models.\nExamples are available in the examples GitHub repository . Several are shown below.\nYAML\nYAML\nInstall and use the CLI (v2)Remarks\nExamples\nYAML: local file\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-file-example  \npath: mlflow-model/model.pkl  \ndescription:  Model created from local file. \nYAML: local folder in MLflow format\n$schema:  https://azuremlschemas.azureedge.net/latest/model.schema.json  \nname: local-mlflow-example  \npath: mlflow-model  \ntype: mlflow_model  \ndescription:  Model created from local MLflow model directory.", "If you want to update the code, model, or environment, update the Y AML file, and\nthen run the az ml online-endpoint update command.\nTo understand how update works:\n1. Open the file online/model-1/onlines coring/s core.py.\n2. Change the last line of the init() function: After logging.info(\"Init\ncomplete\"), add logging.info(\"Updated successfully\").\n3. Save the file.\n4. Run this command:\nAzure CLI(Optional) Update the deployment\nAzure CLI\n\uff17 Note\nIf you update instance count (to scale your deployment) along with other\nmodel settings (such as code, model, or environment) in a single update\ncommand, the scaling operation will be performed first, then the other updates\nwill be applied. It's a good practice to perform these operations separately in a\nproduction environment.\naz ml online-deployment update -n blue --endpoint  $ENDPOINT_NAME  -f \nendpoints/online/managed/sample/blue -deployment .yml\n\uff17 Note\nUpdating by using Y AML is declarative. That is, changes in the Y AML are\nreflected in the underlying Azure R esource Manager resources (endpoints\nand deployments). A declarative approach facilitates GitOps : All\nchanges to endpoints and deployments (even instance_count) go\nthrough the Y AML.\n\uea80 Tip", "then select Regist er.\nThe SDK and CLI (v2) also allow you to manage the lifecycle of your Azure Machine\nLearning model assets.\nList all the models in your workspace:\ncli\nList all the model versions under a given name:\ncli\n\uf80a\nManage models\nList\nAzure CLI\naz ml model list  \nAzure CLI\naz ml model list --name run-model-example"]}
{"question": "How does Azure ML integrate with Azure DevOps for implementing CI/CD pipelines for ML models? Describe the steps involved.", "variant_id": "v1", "line_number": 16, "answer": "Azure Machine Learning integrates with Azure DevOps to implement CI/CD pipelines for ML models by allowing you to automate the entire lifecycle of machine learning models from training to deployment and management. Here are the general steps involved in setting up CI/CD pipelines for ML models with Azure ML and Azure DevOps:\n\n1. **Repository Setup**: Start by setting up a repository for your machine learning project in Azure Repos or GitHub. This repository will contain your machine learning code, training scripts, inference scripts, environment configurations, and other necessary files.\n\n2. **Azure ML Workspace**: Create an Azure Machine Learning workspace, which is the foundational block in Azure ML that provides a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n\n3. **Azure Pipelines Configuration**: Configure Azure Pipelines within Azure DevOps. This involves creating a pipeline that references your repository and includes steps to train the model, evaluate it, and, if the evaluation is successful, deploy the model.\n\n4. **Training Pipeline**: Create a training pipeline in Azure ML that can be triggered by Azure Pipelines. This pipeline will run the training script, log metrics, and save the trained model.\n\n5. **Model Evaluation**: After training, evaluate the model against a validation dataset to ensure it meets the required performance criteria.\n\n6. **Model Registration**: If the model passes the evaluation step, register the model in the Azure ML workspace. Model registration allows you to store and version your models in the workspace.\n\n7. **Release Pipeline**: Set up a release pipeline in Azure DevOps that is triggered once the model is registered. This pipeline can include steps to deploy the model to a target environment, such as Azure Kubernetes Service (AKS) or Azure Container Instances (ACI).\n\n8. **Model Deployment**: Deploy the model as a web service to an Azure ML online endpoint or an edge device using Azure ML. This step involves creating an inference configuration that specifies the environment and scripts needed for the web service.\n\n9. **Monitoring and Management**: After deployment, set up monitoring and management for the deployed models. Azure ML provides capabilities to monitor the health, performance, and usage of your models in production.\n\n10. **Feedback Loop**: Implement a feedback loop to continuously improve the model. This can involve retraining the model with new data, tuning hyperparameters, or updating the model based on performance metrics.\n\nBy integrating Azure ML with Azure DevOps, you can automate these steps to create a robust MLOps workflow that ensures consistent quality and efficiency in deploying and managing machine learning models.", "context": ["Continuous integration and deployment of machine learning models with Azure\nPipelines\nMachine Learning ML Ops  repository\nLearn more by reading and exploring the following resources:\nSet up ML Ops with Azure DevOps\nLearning path: End-to-end ML Ops with Azure Machine Learning\nHow to deploy a model to an online endpoint  with Machine Learning\nTutorial: T rain and deploy a model\nCI/CD of machine learning models with Azure Pipelines\nMachine learning at scale\nAzure AI reference architectures and best practices repo\nNext steps", "Some key features enabling ML Ops include:\ngit integration.\nMLflow integration.\nMachine learning pipeline scheduling.\nAzure Event Grid integration for custom triggers.\nEase of use with CI/CD tools like GitHub Actions or Azure DevOps.\nMachine Learning also includes features for monitoring and auditing:\nJob artifacts, such as code snapshots, logs, and other outputs.\nLineage between jobs and assets, such as containers, data, and compute resources.\nIf you use Apache Airflow, the airflow-provider-azure-machinelearning  package is a\nprovider that enables you to submit workflows to Azure Machine Learning from Apache\nAirFlow.\nStart using Azure Machine Learning:\nSet up an Azure Machine Learning workspace\nTutorial: Build a first machine learning project\nRun training jobs\nNext steps", "What are Azu re Machine Learning\npipelines?\nArticle \u202201/31/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAn Azure Machine Learning pipeline is an independently executable workflow of a\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\nstandardize the best practices of producing a machine learning model, enables the team\nto execute at scale, and improves the model building efficiency.\nThe core of a machine learning pipeline is to split a complete machine learning task into\na multistep workflow. Each step is a manageable component that can be developed,\noptimized, configured, and automated individually. S teps are connected through well-\ndefined interfaces. The Azure Machine Learning pipeline service automatically\norchestrates all the dependencies between pipeline steps. This modular approach brings\ntwo key benefits:\nStandardize the Machine learning operation (ML Ops) practice and support scalable\nteam collaboration\nTraining efficiency and cost reduction\nMachine learning operation (ML Ops) automates the process of building machine\nlearning models and taking the model to production. This is a complex process. It\nusually requires collaboration from different teams with different skills. A well-defined\nmachine learning pipeline can abstract this complex process into a multiple steps\nworkflow, mapping each step to a specific task such that each team can work\nindependently.\nFor example, a typical machine learning project includes the steps of data collection,\ndata preparation, model training, model evaluation, and model deployment. Usually, the\ndata engineers concentrate on data steps, data scientists spend most time on model\nWhy are Azure Machine Learning pipelines\nneeded?\nStandardize the MLOps practice and support scalable\nteam collaboration", "What are Azu re Machine Learning\npipelines?\nArticle \u202201/31/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAn Azure Machine Learning pipeline is an independently executable workflow of a\ncomplete machine learning task. An Azure Machine Learning pipeline helps to\nstandardize the best practices of producing a machine learning model, enables the team\nto execute at scale, and improves the model building efficiency.\nThe core of a machine learning pipeline is to split a complete machine learning task into\na multistep workflow. Each step is a manageable component that can be developed,\noptimized, configured, and automated individually. S teps are connected through well-\ndefined interfaces. The Azure Machine Learning pipeline service automatically\norchestrates all the dependencies between pipeline steps. This modular approach brings\ntwo key benefits:\nStandardize the Machine learning operation (ML Ops) practice and support scalable\nteam collaboration\nTraining efficiency and cost reduction\nMachine learning operation (ML Ops) automates the process of building machine\nlearning models and taking the model to production. This is a complex process. It\nusually requires collaboration from different teams with different skills. A well-defined\nmachine learning pipeline can abstract this complex process into a multiple steps\nworkflow, mapping each step to a specific task such that each team can work\nindependently.\nFor example, a typical machine learning project includes the steps of data collection,\ndata preparation, model training, model evaluation, and model deployment. Usually, the\ndata engineers concentrate on data steps, data scientists spend most time on model\nWhy are Azure Machine Learning pipelines\nneeded?\nStandardize the MLOps practice and support scalable\nteam collaboration", "Use Azu re Pipelines with Azure Machine\nLearning\nArticle \u202209/29/2023\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\nof the operations you can automate are:\nData preparation (extract, transform, load operations)\nTraining machine learning models with on-demand scale-out and scale-up\nDeployment of machine learning models as public or private web services\nMonitoring deployed machine learning models (such as for performance or data-\ndrift analysis)\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\nmachine learning model to Azure Machine Learning .\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\nv2.\nComplete the Create resources to get started  to:\nCreate a workspace\nCreate a cloud-based compute cluster  to use for training your model\nAzure Machine Learning extension for Azure Pipelines. This extension can be\ninstalled from the Visual S tudio marketplace at\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-\nv2.\nFork the following repo at GitHub:Prerequisites\nStep 1: Get the code\nhttps://github.com/azure/azureml-examples"]}
{"question": "Explain the process of setting up a real-time inferencing endpoint using a deep learning model in Azure ML. What are the performance tuning options available?", "variant_id": "v1", "line_number": 17, "answer": "To set up a real-time inferencing endpoint using a deep learning model in Azure Machine Learning, you would typically follow these steps:\n\n1. **Prepare Your Model**: Make sure your deep learning model is trained and ready for deployment. It should be saved in a format that Azure ML can work with, such as ONNX or a framework-specific format like TensorFlow's SavedModel or PyTorch's state dictionary.\n\n2. **Register the Model**: Upload and register your model in the Azure ML workspace. This allows you to manage and version your model.\n\n3. **Define the Scoring Script**: Create a scoring script (entry script) that loads the model and defines how the model will process incoming requests. This script should include two main functions: `init()` to load the model and `run(data)` to run the model on the input data.\n\n4. **Create an Environment**: Define an environment that specifies all the Python packages and dependencies required by your model and scoring script. This can be done using a Conda environment file or a Docker image.\n\n5. **Configure the Endpoint**: Define the endpoint by specifying the endpoint name and the authentication mode (key-based or token-based). You may also need to configure other settings such as compute type (CPU/GPU), instance count, and instance type depending on the load and performance requirements.\n\n6. **Deploy the Model**: Deploy the model to the endpoint. Azure ML will set up a web service that can receive HTTP requests, load the model, and return predictions.\n\n7. **Test the Endpoint**: Once the endpoint is deployed, test it by sending HTTP requests with input data and verifying that you receive the expected predictions.\n\nFor performance tuning, Azure Machine Learning offers several options:\n\n- **Instance Type Selection**: Choose the right virtual machine size or compute cluster that matches your model's resource requirements (CPU/GPU, memory).\n\n- **Scaling**: Configure the endpoint to automatically scale the number of instances based on the load. This helps in handling varying numbers of inference requests.\n\n- **Batch Inference**: If real-time requirements are flexible, batch processing can be used to process multiple requests at once, which can be more efficient.\n\n- **Optimized Model Formats**: Convert your model to an optimized format like ONNX, which can provide faster inferencing times.\n\n- **Concurrency**: Adjust the concurrency level to define how many requests can be processed in parallel by a single instance.\n\n- **Caching**: Implement caching for frequent requests to reduce latency and compute overhead.\n\n- **Monitoring and Diagnostics**: Use Azure ML's built-in monitoring and diagnostics to identify bottlenecks and performance issues, which can then be addressed through tuning.\n\nRemember to follow best practices for security, monitoring, and maintaining your endpoints to ensure they remain reliable and performant.", "context": ["Online endpoints and deployments for\nreal-time inference\nArticle \u202210/24/2023\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nAzure Machine Learning allows you to perform real-time inferencing on data by using\nmodels that are deployed to online endpoints . Inferencing is the process of applying new\ninput data to a machine learning model to generate outputs. While these outputs are\ntypically referred to as \"predictions,\" inferencing can be used to generate outputs for\nother machine learning tasks, such as classification and clustering.\nOnline endpoints  deploy models to a web server that can return predictions under the\nHTTP protocol. Use online endpoints to operationalize models for real-time inference in\nsynchronous low-latency requests. W e recommend using them when:\nTo define an endpoint, you need to specify:\nEndpoint name : This name must be unique in the Azure region. For more\ninformation on the naming rules, see endpoint limits .\nAuthentication mode : You can choose between key-based authentication mode\nand Azure Machine Learning token-based authentication mode for the endpoint. A\nkey doesn't expire, but a token does expire. For more information on\nauthenticating, see Authenticate to an online endpoint .\nAzure Machine Learning provides the convenience of using managed online endpoints\nfor deploying your ML models in a turnkey manner. This is the recommended  way to use\nonline endpoints in Azure Machine Learning. Managed online endpoints work with\npowerful CPU and GPU machines in Azure in a scalable, fully managed way. These\nendpoints also take care of serving, scaling, securing, and monitoring your models, to\nfree you from the overhead of setting up and managing the underlying infrastructure. T o\nOnline  endpoints\nYou have low-latency requirements\uff02\nYour model can answer the request in a relatively short amount of time\uff02\nYour model's inputs fit on the HT TP payload of the request \uff02\nYou need to scale up in terms of number of requests\uff02", "In this article, you trained and registered a T ensorFlow model. Y ou also deployed the\nmodel to an online endpoint. See these other articles to learn more about Azure\nMachine Learning.\nTrack run metrics during training\nTune hyperparameters\nReference architecture for distributed deep learning training in AzureNext steps", "Batch scoring, or batch infer encing , involves invoking an endpoint with a reference to\ndata. The batch endpoint runs jobs asynchronously to process data in parallel on\ncompute clusters and store the data for further analysis.\nReal-time s coring, or online infer encing , involves invoking an endpoint with one or more\nmodel deployments and receiving a response in near real time via HT TPS. T raffic can be\nsplit across multiple deployments, allowing for testing new model versions by diverting\nsome amount of traffic initially and increasing after confidence in the new model is\nestablished.\nFor more information, see:\nDeploy a model with a real-time managed endpoint\nUse batch endpoints for scoring\nDevOps for ML models, often called ML Ops, is a process for developing models for\nproduction. A model's lifecycle from training to deployment must be auditable if not\nreproducible.\nLearn more about MLOps in Azure Machine Learning .\nMachine Learning is built with the model lifecycle in mind. Y ou can audit the model\nlifecycle down to a specific commit and environment.Real-time and batch scoring (inferencing)\nMLOps: DevOps for machine learning\nML model lifecycle\nIntegrations enabling MLOPs", "What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps", "Currently, Azure Machine Learning supports fine-tuning models for the following\nlanguage tasks:\nText classification\nToken classification\nQuestion answering\nSummarization\nTranslation\nTo enable users to quickly get started with fine-tuning, we have published samples (both\nPython notebooks and CLI examples) for each task in the azureml-examples git repo\nFinetune samples . Each model card also links to fine-tuning samples for supported\nfine-tuning tasks.\nYou can deploy foundation models (both pre-trained models from the model catalog,\nand fine-tuned models, once they're registered to your workspace) to an endpoint that\ncan then be used for inferencing. Deployment to both real time endpoints and batch\nendpoints is supported. Y ou can deploy these models by using either the Deploy UI\nwizard or by using the code based samples linked from the model card.\nYou can invoke the Deploy UI form by selecting the Deploy  button on the model card\nfor any foundation model, and selecting either R eal-time endpoint or Batch endpoint\nDeploying foundation models to endpoints for\ninferencing\nDeploying using the studio\n\uf80a\nDeployment settings"]}
{"question": "How can you automate the process of model retraining and deployment using Azure ML Pipelines?", "variant_id": "v1", "line_number": 18, "answer": "To automate the process of model retraining and deployment using Azure ML Pipelines, you can follow these general steps:\n\n1. **Create an Azure ML Workspace**: This is the foundational block for any machine learning operations in Azure. It provides a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n\n2. **Set up a Compute Target**: Create a cloud-based compute cluster that can scale out for training your model. This is where your model will be trained.\n\n3. **Prepare Your Training Script**: Write a training script in Python that defines how your model should be trained. This script is run during the pipeline execution.\n\n4. **Define a Pipeline**: Use the Azure Machine Learning Python SDK v2 to define a pipeline. A pipeline is a workflow of a complete machine learning task. Subtasks are encapsulated as a series of steps within the pipeline.\n\n5. **Create a Pipeline Step for Training**: Define a step in your pipeline to train the model. This step will use the compute resources you've set up to run the training script.\n\n6. **Create a Pipeline Step for Model Registration**: After the model is trained, create another step that registers the model to your Azure ML Workspace. Model registration allows you to store and version your trained models in your workspace.\n\n7. **Create a Pipeline Step for Deployment**: Define a step to deploy the trained model as a web service on Azure. You can deploy the model to either Azure Kubernetes Service (AKS) for high-scale production deployments or Azure Container Instances (ACI) for low-scale testing and development.\n\n8. **Set up Continuous Integration and Delivery (CI/CD)**: Use Azure DevOps Services to set up CI/CD pipelines that trigger the Azure ML Pipeline. You can automate the entire process from code commit to model deployment.\n\n9. **Monitor the Deployed Model**: Implement monitoring for the deployed model to track its performance and data drift. This can be done using Azure ML's monitoring capabilities.\n\n10. **Install Azure Machine Learning Extension for Azure Pipelines**: To integrate Azure Pipelines with Azure Machine Learning, install the Azure Machine Learning extension from the Visual Studio Marketplace.\n\n11. **Fork and Clone the Necessary Code**: Fork and clone the example repository provided in the Azure ML documentation to get started with predefined templates and scripts.\n\nBy following these steps, you can create an end-to-end automated workflow for retraining and deploying machine learning models using Azure ML Pipelines. This allows for consistent, repeatable processes that can be triggered on a schedule or in response to data changes, ensuring that your models are always up-to-date and performing optimally.", "context": ["Use Azu re Pipelines with Azure Machine\nLearning\nArticle \u202209/29/2023\nAzur e DevOps Ser vices | Azur e DevOps Ser ver 2022 - Azur e DevOps Ser ver 2019\nYou can use an Azure DevOps pipeline  to automate the machine learning lifecycle. Some\nof the operations you can automate are:\nData preparation (extract, transform, load operations)\nTraining machine learning models with on-demand scale-out and scale-up\nDeployment of machine learning models as public or private web services\nMonitoring deployed machine learning models (such as for performance or data-\ndrift analysis)\nThis article teaches you how to create an Azure Pipeline that builds and deploys a\nmachine learning model to Azure Machine Learning .\nThis tutorial uses Azure Machine Learning Python SDK v2  and Azure CLI ML extension\nv2.\nComplete the Create resources to get started  to:\nCreate a workspace\nCreate a cloud-based compute cluster  to use for training your model\nAzure Machine Learning extension for Azure Pipelines. This extension can be\ninstalled from the Visual S tudio marketplace at\nhttps://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.azureml-\nv2.\nFork the following repo at GitHub:Prerequisites\nStep 1: Get the code\nhttps://github.com/azure/azureml-examples", "Continuous integration and deployment of machine learning models with Azure\nPipelines\nMachine Learning ML Ops  repository\nLearn more by reading and exploring the following resources:\nSet up ML Ops with Azure DevOps\nLearning path: End-to-end ML Ops with Azure Machine Learning\nHow to deploy a model to an online endpoint  with Machine Learning\nTutorial: T rain and deploy a model\nCI/CD of machine learning models with Azure Pipelines\nMachine learning at scale\nAzure AI reference architectures and best practices repo\nNext steps", "6. Select main as a branch and choose /mlops/devops-pipelines/deploy-model-\ntraining-pipeline.yml, then select Continue .\n7. Save and Run  the pipeline\nThis scenario includes prebuilt workflows for two approaches to deploying a trained\nmodel, batch scoring or a deploying a model to an endpoint for real-time scoring. Y ou\nmay run either or both of these workflows to test the performance of the model in your\nAzure ML workspace. IN this example we will be using real-time scoring.\n1. Go to ADO pipelines\n2. Select New Pipeline .\n3. Select Azur e Repos Git .\uff17 Note\nAt this point, the infrastructure is configured and the Prototyping Loop of the\nMLOps Architecture is deployed. you're ready to move to our trained model to\nproduction.\nDeploying the Trained model\nDeploy ML model endpoint", "3. Specif y the sour ce of the labeled training data : You can bring your data to Azure\nMachine Learning in many different ways .\n4. Configur e the aut omat ed machine learning p aramet ers that determine how many\niterations over different models, hyperparameter settings, advanced\npreprocessing/featurization, and what metrics to look at when determining the\nbest model.\n5. Submit the training job.\n6. Review the r esults\nThe following diagram illustrates this process. \nYou can also inspect the logged job information, which contains metrics  gathered\nduring the job. The training job produces a Python serialized object ( .pkl file) that\ncontains the model and data preprocessing.\nWhile model building is automated, you can also learn how important or relevant\nfeatures are  to the generated models.\nApply automated ML when you want Azure Machine Learning to train and tune a model\nfor you using the target metric you specify. Automated ML democratizes the machine\nlearning model development process, and empowers its users, no matter their data\nscience expertise, to identify an end-to-end machine learning pipeline for any problem.\nML professionals and developers across industries can use automated ML to:\nImplement ML solutions without extensive programming knowledgeWhen to use AutoML: classification, regression,\nforecasting, computer vision & NLP", "4. When the pipeline has complete successfully, you can find your Azure Machine\nLearning W orkspace and associated resources by logging in to the Azure P ortal.\nNext, a model training and scoring pipelines will be deployed into the new\nMachine Learning environment.\nThe solution accelerator includes code and data for a sample end-to-end machine\nlearning pipeline which runs a linear regression to predict taxi fares in NY C. The pipeline\nis made up of components, each serving different functions, which can be registered\nwith the workspace, versioned, and reused with various inputs and outputs. Sample\npipelines and workflows for the Computer Vision and NLP scenarios will have different\nsteps and deployment steps.\nThis training pipeline contains the following steps:\nPrepare Data\nThis component takes multiple taxi datasets (yellow and green) and merges/filters\nthe data, and prepare the train/val and evaluation datasets.\nInput: Local data under ./data/ (multiple .csv files)\nOutput: Single prepared dataset (.csv) and train/val/test datasets.\nTrain Model\nThis component trains a Linear R egressor with the training set.\nInput: T raining dataset\nOutput: T rained model (pickle format)\nEvaluat e Model\nThis component uses the trained model to predict taxi fares on the test set.\nInput: ML model and T est dataset\nOutput: P erformance of model and a deploy flag whether to deploy or not.Sample Training  and Deployment Scenario"]}
{"question": "Describe the approach to use Azure ML for implementing a recommendation system. What kind of data and algorithms are typically involved?", "variant_id": "v1", "line_number": 19, "answer": "To implement a recommendation system using Azure Machine Learning (Azure ML), you would typically follow these steps:\n\n1. **Data Upload and Exploration**:\n   - You would start by uploading your data to Azure ML. This data could be user-item interactions, user or item features, or a combination of both.\n   - You would then explore and preprocess your data using Azure ML notebooks or scripts. This might involve normalizing data, handling missing values, and encoding categorical variables.\n\n2. **Model Development**:\n   - On a cloud workstation provided by Azure ML, you would prototype and develop your recommendation model. Azure ML supports various machine learning frameworks and libraries, such as TensorFlow, PyTorch, and scikit-learn, which you can use to build your model.\n\n3. **Model Training**:\n   - You would train your recommendation model using Azure ML's training capabilities. This involves selecting an algorithm suitable for recommendation systems, such as collaborative filtering, content-based filtering, or hybrid methods.\n   - Azure ML allows you to train models on various scales of compute resources, from a single VM to a cluster of VMs for larger datasets.\n\n4. **Model Deployment**:\n   - Once the model is trained and evaluated, you can deploy it as an online endpoint using Azure ML. This endpoint can then serve real-time recommendations to users.\n\n5. **Machine Learning Pipelines**:\n   - For production systems, you would create machine learning pipelines in Azure ML. These pipelines would automate the workflow of data preprocessing, model training, evaluation, and deployment.\n\n**Data Involved**:\n- User-item interaction data, which could be explicit (e.g., ratings) or implicit (e.g., clicks, purchases).\n- User demographic data or profile information.\n- Item attributes or content information.\n\n**Algorithms Typically Involved**:\n- **Collaborative Filtering**: Algorithms like matrix factorization (e.g., SVD, ALS) that make recommendations based on user-item interactions.\n- **Content-Based Filtering**: Algorithms that use item features to recommend items similar to what the user likes.\n- **Hybrid Methods**: Combining collaborative and content-based approaches to leverage both interaction data and item/user features.\n- **Deep Learning**: Neural network architectures like autoencoders or RNNs for more complex recommendation systems.\n\nAzure ML provides a robust environment to manage the entire lifecycle of a recommendation system, from data handling to model deployment and management.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Tell us about y our PDF experience.\nAzur e Machine Learning documentation\nLearn how to train and deploy models and manage the ML lifecycle (ML Ops) with Azure\nMachine Learning. Tutorials, code examples, API references, and more.\nOverview\n\uff45OVERVIE W\nWhat is Azure Machine Learning?\nSetup & quickstar t\n\uff46QUICKST ART\nCreate resources\nGet started with Azure Machine Learning\nStart with the b asics\n\uff47TUTORIAL\nPrepare and explore data\nDevelop on a cloud workstation\nTrain a model\nDeploy a model\nSet up a reusable pipeline\nWork with data\n\uff43HOW-T O GUIDE\nUse Apache Spark in Azure Machine Learning\nCreate data assets", "What is Azure Machine Learning?\nArticle \u202201/29/2024\nAzure Machine Learning is a cloud service for accelerating and managing the machine\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\nin their day-to-day workflows to train and deploy models and manage machine learning\noperations (ML Ops).\nYou can create a model in Machine Learning or use a model built from an open-source\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\nretrain, and redeploy models.\nMachine Learning is for individuals and teams implementing ML Ops within their\norganization to bring ML models into production in a secure and auditable production\nenvironment.\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\nday workflows. Application developers can use tools for integrating models into\napplications or services. Platform developers can use a robust set of tools, backed by\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\nbased access control for infrastructure. Y ou can set up a project to deny access to\nprotected data and select operations.\nML projects often require a team with a varied skill set to build and maintain. Machine\nLearning has tools that help enable you to:\uea80 Tip\nFree trial!  If you don't have an Azure subscription, create a free account before you\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\nto spend on Azure services. After they're used up, you can keep the account and\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\nchange your settings and ask to be charged.\nWho is Azure Machine Learning for?\nProductivity for everyone on the team", "If you aren't familiar with Azure Machine Learning concepts, start with the What is Azure\nMachine Learning?  and What is an Azure Machine Learning workspace?  articles.\nLearn more about how to set up AutoML to train a time-series forecasting model .\nLearn about calendar features for time series forecasting in AutoML .\nLearn about how AutoML uses machine learning to build forecasting models .\nLearn about AutoML forecasting for lagged features .Next steps", "For training in developing AI and Machine Learning solutions with Microsoft, see\nMicrosoft Learn training .\nChoose a Microsoft cognitive services technology\nArtificial intelligence (AI) architecture design\nHow Azure Machine Learning works: resources and assetsRelated resources"]}
{"question": "How can you leverage Azure ML's built-in support for ONNX models for interoperability across different frameworks?", "variant_id": "v1", "line_number": 20, "answer": "Azure Machine Learning (Azure ML) provides built-in support for ONNX (Open Neural Network Exchange) models, which facilitates interoperability across different machine learning frameworks. Here's how you can leverage this support:\n\n1. **Model Conversion to ONNX**: You can convert models from various frameworks such as TensorFlow, PyTorch, Scikit-Learn, Keras, and others into the ONNX format. Azure ML supports the conversion process through its integration with the ONNX converters available for these frameworks.\n\n2. **Model Deployment**: Once you have your model in ONNX format, you can deploy it using Azure ML services. Azure ML allows you to deploy models as web services on Azure Container Instances (ACI), Azure Kubernetes Service (AKS), or on IoT devices and edge computing platforms.\n\n3. **Inference Optimization**: Azure ML leverages ONNX Runtime for optimized model inference. ONNX Runtime is an inference engine that provides high performance across a variety of platforms and hardware, including cloud and edge devices. It supports integration with hardware-specific accelerators like NVIDIA TensorRT, Intel OpenVINO, and DirectML for Windows.\n\n4. **MLOps**: Azure ML supports MLOps practices, allowing you to manage, version, and monitor ONNX models just like any other model format. This includes the ability to track model performance, data drift, and retraining pipelines.\n\n5. **Scalability and High Availability**: By deploying ONNX models on Azure ML, you can take advantage of Azure's infrastructure for scalability and high availability. This is crucial for production-grade machine learning workloads.\n\n6. **Integration with Azure Services**: ONNX models deployed on Azure ML can easily integrate with other Azure services such as Azure Functions, Azure Logic Apps, and Azure IoT Edge, allowing for seamless workflows and data processing pipelines.\n\nBy using Azure ML's support for ONNX, you can train your model in your preferred framework, convert it to ONNX format, and then deploy and manage it efficiently, while also ensuring compatibility and performance across different platforms and devices.", "context": ["ONNX and Azure Machine Learning:\nCreate and accelerate ML models\nArticle \u202206/13/2023\nLearn how using the Open Neural Network Exchange  (ONNX) can help optimize the\ninference of your machine learning model. Inference, or model scoring, is the phase\nwhere the deployed model is used for prediction, most commonly on production data.\nOptimizing machine learning models for inference (or model scoring) is difficult since\nyou need to tune the model and the inference library to make the most of the hardware\ncapabilities. The problem becomes extremely hard if you want to get optimal\nperformance on different kinds of platforms (cloud/edge, CPU/GPU, etc.), since each one\nhas different capabilities and characteristics. The complexity increases if you have\nmodels from a variety of frameworks that need to run on a variety of platforms. It's very\ntime consuming to optimize all the different combinations of frameworks and hardware.\nA solution to train once in your preferred framework and run anywhere on the cloud or\nedge is needed. This is where ONNX comes in.\nMicrosoft and a community of partners created ONNX as an open standard for\nrepresenting machine learning models. Models from many frameworks  including\nTensorFlow, PyT orch, SciKit-Learn, K eras, Chainer, MXNet, MA TLAB, and SparkML can be\nexported or converted to the standard ONNX format. Once the models are in the ONNX\nformat, they can be run on a variety of platforms and devices.\nONNX Runtime  is a high-performance inference engine for deploying ONNX models\nto production. It's optimized for both cloud and edge and works on Linux, Windows,\nand Mac. Written in C++, it also has C, Python, C#, Java, and JavaScript (Node.js) APIs\nfor usage in a variety of environments. ONNX Runtime supports both DNN and\ntraditional ML models and integrates with accelerators on different hardware such as\nTensorR T on NVidia GPUs, OpenVINO on Intel processors, DirectML on Windows, and\nmore. By using ONNX Runtime, you can benefit from the extensive production-grade\noptimizations, testing, and ongoing improvements.\nONNX Runtime is used in high-scale Microsoft services such as Bing, Office, and Azure\nAI. Performance gains are dependent on a number of factors, but these Microsoft\nservices have seen an average 2x per formance gain on CPU . In addition to Azure\nMachine Learning services, ONNX Runtime also runs in other products that support\nMachine Learning workloads, including:\nWindows: The runtime is built into Windows as part of Windows Machine Learning\nand runs on hundreds of millions of devices.", "Azure SQL product family: Run native scoring on data in Azure SQL Edge  and\nAzure SQL Managed Instance .\nML.NET: Run ONNX models in ML.NET .\nYou can obtain ONNX models in several ways:\nTrain a new ONNX model in Azure Machine Learning (see examples at the bottom\nof this article) or by using automated Machine Learning capabilities\nConvert existing model from another format to ONNX (see the tutorials )\nGet a pre-trained ONNX model from the ONNX Model Zoo\nGenerate a customized ONNX model from Azure Custom Vision service\nMany models including image classification, object detection, and text processing can\nbe represented as ONNX models. If you run into an issue with a model that cannot be\nconverted successfully, please file an issue in the GitHub of the respective converter that\nyou used. Y ou can continue using your existing format model until the issue is\naddressed.\nWith Azure Machine Learning, you can deploy, manage, and monitor your ONNX\nmodels. Using the standard deployment workflow  and ONNX Runtime, you can create a\nREST endpoint hosted in the cloud. See example Jupyter notebooks at the end of this\narticle to try it out for yourself.\n\uf80a\nGet ONNX models\nDeploy ONNX models in Azure\nInstall and use ONNX Runtime with Python", "See the AutoML package  for changing default ensemble settings in automated machine\nlearning.\nWith Azure Machine Learning, you can use automated ML to build a Python model and\nhave it converted to the ONNX format. Once the models are in the ONNX format, they\ncan be run on a variety of platforms and devices. Learn more about accelerating ML\nmodels with ONNX .\nSee how to convert to ONNX format in this Jupyter notebook example . Learn which\nalgorithms are supported in ONNX .\nThe ONNX runtime also supports C#, so you can use the model built automatically in\nyour C# apps without any need for recoding or any of the network latencies that REST\nendpoints introduce. Learn more about using an AutoML ONNX model in a .NET\napplication with ML.NET  and inferencing ONNX models with the ONNX runtime C#\nAPI .\nThere are multiple resources to get you up and running with AutoML.\nTutorials are end-to-end introductory examples of AutoML scenarios.\nFor a code fir st experience , follow the Tutorial: T rain an object detection model\nwith AutoML and Python\nFor a low or no-code experience , see the Tutorial: T rain a classification model with\nno-code AutoML in Azure Machine Learning studio .\nHow-to articles provide additional detail into what functionality automated ML offers.\nFor example,\nConfigure the settings for automatic training experiments\nWithout code in the Azure Machine Learning studio .\nWith the Python SDK .\nLearn how to train computer vision models with Python .\nLearn how to view the generated code from your automated ML models (SDK v1) .AutoML & ONNX\nNext steps\nTutorials/ how-tos", "Make predictions with ONNX on\ncomputer vision models from AutoML\nArticle \u202204/04/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this article, you will learn how to use Open Neural Network Exchange (ONNX) to\nmake predictions on computer vision models generated from automated machine\nlearning (AutoML) in Azure Machine Learning.\nTo use ONNX for predictions, you need to:\n1. Download ONNX model files from an AutoML training run.\n2. Understand the inputs and outputs of an ONNX model.\n3. Preprocess your data so that it's in the required format for input images.\n4. Perform inference with ONNX Runtime for Python.\n5. Visualize predictions for object detection and instance segmentation tasks.\nONNX  is an open standard for machine learning and deep learning models. It enables\nmodel import and export (interoperability) across the popular AI frameworks. For more\ndetails, explore the ONNX GitHub project .\nONNX Runtime  is an open-source project that supports cross-platform inference.\nONNX Runtime provides APIs across programming languages (including Python, C++,\nC#, C, Java, and JavaScript). Y ou can use these APIs to perform inference on input\nimages. After you have the model that has been exported to ONNX format, you can use\nthese APIs on any programming language that your project needs.\nIn this guide, you'll learn how to use Python APIs for ONNX Runtime  to make\npredictions on images for popular vision tasks. Y ou can use these ONNX exported\nmodels across languages.\nGet an AutoML-trained computer vision model for any of the supported image\ntasks: classification, object detection, or instance segmentation. Learn more about\nAutoML support for computer vision tasks .\nInstall the onnxruntime  package. The methods in this article have been tested\nwith versions 1.3.0 to 1.8.0.\nPrerequisites", "runs from experimentation in the studio user interface. Or they can use versioned assets\nfor jobs like environments and storage references.\nFor more information, see Manage Azure Machine Learning workspaces .\nWhen a project is ready for operationalization, users' work can be automated in an ML\npipeline and triggered on a schedule or HT TPS request.\nYou can deploy models to the managed inferencing solution, for both real-time and\nbatch deployments, abstracting away the infrastructure management typically required\nfor deploying models.\nIn Azure Machine Learning, you can run your training script in the cloud or build a\nmodel from scratch. Customers often bring models they've built and trained in open-\nsource frameworks so that they can operationalize them in the cloud.\nData scientists can use models in Azure Machine Learning that they've created in\ncommon Python frameworks, such as:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nLightGBM\nOther languages and frameworks are also supported:\nR\n.NET\nFor more information, see Open-source integration with Azure Machine Learning .\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\nexperience and intuition to select the right data featurization and algorithm for training.\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\nLearning studio UI or the Python SDK.Train models\nOpen and interoperable\nAutomated featurization and algorithm selection"]}
{"question": "What Azure storage services can be integrated with Azure Machine Learning for storing datasets?", "variant_id": "v1", "line_number": 21, "answer": "Azure Machine Learning can be integrated with several Azure storage services for storing datasets. These include:\n\n1. Azure Blob Storage: This is a scalable, object storage solution for unstructured data. It is often used to store large amounts of data for machine learning workloads.\n\n2. Azure Data Lake Storage: This storage service is designed for big data analytics and is optimized for analytics workloads. It provides a hierarchical file system and is suitable for storing large datasets used in machine learning.\n\n3. Azure File Storage: This service offers shared storage for applications using the standard SMB protocol. It can be used for smaller datasets or when a traditional file system structure is required.\n\n4. Azure SQL Database: For structured data, Azure SQL Database can be used to store and retrieve data via SQL queries, which can then be used for machine learning purposes.\n\nThese storage services can be accessed from within Azure Machine Learning notebooks, scripts, and pipelines to store, retrieve, and manage datasets used for training and deploying machine learning models.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Service Usage Example datanumber of nodes; datastore names and tags, and\ndescriptions on assets like models; and data label names.\nAzure AI\nSearchStores indexes that\nhelp with querying\nyour machine learning\ncontent.These indexes are built on top of the data stored in Azure\nCosmos DB.\nAzure\nStorageStores metadata\nrelated to Azure\nMachine Learning\npipeline data.Data can include designer pipeline names, pipeline layout,\nand execution properties.\nFrom the perspective of data lifecycle management, data in the preceding resources is\ncreated and deleted as you create and delete corresponding objects in Azure Machine\nLearning.\nYour Azure Machine Learning workspace reads and writes data by using its managed\nidentity. This identity is granted access to the resources through a role assignment\n(Azure role-based access control) on the data resources. The encryption key that you\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\nused to create indexes for Azure AI Search at runtime.\nExtra networking controls are configured when you create a private link endpoint on\nyour workspace to allow for inbound connectivity. This configuration includes the\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\nNetwork access is restricted to only trusted Microsoft services.\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\nthe data.\nWhen you use a customer-managed key, the resources are in your Azure subscription\nand encrypted with your key. While these resources exist in your subscription, Microsoft\nmanages them. They're automatically created and configured when you create your\nAzure Machine Learning workspace.\nThese Microsoft-managed resources are located in a new Azure resource group that's\ncreated in your subscription. This resource group is separate from the resource group\nfor your workspace. It contains the Microsoft-managed resources that your key is used\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\nresource group name><GUID>.Customer-managed keys"]}
{"question": "How do you authenticate Azure Machine Learning with Azure Blob Storage to access datasets?", "variant_id": "v1", "line_number": 22, "answer": "To authenticate Azure Machine Learning with Azure Blob Storage for accessing datasets, you can use identity-based data access by connecting to an Azure Machine Learning datastore. Here are the steps to set up identity-based authentication:\n\n1. **Managed Identity**: Ensure that the Azure Machine Learning workspace or the specific compute resource (like a compute instance or compute cluster) is assigned a managed identity. A managed identity allows Azure services to authenticate to cloud services without storing credentials in code.\n\n2. **Assign Role**: Assign the appropriate role to the managed identity for the Azure Blob Storage account. At a minimum, you would need to assign the \"Storage Blob Data Reader\" role to the managed identity to read datasets from the blob storage.\n\n3. **Create Datastore**: In Azure Machine Learning, create a datastore that references the Azure Blob Storage. When creating the datastore, select the option to use the managed identity for authentication.\n\n4. **Access Data**: When you access data in your training jobs or Jupyter Notebooks, Azure Machine Learning will use the managed identity to authenticate against the Azure Blob Storage, and you won't need to provide any credentials.\n\nBy following these steps, you can securely connect to Azure Blob Storage from Azure Machine Learning without storing credentials in the datastore, and manage data access at the storage level.", "context": ["You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.", "means that in order to successfully read data from storage, the managed identity of the\ncompute cluster where the deployment is running must have at least Storage Blob Data\nReader  access to the storage account.\nTo configure the compute cluster for data access, follow these steps:\n1. Go to Azure Machine Learning studio .\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\ndeployment is using.\n3. Assign a managed identity to the compute cluster:\na. In the Managed identity  section, verify if the compute has a managed identity\nassigned. If not, select the option Edit.\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\nusing a S ystem-Assigned Managed Identity, it is named as \"[workspace\nname]/computes/[compute cluster name]\".\nc. Save the changes.\n4. Go to the Azure portal  and navigate to the associated storage account where the\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\nstorage account where those assets are placed.", "When you use an Azure S torage Account from Azure Machine Learning studio, you must\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\naccount:\nBlob Data R eader\nIf the storage account uses a private endpoint to connect to the VNet, you must\ngrant the Reader  role for the storage account private endpoint to the managed\nidentity.\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\nNetwork .\nThe following sections explain the limitations of using an Azure S torage Account, with\nyour workspace, in a VNet.\nTo secure communication between Azure Machine Learning and Azure S torage\nAccounts, configure the storage to Grant access to trusted Azure services .\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\ncan normally be used to allow your client to directly connect over the internet. However,\nwhen using studio, your client doesn't connect to the storage account. The Azure\nMachine Learning service that makes the request connects to the storage account. The\nIP address of the service isn't documented, and it changes frequently. Enabling the\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\nconfiguration .\uea80 Tip\nTo access data from outside Azure Machine Learning, for example with Azure\nStorage Explorer, that access probably relies on the user identity. For specific\ninformation, review the documentation for the tool or service you're using. For\nmore information about how Azure Machine Learning works with data, see Setup\nauthentication betw een Azur e Machine Learning and other ser vices .\nAzure Storage Account\nSecure communication with Azure Storage Account\nAzure Storage firewall", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."]}
{"question": "What steps are involved in uploading a dataset to Azure Blob Storage and then accessing it from an Azure Machine Learning experiment?", "variant_id": "v1", "line_number": 23, "answer": "To upload a dataset to Azure Blob Storage and then access it from an Azure Machine Learning experiment, you can follow these steps:\n\n1. **Upload Data to Azure Blob Storage:**\n   - Use Azure Storage Explorer to transfer your data to Azure Blob Storage. This is recommended for a large amount of data due to its robustness and speed.\n   - Alternatively, you can upload files directly through the Azure portal or programmatically using Azure Storage SDKs.\n\n2. **Create a Dataset in Azure Machine Learning:**\n   - In the Azure Machine Learning Studio, select **Create** to create a new dataset.\n   - Enter a **Name** for your dataset and, optionally, a description.\n   - Choose the **Dataset type**. Select **Tabular** if you're using a .csv or .tsv file where each row contains a response. Select **File** if you're using separate .txt files for each response.\n   - Select **Next**.\n\n3. **Specify Datastore and Path:**\n   - Select **From Azure storage**, then select **Next**.\n   - Choose the datastore that points to your Azure Blob Storage.\n   - If your data is in a subfolder within Blob Storage, use **Browse** to select the path. To include all files in subfolders, append `/**` to the path. To include all data in the current container and its subfolders, append `**/*.*` to the path.\n   - Select **Next**.\n\n4. **Finalize Dataset Creation:**\n   - Review your settings and then select **Create** to finalize the creation of the dataset.\n\n5. **Access the Dataset in an Experiment:**\n   - In your Azure Machine Learning experiment, access the dataset by referencing the dataset name you created.\n   - Use the Azure Machine Learning SDK to load the dataset into your experiment code. For example, you can use `Dataset.get_by_name(workspace, name='your_dataset_name')` to retrieve the dataset in your experiment script.\n\nBy following these steps, you will have successfully uploaded your data to Azure Blob Storage, created a dataset in Azure Machine Learning, and accessed it from an experiment.", "context": ["In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data", "2. Select your subscription and the workspace you created.\n3. Select Get star ted.\n4. In the left pane, select Automat ed ML  under the Author  section.\n5. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning dataset. Doing so, allows you to ensure that your\ndata is formatted appropriately for your experiment.\n1. On the Select dataset  form, select From local files  from the +Create dataset  drop-\ndown.\na. On the Basic info  form, give your dataset a name and provide an optional\ndescription. The dataset type should default to Tabular , since automated ML in\nAzure Machine Learning studio currently only supports tabular datasets.\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is the storage location where you upload your data\nfile.\nd. Select Uplo ad files  from the Uplo ad drop-down.\ne. Choose the bike-no.csv  file on your local computer. This is the file you\ndownloaded as a prerequisite .\nf. Select Next\nWhen the upload is complete, the Settings and preview form is pre-populated\nbased on the file type.\ng. Verify that the Settings and pr eview  form is populated as follows and select\nNext .Create and load dataset\n\uff89Expand table", "In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data", "Since this is your first automated ML experiment, you'll see an empty list and links\nto documentation.\n3. Select +New aut omat ed ML job .\nBefore you configure your experiment, upload your data file to your workspace in the\nform of an Azure Machine Learning data asset. In the case of this tutorial, you can think\nof a data asset as your dataset for the AutoML job. Doing so, allows you to ensure that\nyour data is formatted appropriately for your experiment.\n1. Create a new data asset by selecting From local files  from the +Create data asset\ndrop-down.\na. On the Basic info  form, give your data asset a name and provide an optional\ndescription. The automated ML interface currently only supports\nTabularDatasets, so the dataset type should default to Tabular .\nb. Select Next  on the bottom left\nc. On the Datast ore and file selection  form, select the default datastore that was\nautomatically set up during your workspace creation, worksp aceblobst ore\n(Azur e Blob S torage) . This is where you'll upload your data file to make it\navailable to your workspace.\nd. Select Uplo ad files  from the Uplo ad drop-down.Create and load a dataset as a data asset", "b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably."]}
{"question": "Can Azure Machine Learning directly access data stored in Azure Data Lake Storage Gen2? If so, how?", "variant_id": "v1", "line_number": 24, "answer": "Yes, Azure Machine Learning can directly access data stored in Azure Data Lake Storage Gen2. Azure Data Lake Storage Gen2 is a large-scale data lake solution for big data analytics, which integrates with Azure Blob Storage and is commonly used as a datastore for Azure Machine Learning.\n\nTo access data in Azure Data Lake Storage Gen2 from Azure Machine Learning, you can follow these general steps:\n\n1. **Set up authentication**: Ensure that your Azure Machine Learning workspace has the appropriate permissions to access the Azure Data Lake Storage Gen2 account. This typically involves configuring a service principal with the necessary roles and permissions and ensuring that it is registered with your Azure Machine Learning workspace.\n\n2. **Configure datastore**: In Azure Machine Learning, you can register an Azure Data Lake Storage Gen2 account as a datastore. A datastore in Azure Machine Learning is a place where data can be stored and then accessed during training or scoring.\n\n3. **Access data in your experiments**: Once the datastore is registered, you can access the data within your Azure Machine Learning experiments by referencing the datastore and the specific paths to the data you need.\n\n4. **Use Azure Machine Learning SDK**: You can use the Azure Machine Learning SDK for Python to interact with the datastore programmatically. The SDK provides methods to upload, download, and manage data within the datastore.\n\nHere is an example of how you might register an Azure Data Lake Storage Gen2 account as a datastore using the Azure Machine Learning SDK for Python:\n\n```python\nfrom azureml.core import Workspace, Datastore\n\n# Get the workspace\nworkspace = Workspace.from_config()\n\n# Register a new datastore\ndatastore = Datastore.register_azure_data_lake_gen2(\n    workspace=workspace,\n    datastore_name='my_datastore',\n    account_name='myadlsaccount',  # Replace with your account name\n    filesystem='myfilesystem',  # Replace with your filesystem name\n    tenant_id='my-tenant-id',  # Replace with your tenant ID\n    client_id='my-client-id',  # Replace with your service principal's client ID\n    client_secret='my-client-secret'  # Replace with your service principal's client secret\n)\n\n# Now you can use the datastore in your experiments\n```\n\nRemember to replace the placeholders with your actual account details and credentials. It's important to manage and secure your credentials properly, especially when dealing with access keys and service principals.", "context": ["For information on regenerating the access keys, see Regenerate storage account access\nkeys.\nAzure Data Lake S torage Gen2 is built on top of Azure Blob S torage and is designed for\nbig data analytics in enterprises. Data Lake S torage Gen2 is used as a datastore for\nAzure Machine Learning. Like Azure Blob S torage, the data at rest is encrypted with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Data Lake\nStorage, see Azure S torage encryption with customer-managed keys in Azure K ey Vault.\nThe Azure Machine Learning service supports data from the following data sources.\nTransparent data encryption helps protect Azure SQL Database against the threat of\nmalicious offline activity by encrypting data at rest. By default, transparent data\nencryption is enabled for all newly deployed SQL databases that use Microsoft-\nmanaged keys.\nFor information on how to use customer-managed keys for transparent data encryption,\nsee Azure SQL Database transparent data encryption .Azure Data Lake Storage\n\uff17 Note\nOn Feb 29, 2024 , Azure Data Lake S torage Gen1 will be retired. For more\ninformation, see the official announcement . If you use Azure Data Lake S torage\nGen1, make sure to migrate to Azure Data Lake S torage Gen2 before that date. T o\nlearn how, see Migrat e Azur e Data Lak e Storage fr om Gen1 t o Gen2 by using the\nAzur e por tal.\nUnless you already have an Azure Data Lake S torage Gen1 account, you can't create\nnew ones.\nAzure relational databases\nAzure SQL Database\nAzure Database for PostgreSQL", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore"]}
{"question": "How does Azure Machine Learning support versioning of datasets stored in Azure Blob Storage?", "variant_id": "v1", "line_number": 25, "answer": "Azure Machine Learning supports dataset versioning by allowing users to create and manage versions of datasets that are registered in the Azure Machine Learning workspace. When you register a dataset in Azure Machine Learning, you can specify a version number. If you do not specify a version, Azure Machine Learning will automatically assign a version number starting with 1 and incrementing with each new registration of the dataset.\n\nWhen datasets are stored in Azure Blob Storage, you can use Azure Machine Learning to track changes and versions of these datasets. Each time you register a new version of the dataset with updated data or changes, it is tracked within the Azure Machine Learning workspace. This allows for reproducibility and traceability in machine learning experiments, as you can reference specific versions of datasets when training models.\n\nTo manage dataset versions in Azure Machine Learning, you can use the Azure Machine Learning SDK or the Azure Machine Learning studio. The SDK provides methods for registering, retrieving, and managing different versions of datasets. In the studio, you can view and manage dataset versions through the user interface.\n\nIt's important to note that while Azure Machine Learning provides the versioning mechanism, the actual data is stored in Azure Blob Storage, and the storage service itself does not provide versioning of the data. Instead, the versioning is a feature of the dataset registration within the Azure Machine Learning service.", "context": ["Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "You now have two versions of the data, where the name of the version corresponds to\nthe date the images were uploaded to storage:\n1. 20230108:  The images up to 2023-Jan-08.\n2. 20230115:  The images up to 2023-Jan-15.\nIn both cases, ML Table constructs a table of paths that only include the images up t o\nthose dat es.\nIn an Azure Machine Learning job you can mount or download those paths in the\nversioned ML Table to your compute target using either the eval_download or\neval_mount modes:\nPython\nfrom azure.ai.ml import MLClient, command, Input  \nfrom azure.ai.ml.entities import Environment  \nfrom azure.identity import DefaultAzureCredential  \nfrom azure.ai.ml.constants import InputOutputModes  \n# connect to the AzureML workspace  \nml_client = MLClient.from_config(\n    DefaultAzureCredential()  \n) \n# Get the 20230115 version of the data  \ndata_asset = ml_client.data.get(name= \"myimages\" , version= \"20230115\" ) \ninput = {  \n    \"images\" : Input(type= \"mltable\" , \n                   path=data_asset.id,  \n                   mode=InputOutputModes.EVAL_MOUNT  \n            )  \n} \ncmd = \"\"\" \nls ${{inputs.images}}/**  \n\"\"\" \njob = command(  \n    command=cmd,  \n    inputs=input,  \n    compute= \"cpu-cluster\" , \n    environment= \"azureml://registries/azureml/environments/sklearn-\n1.1/versions/4\"  \n) \nml_client.jobs.create_or_update(job)", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore", "Only Azure Blob S torage Accounts are supported for outputs. For instance, Azure Data Lake\nStorage Gen2 isn't supported as output in batch deployment jobs. If you need to output the\ndata to a different location/sink, use the Azure Data F actory Copy activity after the execution of\nthe batch job.\nUse low priority VMs in batch deployments\nAuthorization on batch endpoints\nNetwork isolation in batch endpointsNext steps"]}
{"question": "What is the recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage?", "variant_id": "v1", "line_number": 26, "answer": "The recommended way to handle large datasets in Azure Machine Learning when the data resides in Azure Blob Storage is to use Azure Storage Explorer. It provides a faster and more robust method for transferring a large amount of data compared to uploading local files directly. Azure Storage Explorer should be used as the default method for moving files into Azure Machine Learning.", "context": ["In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity"]}
{"question": "How do you use the Azure Machine Learning SDK to programmatically download data from Azure Blob Storage?", "variant_id": "v1", "line_number": 27, "answer": "To programmatically download data from Azure Blob Storage using the Azure Machine Learning SDK, you would typically follow these steps:\n\n1. **Set up your Azure Machine Learning workspace**: Before you can interact with Azure Blob Storage, you need to have an Azure Machine Learning workspace set up. This workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create.\n\n2. **Create a datastore**: In Azure Machine Learning, a datastore is an abstraction over an Azure Storage Account. The datastore can point to either an Azure Blob container or an Azure File Share. You can register a datastore using the SDK like this:\n\n    ```python\n    from azureml.core import Workspace, Datastore\n    \n    workspace = Workspace.from_config()  # Assuming you have a config.json file with workspace details\n    \n    # Register a new datastore\n    blob_datastore = Datastore.register_azure_blob_container(workspace=workspace,\n                                                            datastore_name='my_blob_datastore',\n                                                            container_name='my-container-name',\n                                                            account_name='my-storage-account-name',\n                                                            account_key='my-storage-account-key')\n    ```\n\n3. **Access the datastore**: Once the datastore is registered, you can access it using the SDK:\n\n    ```python\n    datastore = Datastore.get(workspace, datastore_name='my_blob_datastore')\n    ```\n\n4. **Download data from the datastore**: You can download files from the datastore to your local environment using the `download` method:\n\n    ```python\n    datastore.download(target_path='local/path',\n                       prefix='folder-or-file-prefix',  # Use this to filter which files to download\n                       overwrite=False,  # Overwrite existing files or not\n                       show_progress=True)  # Show download progress\n    ```\n\n5. **Use the data in your machine learning code**: After downloading, you can use the data as needed for training or inference in your machine learning code.\n\nPlease note that the actual code and parameters might vary based on your specific setup and requirements. Always refer to the latest Azure Machine Learning SDK documentation for the most up-to-date information and methods.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Tutorial: Upload, access and explore\nyour data in Azure Machine Learning\nArticle \u202212/27/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nIn this tutorial you learn how to:\nThe start of a machine learning project typically involves exploratory data analysis (ED A),\ndata-preprocessing (cleaning, feature engineering), and the building of Machine\nLearning model prototypes to validate hypotheses. This prototyping  project phase is\nhighly interactive. It lends itself to development in an IDE or a Jupyter notebook, with a\nPython int eractive console. This tutorial describes these ideas.\nThis video shows how to get started in Azure Machine Learning studio so that you can\nfollow the steps in the tutorial. The video shows how to create a notebook, clone the\nnotebook, create a compute instance, and download the data needed for the tutorial.\nThe steps are also described in the following sections.\n1. To use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\n2. Sign in to studio  and select your workspace if it's not already open.\n3. Open or create a notebook in your workspace:\nCreate a new notebook , if you want to copy/paste code into cells.\nOr, open tutorials/get -star ted-not ebooks/explor e-data.ipynb  from the\nSamples  section of studio. Then select Clone  to add the notebook to your\nFiles. (See where to find Samples .)\nUpload your data to cloud storage\uff02\nCreate an Azure Machine Learning data asset\uff02\nAccess your data in a notebook for interactive development\uff02\nCreate new versions of data assets\uff02\nhttps://learn-video.azurefd.net/vod/player?id=514a29e2-0ae7-4a5d-a537-\n8f10681f5545&locale=en-us&embedUrl=%2F azure%2Fmachine-learning%2Ftutorial-\nexplore-data\nPrerequisites", "Data from Azure Machine Learning registered data stores can be directly referenced by\nbatch deployments jobs. In this example, you first upload some data to the default data\nstore in the Azure Machine Learning workspace and then run a batch deployment on it.\nFollow these steps to run a batch endpoint job using data stored in a data store.\n1. Access the default data store in the Azure Machine Learning workspace. If your\ndata is in a different store, you can use that store instead. Y ou're not required to\nuse the default data store.\nAzure CLI\n2. You need to upload some sample data to the data store. This example assumes\nyou already uploaded the sample data included in the repo in the folder\nsdk/python/endpoints/batch/deploy-models/heart-classifier-mlflow/data in the\nfolder heart-disease-uci-unlabeled in the blob storage account. Ensure you've\ndone that before moving forward.\n3. Create the input or request:\nPlace the file path in the following variable:Azure CLI\nDATASTORE_ID=$(az ml datastore show -n workspaceblobstore | jq -r \n'.id')\n\uff17 Note\nData stores ID would look like\n/subscriptions/<subscription>/resourceGroups/<resource-\ngroup>/providers/Microsoft.MachineLearningServices/workspaces/<worksp\nace>/datastores/<data-store>.\n\uea80 Tip\nThe default blob data store in a workspace is called worksp aceblobst ore. You\ncan skip this step if you already know the resource ID of the default data store\nin your workspace.\nAzure CLI", "Acces s data in a job\nArticle \u202202/06/2024\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2 (current)\nIn this article you learn:\nAn Azure subscription. If you don't have an Azure subscription, create a free account before you begin.\nTry the free or paid version of Azure Machine Learning .\nThe Azure Machine Learning SDK for Python v2 .\nAn Azure Machine Learning workspace\nBefore you explore the detailed options available to you when you access data, we first describe the relevant\ncode snippets for data access.\nIn this example, you submit an Azure Machine Learning job that accesses data from a public  blob storage\naccount. However, you can adapt the snippet to access your own data in a private Azure S torage account.\nUpdate the path as described here. Azure Machine Learning seamlessly handles authentication to cloud\nstorage, with Microsoft Entra passthrough. When you submit a job, you can choose:\nUser identity:  Passthrough your Microsoft Entra identity to access the data\nManaged identity:  Use the managed identity of the compute target to access data\nNone:  Don't specify an identity to access the data. Use None when using credential-based (key/SAS\ntoken) datastores or when accessing public data\nHow to read data from Azure storage in an Azure Machine Learning job.\uff02\nHow to write data from your Azure Machine Learning job to Azure S torage. \uff02\nThe difference between mount  and downlo ad modes. \uff02\nHow to use user identity and managed identity to access data.\uff02\nMount settings available in a job.\uff02\nOptimum mount settings for common scenarios.\uff02\nHow to access V1 data assets.\uff02\nPrerequisites\nQuickstart\nRead data from Azure storage in an Azure Machine Learning job\n\uea80 Tip\nIf you use keys or SAS tokens to authenticate, we suggest that you create an Azur e Machine Learning\ndatast ore, because the runtime will automatically connect to storage without exposure of the key/token.\nPython SDK", "Acces s data from Azure cloud storage during\ninteractive development\nArticle \u202209/13/2023\nAPPLIES T O: \n  Python SDK azure-ai-ml v2 (curr ent)\nA machine learning project typically starts with exploratory data analysis (ED A), data-preprocessing\n(cleaning, feature engineering), and includes building prototypes of ML models to validate hypotheses.\nThis prototyping  project phase is highly interactive in nature, and it lends itself to development in a\nJupyter notebook, or an IDE with a Python int eractive console. In this article you'll learn how to:\nAn Azure Machine Learning workspace. For more information, see Manage Azure Machine Learning\nworkspaces in the portal or with the Python SDK (v2) .\nAn Azure Machine Learning Datastore. For more information, see Create datastores .\nAn Azure Machine Learning datastore is a reference to an existing  Azure storage account. The benefits of\ndatastore creation and use include:\nAccess data from a Azure Machine Learning Datastores URI as if it were a file system.\uff02\nMaterialize data into P andas using mltable Python library. \uff02\nMaterialize Azure Machine Learning data assets into P andas using mltable Python library. \uff02\nMaterialize data through an explicit download with the azcopy utility. \uff02\nPrerequisites\n\uea80 Tip\nThe guidance in this article describes data access during interactive development. It applies to any\nhost that can run a Python session. This can include your local machine, a cloud VM, a GitHub\nCodespace, etc. W e recommend use of an Azure Machine Learning compute instance - a fully\nmanaged and pre-configured cloud workstation. For more information, see Create an Azur e\nMachine Learning comput e instance .\n\uff09 Impor tant\nEnsure you have the latest azure-fsspec and mltable python libraries installed in your python\nenvironment:\nBash\npip install -U azureml-fsspec mltable\nAccess data from a datastore URI, like a filesystem\nA common, easy-to-use API to interact with different storage types (Blob/Files/ADLS).\uff02"]}
{"question": "What are the benefits of integrating Azure Data Lake Storage with Azure Machine Learning for big data scenarios?", "variant_id": "v1", "line_number": 28, "answer": "Integrating Azure Data Lake Storage with Azure Machine Learning offers several benefits for big data scenarios:\n\n1. **Scalability**: Azure Data Lake Storage is designed to handle large volumes of data, scaling to petabytes of information. This makes it suitable for big data scenarios where data volume can grow rapidly.\n\n2. **Data Accessibility**: By using Azure Data Lake Storage, you can store data in a central repository that is easily accessible from Azure Machine Learning. This simplifies the management of your data and allows for seamless integration with the machine learning workflows.\n\n3. **Performance**: Azure Data Lake Storage provides high-performance file system operations, which are essential when dealing with big data. This ensures that data can be read and written quickly, which is crucial for machine learning tasks that require frequent data access.\n\n4. **Advanced Analytics**: With Azure Machine Learning, you can build and train advanced analytics models on the data stored in Azure Data Lake Storage. This allows you to leverage the full potential of your big data by applying machine learning algorithms to uncover insights.\n\n5. **Security and Compliance**: Azure Data Lake Storage offers robust security features, including encryption and access control, which are important for protecting sensitive big data. When integrated with Azure Machine Learning, you can maintain compliance with industry standards and regulations.\n\n6. **Cost-Effectiveness**: Storing large volumes of data can be cost-effective with Azure Data Lake Storage due to its optimized storage solutions. When combined with Azure Machine Learning, you can also optimize the compute resources used for training models, leading to a more efficient use of resources.\n\n7. **Streamlined Data Processing Pipelines**: You can create production machine learning pipelines that leverage Azure Data Lake Storage for data input and output. This allows for a structured and efficient way to process data, train models, and deploy them into production.\n\nBy integrating Azure Data Lake Storage with Azure Machine Learning, organizations can build robust, scalable, and efficient big data solutions that harness the power of machine learning to extract valuable insights from their data.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Azur e Machine Learning designer : Use the designer to train and deploy ML\nmodels without writing any code. Drag and drop datasets and components to\ncreate ML pipelines.\nAutomat ed machine learning UI : Learn how to create automated ML experiments\nwith an easy-to-use interface.\nData labeling : Use Machine Learning data labeling to efficiently coordinate image\nlabeling  or text labeling  projects.\nMachine Learning integrates with the Azure cloud platform to add security to ML\nprojects.\nSecurity integrations include:\nAzure Virtual Networks with network security groups.\nAzure K ey Vault, where you can save security secrets, such as access information\nfor storage accounts.\nAzure Container R egistry set up behind a virtual network.\nFor more information, see Tutorial: Set up a secure workspace .\nOther integrations with Azure services support an ML project from end to end. They\ninclude:Enterprise-readiness and security\nAzure integrations for complete solutio ns", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore", "What is Azure Machine Learning?\nArticle \u202201/29/2024\nAzure Machine Learning is a cloud service for accelerating and managing the machine\nlearning (ML) project lifecycle. ML professionals, data scientists, and engineers can use it\nin their day-to-day workflows to train and deploy models and manage machine learning\noperations (ML Ops).\nYou can create a model in Machine Learning or use a model built from an open-source\nplatform, such as PyT orch, T ensorFlow, or scikit-learn. ML Ops tools help you monitor,\nretrain, and redeploy models.\nMachine Learning is for individuals and teams implementing ML Ops within their\norganization to bring ML models into production in a secure and auditable production\nenvironment.\nData scientists and ML engineers can use tools to accelerate and automate their day-to-\nday workflows. Application developers can use tools for integrating models into\napplications or services. Platform developers can use a robust set of tools, backed by\ndurable Azure R esource Manager APIs, for building advanced ML tooling.\nEnterprises working in the Microsoft Azure cloud can use familiar security and role-\nbased access control for infrastructure. Y ou can set up a project to deny access to\nprotected data and select operations.\nML projects often require a team with a varied skill set to build and maintain. Machine\nLearning has tools that help enable you to:\uea80 Tip\nFree trial!  If you don't have an Azure subscription, create a free account before you\nbegin. Try the fr ee or p aid v ersion o f Azur e Machine Learning . You get credits\nto spend on Azure services. After they're used up, you can keep the account and\nuse free Azur e ser vices . Your credit card is never charged unless you explicitly\nchange your settings and ask to be charged.\nWho is Azure Machine Learning for?\nProductivity for everyone on the team", "Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster ."]}
{"question": "How can you secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces?", "variant_id": "v1", "line_number": 29, "answer": "To secure data stored in Azure Storage services when accessed by Azure Machine Learning workspaces, you can use the following methods:\n\n1. **Encryption at Rest**: By default, all data stored in Azure Blob Storage is encrypted at rest with Microsoft-managed keys. This ensures that your data is secure when it is not being accessed or moved.\n\n2. **Customer-Managed Keys**: For additional control, you can use your own encryption keys for data stored in Azure Blob Storage. This is done through Azure Storage encryption with customer-managed keys in Azure Key Vault. By using customer-managed keys, you have the ability to manage the lifecycle of the encryption keys, including rotation and revocation.\n\n3. **Secure Data Access**: When setting up your Azure Machine Learning workspace, ensure that you configure the storage account and other data services with proper access controls. Use Azure role-based access control (RBAC) to grant the minimum necessary permissions to users and services that need to access the data.\n\n4. **Datastore Configuration**: In Azure Machine Learning, you can register Azure Storage services as datastores. When you use datastores, you can manage data connections securely without having to hard-code your credentials.\n\n5. **Network Security**: Use Azure Virtual Network to isolate your Azure Machine Learning compute resources and storage services. You can also use service endpoints or private endpoints to secure and limit access to your storage account.\n\n6. **Monitoring and Auditing**: Enable Azure Monitor and Azure Storage logging to track access and usage of your storage services. This helps in identifying any unauthorized access or anomalies in data usage.\n\nBy combining these methods, you can ensure that your data stored in Azure Storage services is secure when accessed by Azure Machine Learning workspaces.", "context": ["Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "This article is part of a series on securing an Azure Machine Learning workflow. See the\nother articles in this series:\nSecure the workspace resources\nSecure machine learning registries\nSecure the training environment\nSecure the inference environment\nEnable studio functionality\nUse custom DNS\nUse a firewall\nAPI platform network isolationNext steps", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.", "When you use an Azure S torage Account from Azure Machine Learning studio, you must\nadd the managed identity of the workspace to these Azure RBA C roles for the storage\naccount:\nBlob Data R eader\nIf the storage account uses a private endpoint to connect to the VNet, you must\ngrant the Reader  role for the storage account private endpoint to the managed\nidentity.\nFor more information, see Use Azure Machine Learning studio in an Azure Virtual\nNetwork .\nThe following sections explain the limitations of using an Azure S torage Account, with\nyour workspace, in a VNet.\nTo secure communication between Azure Machine Learning and Azure S torage\nAccounts, configure the storage to Grant access to trusted Azure services .\nWhen an Azure S torage account is located behind a virtual network, the storage firewall\ncan normally be used to allow your client to directly connect over the internet. However,\nwhen using studio, your client doesn't connect to the storage account. The Azure\nMachine Learning service that makes the request connects to the storage account. The\nIP address of the service isn't documented, and it changes frequently. Enabling the\nstorage fir ewall will not allow studio t o access the st orage account in a VNet\nconfiguration .\uea80 Tip\nTo access data from outside Azure Machine Learning, for example with Azure\nStorage Explorer, that access probably relies on the user identity. For specific\ninformation, review the documentation for the tool or service you're using. For\nmore information about how Azure Machine Learning works with data, see Setup\nauthentication betw een Azur e Machine Learning and other ser vices .\nAzure Storage Account\nSecure communication with Azure Storage Account\nAzure Storage firewall", "You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity"]}
{"question": "What role do Azure Storage accounts play in deploying Azure Machine Learning models as web services?", "variant_id": "v1", "line_number": 30, "answer": "Azure Storage accounts play a crucial role in deploying Azure Machine Learning models as web services in several ways:\n\n1. **Model Storage**: The storage accounts in your Azure subscription are used to store your machine learning models. When you train a model using Azure Machine Learning, the model is saved in your Azure Storage account.\n\n2. **Data Storage**: Training data, training logs, and Python scripts are stored in Azure Storage accounts. This data can be accessed by the compute resources during the training process and when deploying models as web services.\n\n3. **Operational Artifacts**: When deploying a model as a web service, operational artifacts such as scoring scripts, environment configuration files, and other necessary files are stored in Azure Storage accounts.\n\n4. **Web Service Assets**: For web services, the storage account may also store assets that the web service needs to function, such as additional scripts or files that are required at runtime.\n\n5. **Endpoint Accessibility**: When using a compute instance or compute cluster, the storage account is mounted as a file share via the SMB protocol. This allows the deployed web service to access the necessary files and models securely.\n\n6. **Private Endpoint**: The compute instance and cluster use the private endpoint when they access the storage account, ensuring secure and private connectivity to the storage resources.\n\n7. **Blob Storage**: Azure Blob storage is often used to store large amounts of unstructured data that can be accessed by the deployed web service for inference tasks.\n\nIn summary, Azure Storage accounts provide a secure and scalable way to store and manage the data, models, and artifacts that are essential for deploying and running Azure Machine Learning models as web services.", "context": ["Scenar io Requir ed\ninboundRequir ed outbound Additional\nconfiguration\nUse Azure\nKubernetes\nServiceNot applicable For information on the outbound\nconfiguration for AKS, see Secure\nAzure Kubernetes Service\ninferencing environment .\nUse Docker\nimages that\nAzure\nMachine\nLearning\nmanagesNot applicable Microsoft Artifact R egistry\nviennaglobal.azurecr.io\nglobal container registryIf the container registry\nfor your workspace is\nbehind the virtual\nnetwork, configure the\nworkspace to use a\ncompute cluster to build\nimages. For more\ninformation, see Secure\nan Azure Machine\nLearning workspace with\nvirtual networks .\nAzure Machine Learning uses multiple storage accounts. Each stores different data and\nhas a different purpose:\nYour st orage : The storage accounts in your Azure subscription store your data and\nartifacts, such as models, training data, training logs, and Python scripts. For\nexample, the default  storage account for your workspace is in your subscription.\nThe Azure Machine Learning compute instance and compute cluster access file and\nblob data in this storage over ports 445 (SMB) and 443 (HT TPS).\nWhen you're using a compute instance or compute cluster, your storage account is\nmounted as a file share via the SMB protocol. The compute instance and cluster\nuse this file share to store items like the data, models, Jupyter notebooks, and\ndatasets. The compute instance and cluster use the private endpoint when they\naccess the storage account.\nMicrosoft storage : The Azure Machine Learning compute instance and compute\ncluster rely on Azure Batch. They access storage located in a Microsoft\nsubscription. This storage is used only for the management of the compute\ninstance or cluster. None of your data is stored here. The compute instance and\ncompute cluster access the blob, table, and queue data in this storage, by using\nport 443 (HT TPS).Purposes of storage accounts", "Azure Machine Learning supports storage accounts configured to use either a private\nendpoint or service endpoint.\n1. In the Azure portal, select the Azure S torage Account.\n2. Use the information in Use private endpoints for Azure S torage  to add private\nendpoints for the following storage resources:\nBlob\nFile\nQueue  - Only needed if you plan to use Batch endpoints  or the\nParallelRunS tep in an Azure Machine Learning pipeline.\nTable - Only needed if you plan to use Batch endpoints  or the\nParallelRunS tep in an Azure Machine Learning pipeline.Securing a workspace with private endpoints does not ensure end-to-end security\nby itself. Y ou must follow the steps in the rest of this article, and the VNet series, to\nsecure individual components of your solution. For example, if you use a private\nendpoint for the workspace, but your Azure S torage Account is not behind the\nVNet, traffic between the workspace and storage does not use the VNet for\nsecurity.\nSecure Azure storage accounts\nPrivate endpoint", "What are Azure Machine Learning endpoints?\nDeploy and score a model with an online endpointNext steps", "Azure Machine Learning offers many ways to deploy MLflow models to online and batch\nendpoints. Y ou can deploy models, using the following tools:\nEach workflow has different capabilities, particularly around which type of compute they\ncan target. The following table shows the different capabilities.\nScenar io MLflow SDK Azur e Machine\nLearning CLI/SDKAzur e Machine\nLearning studio\nDeploy to managed online\nendpointsSee example See example See example\nDeploy to managed online\nendpoints (with a scoring script)Not\nsupportedSee example See example\nDeploy to batch endpoints Not\nsupportedSee example See example\nDeploy to batch endpoints (with\na scoring script)Not\nsupportedSee example See example\nDeploy to web services\n(ACI/AKS)Legacy\nsupportNot supported Not supported\nDeploy to web services (A CI/AKS\n- with a scoring script)Not\nsupportedLegacy support Legacy support\n Deployment to online endpoints that are in workspaces with private link enabled\nrequires you to package models before deployment (preview) .\n We recommend switching to managed online endpoints  instead.\n MLflow (OSS) doesn't have the concept of a scoring script and doesn't support batch\nexecution currently.\nUse the MLflow SDK if both of these conditions apply:Deployment tools\nMLflow SDK\uff02\nAzure Machine Learning CLI\uff02\nAzure Machine Learning SDK for Python\uff02\nAzure Machine Learning studio\uff02\n\uff89Expand table\n1 1 1\n3\n3\n3\n22 2\n32 2\n1\n2\n3\nWhich deployment tool to use?", "region basis. When you sign up for Azure Machine Learning studio, you receive a default\nVM quota for several VM families available in the region. Y ou can continue to create\ndeployments until you reach your quota limit. Once you reach this limit, you can request\na quota increase.\nModels deployed as a service with pay-as-you-go are protected by Azure AI content\nsafety. When deployed to real-time endpoints, you can opt out of this capability. With\nAzure AI content safety enabled, both the prompt and completion pass through an\nensemble of classification models aimed at detecting and preventing the output of\nharmful content. The content filtering system detects and takes action on specific\ncategories of potentially harmful content in both input prompts and output\ncompletions. Learn more about Azure AI Content Safety .\nModel Catalog and Collections\nDeploy and score a machine learning model by using an online endpoint\nPlan and manage costs for Azure AI S tudioContent filtering\nRelated content"]}
{"question": "How do you monitor data access and usage when integrating Azure Machine Learning with Azure Storage?", "variant_id": "v1", "line_number": 31, "answer": "To monitor data access and usage when integrating Azure Machine Learning with Azure Storage, you can use Azure Monitor, which provides comprehensive monitoring of your Azure resources and the applications you run on Azure. Here are the steps you can follow:\n\n1. **Enable Diagnostic Settings in Azure Storage**: Configure the diagnostic settings in your Azure Storage account to send logs and metrics to Azure Monitor. This can include read, write, and delete operations on your storage blobs or files, which can be indicative of data access and usage.\n\n2. **Log Analytics Workspace**: Set up a Log Analytics workspace in Azure Monitor to query and visualize the logs sent from your Azure Storage account. You can write queries to analyze the data access patterns or look for specific access events.\n\n3. **Metrics**: Review the metrics provided by Azure Storage in the Azure portal. These metrics can give you insights into the performance and usage of your storage account.\n\n4. **Alerts**: Create alerts in Azure Monitor based on metrics or log query results. For example, you can set an alert for when there's an unusual amount of data read or written, which could indicate a spike in usage or a potential security issue.\n\n5. **Azure Storage Analytics**: Use Azure Storage Analytics to track, analyze, and debug your applications' storage usage.\n\n6. **Azure Machine Learning Monitoring**: Within Azure Machine Learning, you can monitor data drift, model performance, and other operational metrics that can indirectly indicate how your models are using the data stored in Azure Storage.\n\n7. **Role-Based Access Control (RBAC)**: Ensure that you have proper RBAC policies in place to control who has access to your Azure Storage data. Monitoring these access policies can help you understand who is accessing your data and for what purpose.\n\n8. **Audit Logs**: Review the audit logs for any access to the Azure Storage account. These logs can provide detailed information about who accessed the data and what operations were performed.\n\nRemember to consult the Azure documentation for the most up-to-date information on monitoring and logging, as Azure services are frequently updated with new features and capabilities.", "context": ["To learn how to monitor resources and logs from Azure Machine Learning, see\nMonitoring Azure Machine Learning .", "Monitor Azure Machine Learning\nArticle \u202211/06/2023\nWhen you have critical applications and business processes relying on Azure resources, you\nwant to monitor those resources for their availability, performance, and operation. This\narticle describes the monitoring data generated by Azure Machine Learning and how to\nanalyze and alert on this data with Azure Monitor.\nAzure Machine Learning creates monitoring data using Azure Monitor , which is a full stack\nmonitoring service in Azure. Azure Monitor provides a complete set of features to monitor\nyour Azure resources. It can also monitor resources in other clouds and on-premises.\nStart with the article Monitoring Azure resources with Azure Monitor , which describes the\nfollowing concepts:\nWhat is Azure Monitor?\nCosts associated with monitoring\nMonitoring data collected in Azure\nConfiguring data collection\nStandard tools in Azure for analyzing and alerting on monitoring data\nThe following sections build on this article by describing the specific data gathered for\nAzure Machine Learning. These sections also provide examples for configuring data\ncollection and analyzing this data with Azure tools.\uea80 Tip\nThe information in this document is primarily for administrat ors, as it describes\nmonitoring for the Azure Machine Learning service and associated Azure services. If\nyou are a data scientist  or developer , and want to monitor information specific to your\nmodel tr aining r uns, see the following documents:\nStart, monit or, and cancel training runs\nLog metrics for training runs\nTrack experiments with MLflow\nIf you want to monitor information generated by models deployed to online\nendpoints, see Monit or online endpoints .\nWhat is Azure Monitor?", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you.", "Monitor online endpoints\nArticle \u202210/24/2023\nAzure Machine Learning uses integration with Azure Monitor to track and monitor\nmetrics and logs for online endpoints . You can view metrics in charts, compare between\nendpoints and deployments, pin to Azure portal dashboards, configure alerts, query\nfrom log tables, and push logs to supported targets. Y ou can also use Application\nInsights to analyze events from user containers.\nMetrics : For endpoint-level metrics such as request latency, requests per minute,\nnew connections per second, and network bytes, you can drill down to see details\nat the deployment level or status level. Deployment-level metrics such as CPU/GPU\nutilization and memory or disk utilization can also be drilled down to instance\nlevel. Azure Monitor allows tracking these metrics in charts and setting up\ndashboards and alerts for further analysis.\nLogs : You can send metrics to the Log Analytics workspace where you can query\nthe logs using Kusto query syntax. Y ou can also send metrics to Azure S torage\naccounts and/or Event Hubs for further processing. In addition, you can use\ndedicated log tables for online endpoint related events, traffic, and console\n(container) logs. Kusto query allows complex analysis and joining of multiple\ntables.\nApplication insights : Curated environments include integration with Application\nInsights, and you can enable or disable this integration when you create an online\ndeployment. Built-in metrics and logs are sent to Application Insights, and you can\nuse the built-in features of Application Insights (such as Live metrics, T ransaction\nsearch, F ailures, and P erformance) for further analysis.\nIn this article you learn how to:\nDeploy an Azure Machine Learning online endpoint.Choose the right method to view and track metrics and logs\uff02\nView metrics for your online endpoint\uff02\nCreate a dashboard for your metrics\uff02\nCreate a metric alert\uff02\nView logs for your online endpoint\uff02\nUse Application Insights to track metrics and logs\uff02\nPrerequisites", "Use monitoring view in the Azure Machine Learning studio  to check the CPU and memory utilization of\nyour job. If you're low on CPU and memory, consider increasing the compute target VM size.\nConsider increasing RSLEX_DOWNLOADER_THREADS if you're downloading and you don't utilize the CPU and\nmemory. If you use mount, you should increase DATASET_MOUNT_READ_BUFFER_BLOCK_COUNT to do more\nprefetching, and increase DATASET_MOUNT_READ_THREADS for more read threads.\nIf the metrics show low SuccessE2ELat ency and low SuccessSer verLat ency, but the client experiences high\nlatency , you have a delay in the storage request that reaches the service. Y ou should check:\nWhether the number of threads used for mount/download\n(DATASET_MOUNT_READ_THREADS/RSLEX_DOWNLOADER_THREADS) is set too low, relative to the number of cores\navailable on the compute target. If the setting is too low, increase the number of threads.\nWhether the number of retries for downloading ( AZUREML_DATASET_HTTP_RETRY_COUNT) is set too high. If\nso, decrease the number of retries.\nFrom the Azure Machine Learning studio, you can also monitor the compute target disk IO and usage during\nyour job execution. Navigate to your job, and select the Monit oring  tab. This tab provides insights on the\nresources of your job, on a 30 day rolling basis. For example:\nAzure Machine Learning data runtime doesn't use the last RESERVED_FREE_DISK_SPACE bytes of disk space, to\nkeep the compute healthy (the default value is 150MB). If your disk is full, your code is writing files to disk\nwithout declaring the files as an output. Therefore, check your code to make sure that data isn't being written\nerroneously to temporary disk. If you must write files to temporary disk, and that resource is becoming full,\nconsider:\nIncreasing the VM Size to one that has a larger temporary disk\nSetting a T TL on the cached data ( DATASET_MOUNT_ATTRIBUTE_CACHE_TTL), to purge your data from diskMonitor disk usage during a job\n\uf80a\n\uff17 Note\nJob monitoring supports only compute resources that Azure Machine Learning manages. Jobs with a\nruntime of less than 5 minutes will not have enough data to populate this view."]}
{"question": "Can Azure Machine Learning use data stored in Azure File Storage? If so, what are the use cases?", "variant_id": "v1", "line_number": 32, "answer": "Yes, Azure Machine Learning can use data stored in Azure File Storage. Azure File Storage provides shared storage that can be accessed via the SMB protocol, and it can be mounted onto Azure Machine Learning compute instances or pipelines. Here are some use cases for using Azure File Storage with Azure Machine Learning:\n\n1. **Data Access**: You can store your datasets in Azure File Storage and access them from your Azure Machine Learning workspace. This is useful for scenarios where you have large datasets that need to be shared across multiple experiments or users.\n\n2. **Persistent Storage**: When using Azure Machine Learning compute instances, you can mount Azure File Shares to ensure that your data persists even when the compute instance is shut down. This is particularly useful for saving intermediate results or datasets that are used frequently.\n\n3. **Collaboration**: Since Azure File Storage can be accessed from anywhere, it facilitates collaboration among data scientists who are working on the same project but may be located in different geographies.\n\n4. **Training and Model Development**: You can use the data stored in Azure File Storage for training machine learning models directly within Azure Machine Learning. The data can be accessed from notebooks or training scripts.\n\n5. **Pipeline Storage**: In machine learning pipelines, you might need to store output from one step to be used as input in another. Azure File Storage can be used to store these intermediate outputs.\n\nRemember to configure the appropriate permissions and network settings to ensure secure and efficient access to the file shares from your Azure Machine Learning environment.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore", "Attached comput e: You can attach your own compute resources to your\nworkspace and use them for training and inference.\nMachine Learning allows you to work with different types of data:\nURIs (a location in local or cloud storage):\nuri_folder\nuri_file\nTables (a tabular data abstraction):\nmltable\nPrimitives:\nstring\nboolean\nnumber\nFor most scenarios, you use URIs ( uri_folder and uri_file) to identify a location in\nstorage that can be easily mapped to the file system of a compute node in a job by\neither mounting or downloading the storage to the node.\nThe mltable parameter is an abstraction for tabular data that's used for automated\nmachine learning (AutoML) jobs, parallel jobs, and some advanced scenarios. If you're\nstarting to use Machine Learning and aren't using AutoML, we strongly encourage you\nto begin with URIs.\nMachine Learning datastores securely keep the connection information to your data\nstorage on Azure so that you don't have to code it in your scripts. Y ou can register and\ncreate a datastore to easily connect to your storage account and access the data in your\nunderlying storage service. The Azure Machine Learning CLI v2 and SDK v2 support the\nfollowing types of cloud-based storage services:\nAzure Blob S torage container\nAzure Files share\nAzure Data Lake S torage\nAzure Data Lake S torage Gen2Data\nDatastore\nEnvironment", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."]}
{"question": "What are the best practices for managing input and output data for Azure Machine Learning pipelines using Azure Storage?", "variant_id": "v1", "line_number": 33, "answer": "When managing input and output data for Azure Machine Learning pipelines using Azure Storage, consider the following best practices:\n\n1. **Use Azure Machine Learning Datastores**: Register your Azure Storage accounts (Azure Blob Storage, Azure Data Lake Storage Gen1, and Azure Data Lake Storage Gen2) as Azure Machine Learning datastores. This allows you to manage and access data within your Azure Machine Learning workspace easily.\n\n2. **Flatten Nested Folder Structures**: Azure Machine Learning batch endpoint jobs do not support nested folder structures. If your data is organized in nested folders, you will need to flatten the structure before the batch job can process it.\n\n3. **Data Movement with Azure Data Factory**: If your input data resides in a storage service not directly supported by Azure Machine Learning, use Azure Data Factory to copy the data to a compatible Azure Storage account before executing the batch job.\n\n4. **Scoring Script Compatibility**: Ensure that the scoring script used in your deployment is capable of handling the data format and structure as it will be provided to the batch job. This includes being aware of any limitations related to MLflow models if you are using them.\n\n5. **Register Data Stores**: It is recommended to register the storage account you are using with Azure Data Factory as a data store in Azure Machine Learning. This allows you to read from and write back to the same storage account seamlessly.\n\n6. **Monitor Data Access and Permissions**: Make sure that the necessary permissions are set on your Azure Storage account so that the Azure Machine Learning service can access the data. Use Shared Access Signatures (SAS) or Azure Active Directory (Azure AD) authentication to control access.\n\n7. **Optimize Data Transfer**: To reduce latency and potential costs, consider the location of your storage account relative to your Azure Machine Learning workspace and compute resources. Placing them in the same region can improve performance.\n\n8. **Data Output Considerations**: For outputs, ensure that the data store you are writing to is registered with Azure Machine Learning and that your pipeline is configured to write the output to the correct location.\n\nBy following these best practices, you can effectively manage data inputs and outputs for your Azure Machine Learning pipelines, ensuring smooth and efficient operation.", "context": ["7. Your pipeline is ready to be used.\nWhen calling Azure Machine Learning batch deployments consider the following limitations:\nOnly Azure Machine Learning data stores or Azure S torage Accounts (Azure Blob S torage, Azure\nData Lake S torage Gen1, Azure Data Lake S torage Gen2) are supported as inputs. If your input\ndata is in another source, use the Azure Data F actory Copy activity before the execution of the\nbatch job to sink the data to a compatible store.\nBatch endpoint jobs don't explore nested folders and hence can't work with nested folder\nstructures. If your data is distributed in multiple folders, notice that you will have to flatten the\nstructure.\nMake sure that your scoring script provided in the deployment can handle the data as it is\nexpected to be fed into the job. If the model is MLflow, read the limitation in terms of the file\ntype supported by the moment at Using MLflow models in batch deployments .\nOnly registered Azure Machine Learning data stores are supported by the moment. W e\nrecommend you to register the storage account your Azure Data F actory is using as a Data S tore\nin Azure Machine Learning. In that way, you will be able to write back to the same storage\naccount from where you are reading.\nLimitations\nData inputs\nData outputs", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.", "original notebook or Python code, changes the training input from local data to\nparameterized values, split the training code into multiple steps as needed, perform unit\ntest of each step, and finally wraps all steps into a pipeline.\nOnce the teams get familiar with pipelines and want to do more machine learning\nprojects using pipelines, they'll find the first approach is hard to scale. The second\napproach is set up a few pipeline templates, each try to solve one specific machine\nlearning problem. The template predefines the pipeline structure including how many\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\nlearning project, the team first forks one template repo. The team leader then assigns\nmembers which step they need to work on. The data scientists and data engineers do\ntheir regular work. When they're happy with their result, they structure their code to fit\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\nexecuted or automated. If there's any change, each member only needs to work on their\npiece of code without touching the rest of the pipeline code.\nOnce a team has built a collection of machine learnings pipelines and reusable\ncomponents, they could start to build the machine learning pipeline from cloning\nprevious pipeline or tie existing reusable component together. At this stage, the team's\noverall productivity will be improved significantly.\nAzure Machine Learning offers different methods to build a pipeline. For users who are\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\npipelines by using registered components .\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\nfollowing table lists the different pipelines and what they're used for:\nScenar io Primar y\npersonaAzur e\nofferingOSS\nofferingCanonical\npipeStrengths\nModel\norchestration\n(Machine\nlearning)Data\nscientistAzure\nMachine\nLearning\nPipelinesKubeflow\nPipelinesData ->\nModelDistribution,\ncaching, code-first,\nreuse\nData\norchestrationData\nengineerAzure Data\nFactoryApache\nAirflowData -> Data Strongly typed\nmovement, data-Which Azure pipeline technology should I use?\n\uff89Expand table", "original notebook or Python code, changes the training input from local data to\nparameterized values, split the training code into multiple steps as needed, perform unit\ntest of each step, and finally wraps all steps into a pipeline.\nOnce the teams get familiar with pipelines and want to do more machine learning\nprojects using pipelines, they'll find the first approach is hard to scale. The second\napproach is set up a few pipeline templates, each try to solve one specific machine\nlearning problem. The template predefines the pipeline structure including how many\nsteps, each step's inputs and outputs, and their connectivity. T o start a new machine\nlearning project, the team first forks one template repo. The team leader then assigns\nmembers which step they need to work on. The data scientists and data engineers do\ntheir regular work. When they're happy with their result, they structure their code to fit\nin the pre-defined steps. Once the structured codes are checked-in, the pipeline can be\nexecuted or automated. If there's any change, each member only needs to work on their\npiece of code without touching the rest of the pipeline code.\nOnce a team has built a collection of machine learnings pipelines and reusable\ncomponents, they could start to build the machine learning pipeline from cloning\nprevious pipeline or tie existing reusable component together. At this stage, the team's\noverall productivity will be improved significantly.\nAzure Machine Learning offers different methods to build a pipeline. For users who are\nfamiliar with DevOps practices, we recommend using CLI. For data scientists who are\nfamiliar with python, we recommend writing pipelines using the Azure Machine Learning\nSDK v2 . For users who prefer to use the UI, they could use the designer to build\npipelines by using registered components .\nThe Azure cloud provides several types of pipeline, each with a different purpose. The\nfollowing table lists the different pipelines and what they're used for:\nScenar io Primar y\npersonaAzur e\nofferingOSS\nofferingCanonical\npipeStrengths\nModel\norchestration\n(Machine\nlearning)Data\nscientistAzure\nMachine\nLearning\nPipelinesKubeflow\nPipelinesData ->\nModelDistribution,\ncaching, code-first,\nreuse\nData\norchestrationData\nengineerAzure Data\nFactoryApache\nAirflowData -> Data Strongly typed\nmovement, data-Which Azure pipeline technology should I use?\n\uff89Expand table"]}
{"question": "How can you optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training?", "variant_id": "v1", "line_number": 34, "answer": "To optimize data transfer between Azure Machine Learning and Azure Storage services for efficient model training, you can consider the following strategies:\n\n1. **Proximity of Resources**: Ensure that your Azure Machine Learning workspace and Azure Storage account are in the same Azure region to reduce latency and data transfer times.\n\n2. **Datastore Mounting**: Use Azure Machine Learning Datastores to mount your Azure Storage (Blob or File Storage) as a file system in your Azure Machine Learning compute. This allows you to access your data without the need for transferring it, which can be more efficient for large datasets.\n\n3. **Data Caching**: When using Azure Machine Learning Compute Instances or Compute Clusters, you can cache the data on the VMs to reduce the data transfer time for subsequent runs.\n\n4. **Optimized Data Formats**: Store your data in optimized formats such as Apache Parquet or ORC, which are designed for efficient data storage and retrieval.\n\n5. **Incremental Data Transfer**: If your data changes incrementally, consider transferring only the new or changed data instead of the entire dataset.\n\n6. **Pipeline Data Transfer**: Use Azure Machine Learning Pipelines to manage data transfer within the steps of your machine learning workflow. This can help to ensure that data is transferred efficiently between different stages of the pipeline.\n\n7. **Azure Blob Storage's Hot and Cool Access Tiers**: Use the appropriate access tier in Azure Blob Storage based on how frequently you access the data. Hot access tier is optimized for data that is accessed frequently, while the cool access tier is cost-effective for data that is infrequently accessed.\n\n8. **Network Optimization**: Use Azure's high-bandwidth networking options if you are transferring large amounts of data to ensure that network capacity does not become a bottleneck.\n\n9. **Parallelization**: When transferring data, use parallelism to speed up the process. Azure Storage SDKs and tools like AzCopy support parallel data transfer.\n\n10. **Compression**: Compress your data before transferring it to reduce the amount of data that needs to be moved. This can be particularly effective for text-based data formats.\n\nBy implementing these strategies, you can reduce the time and cost associated with transferring data between Azure Machine Learning and Azure Storage services, leading to more efficient model training processes.", "context": ["Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps", "12,000 Mbps) would load the same data in ~33 secs (assuming you saturate the NIC at 1500-MB/s\nthroughput).\nStorage tier : For most scenarios - including Large Language Models (LLM) - standard storage provides\nthe best cost/performance profile. However, if you have many small files , premium  storage offers a\nbetter cost/performance profile. For more information, read Azure S torage options .\nStorage lo ad: If the storage account is under high load - for example, many GPU nodes in a cluster\nrequesting data - then you risk hitting the egress capacity of storage. For more information, read\nStorage load . If you have many small files that need access in parallel, you might hit the request limits\nof storage. R ead up-to-date information on the limits for both egress capacity and storage requests in\nScale targets for standard storage accounts .\nData access p attern in user code : When you use mount mode, data is fetched based on the open/read\nactions in your code. For example, when reading random sections of a large file, the default data\nprefetching settings of mounts can lead to downloads of blocks that won't be read. Y ou might need to\ntune some settings to reach maximum throughput. For more information, read Optimum mount\nsettings for common scenarios .\nTo access the logs of the data runtime from your job:\n1. Select Outputs+Logs  tab from the job page.\n2. Select the system_logs  folder, followed by data_cap ability  folder.\n3. You should see two log files:\nThe log file data-cap ability .log shows the high-level information about the time spent on key data loading\ntasks. For example, when you download data, the runtime logs the download activity start and finish times:\nlog\nIf the download throughput is a fraction of the expected network bandwidth for the VM size, you can inspect\nthe log file rslex.log.<TIMEST AMP> . This file contains all the fine-grain logging from the Rust-based\nruntime; for example, parallelization:\nlogUsing logs to diagnose issues\n\uf80a\nINFO 2023-05-18 17:14:47,790 sdk_logger.py:44 [28] - ActivityStarted, download\nINFO 2023-05-18 17:14:50,295 sdk_logger.py:44 [28] - ActivityCompleted: Activity=download,  \nHowEnded=Success, Duration=2504.39 [ms]", "Manage and optimize Azure Machine\nLearning costs\nArticle \u202208/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.\uff09 Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage"]}
{"question": "What considerations should be taken into account for data privacy and compliance when integrating Azure Machine Learning with Azure Storage?", "variant_id": "v1", "line_number": 35, "answer": "When integrating Azure Machine Learning with Azure Storage for data privacy and compliance, the following considerations should be taken into account:\n\n1. **Encryption at Rest**: Ensure that all data stored in Azure Blob Storage is encrypted at rest. Azure Storage encrypts data with Microsoft-managed keys by default. For additional control, you can use customer-managed keys in Azure Key Vault.\n\n2. **Encryption in Transit**: Data should be encrypted in transit to and from Azure Storage. Azure Machine Learning and Azure Storage use secure protocols such as HTTPS to protect data as it moves between services.\n\n3. **Access Control**: Implement proper access control mechanisms. Use Azure Role-Based Access Control (RBAC) to define who has access to your Azure Storage data. Ensure that only authorized Azure Machine Learning compute resources and users can access the data.\n\n4. **Datastore Configuration**: When configuring datastores in Azure Machine Learning, ensure that they are linked to secure storage accounts and that the data is accessed securely.\n\n5. **Monitoring and Auditing**: Enable monitoring and auditing features to track access and usage of data. Azure provides tools like Azure Monitor and Azure Security Center to help with this.\n\n6. **Compliance Standards**: Be aware of the compliance standards relevant to your industry and region, such as GDPR, HIPAA, or CCPA. Azure has many certifications that can help meet these compliance requirements.\n\n7. **Key Management**: Regularly rotate and manage encryption keys. If you are using customer-managed keys, you should have a process in place for key rotation and revocation. Azure Key Vault can assist with this process.\n\n8. **Data Residency**: Consider the location of your Azure Storage resources to comply with data residency requirements. Data should be stored in a region that aligns with the legal and regulatory requirements pertinent to the data.\n\n9. **Data Retention and Deletion**: Implement policies for data retention and deletion in line with compliance requirements. Ensure that you can securely delete data from Azure Storage when it is no longer needed or when required by compliance regulations.\n\n10. **Secure Development Practices**: When developing applications that use Azure Machine Learning and Azure Storage, follow secure development practices to prevent data leaks or unauthorized access.\n\nBy considering these points, you can help ensure that your integration of Azure Machine Learning with Azure Storage is secure and compliant with relevant data privacy regulations.", "context": ["Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution.", "Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps", "Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .", "Data concepts in Azure Machine\nLearning\nArticle \u202207/13/2023\nWith Azure Machine Learning, you can import data from a local machine or an existing\ncloud-based storage resource. This article describes key Azure Machine Learning data\nconcepts.\nAn Azure Machine Learning datastore serves as a reference to an existing  Azure storage\naccount. An Azure Machine Learning datastore offers these benefits:\nA common, easy-to-use API that interacts with different storage types\n(Blob/Files/ADLS).\nEasier discovery of useful datastores in team operations.\nFor credential-based access (service principal/SAS/key), Azure Machine Learning\ndatastore secures connection information. This way, you won't need to place that\ninformation in your scripts.\nWhen you create a datastore with an existing Azure storage account, you can choose\nbetween two different authentication methods:\nCredential-b ased  - authenticate data access with a service principal, shared access\nsignature (SAS) token, or account key. Users with Reader  workspace access can\naccess the credentials.\nIdentity-b ased  - use your Azure Active Directory identity or managed identity to\nauthenticate data access.\nThe following table summarizes the Azure cloud-based storage services that an Azure\nMachine Learning datastore can create. Additionally, the table summarizes the\nauthentication types that can access those services:\nSuppor ted st orage ser vice Credential-b ased authentication Identity-b ased authentication\nAzure Blob Container \u2713 \u2713\nAzure File Share \u2713\nAzure Data Lake Gen1 \u2713 \u2713\nAzure Data Lake Gen2 \u2713 \u2713Datastore"]}
{"question": "How can you use managed identities to securely access data in Azure Storage from Azure Machine Learning?", "variant_id": "v1", "line_number": 36, "answer": "To securely access data in Azure Storage from Azure Machine Learning using managed identities, you can follow these steps:\n\n1. **Enable Managed Identity on Azure Machine Learning Workspace:**\n   - In the Azure portal, navigate to your Azure Machine Learning workspace.\n   - Under the \"Settings\" section, find and click on \"Identity\".\n   - In the \"System assigned\" tab, switch the \"Status\" to \"On\" to enable a system-assigned managed identity for the workspace. Save the changes.\n\n2. **Grant Permissions to Managed Identity:**\n   - Navigate to the Azure Storage account you want to access from Azure Machine Learning.\n   - Go to the \"Access control (IAM)\" section.\n   - Click on \"Add role assignment\" to grant the managed identity the necessary permissions (e.g., \"Storage Blob Data Reader\" for read-only access).\n\n3. **Create a Datastore in Azure Machine Learning:**\n   - In the Azure Machine Learning studio, go to \"Datastores\" under the \"Data\" section.\n   - Click on \"New datastore\" and select the appropriate storage type (e.g., Azure Blob Storage, Azure Data Lake Storage Gen1/Gen2).\n   - Provide a name for the datastore and select the storage account.\n   - For authentication, choose \"Managed Identity\" instead of providing access keys or SAS tokens.\n\n4. **Use the Datastore in Training Jobs:**\n   - When submitting training jobs, reference the datastore you created. Azure Machine Learning will use the managed identity to authenticate and access the data in the storage account.\n\nBy using managed identities, you avoid the need to store sensitive credentials in your code or configuration files, and you can manage access centrally through Azure role-based access control (RBAC). This approach enhances security and simplifies the management of credentials.", "context": ["You can connect to storage services via identity-based data access with Azure Machine\nLearning datastores .\nWhen you use identity-based data access, Azure Machine Learning prompts you for\nyour Microsoft Entra token for data access authentication instead of keeping your\ncredentials in the datastore. That approach allows for data access management at the\nstorage level and keeps credentials confidential.\nThe same behavior applies when you work with data interactively via a Jupyter\nNotebook on your local computer or compute instance .\nTo help ensure that you securely connect to your storage service on Azure, Azure\nMachine Learning requires that you have permission to access the corresponding data\nstorage.\nIdentity-based data access supports connections to only the following storage services.\nAzure Blob S torage\nAzure Data Lake S torage Gen1\nAzure Data Lake S torage Gen2\nTo access these storage services, you must have at least Storage Blob Data R eader\naccess to the storage account. Only storage account owners can change your access\nlevel via the Azure portal .\uff17 Note\nCredentials stored via credential-based authentication include subscription IDs,\nshared access signature (SAS) tokens, and storage access key and service principal\ninformation, like client IDs and tenant IDs.\n\uff12 Warning\nCross tenant access to storage accounts is not supported. If cross tenant access is\nneeded for your scenario, please reach out to the Azure Machine Learning Data\nSupport team alias at amldatasupport@microsoft.com for assistance with a custom\ncode solution.\nAccess data for training jobs on compute using managed\nidentity", "Certain machine learning scenarios involve working with private data. In such cases, data\nscientists may not have direct access to data as Microsoft Entra users. In this scenario,\nthe managed identity of a compute can be used for data access authentication. In this\nscenario, the data can only be accessed from a compute instance or a machine learning\ncompute cluster executing a training job. With this approach, the admin grants the\ncompute instance or compute cluster managed identity S torage Blob Data R eader\npermissions on the storage. The individual data scientists don't need to be granted\naccess.\nTo enable authentication with compute managed identity:\nCreate compute with managed identity enabled. See the compute cluster  section,\nor for compute instance, the Assign managed identity  section.\nGrant compute managed identity at least S torage Blob Data R eader role on the\nstorage account.\nCreate any datastores with identity-based authentication enabled. See Create\ndatastores .\nOnce the identity-based authentication is enabled, the compute managed identity is\nused by default when accessing data within your training jobs. Optionally, you can\nauthenticate with user identity using the steps described in next section.\nFor information on using configuring Azure RBA C for the storage, see role-based access\ncontrols .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nWhen training on Azure Machine Learning compute clusters , you can authenticate to\nstorage with your user Microsoft Entra token.\nThis authentication mode allows you to:\uff17 Note\nThe name of the created system managed identity for compute instance or cluster\nwill be in the format /workspace-name/computes/compute-name in your Microsoft\nEntra ID.\nAccess data for training jobs on compute clusters using\nuser identity", "means that in order to successfully read data from storage, the managed identity of the\ncompute cluster where the deployment is running must have at least Storage Blob Data\nReader  access to the storage account.\nTo configure the compute cluster for data access, follow these steps:\n1. Go to Azure Machine Learning studio .\n2. Navigate to Comput e, then Comput e clust ers, and select the compute cluster your\ndeployment is using.\n3. Assign a managed identity to the compute cluster:\na. In the Managed identity  section, verify if the compute has a managed identity\nassigned. If not, select the option Edit.\nb. Select Assign a managed identity  and configure it as needed. Y ou can use a\nSystem-Assigned Managed Identity or a User-Assigned Managed Identity. If\nusing a S ystem-Assigned Managed Identity, it is named as \"[workspace\nname]/computes/[compute cluster name]\".\nc. Save the changes.\n4. Go to the Azure portal  and navigate to the associated storage account where the\ndata is located. If your data input is a Data Asset or a Data S tore, look for the\nstorage account where those assets are placed.", "Learn more about enterprise security in Azure Machine Learning\nLearn about data administration\nLearn about managed identities on compute cluster .", "Data administration\nArticle \u202209/26/2023\nLearn how to manage data access and how to authenticate in Azure Machine Learning\nAPPLIES T O: \n  Azure CLI ml extension v2 (current)  \n  Python SDK azure-ai-ml v2\n(current)\nIn general, data access from studio involves these checks:\nWhich user wants to access the resources?\nDepending on the storage type, different types of authentication are available,\nfor example\naccount key\ntoken\nservice principal\nmanaged identity\nuser identity\nFor authentication based on a user identity, you must know which  specific user\ntried to access the storage resource. For more information about user\nauthentication, see authentication for Azure Machine Learning . For more\ninformation about service-level authentication, see authentication between\nAzure Machine Learning and other services .\nDoes this user have permission?\nDoes the user have the correct credentials? If yes, does the service principal,\nmanaged identity, etc., have the necessary permissions for that storage\nresource? P ermissions are granted using Azure role-based access controls\n(Azure RBA C).\nThe storage account Reader  reads the storage metadata.\nThe Storage Blob Data R eader  reads data within a blob container.\nThe Contributor  allows write access to a storage account.\nMore roles may be required, depending on the type of storage.\nWhere does the access come from?\nUser: Is the client IP address in the VNet/subnet range?\n\uff09 Impor tant\nThis article is intended for Azure administrators who want to create the required\ninfrastructure for an Azure Machine Learning solution."]}
{"question": "What are the implications of storage redundancy options in Azure Storage on data availability for Azure Machine Learning experiments?", "variant_id": "v1", "line_number": 37, "answer": "Storage redundancy options in Azure Storage can have significant implications on data availability for Azure Machine Learning experiments. Here are some key points to consider:\n\n1. **Locally Redundant Storage (LRS)**: This option replicates your data within a single storage scale unit in a single data center. While LRS is the least expensive redundancy option, it also poses the highest risk for data availability because if the data center fails, you could lose access to your data. For Azure Machine Learning, this could mean that your experiments may not run or complete if the data is not accessible.\n\n2. **Zone-Redundant Storage (ZRS)**: ZRS replicates your data across multiple data centers or zones within a region. This increases data availability since even if one zone goes down, the other zones can still provide access to your data. For Azure Machine Learning experiments, this means a higher likelihood that your data will be available when needed, even in the event of a partial regional outage.\n\n3. **Geo-Redundant Storage (GRS)** and **Geo-Zone-Redundant Storage (GZRS)**: These options provide the highest level of data availability by replicating data to a secondary region far away from the primary region. GRS and GZRS protect against regional outages, ensuring that your data is still accessible for Azure Machine Learning experiments even if the primary region experiences a significant disruption.\n\nChoosing the right storage redundancy option depends on your specific requirements for data availability, recovery, and cost. For critical Azure Machine Learning experiments where data availability is paramount, it is advisable to opt for higher redundancy options like ZRS, GRS, or GZRS. However, these options come with higher costs compared to LRS.\n\nIt's also important to consider the location of your Azure Machine Learning workspace and compute resources in relation to your storage. To minimize latency and data transfer costs, it's recommended to deploy all resources in the same region. If your storage redundancy option involves replication across regions (like GRS or GZRS), you should still ensure that your primary region for storage aligns with the region of your Azure Machine Learning resources to optimize for performance and cost.", "context": ["setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps", "Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps", "Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Service Usage Example datanumber of nodes; datastore names and tags, and\ndescriptions on assets like models; and data label names.\nAzure AI\nSearchStores indexes that\nhelp with querying\nyour machine learning\ncontent.These indexes are built on top of the data stored in Azure\nCosmos DB.\nAzure\nStorageStores metadata\nrelated to Azure\nMachine Learning\npipeline data.Data can include designer pipeline names, pipeline layout,\nand execution properties.\nFrom the perspective of data lifecycle management, data in the preceding resources is\ncreated and deleted as you create and delete corresponding objects in Azure Machine\nLearning.\nYour Azure Machine Learning workspace reads and writes data by using its managed\nidentity. This identity is granted access to the resources through a role assignment\n(Azure role-based access control) on the data resources. The encryption key that you\nprovide is used to encrypt data that's stored on Microsoft-managed resources. It's also\nused to create indexes for Azure AI Search at runtime.\nExtra networking controls are configured when you create a private link endpoint on\nyour workspace to allow for inbound connectivity. This configuration includes the\ncreation of a private link endpoint connection to the Azure Cosmos DB instance.\nNetwork access is restricted to only trusted Microsoft services.\nWhen you don't use a customer-managed key, Microsoft creates and manages resources\nin a Microsoft-owned Azure subscription and uses a Microsoft-managed key to encrypt\nthe data.\nWhen you use a customer-managed key, the resources are in your Azure subscription\nand encrypted with your key. While these resources exist in your subscription, Microsoft\nmanages them. They're automatically created and configured when you create your\nAzure Machine Learning workspace.\nThese Microsoft-managed resources are located in a new Azure resource group that's\ncreated in your subscription. This resource group is separate from the resource group\nfor your workspace. It contains the Microsoft-managed resources that your key is used\nwith. The formula for naming the resource group is: <Azure Machine Learning workspace\nresource group name><GUID>.Customer-managed keys", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow."]}
{"question": "How do you automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage?", "variant_id": "v1", "line_number": 38, "answer": "To automate the backup and recovery of datasets used in Azure Machine Learning from Azure Storage, you can use Azure services and features that are designed for backup and disaster recovery. Here are the steps you can follow:\n\n1. **Azure Backup Service:**\n   - Use Azure Backup to create backup policies for Azure Blob Storage.\n   - Schedule regular backups according to your requirements.\n   - Azure Backup will manage the lifecycle of the backups, including retention policies.\n\n2. **Azure Data Factory:**\n   - Create a data pipeline in Azure Data Factory that copies data from your primary Azure Blob Storage to a secondary location for backup.\n   - Schedule the pipeline to run at regular intervals.\n   - Use the monitoring features to track the pipeline's performance and ensure backups are completed successfully.\n\n3. **Azure Blob Storage Snapshots:**\n   - Create snapshots of your Azure Blob Storage blobs to maintain recoverable points in time.\n   - Automate snapshot creation using Azure Automation or Azure Functions with a time-triggered schedule.\n   - Manage snapshot lifecycle to delete old snapshots and maintain storage costs.\n\n4. **Azure Site Recovery:**\n   - Although typically used for virtual machines, you can use Azure Site Recovery to replicate your storage accounts to a secondary region for geo-redundancy.\n   - Configure replication and failover policies as per your business continuity plans.\n\n5. **Azure Storage Account Geo-Replication:**\n   - Enable geo-redundant storage (GRS) or read-access geo-redundant storage (RA-GRS) on your Azure Storage account to automatically replicate your data to a secondary region.\n\n6. **Azure Automation:**\n   - Use Azure Automation to run runbooks that can automate the backup process.\n   - Create scripts to manage the backup lifecycle, including the creation and deletion of backups based on a schedule.\n\n7. **Azure Logic Apps:**\n   - Create logic apps that trigger on a schedule or specific events to manage the backup process.\n   - Integrate with other Azure services and custom logic to ensure datasets are backed up and recoverable.\n\n8. **Monitoring and Alerts:**\n   - Set up monitoring and alerts using Azure Monitor to track the status of your backups.\n   - Receive notifications in case of failures or issues with the backup process.\n\nRemember to test your backup and recovery process regularly to ensure that it meets your recovery time objectives (RTO) and recovery point objectives (RPO). Additionally, consider the cost implications of the backup strategy you choose, as data storage and transfer can incur costs.", "context": ["Data encryption with Azure Machine\nLearning\nArticle \u202201/31/2024\nAzure Machine Learning relies on various Azure data storage services and compute\nresources when you're training models and performing inferences. In this article, learn\nabout the data encryption for each service both at rest and in transit.\nFor production-grade encryption during training, we recommend that you use an Azure\nMachine Learning compute cluster. For production-grade encryption during inference,\nwe recommend that you use Azure Kubernetes Service (AKS).\nAn Azure Machine Learning compute instance is a dev/test environment. When you use\nit, we recommend that you store your files, such as notebooks and scripts, in a file share.\nStore your data in a datastore.\nAzure Machine Learning end-to-end projects integrate with services like Azure Blob\nStorage, Azure Cosmos DB, and Azure SQL Database. This article describes encryption\nmethods for such services.\nAzure Machine Learning stores snapshots, output, and logs in the Azure Blob S torage\naccount (default storage account) that's tied to the Azure Machine Learning workspace\nand your subscription. All the data stored in Azure Blob S torage is encrypted at rest with\nMicrosoft-managed keys.\nFor information on how to use your own keys for data stored in Azure Blob S torage, see\nAzure S torage encryption with customer-managed keys in Azure K ey Vault.\nTraining data is typically also stored in Azure Blob S torage so that training compute\ntargets can access it. Azure Machine Learning doesn't manage this storage. This storage\nis mounted to compute targets as a remote file system.\nIf you need to rotate or r evoke your key, you can do so at any time. When you rotate a\nkey, the storage account starts using the new key (latest version) to encrypt data at rest.\nWhen you revoke (disable) a key, the storage account takes care of failing requests. It\nusually takes an hour for the rotation or revocation to be effective.Encryption at rest\nAzure Blob Storage", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "Azure Machine Learning uses various compute resources and datastores on the Azure\nplatform. T o learn more about how each of these resources supports data encryption at\nrest and in transit, see Data encryption with Azure Machine Learning .\nAzure Machine Learning has several inbound and outbound network dependencies.\nSome of these dependencies can expose a data exfiltration risk by malicious agents\nwithin your organization. These risks are associated with the outbound requirements to\nAzure S torage, Azure Front Door, and Azure Monitor. For recommendations on\nmitigating this risk, see Azure Machine Learning data exfiltration prevention .\nMicrosoft Defender for Cloud  provides unified security management and advanced\nthreat protection across hybrid cloud workloads. For Azure Machine Learning, you\nshould enable scanning of your Azure Container R egistry  resource and AKS resources.\nFor more information, see Introduction to Microsoft Defender for container registries\nand Introduction to Microsoft Defender for Kubernetes .\nAzure P olicy is a governance tool that helps you ensure that Azure resources comply\nwith your policies. Y ou can set policies to allow or enforce specific configurations, such\nas whether your Azure Machine Learning workspace uses a private endpoint.\nFor more information on Azure P olicy, see the Azure P olicy documentation . For more\ninformation on the policies that are specific to Azure Machine Learning, see Audit and\nmanage Azure Machine Learning .\nAzure Machine Learning best practices for enterprise security\nUse Azure Machine Learning with Azure Firewall\nUse Azure Machine Learning with Azure Virtual Network\nEncrypt data at rest and in transit\nBuild a real-time recommendation API on AzurePrevent data exfiltration\nScan for vulnerabilities\nAudit and manage compliance\nNext steps", "setting up policies to manage your data throughout its lifecycle to archive and delete\nyour datasets. For more information, see optimize costs by automating Azure Blob\nStorage access tiers .\nComputes located in different regions may experience network latency and increased\ndata transfer costs. Azure network costs are incurred from outbound bandwidth from\nAzure data centers. T o help reduce network costs, deploy all your resources in the\nregion. Provisioning your Azure Machine Learning workspace and dependent resources\nin the same region as your data can help lower cost and improve performance.\nFor hybrid cloud scenarios like those using ExpressR oute, it can sometimes be more cost\neffective to move all resources to Azure to optimize network costs and latency.\nPlan to manage costs for Azure Machine Learning\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscaleDeploy resources to the same region\nNext steps", "Machine Learning also stores metadata in an Azure Cosmos DB instance. By default, this\ninstance is hosted in a Microsoft subscription, and Microsoft manages it. Y ou can\noptionally use an Azure Cosmos DB instance in your Azure subscription. For more\ninformation, see Data encryption with Azure Machine Learning .\nWhen you access your workspace from the studio, the network traffic flows are as\nfollows:\nTo authenticate to resources, the configuration uses Microsoft Entra ID.\nFor management and deployment operations, the configuration uses Azure\nResource Manager.\nFor tasks that are specific to Azure Machine Learning, the configuration uses the\nAzure Machine Learning service.\nFor access to Azure Machine Learning studio , the configuration uses Azure Front\nDoor.\nFor most storage operations, traffic flows through the private endpoint of the\ndefault storage for your workspace. The Use AutoML, the designer, the dataset,\nand the datastore from the studio  section of this article discusses exceptions.\nYou also need to configure a DNS solution that allows you to resolve the names of\nthe resources within the virtual network. For more information, see Use your\nworkspace with a custom DNS server .Scenario: Access a workspace from the studio\n\uff17 Note\nThe information in this section is specific to using the workspace from Azure\nMachine Learning studio. If you use the Azure Machine Learning SDK, the REST API,\nthe Azure CLI, or Visual S tudio Code, the information in this section doesn't apply\nto you."]}
{"question": "What tools and techniques are available for analyzing storage costs associated with Azure Machine Learning projects?", "variant_id": "v1", "line_number": 39, "answer": "To analyze storage costs associated with Azure Machine Learning projects, you can use the following tools and techniques:\n\n1. **Azure Pricing Calculator**: Before adding any resources, you can use the Azure Pricing Calculator to estimate the costs of the storage you plan to use with Azure Machine Learning.\n\n2. **Cost Management Features**: After you've started using Azure Machine Learning resources, you can utilize Azure Cost Management features to monitor and manage your costs. This includes setting budgets, reviewing forecasted costs, and identifying spending trends.\n\n3. **Cost Analysis in Azure Cost Management**: This tool supports most Azure account types and allows you to analyze your spending on Azure services, including storage used by Azure Machine Learning. You can view and analyze your costs, usage patterns, and trends over time.\n\n4. **Assign Access to Cost Data**: To view cost data, you need at least read access to an Azure account. You can assign access to Azure Cost Management data to ensure the right team members can monitor and manage costs.\n\nRemember to review the Azure documentation for any updates or changes to these tools and techniques, as Azure services are continuously evolving. Additionally, consider applying similar cost management methods to other Azure services you use in conjunction with Azure Machine Learning to get a comprehensive view of your overall costs.", "context": ["Plan to manage costs for Azure Machine\nLearning\nArticle \u202203/31/2023\nThis article describes how to plan and manage costs for Azure Machine Learning. First,\nyou use the Azure pricing calculator to help plan for costs before you add any resources.\nNext, as you add the Azure resources, review the estimated costs.\nAfter you've started using Azure Machine Learning resources, use the cost management\nfeatures to set budgets and monitor costs. Also review the forecasted costs and identify\nspending trends to identify areas where you might want to act.\nUnderstand that the costs for Azure Machine Learning are only a portion of the monthly\ncosts in your Azure bill. If you are using other Azure services, you're billed for all the\nAzure services and resources used in your Azure subscription, including the third-party\nservices. This article explains how to plan for and manage costs for Azure Machine\nLearning. After you're familiar with managing costs for Azure Machine Learning, apply\nsimilar methods to manage costs for all the Azure services used in your subscription.\nFor more information on optimizing costs, see how to manage and optimize cost in\nAzure Machine Learning .\nCost analysis in Cost Management supports most Azure account types, but not all of\nthem. T o view the full list of supported account types, see Understand Cost\nManagement data .\nTo view cost data, you need at least read access for an Azure account. For information\nabout assigning access to Azure Cost Management data, see Assign access to data .\uff09 Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nPrerequisites", "Use the Azure pricing calculator  to estimate costs before you create the\nresources in an Azure Machine Learning workspace. On the left, select AI +\nMachine Learning , then select Azur e Machine Learning  to begin.\nThe following screenshot shows the cost estimation by using the calculator:\nAs you add new resources to your workspace, return to this calculator and add the same\nresource here to update your cost estimates.\nFor more information, see Azure Machine Learning pricing .\nAzure Machine Learning runs on Azure infrastructure that accrues costs along with\nAzure Machine Learning when you deploy the new resource. It's important toEstimate costs before using Azure Machine\nLearning\nUnderstand the full billing  model for Azure\nMachine Learning", "Manage and optimize Azure Machine\nLearning costs\nArticle \u202208/01/2023\nLearn how to manage and optimize costs when training and deploying machine learning\nmodels to Azure Machine Learning.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nFor information on planning and monitoring costs, see the plan to manage costs for\nAzure Machine Learning  guide.\nWith constantly changing data, you need fast and streamlined model training and\nretraining to maintain accurate models. However, continuous training comes at a cost,\nespecially for deep learning models on GPUs.\uff09 Impor tant\nItems marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nUse Azure Machine Learning compute cluster\n(AmlCompute)", "monthly schedule and set a custom date range. Exporting cost data is the\nrecommended way to retrieve cost datasets.\nUse the following tips to help you manage and optimize your compute resource costs.\nConfigure your training clusters for autoscaling\nSet quotas on your subscription and workspaces\nSet termination policies on your training job\nUse low-priority virtual machines (VM)\nSchedule compute instances to shut down and start up automatically\nUse an Azure R eserved VM Instance\nTrain locally\nParallelize training\nSet data retention and deletion policies\nDeploy resources to the same region\nDelete instances and clusters if you do not plan on using them in the near future.\nFor more information, see manage and optimize costs in Azure Machine Learning .\nManage and optimize costs in Azure Machine Learning .\nManage budgets, costs, and quota for Azure Machine Learning at organizational\nscale\nLearn how to optimize your cloud investment with Azure Cost Management .\nLearn more about managing costs with cost analysis .\nLearn about how to prevent unexpected costs .\nTake the Cost Management  guided learning course.Other ways to manage and reduce costs for\nAzure Machine Learning\nNext steps", "To view Azure Machine Learning costs in cost analysis:\n1. Sign in to the Azure portal.\n2. Open the scope in the Azure portal and select Cost analysis  in the menu. For\nexample, go to Subscriptions , select a subscription from the list, and then select\nCost analysis  in the menu. Select Scope  to switch to a different scope in cost\nanalysis.\n3. By default, cost for services are shown in the first donut chart. Select the area in the\nchart labeled Azure Machine Learning.\nActual monthly costs are shown when you initially open cost analysis. Here's an example\nshowing all monthly usage costs.\nTo narrow costs for a single service, like Azure Machine Learning, select Add filt er and\nthen select Service name . Then, select virtual machines .\nHere's an example showing costs for just Azure Machine Learning.\n\uf80a"]}
{"question": "How can data engineers streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics?", "variant_id": "v1", "line_number": 40, "answer": "Data engineers can streamline the data ingestion process from Azure Storage to Azure Machine Learning for real-time analytics by leveraging several Azure services and features that work together to facilitate a smooth and efficient data pipeline. Here are some steps and best practices:\n\n1. **Azure Data Lake Storage (ADLS) Gen 2**: Utilize ADLS Gen 2 as the primary data storage, which provides a scalable and secure data lake that supports high-throughput analytics.\n\n2. **Azure Synapse Analytics**: Integrate Azure Machine Learning with Azure Synapse Analytics to access an Apache Spark pool. This allows for interactive data wrangling and preprocessing within Azure Machine Learning Notebooks.\n\n3. **Azure Machine Learning Pipelines**: Create Azure Machine Learning pipelines that can automate the data ingestion and preprocessing steps. These pipelines can be triggered on a schedule or in response to events, ensuring that the data is ready for real-time analytics.\n\n4. **Real-time Ingestion with Azure Event Hubs or Azure IoT Hub**: For real-time analytics, use Azure Event Hubs or Azure IoT Hub to ingest streaming data directly into Azure Machine Learning.\n\n5. **Azure Functions**: Use Azure Functions to create serverless components that can process or transform data as it arrives, reducing latency for real-time analytics.\n\n6. **Azure Key Vault**: Store sensitive information such as connection strings, access keys, and secrets securely in Azure Key Vault. This ensures that the data ingestion process is secure and that credentials are managed properly.\n\n7. **Role Assignments and Access Control**: Manage role assignments and access control for Azure Storage accounts to ensure that only authorized services and users can access the data.\n\n8. **Monitoring and Logging**: Implement monitoring and logging using Azure Monitor and Azure Log Analytics to track the performance of the data ingestion process and to quickly identify and troubleshoot any issues.\n\nBy following these steps, data engineers can create a robust and efficient data ingestion pipeline that feeds into Azure Machine Learning for real-time analytics, ensuring that the data is always up-to-date and ready for analysis.", "context": ["Interactive Data Wrangling with Apache\nSpark in Azure Machine Learning\nArticle \u202210/05/2023\nData wrangling becomes one of the most important steps in machine learning projects.\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides access\nto an Apache Spark pool - backed by Azure S ynapse - for interactive data wrangling\nusing Azure Machine Learning Notebooks.\nIn this article, you'll learn how to perform data wrangling using\nServerless Spark compute\nAttached S ynapse Spark pool\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an Azure\nData Lake S torage (ADLS) Gen 2 storage account .\n(Optional): An Azure K ey Vault. See Create an Azure K ey Vault.\n(Optional): A Service Principal. See Create a Service Principal .\n(Optional): An attached S ynapse Spark pool in the Azure Machine Learning\nworkspace .\nBefore you start your data wrangling tasks, learn about the process of storing secrets\nAzure Blob storage account access key\nShared Access Signature (SAS) token\nAzure Data Lake S torage (ADLS) Gen 2 service principal information\nin the Azure K ey Vault. Y ou also need to know how to handle role assignments in the\nAzure storage accounts. The following sections review these concepts. Then, we'll\nexplore the details of interactive data wrangling using the Spark pools in Azure Machine\nLearning Notebooks.Prerequisites\n\uea80 Tip", "Tutorial Descr iption\nUpload, access and explore your data in\nAzure Machine LearningStore large data in the cloud and retrieve it from\nnotebooks and scripts\nModel development on a cloud workstation Start prototyping and developing machine\nlearning models\nTrain a model in Azure Machine Learning Dive in to the details of training a model\nDeploy a model as an online endpoint Dive in to the details of deploying a model\nCreate production machine learning pipelines Split a complete machine learning task into a\nmultistep workflow.", "runs from experimentation in the studio user interface. Or they can use versioned assets\nfor jobs like environments and storage references.\nFor more information, see Manage Azure Machine Learning workspaces .\nWhen a project is ready for operationalization, users' work can be automated in an ML\npipeline and triggered on a schedule or HT TPS request.\nYou can deploy models to the managed inferencing solution, for both real-time and\nbatch deployments, abstracting away the infrastructure management typically required\nfor deploying models.\nIn Azure Machine Learning, you can run your training script in the cloud or build a\nmodel from scratch. Customers often bring models they've built and trained in open-\nsource frameworks so that they can operationalize them in the cloud.\nData scientists can use models in Azure Machine Learning that they've created in\ncommon Python frameworks, such as:\nPyTorch\nTensorFlow\nscikit-learn\nXGBoost\nLightGBM\nOther languages and frameworks are also supported:\nR\n.NET\nFor more information, see Open-source integration with Azure Machine Learning .\nIn a repetitive, time-consuming process, in classical ML, data scientists use prior\nexperience and intuition to select the right data featurization and algorithm for training.\nAutomated ML (AutoML) speeds this process. Y ou can use it through the Machine\nLearning studio UI or the Python SDK.Train models\nOpen and interoperable\nAutomated featurization and algorithm selection", "training and evaluation, the machine learning engineers focus on model deployment\nand automation of the entire workflow. By leveraging machine learning pipeline, each\nteam only needs to work on building their own steps. The best way of building steps is\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\none step in a machine learning pipeline. All these steps built by different users are finally\nintegrated into one workflow through the pipeline definition. The pipeline is a\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\nits steps can be standardized by each company's preferred DevOps practice. The\npipeline can be further versioned and automated. If the ML projects are described as a\npipeline, then the best ML Ops practice is already applied.\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\nimproves large model training's efficiency and reduces cost. T aking modern natural\nlanguage model training as an example. It requires pre-processing large amounts of\ndata and GPU intensive transformer model training. It takes hours to days to train a\nmodel each time. When the model is being built, the data scientist wants to test\ndifferent training code or hyperparameters and run the training many times to get the\nbest model performance. For most of these trainings, there's usually small changes from\none training to another one. It will be a significant waste if every time the full training\nfrom data processing to model training takes place. By using machine learning pipeline,\nit can automatically calculate which steps result is unchanged and reuse outputs from\nprevious training. Additionally, the machine learning pipeline supports running each\nstep on different computation resources. Such that, the memory heavy data processing\nwork and run-on high memory CPU machines, and the computation intensive training\ncan run on expensive GPU machines. By properly choosing which step to run on which\ntype of machines, the training cost can be significantly reduced.\nDepending on what a machine learning project already has, the starting point of\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\nbuilding a pipeline.\nThe first approach usually applies to the team that hasn't used pipeline before and\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\ntypically have developed some machine learning models on their local environment\nusing their favorite tools. Machine learning engineers need to take data scientists'\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\nGetting  started best practices", "training and evaluation, the machine learning engineers focus on model deployment\nand automation of the entire workflow. By leveraging machine learning pipeline, each\nteam only needs to work on building their own steps. The best way of building steps is\nusing Azure Machine Learning component (v2) , a self-contained piece of code that does\none step in a machine learning pipeline. All these steps built by different users are finally\nintegrated into one workflow through the pipeline definition. The pipeline is a\ncollaboration tool for everyone in the project. The process of defining a pipeline and all\nits steps can be standardized by each company's preferred DevOps practice. The\npipeline can be further versioned and automated. If the ML projects are described as a\npipeline, then the best ML Ops practice is already applied.\nBesides being the tool to put ML Ops into practice, the machine learning pipeline also\nimproves large model training's efficiency and reduces cost. T aking modern natural\nlanguage model training as an example. It requires pre-processing large amounts of\ndata and GPU intensive transformer model training. It takes hours to days to train a\nmodel each time. When the model is being built, the data scientist wants to test\ndifferent training code or hyperparameters and run the training many times to get the\nbest model performance. For most of these trainings, there's usually small changes from\none training to another one. It will be a significant waste if every time the full training\nfrom data processing to model training takes place. By using machine learning pipeline,\nit can automatically calculate which steps result is unchanged and reuse outputs from\nprevious training. Additionally, the machine learning pipeline supports running each\nstep on different computation resources. Such that, the memory heavy data processing\nwork and run-on high memory CPU machines, and the computation intensive training\ncan run on expensive GPU machines. By properly choosing which step to run on which\ntype of machines, the training cost can be significantly reduced.\nDepending on what a machine learning project already has, the starting point of\nbuilding a machine learning pipeline might vary. There are a few typical approaches to\nbuilding a pipeline.\nThe first approach usually applies to the team that hasn't used pipeline before and\nwants to take some advantage of pipeline like ML Ops. In this situation, data scientists\ntypically have developed some machine learning models on their local environment\nusing their favorite tools. Machine learning engineers need to take data scientists'\noutput into production. The work involves cleaning up some unnecessary code fromTraining efficiency and cost reduction\nGetting  started best practices"]}
{"question": "How can I using MSI with Storage?", "variant_id": "v1", "line_number": 41, "answer": "To use Managed Service Identity (MSI) with Azure Storage within the context of Azure Machine Learning, you would typically set up an identity for your Azure Machine Learning workspace and grant it access to your Azure Storage account. Here's a general outline of the steps you would follow:\n\n1. **Enable Managed Identity for Azure Machine Learning Workspace:**\n   - In the Azure portal, navigate to your Azure Machine Learning workspace.\n   - Under the \"Settings\" section, find and click on \"Identity\".\n   - In the \"System assigned\" tab, switch the Status to \"On\" to enable a system-assigned managed identity for the workspace. Save the changes.\n\n2. **Grant Access to Azure Storage:**\n   - Navigate to your Azure Storage account in the Azure portal.\n   - Under the \"Access control (IAM)\" section, click on \"Add role assignment\".\n   - Choose the appropriate role that you want to assign to the Azure Machine Learning workspace (e.g., Storage Blob Data Contributor).\n   - In the \"Assign access to\" dropdown, select \"Managed Identity\".\n   - Search for your Azure Machine Learning workspace and select it.\n   - Click \"Save\" to grant the managed identity access to the storage account.\n\n3. **Configure Azure Machine Learning to Use the Managed Identity:**\n   - When setting up datastores or other storage-related configurations in Azure Machine Learning, you can specify that the managed identity should be used for authentication.\n\n4. **Access Storage from Azure Machine Learning:**\n   - In your Azure Machine Learning code or pipeline, when accessing the Azure Storage, the managed identity will be used automatically for authentication, assuming that the environment is correctly configured to use the identity.\n\nPlease note that the exact steps and configurations might vary based on the specific requirements of your project and the current features available in Azure Machine Learning and Azure Storage services. Always refer to the latest Azure documentation for the most accurate and up-to-date instructions.", "context": ["In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Choose the Dataset type :\nIf you're using a .csv or .tsv file and each row contains a response, select\nTabular .\nIf you're using separate .txt files for each response, select File.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. The default uploads to the default blob store\n(worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\nCreate a dataset from uploaded data", "Service tag Protocol Port\nStorage.<region> TCP 443\nFor more information, see How to secure training environments  and Configure inbound\nand outbound network traffic .\nUse the following steps to enable a storage endpoint for the subnet that contains your\nAzure Machine Learning compute clusters and compute instances:\n1. From the Azure portal , select the Azur e Vir tual Netw ork for your Azure Machine\nLearning workspace.\n2. From the left of the page, select Subnets  and then select the subnet that contains\nyour compute cluster and compute instance.\n3. In the form that appears, expand the Services  dropdown and then enable\nMicrosoft.Storage . Select Save to save these changes.\n4. Apply the service endpoint policy to your workspace subnet.\uff17 Note\nFor the storage outbound, a Service Endpoint P olicy will be applied in a later\nstep to limit outbound traffic.\n3. Enable storage endpoint for the subnet\n\uf80a", "In many cases, you can upload local files. However, Azure S torage Explorer  provides a\nfaster and more robust way to transfer a large amount of data. W e recommend S torage\nExplorer as the default way to move files.\nTo create a dataset from data that's already stored in Blob S torage:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From Azur e storage , and then select Next .\n6. Select the datastore, and then select Next .\n7. If your data is in a subfolder within Blob S torage, choose Browse  to select the path.\nTo include all the files in the subfolders of the selected path, append /** to\nthe path.\nTo include all the data in the current container and its subfolders, append\n**/*.* to the path.\n8. Select Create.\n9. Select the data asset you created.\nTo directly upload your data:\n1. Select Create.\n2. For Name , enter a name for your dataset. Optionally, enter a description.\n3. Ensure that Dataset type  is set to File. Only file dataset types are supported for\nimages.\n4. Select Next .\n5. Select From local files , and then select Next .\n6. (Optional) Select a datastore. Y ou can also leave the default to upload to the\ndefault blob store ( worksp aceblobst ore) for your Machine Learning workspace.\n7. Select Next .\n8. Select Uplo ad > Uplo ad files  or Uplo ad > Uplo ad folder  to select the local files or\nfolders to upload.\n9. In the browser window, find your files or folders, and then select Open .\n10. Continue to select Uplo ad until you specify all your files and folders.Create a dataset from an Azure datastore\nCreate a dataset from uploaded data", "b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably.", "b. For Connection name , keep the default populated value.\nc. For Authentication kind , select Organizational account  to use the credentials\nof the connected user via O Auth 2.0.\nd. Select Sign in  to sign in.\n8. Select Next .\n9. Configure the path to the shortcut, relative to the storage account, if needed. Use\nthis setting to configure the folder that the shortcut will point to.\n10. Configure the Name  of the shortcut. This name will be a path inside the lakehouse.\nIn this example, name the shortcut datasets .\n11. Save the changes.\n1. Open the Azure Machine Learning studio .\n2. Go to your Azure Machine Learning workspace.\n3. Go to the Data  section.\n4. Select the Datast ores tab.\n5. Select Create.\n6. Configure the datastore as follows:\na. For Datast ore name , enter trust ed_blob .\nb. For Datast ore type  select Azur e Blob S torage .\nc. Select the storage account from the wizard, using the Subscription ID , Storage\naccount , and Blob container  (file system).Create a datastore that points to the storage account\n\uea80 Tip\nWhy should you configure Azur e Blob S torage  instead of Azur e Data Lak e\nGen2 ? Batch endpoints can only write predictions to Blob S torage\naccounts. However, every Azure Data Lake Gen2 storage account is also a\nblob storage account; therefore, they can be used interchangeably."]}
{"question": "How can I browse data in the AML workspace?", "variant_id": "v1", "line_number": 42, "answer": "To browse data in an Azure Machine Learning (AML) workspace, you can follow these steps:\n\n1. **Access Azure Machine Learning Studio**: Open Azure Machine Learning Studio by navigating to https://ml.azure.com. You will need to sign in with the credentials associated with your Azure subscription.\n\n2. **Select Your Workspace**: Once you are in the Azure Machine Learning Studio, select the appropriate workspace from the top right corner if it's not already selected.\n\n3. **Navigate to the Datasets**: On the left side navigation panel, click on \"Datasets\" to view the datasets that have been registered or uploaded to your workspace.\n\n4. **Explore Datasets**: In the Datasets section, you can explore the list of datasets. You can click on any dataset to see more details, such as the schema, profile, and explore the data if the dataset is not too large.\n\n5. **Preview Data**: For many datasets, you can preview the data directly in the Azure Machine Learning Studio by selecting the dataset and clicking on the \"Preview\" tab.\n\n6. **Use Notebooks**: If you want to interact with the data programmatically, you can use Azure Notebooks. Create a new notebook or open an existing one and use the Azure ML SDK to load and work with your data.\n\nRemember that the exact steps and UI elements might change over time as Azure Machine Learning continues to evolve. Always refer to the latest Azure Machine Learning documentation for the most current information.", "context": ["You'll need the following items:\nTo use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\nRun one or more jobs in your workspace to have results available in the\ndashboard. Complete Tutorial: T rain a model in Azure Machine Learning  if you\ndon't have any jobs yet.\nEnable this preview feature via the preview panel.\nNext, let's view your jobs in the studio:\nSelect Jobs on the left side navigation panel.\nSelect either All experiments  to view all the jobs in an experiment or select All jobs\nto view all the jobs submitted in the workspace.\nYou are now on the default dashboard view where you will find your job list\nconsolidated into the left side bar and dashboard content on the right.Items marked (preview) in this article are currently in public preview. The preview\nversion is provided without a service level agreement, and it's not recommended\nfor production workloads. Certain features might not be supported or might have\nconstrained capabilities. For more information, see Supplemental T erms o f Use for\nMicrosoft Azur e Previews .\nPrerequisites\nExplore the dashboard view", "The following features of Azure Machine Learning studio use data profiling:\nDataset: Explore the dataset from the studio.\nDesigner: Visualize module output data.\nAutoML: View a data preview or profile and choose a target column.\nLabeling: Use labels to prepare data for a machine learning project.\nData profiling depends on the ability of the Azure Machine Learning managed service to\naccess the default Azure storage account for your workspace. The managed service\ndoesn 't exist in y our vir tual netw ork, so it can't directly access the storage account in the\nvirtual network. Instead, the workspace uses a service principal to access storage.\nTo allow access to the storage account, configure the storage account to allow a\nresource instance for your workspace or select Allow Azur e ser vices on the trust edScenario: Use AutoML, the designer, the\ndataset, and the datastore from the studio\n\uea80 Tip\nYou can provide a service principal when you're creating the workspace. If you\ndon't, one is created for you and has the same name as your workspace.", "You'll need the following items:\nTo use Azure Machine Learning, you'll first need a workspace. If you don't have\none, complete Create resources you need to get started  to create a workspace and\nlearn more about using it.\nRun one or more jobs in your workspace to have results available in the\ndashboard. Complete Tutorial: T rain a model in Azure Machine Learning  if you\ndon't have any jobs yet.\nEnable this preview feature via the preview panel.\nSelect Jobs on the left side navigation panel.\nSelect either All experiments  to view all the jobs in an experiment or select All jobs\nto view all the jobs submitted in the workspace.\nSelect List view  at the top to switch into List view .\nThe job display name is an optional and customizable name that you can provide for\nyour job. Y ou can edit this directly in your jobs list view by selecting the pencil icon\nwhen you move your mouse over a job name.\nCustomizing the name may help you organize and label your training jobs easily.Prerequisites\nView jobs list\nJob display name\nSelect and pin columns", "Azure Machine Learning extension to view, update and create workspace assets like\ncomputes, data, environments, jobs and more.\nWhen you use VS Code for the W eb, the latest versions of these extensions are\nautomatically available to you. If you use the desktop application , you may need to\ninstall them.\nWhen you launch VS Code connected to a compute instance  for the first time, make\nsure you follow these steps and take a few moments to orient yourself to the tools in\nyour integrated development environment.\n1. Locate the Azure extension and sign in\n2. Once your subscriptions are listed, you can filter to the ones you use frequently.\nYou can also pin workspaces you use most often within the subscriptions.\n3. The workspace you launched the VS Code remote connection from (the workspace\nthe compute instance is in) should be automatically set as the default. Y ou can\nupdate the default workspace from the VS Code status bar.\n4. If you plan to use the Azure Machine Learning CLI, open a terminal from the menu,\nand sign in to the Azure Machine Learning CLI using az login --identity.", "2. Expand your workspace node.\n3. Expand the Envir onments  node.\n4. Right-click the environment you want to view and select View Envir onment .\nAlternatively, use the > Azure ML: View Environment command in the command palette.\nThe quickest way to create a job is by clicking the Create Job  icon in the extension's\nactivity bar.\nUsing the resource nodes in the Azure Machine Learning view:\n1. Expand the subscription node that contains your workspace.\n2. Expand your workspace node.\n3. Right-click the Experiments  node in your workspace and select Create Job .\n4. Choose your job type.\n5. A specification file appears. Configure the specification file.\n6. Right-click the specification file and select Azur eML: Ex ecute YAML .\nAlternatively, use the > Azure ML: Create Job command in the command palette.\nTo view your job in Azure Machine Learning studio:\n1. Expand the subscription node that contains your workspace.\n2. Expand the Experiments  node inside your workspace.\n3. Right-click the experiment you want to view and select View Experiment in S tudio .\n4. A prompt appears asking you to open the experiment URL in Azure Machine\nLearning studio. Select Open .\nAlternatively, use the > Azure ML: View Experiment in Studio command respectively in\nthe command palette.\nAs you're running your job, you may want to see its progress. T o track the progress of a\njob in Azure Machine Learning studio from the extension:\n1. Expand the subscription node that contains your workspace.\n2. Expand the Experiments  node inside your workspace.\n3. Expand the job node you want to track progress for.Create job\nView job\nTrack job progress"]}
{"question": "How to use run spark jobs and integrate with ADLS Gen2?", "variant_id": "v1", "line_number": 43, "answer": "To run Apache Spark jobs and integrate with Azure Data Lake Storage (ADLS) Gen 2 using Azure Machine Learning, you can follow these steps:\n\n1. **Prerequisites**:\n   - Ensure you have an Azure subscription. If you don't, you can create a free account.\n   - Create an Azure Machine Learning workspace. You can find instructions on how to do this in the Azure documentation.\n   - Set up an ADLS Gen 2 storage account. Instructions for creating one are available in the Azure documentation.\n   - Create an Azure Machine Learning compute instance if you don't already have one.\n   - Install the Azure Machine Learning CLI (Command Line Interface) on your local machine or on a cloud-based compute instance.\n\n2. **Assign Roles**:\n   - Before submitting a Spark job, make sure that the input and output data paths in your ADLS Gen 2 storage account are accessible.\n   - Assign the 'Contributor' and 'Storage Blob Data Contributor' roles to the user identity of the logged-in user. This will enable read and write access to the storage account.\n\n3. **Submit a Spark Job**:\n   - Use the Azure Machine Learning CLI or SDK to configure and submit your Spark job.\n   - When configuring your Spark job, specify the ADLS Gen 2 storage account details as part of the input and output data paths.\n   - If you are using the Azure Machine Learning CLI, you would typically use the `az ml job create` command with a job YAML file that defines the Spark job configuration, including the compute target, which would be the Azure Machine Learning serverless Spark compute in this case.\n\n4. **User Identity Passthrough**:\n   - For security and governance, you may want to use user identity passthrough. This ensures that the job runs under the identity of the user who submitted it, leveraging their permissions for data access.\n\n5. **Monitor and Manage the Job**:\n   - After submitting the job, you can monitor its progress using the Azure Machine Learning studio or the CLI.\n   - Once the job is completed, you can review the outputs and logs to verify the results.\n\nFor more detailed instructions and examples, you should refer to the Azure Machine Learning documentation, specifically the section on running Apache Spark jobs. The documentation will provide step-by-step guidance and examples of the necessary configurations and commands.", "context": ["Quickstart: Apache Spark jobs in Azure\nMachine Lea rning\nArticle \u202205/23/2023\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides easy\naccess to distributed computing capability - backed by Azure S ynapse - for scaling\nApache Spark jobs on Azure Machine Learning.\nIn this quickstart guide, you learn how to submit a Spark job using Azure Machine\nLearning serverless Spark compute, Azure Data Lake S torage (ADLS) Gen 2 storage\naccount, and user identity passthrough in a few simple steps.\nFor more information about Apache Sp ark in Azur e Machine Learning  concepts, see\nthis resource .\nAPPLIES T O: \n  Azure CLI ml extension v2 (curr ent)\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an\nAzure Data Lake S torage (ADLS) Gen 2 storage account .\nCreate an Azure Machine Learning compute instance .\nInstall Azure Machine Learning CLI .\nBefore we submit an Apache Spark job, we must ensure that input, and output, data\npaths are accessible. Assign Contribut or and Storage Blob Data Contribut or roles to\nthe user identity of the logged-in user to enable read and write access.\nTo assign appropriate roles to the user identity:Prerequisites\nCLI\nAdd role assignments in Azure storage\naccounts", "16. Repeat steps 2-13 for Storage Blob Contribut or role assignment.\nData in the Azure Data Lake S torage (ADLS) Gen 2 storage account should become\naccessible once the user identity has appropriate roles assigned.\nA Spark job requires a Python script that takes arguments, which can be developed by\nmodifying the Python code developed from interactive data wrangling . A sample Python\nscript is shown here.\nPython\n\uf80a\nCreate parametrized Python code\n# titanic.py  \nimport argparse  \nfrom operator import add \nimport pyspark.pandas as pd \nfrom pyspark.ml.feature import Imputer  \nparser = argparse.ArgumentParser()  \nparser.add_argument( \"--titanic_data\" ) \nparser.add_argument( \"--wrangled_data\" ) \nargs = parser.parse_args()  \nprint(args.wrangled_data)  \nprint(args.titanic_data)  \ndf = pd.read_csv(args.titanic_data, index_col= \"PassengerId\" ) \nimputer = Imputer(inputCols=[ \"Age\"], outputCol= \"Age\").setStrategy(\n    \"mean\" \n)  # Replace missing values in Age column with the mean value  \ndf.fillna(  \n    value={ \"Cabin\": \"None\"}, inplace= True \n)  # Fill Cabin column with value \"None\" if missing", "Interactive Data Wrangling with Apache\nSpark in Azure Machine Learning\nArticle \u202210/05/2023\nData wrangling becomes one of the most important steps in machine learning projects.\nThe Azure Machine Learning integration, with Azure S ynapse Analytics, provides access\nto an Apache Spark pool - backed by Azure S ynapse - for interactive data wrangling\nusing Azure Machine Learning Notebooks.\nIn this article, you'll learn how to perform data wrangling using\nServerless Spark compute\nAttached S ynapse Spark pool\nAn Azure subscription; if you don't have an Azure subscription, create a free\naccount  before you begin.\nAn Azure Machine Learning workspace. See Create workspace resources .\nAn Azure Data Lake S torage (ADLS) Gen 2 storage account. See Create an Azure\nData Lake S torage (ADLS) Gen 2 storage account .\n(Optional): An Azure K ey Vault. See Create an Azure K ey Vault.\n(Optional): A Service Principal. See Create a Service Principal .\n(Optional): An attached S ynapse Spark pool in the Azure Machine Learning\nworkspace .\nBefore you start your data wrangling tasks, learn about the process of storing secrets\nAzure Blob storage account access key\nShared Access Signature (SAS) token\nAzure Data Lake S torage (ADLS) Gen 2 service principal information\nin the Azure K ey Vault. Y ou also need to know how to handle role assignments in the\nAzure storage accounts. The following sections review these concepts. Then, we'll\nexplore the details of interactive data wrangling using the Spark pools in Azure Machine\nLearning Notebooks.Prerequisites\n\uea80 Tip", "Standard_E4s_v3\nStandard_E8s_v3\nStandard_E16s_v3\nStandard_E32s_v3\nStandard_E64s_v3\n4. Input a Spark Session timeout  value, in minutes.\n5. Select whether to Dynamically allocat e executors\n6. Select the number of Executors for the Spark session.\n7. Select Executor size  from the dropdown menu.\n8. Select Driver size  from the dropdown menu.\n9. To use a Conda file to configure a Spark session, check the Uplo ad conda file\ncheckbox. Then, select Browse , and choose the Conda file with the Spark session\nconfiguration you want.\n10. Add Configuration settings  properties, input values in the Proper ty and Value\ntextboxes, and select Add.\n11. Select Apply .\n12. Select Stop session  in the Configur e new session?  pop-up.\nThe session configuration changes persist and become available to another notebook\nsession that is started using the serverless Spark compute.\nYou can access and wrangle data stored in Azure Data Lake S torage (ADLS) Gen 2\nstorage accounts with abfss:// data URIs following one of the two data access\nmechanisms:\nUser identity passthrough\nService principal-based data access\uea80 Tip\nIf you use session-level Conda packages, you can impr ove the Spark session cold\nstart time if you set the configuration variable spark.hadoop.aml.enable_cache to\ntrue.\nImport and wrangle data from Azure Data Lake Storage\n(ADLS) Gen 2\n\uea80 Tip", "Install and use the CLI (v2)\nSubmit Spark jobs in Azure Machine Learning    outputs:  \n      wrangled_data:  \n        type:  uri_folder  \n        path:  azureml://datastores/workspaceblobstore/paths/data/wrangled/  \n        mode:  direct \n    identity:  \n      type:  user_identity  \n    compute:  mysparkcompute  \nNext steps"]}
{"question": "How does batch inferencing work in parallel mode?", "variant_id": "v1", "line_number": 44, "answer": "In Azure Machine Learning, batch inferencing in parallel mode involves processing multiple mini-batches of data simultaneously across different workers and nodes. Here's how it generally works:\n\n1. **Deployment Configuration**: You configure the deployment settings to control the size of each mini-batch and the number of workers on each node. This configuration is crucial for parallel processing.\n\n2. **Mini-Batch Processing**: The `run()` function in your scoring script is called once per mini-batch. You have the option to process the entire mini-batch at once or to process the data file by file or row by row, depending on your data type (e.g., tabular).\n\n3. **Multiple Workers**: When running multiple workers on the same instance, it's important to remember that they share memory. To ensure efficient memory usage and to prevent out-of-memory exceptions, you may need to adjust the mini-batch size or change the scoring strategy if you increase the number of workers per node.\n\n4. **Parallel Execution**: Each worker can process a mini-batch of data independently. If you have multiple workers across different nodes, they can all work in parallel, each handling a separate mini-batch, thus speeding up the overall inferencing process.\n\n5. **Memory Management**: Running inference at the batch level requires careful control over the input data size to manage memory requirements effectively. The ability to load the entire mini-batch into memory depends on various factors, including the mini-batch size, the size of the instances in the cluster, the number of workers on each node, and the memory capacity.\n\n6. **High Throughput**: For high throughput scenarios, such as when using GPUs, you might want to process the entire batch at once to saturate the inference device. Some data loaders, like those provided by TensorFlow or PyTorch, can handle batching themselves, which can be beneficial when data doesn't fit in memory.\n\nIn summary, batch inferencing in parallel mode in Azure Machine Learning is about distributing the workload across multiple workers and nodes, with each worker handling a portion of the data to achieve faster processing times. The specific approach to parallel processing will depend on the deployment configuration and the characteristics of the data and compute resources.", "context": ["Your deployment configuration controls the size of each mini-batch and the number of\nworkers on each node. T ake into account them when deciding if you want to read the\nentire mini-batch to perform inference, or if you want to run inference file by file, or row\nby row (for tabular). See Running inference at the mini-batch, file or the row level  to see\nthe different approaches.\nWhen running multiple workers on the same instance, take into account that memory is\nshared across all the workers. Usually, increasing the number of workers per node\nshould be accompanied by a decrease in the mini-batch size or by a change in the\nscoring strategy (if data size and compute SKU remains the same).\nBatch endpoints will call the run() function in your scoring script once per mini-batch.\nHowever, you will have the power to decide if you want to run the inference over the\nentire batch, over one file at a time, or over one row at a time (if your data happens to\nbe tabular).\nYou will typically want to run inference over the batch all at once when you want to\nachieve high throughput in your batch scoring process. This is the case for instance if\nyou run inference over a GPU where you want to achieve saturation of the inference\ndevice. Y ou may also be relying on a data loader that can handle the batching itself if\ndata doesn't fit on memory, like TensorFlow or PyTorch data loaders. On those cases,\nyou may want to consider running inference on the entire batch.\nFor an example about how to achieve it, see High throughput deployments . This\nexample processes an entire batch of files at a time.Running inference at the mini-batch, file or the row level\nMini-batch level\n\uff12 Warning\nRunning inference at the batch level may require having high control over the input\ndata size to be able to correctly account for the memory requirements and avoid\nout of memory exceptions. Whether you are able or not of loading the entire mini-\nbatch in memory will depend on the size of the mini-batch, the size of the instances\nin the cluster, the number of workers on each node, and the size of the mini-batch.\nFile level", "One of the easiest ways to perform inference is by iterating over all the files in the mini-\nbatch and run your model over it. In some cases, like image processing, this may be a\ngood idea. If your data is tabular, you may need to make a good estimation about the\nnumber of rows on each file to estimate if your model is able to handle the memory\nrequirements to not just load the entire data into memory but also to perform inference\nover it. R emember that some models (specially those based on recurrent neural\nnetworks) will unfold and present a memory footprint that may not be linear with the\nnumber of rows. If your model is expensive in terms of memory, please consider running\ninference at the row level.\nFor an example about how to achieve it see Image processing with batch deployments .\nThis example processes a file at a time.\nFor models that present challenges in the size of their inputs, you may want to consider\nrunning inference at the row level. Y our batch deployment will still provide your scoring\nscript with a mini-batch of files, however, you will read one file, one row at a time. This\nmay look inefficient but for some deep learning models may be the only way to perform\ninference without scaling up your hardware requirements.\nFor an example about how to achieve it see Text processing with batch deployments .\nThis example processes a row at a time.\nThe environment variable AZUREML_MODEL_DIR contains the path to where the selected\nmodel is located and it is typically used in the init() function to load the model into\nmemory. However, some models may contain their files inside of a folder and you may\nneed to account for that when loading them. Y ou can identify the folder structure of\nyour model as follows:\n1. Go to Azure Machine Learning portal .\n2. Go to the section Models .\uea80 Tip\nIf file sizes are too big to be readed even at once, please consider breaking down\nfiles into multiple smaller files to account for better parallelization.\nRow level (tabular)\nUsing models that are folders", "Batch scoring, or batch infer encing , involves invoking an endpoint with a reference to\ndata. The batch endpoint runs jobs asynchronously to process data in parallel on\ncompute clusters and store the data for further analysis.\nReal-time s coring, or online infer encing , involves invoking an endpoint with one or more\nmodel deployments and receiving a response in near real time via HT TPS. T raffic can be\nsplit across multiple deployments, allowing for testing new model versions by diverting\nsome amount of traffic initially and increasing after confidence in the new model is\nestablished.\nFor more information, see:\nDeploy a model with a real-time managed endpoint\nUse batch endpoints for scoring\nDevOps for ML models, often called ML Ops, is a process for developing models for\nproduction. A model's lifecycle from training to deployment must be auditable if not\nreproducible.\nLearn more about MLOps in Azure Machine Learning .\nMachine Learning is built with the model lifecycle in mind. Y ou can audit the model\nlifecycle down to a specific commit and environment.Real-time and batch scoring (inferencing)\nMLOps: DevOps for machine learning\nML model lifecycle\nIntegrations enabling MLOPs", "There are two types of deployments in batch endpoints:\nModel deployments\nPipeline component deployment\nModel deployment allows operationalizing model inference at scale, processing big\namounts of data in a low latency and asynchronous way. Scalability is automatically\ninstrumented by Azure Machine Learning by providing parallelization of the inferencing\nprocesses across multiple nodes in a compute cluster.\nUse Model deployments  when:\nThe main benefit of this kind of deployments is that you can use the very same assets\ndeployed in the online world (Online Endpoints) but now to run at scale in batch. If your\nmodel requires simple pre or pos processing, you can author an scoring script  that\nperforms the data transformations required.\nTo create a model deployment in a batch endpoint, you need to specify the following\nelements:Model deployments\nYou have expensive models that requires a longer time to run inference.\uff02\nYou need to perform inference over large amounts of data, distributed in multiple\nfiles.\uff02\nYou don't have low latency requirements.\uff02\nYou can take advantage of parallelization.\uff02", "You use the updated settings to save the Y AML file.\nPython\nThe batch inference has these steps:\n1. You use the same built-in feature retrieval component for feature retrieval that you\nused in the training pipeline (covered in the third tutorial). For pipeline training,\nyou provided a feature retrieval specification as a component input. For batch\ninference, you pass the registered model as the input. The component looks for\nthe feature retrieval specification in the model artifact.\nAdditionally, for training, the observation data had the target variable. However,\nthe batch inference observation data doesn't have the target variable. The feature\nretrieval step joins the observation data with the features and outputs the data for\nbatch inference.\n2. The pipeline uses the batch inference input data from previous step, runs inference\non the model, and appends the predicted value as output.fs_client.feature_sets.begin_create_or_update(transactions_fset_config)\nprint(fs_poller.result())\n(Optional) Save the YAML file for the feature\nset asset\nPython SDK\n## uncomment and run\n# transactions_fset_config.dump(root_dir +  \n\"/featurestore/featuresets/transactions/featureset_asset_offline_enabled\n_with_schedule.yaml\")\nRun the batch inference pipeline\n\uff17 Note\nYou use a job for batch inference in this example. Y ou can also use batch\nendpoints in Azure Machine Learning."]}
{"question": "How to deploy custom vision model to real time endpoint?", "variant_id": "v1", "line_number": 45, "answer": "To deploy a custom vision model to a real-time endpoint in Azure Machine Learning, you can follow these general steps:\n\n1. **Register the Model**: First, you need to register your trained custom vision model in Azure Machine Learning. You can do this through the Azure Machine Learning Studio or by using the SDK. Here's an example of how to register a model using the SDK:\n\n    ```python\n    from azureml.core import Workspace, Model\n\n    workspace = Workspace.from_config()\n    model = Model.register(workspace=workspace,\n                           model_name='your-model-name',                # Name of the registered model in your workspace.\n                           model_path='path/to/your/model',             # Local path to the model\n                           description='Your custom vision model')\n    ```\n\n2. **Create Scoring Script**: Create a scoring script (`score.py`) that will be used to load your model and make predictions. This script must have two required functions, `init()` and `run(raw_data)`. The `init()` function is called when the deployment starts and is typically used to load the model. The `run(raw_data)` function is called when new data is submitted to the endpoint.\n\n    ```python\n    def init():\n        # Load the model from a file\n        global model\n        model = Model.get_model_path('your-model-name')\n\n    def run(raw_data):\n        # Make predictions using the model\n        data = ... # Parse the raw_data\n        result = model.predict(data)\n        return result\n    ```\n\n3. **Create Inference Configuration**: Create an inference configuration that specifies the environment and the scoring script.\n\n    ```python\n    from azureml.core.model import InferenceConfig\n    from azureml.core.environment import Environment\n\n    environment = Environment(name='your-environment-name')\n    environment.docker.enabled = True\n    environment.python.conda_dependencies = ... # Define your conda dependencies\n\n    inference_config = InferenceConfig(entry_script='score.py',\n                                       environment=environment)\n    ```\n\n4. **Deploy the Model**: Deploy the model as a web service to an Azure Container Instance (ACI) or Azure Kubernetes Service (AKS) for real-time scoring.\n\n    ```python\n    from azureml.core.webservice import AciWebservice, Webservice\n\n    deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n    service = Model.deploy(workspace=workspace,\n                           name='your-service-name',\n                           models=[model],\n                           inference_config=inference_config,\n                           deployment_config=deployment_config)\n    service.wait_for_deployment(show_output=True)\n    ```\n\n5. **Test the Endpoint**: Once the service is deployed, you can test the real-time endpoint by sending data to it and getting predictions back.\n\n    ```python\n    import requests\n\n    url = service.scoring_uri\n    data = {...}  # Your input data\n    headers = {'Content-Type': 'application/json'}\n    response = requests.post(url, json=data, headers=headers)\n    print(response.json())\n    ```\n\nPlease note that the actual code and parameters will depend on the specifics of your model and environment. Make sure to replace placeholders like `'your-model-name'`, `'path/to/your/model'`, and `'your-environment-name'` with your actual model and environment details.", "context": ["Deploy a trained model\nDeploy a pipeline with batch endpoints", "Deploy a trained model", "With the trained model registered in the Machine learning workspace, you are ready to\ndeploy the model for scoring.\nThis scenario includes prebuilt workflows for two approaches to deploying a trained\nmodel, batch scoring or a deploying a model to an endpoint for real-time scoring. Y ou\nmay run either or both of these workflows to test the performance of the model in your\nAzure Machine Learning workspace.\n1. In your GitHub project repository (ex: taxi-fare-regression), select Actions\n2. Select the deploy-online-endpoint -pipeline  from the workflows listed on the left\nand click Run w orkflow  to execute the online endpoint deployment pipeline\nworkflow. The steps in this pipeline will create an online endpoint in your Machine\nLearning workspace, create a deployment of your model to this endpoint, then\nallocate traffic to the endpoint.\uff17 Note\nIf you want to check the output of each individual step, for example to view output\nof a failed run, click a job output, and then click each step in the job to view any\noutput of that step.\nDeploying the Trained Model\nOnline Endpoint", "Alternatively Y ou can deploy the model from the Azure Machine Learning studio UI .\nNavigate to the model you wish to deploy in the Models  tab of the automated ML job\nand select on Deploy  and select Deploy t o real-time endpoint  .\n.\nthis is how your review page looks like. we can select instance type, instance count and\nset traffic percentage for the current deployment.\n.az ml online-endpoint update --name 'od-fridge-items-endpoint'  --traffic  \n'od-fridge-items-mlflow-deploy=100'  --workspace-name  \n[YOUR_AZURE_ WORKSPACE]  --resource-group  [YOUR_AZURE_RESOURCE_ GROUP] --\nsubscription  [YOUR_AZURE_ SUBSCRIPTION]", "If you directly deploy real-time endpoint from a previous completed real-time inference\npipeline, it may run into errors.\nRecommendation : clone the inference pipeline and submit it again, then deploy to real-\ntime endpoint.\nTutorial: Build a model in designer to predict auto pricesDeploy real-time endpoint from real-time inference\npipeline issue\nNext steps"]}
